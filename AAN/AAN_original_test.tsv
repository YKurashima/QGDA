we consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. <eos> in contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. <eos> in a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies ( one weakly supervised and one indirectly supervised ) and two different dictionary sources ( one manually constructed and one automatically constructed ).
in this paper, we address the problem of building a multilingual transliteration system using an interlingual representation. <eos> our approach uses international phonetic alphabet ( ipa ) to learn the interlingual representation and thus allows us to use any word and its ipa representation as a training example. <eos> thus, our approach requires only monolingual resources : a phoneme dictionary that lists words and their ipa representations.1 by adding a phoneme dictionary of a new language, we can readily build a transliteration system into any of the existing previous languages, without the expense of all-pairs data or computation. <eos> we also propose a regularization framework for learning the interlingual representation, which accounts for language specific phonemic variability, and thus it can find better mappings between languages. <eos> experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29 % accuracy and an average improvement of 17 % accuracy compared to a state-of-the-art baseline system.
this paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. <eos> in previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. <eos> we resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. <eos> the seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. <eos> we propose two types of the graphs : a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. <eos> evaluations using english and japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. <eos> further, the similarity graph achieved improved performance by clustering synonyms into the same translation.
we present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. <eos> the focus of the study is to identify the lexical features that distinguish the two populations. <eos> we report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9 % with only a small set of features. <eos> we provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.
inferring attributes of discourse participants has been treated as a batch-processing task : data such as all tweets from a given author are gathered in bulk, processed, analyzed for a particular feature, then reported as a result of academic interest. <eos> given the sources and scale of material used in these efforts, along with potential use cases of such analytic tools, discourse analysis should be reconsidered as a streaming challenge. <eos> we show that under certain common formulations, the batchprocessing analytic framework can be decomposed into a sequential series of updates, using as an example the task of gender classification. <eos> once in a streaming framework, and motivated by large data sets generated by social media services, we present novel results in approximate counting, showing its applicability to space efficient streaming classification.
a mixture of positive ( friendly ) and negative ( antagonistic ) relations exist among users in most social media applications. <eos> however, many such applications do not allow users to explicitly express the polarity of their interactions. <eos> as a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed ( e.g. <eos> epinions trust/distrust ). <eos> we study text exchanged between users in online communities. <eos> we find that the polarity of the links between users can be predicted with high accuracy given the text they exchange. <eos> this allows us to build a signed network representation of discussions ; where every edge has a sign : positive to denote a friendly relation, or negative to denote an antagonistic relation. <eos> we also connect our analysis to social psychology theories of balance. <eos> we show that the automatically predicted networks are consistent with those theories. <eos> inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them.
user simulation is frequently used to train statistical dialog managers for task-oriented domains. <eos> at present, goal-driven simulators ( those that have a persistent notion of what they wish to achieve in the dialog ) require some task-specific engineering, making them impossible to evaluate intrinsically. <eos> instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. <eos> in this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. <eos> our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. <eos> we evaluate on two standard dialog resources, the communicator and let ? s go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. <eos> we also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs.
incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. <eos> unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. <eos> we present a novel approach to incremental decision making that is based on hierarchical reinforcement learning to achieve an interactive optimisation of information presentation ( ip ) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density ( id ) ( jaeger, 2010 ). <eos> results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to id by more than 23 %.
recent work has explored the use of hidden markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence. <eos> we extend this approach to allow each block of text to be a mixture of multiple classes. <eos> under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block. <eos> we show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard hmm. <eos> additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models.
entity linking ( el ) has received considerable attention in recent years. <eos> given many name mentions in a document, the goal of el is to predict their referent entities in a knowledge base. <eos> traditionally, there have been two distinct directions of el research : one focusing on the effects of mention ? s context compatibility, assuming that ? the referent entity of a mention is reflected by its context ? <eos> ; the other dealing with the effects of document ? s topic coherence, assuming that ? a mention ? s referent entity should be coherent with the document ? s main topics ?. <eos> in this paper, we propose a generative model ? <eos> called entitytopic model, to effectively join the above two complementary directions together. <eos> by jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can accurately link all mentions in a document using both the local information ( including the words and the mentions in a document ) and the global knowledge ( including the topic knowledge, the entity context knowledge and the entity name knowledge ). <eos> experimental results demonstrate the effectiveness of the proposed model.
existing techniques for disambiguating named entities in text mostly focus on wikipedia as a target catalog of entities. <eos> yet for many types of entities, such as restaurants and cult movies, relational databases exist that contain far more extensive information than wikipedia. <eos> this paper introduces a new task, called open-database named-entity disambiguation ( open-db ned ), in which a system must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. <eos> we introduce two techniques for open-db ned, one based on distant supervision and the other based on domain adaptation. <eos> in experiments on two domains, one with poor coverage by wikipedia and the other with near-perfect coverage, our open-db ned strategies outperform a state-of-the-art wikipedia ned system by over 25 % in accuracy.
generic rule-based systems for information extraction ( ie ) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization. <eos> however, it is generally recognized that manually building and customizing rules is a complex and labor intensive process. <eos> in this paper, we discuss an approach that facilitates the process of building customizable rules for named-entity recognition ( ner ) tasks via rule induction, in the annotation query language ( aql ). <eos> given a set of basic features and an annotated document collection, our goal is to generate an initial set of rules with reasonable accuracy, that are interpretable and thus can be easily refined by a human developer. <eos> we present an efficient rule induction process, modeled on a fourstage manual rule development process and present initial promising results with our system. <eos> we also propose a simple notion of extractor complexity as a first step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. <eos> we demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure.
active learning is a promising way for sentiment classification to reduce the annotation cost. <eos> in this paper, we focus on the imbalanced class distribution scenario for sentiment classification, wherein the number of positive samples is quite different from that of negative samples. <eos> this scenario posits new challenges to active learning. <eos> to address these challenges, we propose a novel active learning approach, named co-selecting, by taking both the imbalanced class distribution issue and uncertainty into account. <eos> specifically, our co-selecting approach employs two feature subspace classifiers to collectively select most informative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. <eos> extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. <eos> 1
we propose the weakly supervised multiexperts model ( mem ) for analyzing the semantic orientation of opinions expressed in natural language reviews. <eos> in contrast to most prior work, mem predicts both opinion polarity and opinion strength at the level of individual sentences ; such fine-grained analysis helps to understand better why users like or dislike the entity under review. <eos> a key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. <eos> for this reason, mem is weakly supervised : it starts with potentially noisy indicators obtained from coarse-grained training data ( i.e., document-level ratings ), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. <eos> we integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. <eos> our experiments indicate that mem outperforms state-of-the-art methods by a significant margin.
this paper focuses on the task of collocation polarity disambiguation. <eos> the collocation refers to a binary tuple of a polarity word and a target ( such as ? long, battery life ? <eos> or ? long, startup ? <eos> ), in which the sentiment orientation of the polarity word ( ? long ? ) <eos> changes along with different targets ( ? battery life ? <eos> or ? startup ? ). <eos> to disambiguate a collocation ? s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. <eos> however, these contexts are limited, thus the resulting polarity is insufficient to be reliable. <eos> we therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation ? s polarity.without using any additional labeled data, experiments show that our method is effective.
generating coherent discourse is an important aspect in natural language generation. <eos> our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. <eos> we present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. <eos> the contribution of this work is two-fold : we first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. <eos> secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with mincuts. <eos> our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in f1-score.
computational approaches to metonymy resolution have focused almost exclusively on the local context, especially the constraints placed on a potentially metonymic word by its grammatical collocates. <eos> we expand such approaches by taking into account the larger context. <eos> our algorithm is tested on the data from the metonymy resolution task ( task 8 ) at semeval 2007. <eos> the results show that incorporation of the global context can improve over the use of the local context alone, depending on the types of metonymies addressed. <eos> as a second contribution, we move towards unsupervised resolution of metonymies, made feasible by considering ontological relations as possible readings. <eos> we show that such an unsupervised approach delivers promising results : it beats the supervised most frequent sense baseline and performs close to a supervised approach using only standard lexico-syntactic features.
learning inference relations between verbs is at the heart of many semantic applications. <eos> however, most prior work on learning such rules focused on a rather narrow set of information sources : mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. <eos> in this paper, we claim that it is imperative to utilize information from various textual scopes : verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. <eos> to this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. <eos> we empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task.
recently there has been substantial interest in using spectral methods to learn generative sequence models like hmms. <eos> spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike em methods, which can get stuck in local minima. <eos> in this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures. <eos> we propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efficiently estimate it. <eos> as a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. <eos> our approach gives us a moderate reduction in error of up to 4.6 % over the baseline re-ranker.
topic models traditionally rely on the bagof-words assumption. <eos> in data mining applications, this often results in end-users being presented with inscrutable lists of topical unigrams, single words inferred as representative of their topics. <eos> in this article, we present a hierarchical generative probabilistic model of topical phrases. <eos> the model simultaneously infers the location, length, and topic of phrases within a corpus and relaxes the bagof-words assumption within phrases by using a hierarchy of pitman-yor processes. <eos> we use markov chain monte carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters. <eos> we show via an experiment on human subjects that our model finds substantially better, more interpretable topical phrases than do competing models.
we describe a nonparametric model and corresponding inference algorithm for learning synchronous context free grammar derivations for parallel text. <eos> the model employs a pitman-yor process prior which uses a novel base distribution over synchronous grammar rules. <eos> through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences ? <eos> including discontiguous, many-to-many alignments ? and produces competitive translation results. <eos> further, inference is efficient and we present results on significantly larger corpora than prior work.
multi-document summarization involves many aspects of content selection and surface realization. <eos> the summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. <eos> we present a method where such individual aspects are learned separately from data ( without any hand-engineering ) but optimized jointly using an integer linear programme. <eos> the ilp framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. <eos> experimental results on the tac-08 data set show that our model achieves state-of-the-art performance using rouge and significantly improves the informativeness of the summaries.
comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices. <eos> in this paper, we investigate dependency length minimization in the context of discriminative realization ranking, focusing on its potential to eliminate egregious ordering errors as well as better match the distributional characteristics of sentence orderings in news text. <eos> we find that with a stateof-the-art, comprehensive realization ranking model, dependency length minimization yields statistically significant improvements in bleu scores and significantly reduces the number of heavy/light ordering errors. <eos> through distributional analyses, we also show that with simpler ranking models, dependency length minimization can go overboard, too often sacrificing canonical word order to shorten dependencies, while richer models manage to better counterbalance the dependency length minimization preference against ( sometimes ) competing canonical word order preferences.
we present a new approach to the problem of automatic text summarization called automatic summarization using reinforcement learning ( asrl ) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary. <eos> we demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. <eos> the experimental results indicated asrl was superior to the best performing method in duc2004 and comparable to the state of the art ilp-style method, in terms of rouge scores. <eos> the results also revealed asrl can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.
we apply slice sampling to bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. <eos> compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. <eos> we decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 bleu points.
tense is a small element to a sentence, however, error tense can raise odd grammars and result in misunderstanding. <eos> recently, tense has drawn attention in many natural language processing applications. <eos> however, most of current statistical machine translation ( smt ) systems mainly depend on translation model and language model. <eos> they never consider and make full use of tense information. <eos> in this paper, we propose n-gram-based tense models for smt and successfully integrate them into a state-of-the-art phrase-based smt system via two additional features. <eos> experimental results on the nist chinese-english translation task show that our proposed tense models are very effective, contributing performance improvement by 0.62 blue points over a strong baseline.
we propose a novel, language-independent approach for improving machine translation from a resource-poor language to x by adapting a large bi-text for a related resource-rich language and x ( the same target language ). <eos> we assume a small bi-text for the resourcepoor language to x pair, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-rich and the resource-poor language ; we then adapt the former to get closer to the latter. <eos> our experiments for indonesian/malay ? english translation show that using the large adapted resource-rich bitext yields 6.7 bleu points of improvement over the unadapted one and 2.6 bleu points over the original small bi-text. <eos> moreover, combining the small bi-text with the adapted bi-text outperforms the corresponding combinations with the unadapted bi-text by 1.5 ? <eos> 3 bleu points. <eos> we also demonstrate applicability to other languages and domains.
the possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency. <eos> we introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure. <eos> we perform experiments across 18 languages available in conll data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results.
in this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. <eos> first, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of nivre ? s arc-eager projective parser on a number of datasets from the conll-x shared task. <eos> then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node ( allowing a limited form of non-projectivity ). <eos> none of these transitions has a negative impact on the computational complexity of the algorithm. <eos> although the experiments in this paper use the arc-eager parser, the approach is generic enough to be applicable to any stackbased dependency parser.
state-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. <eos> on the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. <eos> in this paper, we attempt to address this imbalance for graph-based parsing by generalizing the eisner ( 1996 ) algorithm to handle arbitrary features over higherorder dependencies. <eos> the generalization is at the cost of asymptotic efficiency. <eos> to account for this, cube pruning for decoding is utilized ( chiang, 2007 ). <eos> for the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. <eos> our parser achieves the state-of-art unlabeled accuracy of 93.06 % and labeled accuracy of 91.86 % on the standard test set for english, at a faster speed than a reimplementation of the third-order model of koo et al2010 ).
we consider the problem of inducing grapheme-to-phoneme mappings for unknown languages written in a latin alphabet. <eos> first, we collect a data-set of 107 languages with known grapheme-phoneme relationships, along with a short text in each language. <eos> we then cast our task in the framework of supervised learning, where each known language serves as a training example, and predictions are made on unknown languages. <eos> we induce an undirected graphical model that learns phonotactic regularities, thus relating textual patterns to plausible phonemic interpretations across the entire range of languages. <eos> our model correctly predicts grapheme-phoneme pairs with over 88 % f1-measure.
many linguistic and textual processes involve transduction of strings. <eos> we show how to learn a stochastic transducer from an unorganized collection of strings ( rather than string pairs ). <eos> the role of the transducer is to organize the collection. <eos> our generative model explains similarities among the strings by supposing that some strings in the collection were not generated ab initio, but were instead derived by transduction from other, ? similar ? <eos> strings in the collection. <eos> our variational em learning algorithm alternately reestimates this phylogeny and the transducer parameters. <eos> the final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. <eos> we find that our method can effectively find name variants in a corpus of web strings used to refer to persons inwikipedia, improving over standard untrained distances such as jaro-winkler and levenshtein distance.
we present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. <eos> we provide the first evidence for an association between syntactic surprisal and word duration in recorded speech. <eos> using the ami corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. <eos> this result supports the uniform information density ( uid ) hypothesis and points a way to more realistic artificial speech generation.
in this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus. <eos> our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable. <eos> to the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering. <eos> we combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2 % improvement in precision at the top-1 answer over a baseline state-of-the-art qa system that achieved the best performance in a shared task of japanese non-factoid qa in ntcir-6.
the linked data initiative comprises structured databases in the semantic-web data model rdf. <eos> exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. <eos> to ease the task, this paper presents a methodology for translating natural language questions into structured sparql queries over linked-data sources. <eos> our method is based on an integer linear program to solve several disambiguation tasks jointly : the segmentation of questions into phrases ; the mapping of phrases to semantic entities, classes, and relations ; and the construction of sparql triple patterns. <eos> our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. <eos> we present experiments on both the question translation and the resulting query answering.
this paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. <eos> the hierarchy organizes product aspects as nodes following their parent-child relations. <eos> for each aspect, the reviews and corresponding opinions on this aspect are stored. <eos> we develop a new framework for opinion questions answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. <eos> in particular, we first identify the ( explicit/implicit ) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. <eos> we then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. <eos> in order to generate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. <eos> we conduct evaluations on 11 popular products in four domains. <eos> the evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. <eos> experimental results demonstrate the effectiveness of our approach.
in statistical machine translation, minimum error rate training ( mert ) is a standard method for tuning a single weight with regard to a given development data. <eos> however, due to the diversity and uneven distribution of source sentences, there are two problems suffered by this method. <eos> first, its performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing. <eos> second, translations become inconsistent at the sentence level since tuning is performed globally on a document level. <eos> in this paper, we propose a novel local training method to address these two problems. <eos> unlike a global training method, such as mert, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. <eos> we propose efficient incremental training methods to put the local training into practice. <eos> in nist chinese-to-english translation tasks, our local training method significantly outperforms mert with the maximal improvements up to 2.0 bleu points, meanwhile its efficiency is comparable to that of the global method.
in this paper we first describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm ( jiang et al2009 ). <eos> it can automatically transform a human-annotated corpus from one annotation guideline to another. <eos> we then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. <eos> experiments on chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline, and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does. <eos> on the penn chinese treebank 5.0, it achieves an f-measure of 98.43 %, significantly outperforms previous works although using a single classifier with only local features.
? grounded ? <eos> language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts. <eos> bo ? rschinger et al2011 ) introduced an approach to grounded language learning based on unsupervised pcfg induction. <eos> their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task. <eos> however, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by chen and mooney ( 2011 ). <eos> this paper presents an enhancement of the pcfg approach that scales to such problems with highly-ambiguous supervision. <eos> experimental results on the navigation task demonstrates the effectiveness of our approach.
a forced derivation tree ( fdt ) of a sentence pair { f, e } denotes a derivation tree that can translate f into its accurate target translation e. in this paper, we present an approach that leverages structured knowledge contained in fdts to train component models for statistical machine translation ( smt ) systems. <eos> we first describe how to generate different fdts for each sentence pair in training corpus, and then present how to infer the optimal fdts based on their derivation and alignment qualities. <eos> as the first step in this line of research, we verify the effectiveness of our approach in a btgbased phrasal system, and propose four fdtbased component models. <eos> experiments are carried out on large scale english-to-japanese and chinese-to-english translation tasks, and significant improvements are reported on both translation quality and alignment quality.
distant supervision for relation extraction ( re ) ? <eos> gathering training data by aligning a database of facts with text ? <eos> is an efficient approach to scale re to thousands of different relations. <eos> however, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. <eos> for example, a sentence containing balzac and france may express bornin or died, an unknown relation, or no relation at all. <eos> because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. <eos> we propose a novel approach to multi-instance multi-label learning for re, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. <eos> our model performs competitively on two difficult domains.
this paper present a new readability formula for french as a foreign language ( ffl ), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the ffl context. <eos> we report comparisons between several techniques for feature selection and various learning algorithms. <eos> our best model, based on support vector machines ( svm ), significantly outperforms previous ffl formulas. <eos> we also found that semantic features behave poorly in our case, in contrast with some previous readability studies on english as a first language.
we introduce gap inheritance, a new structural property on trees, which provides a way to quantify the degree to which intervals of descendants can be nested. <eos> based on this property, two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees : unlike projective trees, a word can have descendants in more than one interval ; unlike spanning trees, these intervals can not be nested in arbitrary ways. <eos> the 1-inherit class of trees has exactly the same empirical coverage of natural language sentences as the class of mildly nonprojective trees, yet the optimal scoring tree can be found in an order of magnitude less time. <eos> gap-minding trees ( the second class ) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals.
we introduce a novel coreference resolution system that models entities and events jointly. <eos> our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. <eos> as clusters are built, information flows between entity and event clusters through features that model semantic role dependencies. <eos> our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. <eos> in a cross-document domain with comparable documents, joint coreference resolution performs significantly better ( over 3 conll f1 points ) than two strong baselines that resolve entities and events separately.
in this paper, we propose a novel decoding algorithm for discriminative joint chinese word segmentation, part-of-speech ( pos ) tagging, and parsing. <eos> previous work often used a pipeline method ? <eos> chinese word segmentation followed by pos tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. <eos> in our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding. <eos> we extend the cyk parsing algorithm so that it can deal with word segmentation and pos tagging features. <eos> as far as we know, this is the first work on joint chinese word segmentation, pos tagging and parsing. <eos> our experimental results on chinese tree bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system.
in this paper, we propose a novel translation model ( tm ) based cross-lingual data selection model for language model ( lm ) adaptation in statistical machine translation ( smt ), from word models to phrase models. <eos> given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target lm training corpus is similar. <eos> compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. <eos> furthermore, phrase tm based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based tm, because it captures contextual information in modeling the selection of phrase as a whole. <eos> experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both lm perplexity and smt performance.
open information extraction ( ie ) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. <eos> however, stateof-the-art open ie systems such as reverb and woe share two important weaknesses ? <eos> ( 1 ) they extract only relations that are mediated by verbs, and ( 2 ) they ignore context, thus extracting tuples that are not asserted as factual. <eos> this paper presents ollie, a substantially improved open ie system that addresses both these limitations. <eos> first, ollie achieves high yield by extracting relations mediated by nouns, adjectives, and more. <eos> second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. <eos> ollie obtains 2.7 times the area under precision-yield curve ( auc ) compared to reverb and 1.9 times the auc of woeparse.
topic models are increasingly being used for text analysis tasks, often times replacing earlier semantic techniques such as latent semantic analysis. <eos> in this paper, we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document. <eos> for this proposed model, a gibbs sampler is developed for doing posterior inference. <eos> experimental results show that with topic adaptation, our model significantly improves over existing approaches in terms of perplexity, and is able to uncover clear sequential structure on, for example, herman melville ? s book ? moby dick ?.
in this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. <eos> we experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. <eos> some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. <eos> we find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests : ( 1 ) phrase similarity and ( 2 ) paraphrase detection. <eos> the sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.
most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates can not be treated as a whole during parsing process so that the chunk-level features can not be exploited in a natural way. <eos> in this paper, we formulate phrase chunking as a joint segmentation and labeling task. <eos> we propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. <eos> a relaxed, online maximum margin training algorithm is used for learning. <eos> within this framework, we explored a variety of effective feature representations for chinese phrase chunking. <eos> the experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. <eos> in particular, our approach is much better at recognizing long and complicated phrases.
we present a novel beam-search decoder for grammatical error correction. <eos> the decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. <eos> these features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. <eos> unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. <eos> our decoder achieves an f1 correction score significantly higher than all previous published scores on the helping our own ( hoo ) shared task data set.
evidence-based medicine is an approach whereby clinical decisions are supported by the best available findings gained from scientific research. <eos> this requires efficient access to such evidence. <eos> to this end, abstracts in evidence-based medicine can be labeled using a set of predefined medical categories, the socalled pico criteria. <eos> this paper presents an approach to automatically annotate sentences in medical abstracts with these labels. <eos> since both structural and sequential information are important for this classification task, we use klog, a new language for statistical relational learning with kernels. <eos> our results show a clear improvement with respect to state-of-the-art systems.
in this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. <eos> we introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. <eos> we show that textual and musical features can both be successfully used for emotion recognition in songs. <eos> moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31 %.
this study presents a novel method that measures english language learners ? <eos> syntactic competence towards improving automated speech scoring systems. <eos> in contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. <eos> we estimated the syntactic competence through the use of corpus-based nlp techniques. <eos> assuming that the range and sophistication of grammatical expressions can be captured by the distribution of part-ofspeech ( pos ) tags, vector space models of pos tags were constructed. <eos> we use a large corpus of english learners ? <eos> responses that are classified into four proficiency levels by human raters. <eos> our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner ? s syntactic competence level. <eos> widely outperforming the state-of-the-art measures of syntactic complexity, our method attained a significant correlation with humanrated scores. <eos> the correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on asr-hypothesis was slightly lower, 0.42. <eos> an important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors.
this paper presents an integrated, end-to-end approach to online spelling correction for text input. <eos> online spelling correction refers to the spelling correction as you type, as opposed to post-editing. <eos> the online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as chinese and japanese, because the desired target characters can not be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. <eos> for example, a user might type suesheng by mistake to mean xuesheng ? ? <eos> 'student ' in chinese ; existing input methods fail to convert this misspelled input to the desired target chinese characters. <eos> in this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. <eos> at the phrase ( substring ) level, k most probable pinyin ( romanized chinese ) corrections are generated using a monotone decoder ; at the sentence level, input pinyin strings are directly transliterated into target chinese characters by a decoder using a loglinear model that refer to the features of both levels. <eos> a new method of automatically deriving parallel training data from user keystroke logs is also presented. <eos> experiments on chinese pinyin conversion show that our integrated method reduces the character error rate by 20 % ( from 8.9 % to 7.12 % ) over the previous state-of-the art based on a noisy channel model.
we propose a new semantic orientation, excitation, and its automatic acquisition method. <eos> excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. <eos> we show that excitation is useful for extracting contradiction pairs ( e.g., destroy cancer ? <eos> develop cancer ) and causality pairs ( e.g., increase in crime ? <eos> heighten anxiety ). <eos> our experiments show that with automatically acquired excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70 % precision from a 600 million page web corpus. <eos> furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus with reasonable precision.
this paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases : paraphrase patterns are first induced and then used to collect novel instances. <eos> unlike existing methods, ours uses both bilingual parallel and monolingual corpora. <eos> while the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. <eos> we show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. <eos> in our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. <eos> human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are of reasonable quality. <eos> remaining noise can be further reduced by filtering seed paraphrases.
this paper presents a comparative study of graph-based approaches for cross-domain sentiment classification. <eos> in particular, the paper analyses two existing methods : an optimisation problem and a ranking algorithm. <eos> we compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. <eos> analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains.
this paper explores log-based query expansion ( qe ) models for web search. <eos> three lexicon models are proposed to bridge the lexical gap between web documents and user queries. <eos> these models are trained on pairs of user queries and titles of clicked documents. <eos> evaluations on a real world data set show that the lexicon models, integrated into a ranker-based qe system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based qe methods.
this paper addresses the task of constructing a timeline of events mentioned in a given text. <eos> to accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. <eos> we then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. <eos> moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. <eos> overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2 % of relative improvement in f1. <eos> the experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.
we present a new family of models for unsupervised parsing, dependency and boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. <eos> we build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations : ( i ) distributions of words that occur at sentence boundaries ? <eos> such as english determiners ? <eos> resemble constituent edges. <eos> ( ii ) punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. <eos> ( iii ) sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. <eos> our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.
the task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags ( for small and fixed n ) and unigram function words. <eos> to capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. <eos> in this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of pos tags and words, using both maxent and induced syntactic language model approaches to classification. <eos> after presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model.
we propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. <eos> as in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. <eos> we are particularly interested in revealing and exploiting relationships between documents. <eos> to this end, we focus on extracting diverse sets of threads ? singlylinked, coherent chains of important documents. <eos> to illustrate, we extract research threads from citation graphs and construct timelines from news articles. <eos> our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. <eos> finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.
this paper describes a study on the impact of the original signal ( text, speech, visual scene, event ) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. <eos> a corpus of 2,500 annotated sentences in english and french is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. <eos> a detailed quantified typology of subsentential paraphrases found in our corpus types is given.
graph-based dependency parsers suffer from the sheer number of higher order edges they need to ( a ) score and ( b ) consider during optimization. <eos> here we show that when working with lp relaxations, large fractions of these edges can be pruned before they are fully scored ? without any loss of optimality guarantees and, hence, accuracy. <eos> this is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. <eos> this amounts to delayed column and row generation in the lp relaxation and is guaranteed to provide the optimal lp solution. <eos> for second order grandparent models, our method considers, or scores, no more than 6 ? 13 % of the second order edges of the full model. <eos> this yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full lp relaxation. <eos> we also provide a tighter lp formulation for grandparent models that leads to a smaller integrality gap and higher speed.
we propose an adaptive ensemble method to adapt coreference resolution across domains. <eos> this method has three features : ( 1 ) it can optimize for any user-specified objective measure ; ( 2 ) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models ; ( 3 ) it can automatically adjust the active ensemble members during prediction. <eos> with simplification, this method can be used in the traditional withindomain case, while still retaining the above features. <eos> to the best of our knowledge, this work is the first to both ( i ) develop a domain adaptation algorithm for the coreference resolution problem and ( ii ) have the above features as an ensemble method. <eos> empirically, we show the benefits of ( i ) on the six domains of the ace 2005 data set in domain adaptation setting, and of ( ii ) on both the muc-6 and the ace 2005 data sets in within-domain setting.
we present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. <eos> our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser : semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. <eos> we apply our approach to train a semantic parser that uses 77 relations from freebase in its knowledge representation. <eos> this semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. <eos> we demonstrate recovery of this richer structure by extracting logical forms from natural language queries against freebase. <eos> on this task, the trained semantic parser achieves 80 % precision and 56 % recall, despite never having seen an annotated logical form.
this paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. <eos> our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. <eos> the most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. <eos> we therefore propose syntactic reordering for cross-lingual language modeling, and present a first result that compares inversion transduction grammar ( itg ) reordering constraints to ibm and local constraints in an integrated speech transcription and translation system. <eos> evaluations on resource-poor cantonese speech transcription and cantonese to resource-rich mandarin translation tasks show that our proposed approach improves the system performance significantly, up to 3.4 % relative wer reduction in cantonese transcription and 13.3 % relative bilingual evaluation understudy ( bleu ) score improvement in mandarin transcription compared with the system without reordering.
we examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference ( e.g., binding constraints, gender and number agreement ) as well as commonly-used resolution heuristics ( e.g., string-matching facilities, syntactic salience ) are not useful. <eos> being able to solve this task has broader implications in artificial intelligence : a restricted version of it, sometimes referred to as the winograd schema challenge, has been suggested as a conceptually and practically appealing alternative to the turing test. <eos> we employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.
quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker. <eos> the present state-of-the-art system uses gold standard information from previous decisions in its features, which, when removed, results in a large drop in performance. <eos> we treat the problem as a sequence labelling task, which allows us to incorporate sequence features without using gold standard information. <eos> we present results on two new corpora and an augmented version of a third, achieving a new state-of-the-art for systems using only realistic features.
supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics, such as hllda and hlda. <eos> supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels, but can not explore new topics ; while unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, but does not make use of any information from hierarchical labels. <eos> in this paper, we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical labels into the modeling process, called semisupervised hierarchical latent dirichlet allocation ( sshlda ). <eos> we also prove that hlda and hllda are special cases of sshlda.we conduct experiments on yahoo ! <eos> answers and odp datasets, and assess the performance in terms of perplexity and clustering. <eos> the experimental results show that predictive ability of sshlda is better than that of baselines, and sshlda can also achieve significant improvement over baselines for clustering on the fscore measure.
many nlp tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. <eos> for others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. <eos> we propose a novel method which avoids the need for any syntactically annotated data when predicting a related nlp task. <eos> our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a markov random field. <eos> at both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. <eos> results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an english relation extraction task, and performing comparably to them in semantic role labeling.
past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic : due to historical precedents, they assume access to information about labels in the raw and test sets. <eos> here, we demonstrate ways to learn hidden markov model taggers from incomplete tag dictionaries. <eos> taking the mingreedy algorithm ( ravi et al2010 ) as a starting point, we improve it with several intuitive heuristics. <eos> we also define a simple hmm emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. <eos> altogether, our augmentations produce improvements to performance over the original min-greedy algorithm for both english and italian data.
in this paper, we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments. <eos> in literature, the latter receives less attention and remains more challenging. <eos> we explore the feature space in this task and argue that collecting person specific evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature ? s importance in a given web page. <eos> this can alleviate the lack of clues where discriminative features can be reasonably weighted by taking their corpus level importance into account, not just relying on the current local context. <eos> we therefore propose a topic-based model to exploit the person specific global importance and embed it into the person name similarity. <eos> the experimental results show that the corpus level topic information provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three weps datasets.
this paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. <eos> this is done by treating the parser ? s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. <eos> we demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. <eos> using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased smt and previously proposed unsupervised syntax induction methods.
the training of most syntactic smt approaches involves two essential components, word alignment and monolingual parser. <eos> in the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. <eos> in this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. <eos> one is targeted self-training with a simple evaluation function ; the other is based on training data selection from forced alignment of bilingual data. <eos> we also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. <eos> the best combination of these novel methods achieves 3 bleu point gain in an iwslt task and more than 1 bleu point gain in nist tasks.
we describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. <eos> using the manually annotated english chinese translation treebank, we show how our method automatically discovers transformations that accommodate differences in english and chinese syntax. <eos> furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic mt system, the transformed trees achieve a 0.9 bleu improvement over baseline trees.
we present a distantly supervised system for extracting the temporal bounds of fluents ( relations which only hold during certain times, such as attends school ). <eos> unlike previous pipelined approaches, our model does not assume independence between each fluent or even between named entities with known connections ( parent, spouse, employer, etc. ). <eos> instead, we model what makes timelines of fluents consistent by learning cross-fluent constraints, potentially spanning entities as well. <eos> for example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn ? t been born yet. <eos> our system achieves a 36 % error reduction over a pipelined baseline.
because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. <eos> this study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. <eos> we exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. <eos> experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification.
entity linking systems link noun-phrase mentions in text to their corresponding wikipedia articles. <eos> however, nlp applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own wikipedia articles. <eos> in this paper we show that once the wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. <eos> our proposed method for detecting unlinkable entities achieves 24 % greater accuracy than a named entity recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked wikipedia entities to unlinkable entities. <eos> detection and typing of unlinkable entities can increase yield for nlp applications such as typed question answering.
we propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis. <eos> our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal cky-like parsing algorithm to probabilities inferred from a dynamic conditional random field. <eos> we show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin.
previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents ? <eos> discourse structure as a useful information source. <eos> we propose a novel method for collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. <eos> we show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. <eos> our system beats an informed baseline by a margin of 50 %.
we propose a technique to generate nonprojective word orders in an efficient statistical linearization system. <eos> our approach predicts liftings of edges in an unordered syntactic tree by means of a classifier, and uses a projective algorithm for tree linearization. <eos> we obtain statistically significant improvements on six typologically different languages : english, german, dutch, danish, hungarian, and czech.
we investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. <eos> paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. <eos> we compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. <eos> our best model based on euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80 % many-to-one accuracy on a 45-tag 1m word corpus.
we apply two new automated semantic evaluations to three distinct latent topic models. <eos> both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. <eos> we improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. <eos> we further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. <eos> our experiments reveal that lda and lsa each have different strengths ; lda best learns descriptive topics while lsa is best at creating a compact semantic representation of documents and words in a corpus.
phrase-based machine translation models have shown to yield better translations than word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. <eos> however, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. <eos> in this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. <eos> this model is then applied to phrase table pruning. <eos> tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. <eos> in fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.
when trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. <eos> in this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. <eos> systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods.
accurate and robust metrics for automatic evaluation are key to the development of statistical machine translation ( mt ) systems. <eos> we first introduce a new regression model that uses a probabilistic finite state machine ( pfsm ) to compute weighted edit distance as predictions of translation quality. <eos> we also propose a novel pushdown automaton extension of the pfsm model for modeling word swapping and cross alignments that can not be captured by standard edit distance models. <eos> our models can easily incorporate a rich set of linguistic features, and automatically learn their weights, eliminating the need for ad-hoc parameter tuning. <eos> our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations ( nist openmt06,08 ; wmt0608 ).
we investigate two aspects of the empirical behavior of paired significance tests for nlp systems. <eos> first, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on ? <eos> is it true that for each task there is a gain which roughly implies significance ? <eos> we explore these issues across a range of nlp tasks using both large collections of past systems ? <eos> outputs and variants of single systems. <eos> next, once significance levels are computed, how well does the standard i.i.d. <eos> notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains ? <eos> we explore this question using a range of test set variations for constituency parsing.
current chinese event extraction systems suffer much from two problems in trigger identification : unknown triggers and word segmentation errors to known triggers. <eos> to resolve these problems, this paper proposes two novel inference mechanisms to explore special characteristics in chinese via compositional semantics inside chinese triggers and discourse consistency between chinese trigger mentions. <eos> evaluation on the ace 2005 chinese corpus justifies the effectiveness of our approach over a strong baseline.
we study how to extend a large knowledge base ( freebase ) by reading relational information from a large web text corpus. <eos> previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. <eos> we describe a distributed, web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. <eos> experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.
discovering significant types of relations from the web is challenging because of its open nature. <eos> unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. <eos> recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open ie algorithm as input. <eos> however, it can not handle polysemy of relation phrases and fails to group many similar ( ? synonymous ? ) <eos> relation instances because of the sparseness of features. <eos> in this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. <eos> the algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. <eos> moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. <eos> while maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. <eos> it is also very efficient. <eos> experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web.
we propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. <eos> for the training of the reranker, we extract competing local subtrees, hence the training instances ( candidate subtree sets ) are very similar to those used during beamsearch parsing. <eos> this leads to better parameter optimization. <eos> another chief advantage of the framework is that arbitrary learning to rank methods can be applied. <eos> we evaluated our reranking approach on german and english phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. <eos> the subtree ranking approach with a maximum entropy model significantly outperformed the other approaches.
constituency parser performance is primarily interpreted through a single metric, f-score on wsj section 23, that conveys no linguistic information regarding the remaining errors. <eos> we classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. <eos> we use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.
this paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level. <eos> as a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning. <eos> a comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine. <eos> various ways to apply this feature to evaluate machinetranslated documents are presented, including one without reliance on reference translation. <eos> experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements.
many natural language processing problems involve constructing large nearest-neighbor graphs. <eos> we propose a system called flag to construct such graphs approximately from large data sets. <eos> to handle the large amount of data, our algorithm maintains approximate counts based on sketching algorithms. <eos> to find the approximate nearest neighbors, our algorithm pairs a new distributed online-pmi algorithm with novel fast approximate nearest neighbor search algorithms ( variants of pleb ). <eos> these algorithms return the approximate nearest neighbors quickly. <eos> we show our system ? s efficiency in both intrinsic and extrinsic experiments. <eos> we further evaluate our fast search algorithms both quantitatively and qualitatively on two nlp applications.
short listings such as classified ads or product listings abound on the web. <eos> if a computer can reliably extract information from them, it will greatly benefit a variety of applications. <eos> short listings are, however, challenging to process due to their informal styles. <eos> in this paper, we present an unsupervised information extraction system for short listings. <eos> given a corpus of listings, the system builds a semantic model that represents typical objects and their attributes in the domain of the corpus, and then uses the model to extract information. <eos> two key features in the system are a semantic parser that extracts objects and their attributes and a listing-focused clustering module that helps group together extracted tokens of same type. <eos> our evaluation shows that the semantic model learned by these two modules is effective across multiple domains.
many nlp tasks rely on accurate statistics from large corpora. <eos> tracking complete statistics is memory intensive, so recent work has proposed using compact approximate ? sketches ? <eos> of frequency distributions. <eos> we describe 10 sketch methods, including existing and novel variants. <eos> we compare and study the errors ( over-estimation and underestimation ) made by the sketches. <eos> we evaluate several sketches on three important nlp problems. <eos> our experiments show that one sketch performs best for all the three tasks.
conditional random fields and other graphical models have achieved state of the art results in a variety of tasks such as coreference, relation extraction, data integration, and parsing. <eos> increasingly, practitioners are using models with more complex structure ? higher treewidth, larger fan-out, more features, and more data ? rendering even approximate inference methods such as mcmc inefficient. <eos> in this paper we propose an alternative mcmc sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors. <eos> we demonstrate that our method converges more quickly than a traditional mcmc sampler for both marginal and map inference. <eos> in an author coreference task with over 5 million mentions, we achieve a 13 times speedup over regular mcmc inference.
this paper deals with the problem of predicting structures in the context of nlp. <eos> typically, in structured prediction, an inference procedure is applied to each example independently of the others. <eos> in this paper, we seek to optimize the time complexity of inference over entire datasets, rather than individual examples. <eos> by considering the general inference representation provided by integer linear programs, we propose three exact inference theorems which allow us to re-use earlier solutions for certain instances, thereby completely avoiding possibly expensive calls to the inference procedure. <eos> we also identify several approximation schemes which can provide further speedup. <eos> we instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance.
we present a method for exact optimization and sampling from high order hidden markov models ( hmms ), which are generally handled by approximation techniques. <eos> motivated by adaptive rejection sampling and heuristic search, we propose a strategy based on sequentially refining a lower-order language model that is an upper bound on the true model we wish to decode and sample from. <eos> this allows us to build tractable variable-order hmms. <eos> the arpa format for language models is extended to enable an efficient use of the max-backoff quantities required to compute the upper bound. <eos> we evaluate our approach on two problems : a sms-retrieval task and a pos tagging experiment using 5-gram models. <eos> results show that the same approach can be used for exact optimization and sampling, while explicitly constructing only a fraction of the total implicit state-space.
this paper presents patty : a large resource for textual patterns that denote binary relations between entities. <eos> the patterns are semantically typed and organized into a subsumption taxonomy. <eos> the patty system is based on efficient algorithms for frequent itemset mining and can process web-scale corpora. <eos> it harnesses the rich type system and entity population of large knowledge bases. <eos> the patty taxonomy comprises 350,569 pattern synsets. <eos> random-sampling-based evaluation shows a pattern accuracy of 84.7 %. <eos> patty has 8,162 subsumptions, with a random-sampling-based precision of 75 %. <eos> the patty resource is freely available for interactive access and download.
pcfgs can grow exponentially as additional annotations are added to an initially simple base grammar. <eos> we present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. <eos> our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. <eos> we use a structured expectation propagation algorithm that makes use of the factored structure in two ways. <eos> first, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. <eos> second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. <eos> using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving f1 scores up to 88.4 on the penn treebank while using two orders of magnitudes fewer parameters compared to the na ? <eos> ? ve approach. <eos> combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 f1 on all sentences from section 23 of the penn treebank.
we introduce a model of coherence which captures the intentional discourse structure in text. <eos> our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns. <eos> results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90 %. <eos> we also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles.
approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. <eos> we contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. <eos> common practice uses lowerorder entries in an n -gram model to score the first few words of a fragment ; this violates assumptions made by common smoothing strategies, including kneser-ney. <eos> instead, we use a unigram model to score the first word, a bigram for the second, etc. <eos> this improves search at the expense of memory. <eos> conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments. <eos> these changes can be stacked, achieving better estimates with unchanged memory usage. <eos> in order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in cpu time. <eos> in a germanenglish moses system with target-side syntax, improved estimates yielded a 63 % reduction in cpu time ; for a hiero-style version, the reduction is 21 %. <eos> the compressed language model uses 26 % less ram while equivalent search quality takes 27 % more cpu. <eos> source code is released as part of kenlm.
decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. <eos> left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. <eos> this paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. <eos> our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.
single-word vector space models have been very successful at learning lexical information. <eos> however, they can not capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. <eos> we introduce a recursive neural network ( rnn ) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. <eos> our model assigns a vector and a matrix to every node in a parse tree : the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. <eos> this matrix-vector rnn can learn the meaning of operators in propositional logic and natural language. <eos> the model obtains state of the art performance on three different experiments : predicting fine-grained sentiment distributions of adverb-adjective pairs ; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.
existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. <eos> we introduce a new vector space representation where antonyms lie on opposite sides of a sphere : in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. <eos> we derive this representation with the aid of a thesaurus and latent semantic analysis ( lsa ). <eos> each entry in the thesaurus ? <eos> a word sense along with its synonyms and antonyms ? <eos> is treated as a ? document, ? <eos> and the resulting document collection is subjected to lsa. <eos> the key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which lsa operates, so as to induce a subspace with the desired property. <eos> we evaluate this procedure with the graduate record examination questions of ( mohammed et al 2008 ) and find that the method improves on the results of that study. <eos> further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. <eos> altogether, we improve on the best previous results by 11 points absolute in f measure.
adjectival modification, particularly by expressions that have been treated as higherorder modifiers in the formal semantics tradition, raises interesting challenges for semantic composition in distributional semantic models. <eos> we contrast three types of adjectival modifiers ? <eos> intersectively used color terms ( as in white towel, clearly first-order ), subsectively used color terms ( white wine, which have been modeled as both first- and higher-order ), and intensional adjectives ( former bassist, clearly higher-order ) ? <eos> and test the ability of different composition strategies to model their behavior. <eos> in addition to opening up a new empirical domain for research on distributional semantics, our observations concerning the attested vectors for the different types of adjectives, the nouns they modify, and the resulting noun phrases yield insights into modification that have been little evident in the formal semantics literature to date.
we explore the interplay of knowledge and structure in co-reference resolution. <eos> to inject knowledge, we use a state-of-the-art system which cross-links ( or ? grounds ? ) <eos> expressions in free text to wikipedia. <eos> we explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. <eos> to maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. <eos> our end system outperforms the state-of-the-art baseline by 2 b3 f1 points on non-transcript portion of the ace 2004 dataset.
pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. <eos> traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. <eos> we propose a joint learning model which combines pairwise classification and mention clustering with markov logic. <eos> experimental results show that our joint learning system outperforms independent learning systems. <eos> our system gives a better performance than all the learning-based systems from the conll-2011 shared task on the same dataset. <eos> compared with the best system from conll2011, which employs a rule-based method, our system shows competitive performance.
we annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. <eos> we propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. <eos> the model is not restricted to nominal or verbal antecedents ; rather, it is able to identify antecedents that are arbitrary spans of text. <eos> our results show that ( a ) the model outperforms the strong adjacent-sentence baseline ; ( b ) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other nps such as this problem and this debate ; and ( c ) it is possible to reduce the search space in order to improve performance.
bridging the lexical gap between the user ? s question and the question-answer pairs in the q & a archives has been a major challenge for q & a retrieval. <eos> state-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. <eos> while useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. <eos> moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. <eos> this results in degraded retrieval performance. <eos> in this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts ( entities ). <eos> we explore strategies to learn the translation probabilities between words and the concepts using the q & a archives and a popular entity catalog. <eos> experiments conducted on a large scale real data show that the proposed techniques are promising.
taxonomies can serve as browsing tools for document collections. <eos> however, given an arbitrary collection, pre-constructed taxonomies could not easily adapt to the specific topic/task present in the collection. <eos> this paper explores techniques to quickly derive task-specific taxonomies supporting browsing in arbitrary document collections. <eos> the supervised approach directly learns semantic distances from users to propose meaningful task-specific taxonomies. <eos> the approach aims to produce globally optimized taxonomy structures by incorporating path consistency control and usergenerated task specification into the general learning framework. <eos> a comparison to stateof-the-art systems and a user study jointly demonstrate that our techniques are highly effective.
cost-sensitive classification, where the features used in machine learning tasks have a cost, has been explored as a means of balancing knowledge against the expense of incrementally obtaining new features. <eos> we introduce a setting where humans engage in classification with incrementally revealed features : the collegiate trivia circuit. <eos> by providing the community with a web-based system to practice, we collected tens of thousands of implicit word-by-word ratings of how useful features are for eliciting correct answers. <eos> observing humans ? <eos> classification process, we improve the performance of a state-of-the art classifier. <eos> we also use the dataset to evaluate a system to compete in the incremental classification task through a reduction of reinforcement learning to classification. <eos> our system learns when to answer a question, performing better than baselines and most human players.
we present a systematic analysis of existing multi-domain learning approaches with respect to two questions. <eos> first, many multidomain learning algorithms resemble ensemble learning algorithms. <eos> ( 1 ) are multi-domain learning improvements the result of ensemble learning effects ? <eos> second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. <eos> when multi-domain learning is applied to these settings, ( 2 ) are multidomain methods improving because they capture domain-specific class biases ? <eos> an understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.
representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. <eos> we present a novel, formal statement of the representation learning task. <eos> we argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. <eos> leveraging the posterior regularization framework, we develop an architecture for incorporating biases into representation learning. <eos> we investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16 % for both tasks, with respect to existing state-of-the-art representation learning techniques.
we introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. <eos> the approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. <eos> we incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. <eos> the resulting family of algorithms includes the expectation-maximization algorithm ( em ) and its variant, viterbi em, as well as a so-called softmax-em algorithm. <eos> the softmax-em algorithm can be implemented with a simple and computationally efficient extension to standard em. <eos> in our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing ( of the regularization strength ) and sparsity priors it leads to improvement over the current state of the art.
extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using conditional random fields ( crfs ). <eos> crfs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. <eos> thus, we propose a semi-crf-based approach to the task that can perform sequence labeling at the segment level. <eos> we extend the original semi-crf model ( sarawagi and cohen, 2004 ) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. <eos> we evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segmentlevel syntactic parse features. <eos> experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks.
this paper proposes a novel approach to extract opinion targets based on wordbased translation model ( wtm ). <eos> at first, we apply wtm in a monolingual scenario to mine the associations between opinion targets and opinion words. <eos> then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. <eos> by using wtm, our method can capture opinion relations more precisely, especially for long-span relations. <eos> in particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large web corpora. <eos> by using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as double propagation. <eos> the experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods.
we investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. <eos> our approach is to build predictive models of concrete external variables, such as restaurant menu prices. <eos> we make use of a dataset of menus and customer reviews for thousands of restaurants in several u.s. cities. <eos> by focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to describe food in restaurants. <eos> we also explore interactions in language use between menu prices and sentiment as expressed in user reviews.
we present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. <eos> this unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. <eos> until now, however, such conversion schemes have been created manually. <eos> our central hypothesis is that a valid mapping yields pos annotations with coherent linguistic properties which are consistent across source and target languages. <eos> we encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. <eos> given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. <eos> our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing.1
in modern chinese articles or conversations, it is very popular to involve a few english words, especially in emails and internet literature. <eos> therefore, it becomes an important and challenging topic to analyze chinese-english mixed texts. <eos> the underlying problem is how to tag part-of-speech ( pos ) for the english words involved. <eos> due to the lack of specially annotated corpus, most of the english words are tagged as the oversimplified type, ? foreign words ?. <eos> in this paper, we present a method using dynamic features to tag pos of mixed texts. <eos> experiments show that our method achieves higher performance than traditional sequence labeling methods. <eos> meanwhile, our method also boosts the performance of pos tagging for pure chinese texts.
despite significant recent work, purely unsupervised techniques for part-of-speech ( pos ) tagging have not achieved useful accuracies required by many language processing tasks. <eos> use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy. <eos> however, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. <eos> in this paper we show that we can build pos-taggers exceeding state-of-the-art bilingual methods by using simple hidden markov models and a freely available and naturally growing resource, the wiktionary. <eos> across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods. <eos> we achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised penn treebank.
we present a multilingual joint approach to word sense disambiguation ( wsd ). <eos> our method exploits babelnet, a very large multilingual knowledge base, to perform graphbased wsd across different languages, and brings together empirical evidence from these languages using ensemble methods. <eos> the results show that, thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods, we are able to achieve the state of the art in both monolingual and multilingual wsd settings.
we present a new minimally-supervised framework for performing domain-driven word sense disambiguation ( wsd ). <eos> glossaries for several domains are iteratively acquired from the web by means of a bootstrapping technique. <eos> the acquired glosses are then used as the sense inventory for fullyunsupervised domain wsd. <eos> our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.
a popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. <eos> in this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. <eos> linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms ( i.e., attributes native speakers consider important in describing the meaning of a word ). <eos> the models differ in terms of the mechanisms by which they integrate the two modalities. <eos> experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two.
state-of-the-art statistical parsers and pos taggers perform very well when trained with large amounts of in-domain data. <eos> when training data is out-of-domain or limited, accuracy degrades. <eos> in this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. <eos> we show how to augment sentencelevel models for parsing and pos tagging with inter-sentence consistency constraints. <eos> to deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. <eos> in experiments, we add consistency constraints to the mst parser and the stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.
most previous approaches to syntactic parsing of chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in chinese. <eos> we show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. <eos> hence in practice the strict separation of morphology and syntax in the chinese language proves to be untenable. <eos> we present a unified dependency parsing approach for chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. <eos> by removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. <eos> evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sentences simultaneously. <eos> 1
most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. <eos> we present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. <eos> experimental evaluation on chinese, czech, english and german shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.
activities on social media increase at a dramatic rate. <eos> when an external event happens, there is a surge in the degree of activities related to the event. <eos> these activities may be temporally correlated with one another, but they may also capture different aspects of an event and therefore exhibit different bursty patterns. <eos> in this paper, we propose to identify event-related bursts via social media activities. <eos> we study how to correlate multiple types of activities to derive a global bursty pattern. <eos> to model smoothness of one state sequence, we propose a novel function which can capture the state context. <eos> the experiments on a large twitter dataset shows our methods are very effective.
we consider the task of predicting the gender of the youtube1 users and contrast two information sources : the comments they leave and the social environment induced from the affiliation graph of users and videos. <eos> we propagate gender information through the videos and show that a user ? s gender can be predicted from her social environment with the accuracy above 90 %. <eos> we also show that the gender can be predicted from language alone ( 89 % ). <eos> a surprising result of our study is that the latter predictions correlate more strongly with the gender predominant in the user ? s environment than with the sex of the person as reported in the profile. <eos> we also investigate how the two views ( linguistic and social ) can be combined and analyse how prediction accuracy changes over different age groups.
the question ? how predictable is english ? ? <eos> has long fascinated researchers. <eos> while prior work has focused on formal english typically used in news articles, we turn to texts generated by users in online settings that are more informal in nature. <eos> we are motivated by a novel application scenario : given the difficulty of typing on mobile devices, can we help reduce typing effort with message completion, especially in conversational settings ? <eos> we propose a method for automatic response completion. <eos> our approach models both the language used in responses and the specific context provided by the original message. <eos> our experimental results on a large-scale dataset show that both components help reduce typing effort. <eos> we also perform an information-theoretic study in this setting and examine the entropy of user-generated content, especially in conversational scenarios, to better understand predictability of user generated english.
the geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. <eos> one common approach for geolocating texts is rooted in information retrieval. <eos> given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the earth and pseudo-documents constructed by concatenating the documents within a given grid cell ; then a location for a test document is chosen based on the most similar pseudo-document. <eos> uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. <eos> we define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. <eos> we also provide a better way of choosing the locations for pseudo-documents. <eos> we evaluate these strategies on existing wikipedia and twitter corpora, as well as a new, larger twitter corpus. <eos> the adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large twitter corpus. <eos> the two grid constructions can also be combined to produce consistently strong results across all training sets.
discriminative training in query spelling correction is difficult due to the complex internal structures of the data. <eos> recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. <eos> the ranker, however, suffers from the fact the low recall of the first, suboptimal, search stage. <eos> this paper proposes to directly optimize the search stage with a discriminative model based on latent structural svm. <eos> in this model, we treat query spelling correction as a multiclass classification problem with structured input and output. <eos> the latent structural information is used to model the alignment of words in the spelling correction process. <eos> experiment results show that as a standalone speller, our model outperforms all the baseline systems. <eos> it also attains a higher recall compared with the noisy channel model, and can therefore serve as a better filtering stage when combined with a ranker.
much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements. <eos> however, most previous research for computational stylometric analysis has relied on shallow lexico-syntactic patterns. <eos> some very recent work has shown that pcfg models can detect distributional difference in syntactic styles, but without offering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship. <eos> in this paper, we present a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on interpretable characterization of stylistic elements. <eos> we present analytic insights with respect to the authorship attribution task in two different domains.
since many applications such as timeline summaries and temporal ir involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. <eos> instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. <eos> based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. <eos> the experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a maxent classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small.
in this paper we classify the temporal relations between pairs of events on an article-wide basis. <eos> this is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. <eos> to achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. <eos> we propose the use of several discourse analysis frameworks, including 1 ) rhetorical structure theory ( rst ), 2 ) pdtb-styled discourse relations, and 3 ) topical text segmentation. <eos> we explain how features derived from these frameworks can be effectively used with support vector machines ( svm ) paired with convolution kernels. <eos> experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16 % in terms of f1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. <eos> making use of more accurate discourse analysis can further boost gains to 35 %.
distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. <eos> in this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. <eos> the combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. <eos> a simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting.
children learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. <eos> developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning. <eos> here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. <eos> we test on child-directed utterances in english and spanish and compare to single-task baselines. <eos> in the morphologically poorer language ( english ), the model improves morphological segmentation, while in the morphologically richer language ( spanish ), it leads to better syntactic categorization. <eos> these results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.
we present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. <eos> we define the model as a bayesian noisy channel ; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. <eos> compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. <eos> on data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. <eos> in contrast, our joint model, like infant learners, tends to learn multiword collocations. <eos> we also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.
animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. <eos> we present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. <eos> we show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.
we present a unified unsupervised statistical model for text normalization. <eos> the relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. <eos> the weights of these features are trained in a maximumlikelihood framework, employing a novel sequential monte carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. <eos> this model is implemented in a normalization system called unlol, which achieves the best known results on two normalization datasets, outperforming more complex systems. <eos> we use the output of unlol to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.
compared to the edited genres that have played a central role in nlp research, microblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation. <eos> when confronted with such input, conventional text analysis tools often perform poorly. <eos> normalization ? <eos> replacing orthographically or lexically idiosyncratic forms with more standard variants ? <eos> can improve performance. <eos> we propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages. <eos> to validate the utility of our approach, we evaluate extrinsically, showing that normalizing english tweets and then translating improves translation quality ( compared to translating unnormalized text ) using three standard web translation services as well as a phrase-based translation system trained on parallel microblog data.
in this paper, we address the problem of estimating question difficulty in community question answering services. <eos> we propose a competition-based model for estimating question difficulty by leveraging pairwise comparisons between questions and users. <eos> our experimental results show that our model significantly outperforms a pagerank-based approach. <eos> most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. <eos> this implies the possibility of predicting question difficulty from the text of question descriptions.
we seek to measure political candidates ? <eos> ideological positioning from their speeches. <eos> to accomplish this, we infer ideological cues from a corpus of political writings annotated with known ideologies. <eos> we then represent the speeches of u.s. presidential candidates as sequences of cues and lags ( filler distinguished only by its length in words ). <eos> we apply a domain-informed bayesian hmm to infer the proportions of ideologies each candidate uses in each campaign. <eos> the results are validated against a set of preregistered, domain expertauthored hypotheses.
we present a novel model, freestyle, that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics, by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations. <eos> in this attack on the woefully under-explored natural language genre of music lyrics, we exploit a strictly unsupervised transduction grammar induction approach. <eos> our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed, even though the domain of hip hop lyrics is particularly noisy and unstructured. <eos> we evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction, and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses, measured on fluency and rhyming criteria as judged by human evaluators. <eos> to highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based smt system. <eos> we tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module, which is also acquired via unsupervised learning and report improved quality of the generated responses. <eos> finally, we report results with maghrebi french hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages.
when reviewing scientific literature, it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles. <eos> in this context, this paper introduces topical influence, a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it. <eos> given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles. <eos> experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach.
we introduce a novel method to jointly parse and detect disfluencies in spoken utterances. <eos> our model can use arbitrary features for parsing sentences and adapt itself with out-ofdomain data. <eos> we show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks. <eos> additionally, our method is the fastest for the joint task, running in linear time.
we present a novel vector space model for semantic co-compositionality. <eos> inspired by generative lexicon theory ( pustejovsky, 1995 ), our goal is a compositional model where both predicate and argument are allowed to modify each others ? <eos> meaning representations while generating the overall semantics. <eos> this readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. <eos> we implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. <eos> we further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. <eos> the model achieves the best result to date ( ? <eos> = 0.47 ) on the semantic similarity task of transitive verbs ( grefenstette and sadrzadeh, 2011 ).
in this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. <eos> specifically, we focus on properties distinguishing adjectiveadjective-noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. <eos> we explore a number of measures extracted from the distributional representation of aan phrases which may indicate a word order restriction. <eos> we find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. <eos> our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.
domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. <eos> in this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. <eos> specifically, we propose a hierarchical multinomial naive bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. <eos> we train this latent graphical model using a simple expectation-maximization ( em ) algorithm. <eos> we empirically evaluate the proposed method with both cross-domain document categorization tasks on reuters-21578 dataset and cross-domain sentiment classification tasks on amazon product review dataset. <eos> the experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.
two recent measures incorporate the notion of statistical significance in basic pmi formulation. <eos> in some tasks, we find that the new measures perform worse than the pmi. <eos> our analysis shows that while the basic ideas in incorporating statistical significance in pmi are reasonable, they have been applied slightly inappropriately. <eos> by fixing this, we get new measures that improve performance over not just pmi but on other popular co-occurrence measures as well. <eos> in fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.
in this paper we present a minimallysupervised approach to the multi-domain acquisition of wide-coverage glossaries. <eos> we start from a small number of hypernymy relation seeds and bootstrap glossaries from the web for dozens of domains using probabilistic topic models. <eos> our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions.
the creation of a pronunciation lexicon remains the most inefficient process in developing an automatic speech recognizer ( asr ). <eos> in this paper, we propose an unsupervised alternative ? <eos> requiring no language-specific knowledge ? <eos> to the conventional manual approach for creating pronunciation dictionaries. <eos> we present a hierarchical bayesian model, which jointly discovers the phonetic inventory and the letter-to-sound ( l2s ) mapping rules in a language using only transcribed data. <eos> when tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and l2s mappings are learned separately. <eos> furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using conventional supervised procedures.
this paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. <eos> the model is an extension of a bayesian many-to-many alignment method for distinguishing nontransliteration ( noise ) parts in phrase pairs. <eos> it worked effectively in the experiments of bootstrapping japanese-to-english statistical machine transliteration in patent domain using patent bilingual corpora.
beam search is a fast and empirically effective method for translation decoding, but it lacks formal guarantees about search error. <eos> we develop a new decoding algorithm that combines the speed of beam search with the optimal certificate property of lagrangian relaxation, and apply it to phrase- and syntax-based translation decoding. <eos> the new method is efficient, utilizes standard mt algorithms, and returns an exact solution on the majority of translation examples in our test data. <eos> the algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation.
ngram language models tend to increase in size with inflating the corpus size, and consume considerable resources. <eos> in this paper, we propose an efficient method for implementing ngram models based on doublearray structures. <eos> first, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency. <eos> next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures. <eos> embedding probabilities into unused spaces in double-array structures reduces the model size. <eos> moreover, tuning the word ids in the language model makes the model smaller and faster. <eos> we also show that our method can be used for building large language models using the division method. <eos> lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used.
language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. the complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. <eos> we present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. <eos> we account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization.
current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. <eos> alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. <eos> here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. <eos> in our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48 % relative over a traditional post-edition system.
traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. <eos> alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by bleu. <eos> in the max-margin estimation of parameters, we only need to calculate viterbi translations. <eos> this further facilitates the incorporation of various non-local features that are defined on the target side. <eos> we test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. <eos> experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 bleu points and is also better than previous max-likelihood estimation method.
coreference resolution metrics quantify errors but do not analyze them. <eos> here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. <eos> using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.
coreference resolution plays a critical role in discourse analysis. <eos> this paper focuses on exploiting zero pronouns to improve chinese coreference resolution. <eos> in particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods ( refining syntactic parser and refining learning example generation ) are employed to exploit zero pronouns for chinese coreference resolution. <eos> evaluation on the conll-2012 shared task data set shows that zero pronouns can significantly improve chinese coreference resolution.
many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. <eos> errors in named-entity linking ( nel ), on the other hand, are often caused by superficial modeling of entity context. <eos> this paper demonstrates that these two tasks are complementary. <eos> we introduce neco, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. <eos> neco extends the stanford deterministic coreference system by automatically linking mentions to wikipedia and introducing new nel-informed mention-merging sieves. <eos> linking improves mention-detection and enables new semantic attributes to be incorporated from freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. <eos> experiments show consistent improvements across a number of datasets and experimental conditions, including over 11 % reduction in muc coreference error and nearly 21 % reduction in f1 nel error on ace 2004 newswire data.
interpreting anaphoric shell nouns ( asns ) such as this issue and this fact is essential to understanding virtually any substantial natural language text. <eos> one obstacle in developing methods for automatically interpreting asns is the lack of annotated data. <eos> we tackle this challenge by exploiting cataphoric shell nouns ( csns ) whose construction makes them particularly easy to interpret ( e.g., the fact that x ). <eos> we propose an approach that uses automatically extracted antecedents of csns as training data to interpret asns. <eos> we achieve precisions in the range of 0.35 ( baseline = 0.21 ) to 0.72 ( baseline = 0.44 ), depending upon the shell noun.
nowadays supervised sequence labeling models can reach competitive performance on the task of chinese word segmentation. <eos> however, the ability of these models is restricted by the availability of annotated data and the design of features. <eos> we propose a scalable semi-supervised feature engineering approach. <eos> in contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. <eos> we update the representation values with a semi-supervised approach. <eos> experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. <eos> the feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.
training higher-order conditional random fields is prohibitive for huge tag sets. <eos> we present an approximated conditional random field using coarse-to-fine decoding and early updating. <eos> we show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1st-order models.
morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. <eos> we analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. <eos> we show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.
this paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in semitic languages, in terms of a mildly context-sensitive grammar formalism. <eos> this offers a convenient level of modelling abstraction while remaining computationally tractable. <eos> the nonparametric bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. <eos> our experiments on hebrew and three variants of arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. <eos> we obtain 74 % accuracy in identifying triliteral hebrew roots, while performing morphological segmentation with an f1-score of 78.1.
this paper describes a method that predicts which trades players execute during a winlose game. <eos> our method uses data collected from chat negotiations of the game the settlers of catan and exploits the conversation to construct dynamically a partial model of each player ? s preferences. <eos> this in turn yields equilibrium trading moves via principles from game theory. <eos> we compare our method against four baselines and show that tracking how preferences evolve through the dialogue and reasoning about equilibrium moves are both crucial to success.
human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. <eos> researchers in nlp have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the contingent discourse relation, or as a type of common-sense causal reasoning. <eos> our approach is to model likelihood between events by drawing on several of these lines of previous work. <eos> we implement and evaluate different unsupervised methods for learning event pairs that are likely to be contingent on one another. <eos> we refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. <eos> our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64 % over a baseline of 50 %, as compared to an average accuracy of 75.15 % without web search.
in situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. <eos> their representations of the shared world are misaligned. <eos> thus referring expression generation ( reg ) will need to take this discrepancy into consideration. <eos> to address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. <eos> our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9 %. <eos> however, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment ( e.g., 84 % ), they perform rather poorly when the agent has imperfect perception of the environment ( e.g., 45 % ). <eos> this big performance gap calls for new solutions to reg that can mediate a shared perceptual basis in situated dialogue.
this paper introduces a method for extracting fine-grained class labels ( ? countries with double taxation agreements with india ? ) <eos> from web search queries. <eos> the class labels are more numerous and more diverse than those produced by current extraction methods. <eos> also extracted are representative sets of instances ( singapore, united kingdom ) for the class labels.
in this paper we present an unsupervised approach to relational information extraction. <eos> our model partitions tuples representing an observed syntactic relationship between two named entities ( e.g., ? x was born in y ? <eos> and ? x is from y ? ) <eos> into clusters corresponding to underlying semantic relation types ( e.g., bornin, located ). <eos> our approach incorporates general domain knowledge which we encode as first order logic rules and automatically combine with a topic model developed specifically for the relation extraction task. <eos> evaluation results on the ace 2007 english relation detection and categorization ( rdc ) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules.
entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. <eos> recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. <eos> we propose a fast collective disambiguation approach based on stacking. <eos> first, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. <eos> second, top k candidates of related instances are searched for constructing expressive global coherence features. <eos> a global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. <eos> the proposed method is fast and easy to implement. <eos> experiments show its effectiveness over various algorithms on several public datasets. <eos> by learning a rich semantic relatedness measure between entity categories and context document, performance is further improved.
web search can be enhanced in powerful ways if token spans in web text are annotated with disambiguated entities from large catalogs like freebase. <eos> entity annotators need to be trained on sample mention snippets. <eos> wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. <eos> unfortunately, wikipedia features only one-ninth the number of entities as freebase, and these are a highly biased sample of well-connected, frequently mentioned ? head ? <eos> entities. <eos> to bring hope to ? tail ? <eos> entities, we broaden our goal to a second task : assigning types to entities in freebase but not wikipedia. <eos> the two tasks are synergistic : knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. <eos> we present tmi, a bipartite graphical model for joint type-mention inference. <eos> tmi attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. <eos> in experiments involving 780,000 people in wikipedia, 2.3 million people in freebase, 700 million web pages, and over 20 professional editors, tmi shows considerable annotation accuracy improvement ( e.g., 70 % ) compared to baselines ( e.g., 46 % ), especially for ? tail ? <eos> and emerging entities. <eos> we also compare with google ? s recent annotations of the same corpus with freebase entities, and report considerable improvements within the people domain.
a large number of open relation extraction approaches have been proposed recently, covering a wide range of nlp machinery, from ? shallow ? <eos> ( e.g., part-of-speech tagging ) to ? deep ? <eos> ( e.g., semantic role labeling ? srl ). <eos> a natural question then is what is the tradeoff between nlp depth ( and associated computational cost ) versus effectiveness. <eos> this paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. <eos> the paper also describes a novel method, exemplar, which adapts ideas from srl to less costly nlp machinery, resulting in substantial gains both in efficiency and effectiveness, over binary and n-ary relation extraction tasks.
this paper proposes a framework for automatically engineering features for two important tasks of question answering : answer sentence selection and answer extraction. <eos> we represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and named entity recognizer. <eos> tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. <eos> we conduct experiments on a public benchmark from trec to compare with previous systems for answer sentence selection and answer extraction. <eos> the results show that our models greatly improve on the state of the art, e.g., up to 22 % on f1 ( relative improvement ) for answer extraction, while using no additional resources and no manual feature engineering.
bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. <eos> their manual construction requires strong expertise in both languages involved and is a costly process. <eos> several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. <eos> we introduce a novel approach to the creation of specific domain bilingual lexicon that relies on wikipedia. <eos> this massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. <eos> wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. <eos> the approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs : frenchenglish and romanian-english. <eos> the newly introduced method compares favorably to existing methods in all configurations tested.
joint compression and summarization has been used recently to generate high quality summaries. <eos> however, such word-based joint optimization is computationally expensive. <eos> in this paper we adopt the ? sentence compression + sentence selection ? <eos> pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. <eos> to create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. <eos> using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. <eos> during summarization, we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences. <eos> our results on the tac 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.
reordering poses one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units. <eos> we present the ? anchor graph ? <eos> ( ag ) model where we use a graph structure to model global contextual information that is crucial for reordering. <eos> the key ingredient of our ag model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. <eos> as the edges link anchors that may span multiple translation units at decoding time, our ag model effectively encodes global contextual information that is previously absent. <eos> we integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale chinese-to-english translation task.
we present a simple and novel classifier-based preordering approach. <eos> unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. <eos> our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. <eos> we present extensive experiments on 22 language pairs, including preordering into english from 7 other languages. <eos> we obtain improvements of up to 1.4 bleu on language pairs in the wmt 2010 shared task. <eos> for languages from different families the improvements often exceed 2 bleu. <eos> many of these gains are also significant in human evaluations.
this paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation ( smt ). <eos> for language pairs with few bilingual data, a possible solution in pivot-based smt using another language as a `` bridge '' to generate source-target translation. <eos> however, one of the weaknesses is that some useful sourcetarget translations can not be generated if the corresponding source phrase and target phrase connect to different pivot phrases. <eos> to alleviate the problem, we utilize markov random walks to connect possible translation phrases between source and target language. <eos> experimental results on european parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system.
this paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. <eos> in this area, most of techniques usually utilize confusion networks ( cn ) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results. <eos> however, these information may be heterogeneous, so the widely-used viterbi algorithm for searching the best alignment may not apply here. <eos> in the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms. <eos> the solutions from this framework, termed pareto optimal solutions, are then combined to construct confusion networks. <eos> experiments on two chinese-to-english translation datasets show significant improvements, 0.97 and 1.06 bleu points over a strong indirected hidden markov model-based ( ihmm ) system, and 4.75 and 3.53 points over the best single machine translation systems.
machine translation benefits from system combination. <eos> we propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. <eos> we introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models ? specification, generalization, and interchange. <eos> the experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. <eos> all three schemes significantly improve results of any single system on four testsets. <eos> we find that specification ? a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans ? comes out as the strongest, yielding improvement up to 0.9 ( ter-bleu ) /2 points. <eos> we also provide a detailed experimental and qualitative analysis of the results.
this paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. <eos> in contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models : 1 ) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans ; 2 ) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. <eos> the features produced by both models are used as soft constraints to guide the translation process. <eos> experiments on chinese-english translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.
while inversion transduction grammar ( itg ) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules ( i.e., straight and inverted ) dependent on actual blocks being merged remains a challenge. <eos> unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. <eos> the recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling ? s perspective. <eos> experiments on the nist 2008 dataset show that our system significantly improves over the maxent classifier by 1.07 bleu points.
in this paper, we analyze a novel set of features for the task of automatic edit category classification. <eos> edit category classification assigns categories such as spelling error correction, paraphrase or vandalism to edits in a document. <eos> our features are based on differences between two versions of a document including meta data, textual and language properties and markup. <eos> in a supervised machine learning experiment, we achieve a micro-averaged f1 score of.62 on a corpus of edits from the english wikipedia. <eos> in this corpus, each edit has been multi-labeled according to a 21-category taxonomy. <eos> a model trained on the same data achieves state-of-the-art performance on the related task of fluency edit classification. <eos> we apply pattern mining to automatically labeled edits in the revision histories of different wikipedia articles. <eos> our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles.
we introduce a novel discriminative model for phrase-based monolingual alignment using a semi-markov crf. <eos> our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets ( rte and paraphrase ), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. <eos> additional experiments highlight the potential benefit of our alignment model to rte, paraphrase identification and question answering, where even a naive application of our model ? s alignment score approaches the state of the art.
coreference resolution is a well known clustering task in natural language processing. <eos> in this paper, we describe the latent left linking model ( l3m ), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. <eos> we show that l3m admits efficient inference and can be augmented with knowledge-based constraints ; we also present a fast stochastic gradient based learning. <eos> experiments on ace and ontonotes data show that l3m and its constrained version, cl3m, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.
the performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. <eos> in this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs. <eos> we show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. <eos> to further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. <eos> our experimental results show that ( weighted ) centering is effective for natural language data ; it improves the performance of the k-nearest neighbor classifiers considerably in word sense disambiguation and document classification tasks.
we derive a spectral method for unsupervised learning of weighted context free grammars. <eos> we frame wcfg induction as finding a hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions. <eos> the proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the hankel matrix.
we address the problem of identifying multiword expressions in a language, focusing on english phrasal verbs. <eos> our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages. <eos> our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of english phrasal verbs, achieving performance comparable to a human-curated set.
this study explores the feasibility of performing chinese word segmentation ( cws ) and pos tagging by deep learning. <eos> we try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. <eos> we leverage large-scale unlabeled data to improve internal representation of chinese characters, and use these improved representations to enhance supervised word segmentation and pos tagging models. <eos> our networks achieved close to state-of-theart performance with minimal computational cost. <eos> we also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.
chinese word segmentation and part-ofspeech tagging ( s & t ) are fundamental steps for more advanced chinese language processing tasks. <eos> recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for chinese s & t. <eos> in this paper, we propose a unified model for chinese s & t with heterogeneous annotation corpora. <eos> we first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, penn chinese treebank ( ctb ) and pku ? s people ? s daily ( ppd ). <eos> then we regard the chinese s & t with heterogeneous corpora as two ? related ? <eos> tasks and train our model on two heterogeneous corpora simultaneously. <eos> experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods.
studies of the graph of dictionary definitions ( dd ) ( picard et al, 2009 ; levary et al, 2012 ) have revealed strong semantic coherence of local topological structures. <eos> the techniques used in these papers are simple and the main results are found by understanding the structure of cycles in the directed graph ( where words point to definitions ). <eos> based on our earlier work ( levary et al, 2012 ), we study a different class of word definitions, namely those of the free association ( fa ) dataset ( nelson et al, 2004 ). <eos> these are responses by subjects to a cue word, which are then summarized by a directed, free association graph. <eos> we find that the structure of this network is quite different from both the wordnet and the dictionary networks. <eos> this difference can be explained by the very nature of free association as compared to the more ? logical ? <eos> construction of dictionaries. <eos> it thus sheds some ( quantitative ) light on the psychology of free association. <eos> in nlp, semantic groups or clusters are interesting for various applications such as word sense disambiguation. <eos> the fa graph is tighter than the dd graph, because of the large number of triangles. <eos> this also makes drift of meaning quite measurable so that fa graphs provide a quantitative measure of the semantic coherence of small groups of words.
creating a language-independent meaning representation would benefit many crosslingual nlp tasks. <eos> we introduce the first unsupervised approach to this problem, learning clusters of semantically equivalent english and french relations between referring expressions, based on their named-entity arguments in large monolingual corpora. <eos> the clusters can be used as language-independent semantic relations, by mapping clustered expressions in different languages onto the same relation. <eos> our approach needs no parallel text for training, but outperforms a baseline that uses machine translation on a cross-lingual question answering task. <eos> we also show how to use the semantics to improve the accuracy of machine translation, by using it in a simple reranker.
in this paper we propose a two-stage method to acquire contradiction relations between typed lexico-syntactic patterns such as xdrug prevents ydisease and ydisease caused by xdrug. <eos> in the first stage, we train an svm classifier to detect contradiction pattern pairs in a large web archive by exploiting the excitation polarity ( hashimoto et al, 2012 ) of the patterns. <eos> in the second stage, we enlarge the first stage classifier ? s training data with new contradiction pairs obtained by combining the output of the first stage ? s classifier and that of an entailment classifier. <eos> we acquired this way 750,000 typed japanese contradiction pattern pairs with an estimated precision of 80 %. <eos> we plan to release this resource to the nlp community.
a common form of sarcasm on twitter consists of a positive sentiment contrasted with a negative situation. <eos> for example, many sarcastic tweets include a positive sentiment, such as ? love ? <eos> or ? enjoy ?, followed by an expression that describes an undesirable activity or state ( e.g., ? taking exams ? <eos> or ? being ignored ? ). <eos> we have developed a sarcasm recognizer to identify this type of sarcasm in tweets. <eos> we present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. <eos> we show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition.
personal profile information on social media like linkedin.com and facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. <eos> however, personal profiles usually lack organization confronted with the large amount of available information. <eos> therefore, it is always a challenge for people to find desired information from them. <eos> in this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. <eos> here, using social networks is motivated by the intuition that, people with similar academic, business or social connections ( e.g. <eos> co-major, co-university, and cocorporation ) tend to have similar experience and summaries. <eos> to achieve the learning process, we propose a collective factor graph ( cofg ) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. <eos> extensive evaluation on a large-scale dataset from linkedin.com demonstrates the effectiveness of the proposed approach. <eos> *
recently, much research focuses on event storyline generation, which aims to produce a concise, global and temporal event summary from a collection of articles. <eos> generally, each event contains multiple sub-events and the storyline should be composed by the component summaries of all the sub-events. <eos> however, different sub-events have different part-whole relationship with the major event, which is important to correspond to users ? <eos> interests but seldom considered in previous work. <eos> to distinguish different types of sub-events, we propose a mixture-event-aspect model which models different sub-events into local and global aspects. <eos> combining these local/global aspects with summarization requirements together, we utilize an optimization method to generate the component summaries along the timeline. <eos> we develop experimental systems on 6 distinctively different datasets. <eos> evaluation and comparison results indicate the effectiveness of our proposed method.
document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. <eos> in various summarization tasks, the summary length is manually defined. <eos> however, how to find the proper summary length is quite a problem ; and keeping all summaries restricted to the same length is not always a good choice. <eos> it is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. <eos> in this paper, we propose a bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. <eos> assuming that an original document can be reconstructed from its summary, we describe the ? reconstruction ? <eos> by a bayesian framework which selects sentences to form a good summary. <eos> experimental results on duc2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination.
we present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. <eos> full papers are first automatically annotated using the coresc scheme, which captures 11 contentbased concepts such as hypothesis, result, conclusion etc at the sentence level. <eos> a content model which follows the sequence of coresc categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. <eos> summary creation is also guided by the distribution of coresc categories found in the full articles, in order to adequately represent the article content. <eos> finally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. <eos> results are very encouraging as summaries of papers from automatically obtained corescs enable experts to answer 66 % of complex content-related questions designed on the basis of paper abstracts. <eos> the questions were answered with a precision of 75 %, where the upper bound for human summaries ( abstracts ) was 95 %.
we present the first provably optimal polynomial time dynamic programming ( dp ) algorithm for best-first shift-reduce parsing, which applies the dp idea of huang and sagae ( 2010 ) to the best-first parser of sagae and lavie ( 2006 ) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. <eos> we prove the correctness of our algorithm rigorously. <eos> experiments confirm that dp leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time.
the problem of learning language models from large text corpora has been widely studied within the computational linguistic community. <eos> however, little is known about the performance of these language models when applied to the computer vision domain. <eos> in this work, we compare representative models : a window-based model, a topic model, a distributional memory and a commonsense knowledge database, conceptnet, in two visual recognition scenarios : human action recognition and object prediction. <eos> we examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. <eos> we determine the usefulness of different language models in aiding the two visual recognition tasks. <eos> the study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.
this paper presents defminer, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions. <eos> defminer achieves 85 % f1 on a wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8 %. <eos> we exploit defminer to process the acl anthology reference corpus ( arc ) ? <eos> a large, real-world digital library of scientific articles in computational linguistics. <eos> the resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles. <eos> we highlight several interesting observations : more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. <eos> obtaining a list of popular defined terms in a corpus of computational linguistics papers, we find that concepts can often be categorized into one of three categories : resources, methodologies and evaluation metrics.
state-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. <eos> such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. <eos> in this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. <eos> we show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. <eos> furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. <eos> overall, our model significantly outperforms the illinois system that placed first in the conll-2013 shared task on grammatical error correction.
nilsson and nivre ( 2009 ) introduced a treebased model of persons ? <eos> eye movements in reading. <eos> the individual variation between readers reportedly made application across readers impossible. <eos> while a tree-based model seems plausible for eye movements, we show that competitive results can be obtained with a linear crf model. <eos> increasing the inductive bias also makes learning across readers possible. <eos> in fact we observe next-to-no performance drop when evaluating models trained on gaze records of multiple readers on new readers.
this paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. <eos> the results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.
recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. <eos> we develop linguistically motivated discourse structure, lexico-semantic and genericity detection features and integrate these into a cascaded minority preference algorithm that models bridging recognition as a subtask of learning finegrained information status ( is ). <eos> we substantially improve bridging recognition without impairing performance on other is classes.
we present an approach to time normalization ( e.g. <eos> the day before yesterday ? 2013-04-12 ) based on a synchronous context free grammar. <eos> synchronous rules map the source language to formally defined operators for manipulating times ( findenclosed, startatendof, etc. ). <eos> time expressions are then parsed using an extended cyk+ algorithm, and converted to a normalized form by applying the operators recursively. <eos> for evaluation, a small set of synchronous rules for english time expressions were developed. <eos> our model outperforms heideltime, the best time normalization system in tempeval 2013, on four different time normalization corpora.
the rise of ? big data ? <eos> analytics over unstructured text has led to renewed interest in information extraction ( ie ). <eos> we surveyed the landscape of ie technologies and identified a major disconnect between industry and academia : while rule-based ie dominates the commercial world, it is widely regarded as dead-end technology by the academia. <eos> we believe the disconnect stems from the way in which the two communities measure the benefits and costs of ie, as well as academia ? s perception that rulebased ie is devoid of research challenges. <eos> we make a case for the importance of rule-based ie to industry practitioners. <eos> we then lay out a research agenda in advancing the state-of-theart in rule-based ie systems which we believe has the potential to bridge the gap between academic research and industry practice.
automatically constructed knowledge bases ( kbs ) are often incomplete and there is a genuine need to improve their coverage. <eos> path ranking algorithm ( pra ) is a recently proposed method which aims to improve kb coverage by performing inference directly over the kb graph. <eos> for the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million web documents can significantly outperform previous prabased approaches on the kb inference task. <eos> we present extensive experimental results validating this finding. <eos> the resources presented in this paper are publicly available.
most of the machine translation systems rely on a large set of translation rules. <eos> these rules are treated as discrete and independent events. <eos> in this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. <eos> we present a preliminary generative model to test this idea. <eos> experimental results show about one point improvement on ter-bleu over a strong baseline in chinese-to-english translation.
neural network language models, or continuous-space language models ( cslms ), have been shown to improve the performance of statistical machine translation ( smt ) when they are used for reranking n-best translations. <eos> however, cslms have not been used in the first pass decoding of smt, because using cslms in decoding takes a lot of time. <eos> in contrast, we propose a method for converting cslms into back-off n-gram language models ( bnlms ) so that we can use converted cslms in decoding. <eos> we show that they outperform the original bnlms and are comparable with the traditional use of cslms in reranking.
mira based tuning methods have been widely used in statistical machine translation ( smt ) system with a large number of features. <eos> since the corpus-level bleu is not decomposable, these mira approaches usually define a variety of heuristic-driven sentencelevel bleus in their model losses. <eos> instead, we present a new mira method, which employs an exact corpus-level bleu to compute the model loss. <eos> our method is simpler in implementation. <eos> experiments on chinese-toenglish translation show its effectiveness over two state-of-the-art mira implementations.
multilingual speakers switch between languages in online and spoken communication. <eos> analyses of large scale multilingual data require automatic language identification at the word level. <eos> for our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. <eos> secondly, we incorporate context to improve the performance. <eos> we achieve an accuracy of 98 %. <eos> besides word level accuracy, we use two new metrics to evaluate this task.
linking name mentions in microblog posts to a knowledge base, namely microblog entity linking, is useful for text mining tasks on microblog. <eos> entity linking in long text has been well studied in previous works. <eos> however few work has focused on short text such as microblog post. <eos> microblog posts are short and noisy. <eos> previous method can extract few features from the post context. <eos> in this paper we propose to use extra posts for the microblog entity linking task. <eos> experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3 % and 7.5 % respectively.
multi-domain learning ( mdl ) assumes that the domain labels in the dataset are known. <eos> however, when there are multiple metadata attributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes ( including continuous attributes ) can lead to better mdl performance. <eos> in this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for mdl. <eos> we use a supervised clustering approach that learns the domain distance between data instances, and then cluster the data into better domains for mdl. <eos> our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular mdl methods.
this paper investigates the utility and effect of running numerous random restarts when using em to attack decipherment problems. <eos> we find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. <eos> for particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100k random restarts, which we accomplish efficiently using a gpu-based parallel implementation. <eos> we run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked zodiac 340.
we explore a model of stress prediction in russian using a combination of local contextual features and linguisticallymotivated features associated with the word ? s stem and suffix. <eos> we frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. <eos> we train our models using a simple maximum entropy ranking framework allowing for efficient prediction. <eos> an empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress.
we introduce a new highly scalable approach for computing distributional thesauri ( dts ). <eos> by employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. <eos> we demonstrate this by releasing a dt for the whole vocabulary of google books syntactic n-grams. <eos> evaluating against lexical resources using two measures, we show that our approach produces higher quality dts than previous approaches, and is thus preferable in terms of speed and quality for large corpora.
matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. <eos> the key idea is that similarity in the latent space implies semantic relatedness. <eos> we describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. <eos> first, we design a new discriminative term-weighting metric called tf-kld, which outperforms tf-idf. <eos> next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. <eos> finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3 % more accurate than the prior state-of-the-art.
extensive experiments have validated the effectiveness of the corpus-based method for classifying the word ? s sentiment polarity. <eos> however, no work is done for comparing different corpora in the polarity classification task. <eos> nowadays, twitter has aggregated huge amount of data that are full of people ? s sentiments. <eos> in this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. <eos> experiment results show that the twitter data can achieve a much better performance than the google, web1t and wikipedia based methods.
implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. <eos> we think, based on the explicit sentences, several support vector machine ( svm ) classifiers can be established to do this task. <eos> nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. <eos> experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.
online learning algorithms like the perceptron are widely used for structured prediction tasks. <eos> for sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors ( collins and roark, 2004 ; huang et al., 2012 ). <eos> however, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. <eos> in this paper, we generalize the violation-fixing perceptron of huang et al. <eos> ( 2012 ) to hypergraphs and apply it to the cube-pruning parser of zhang and mcdonald ( 2012 ). <eos> this results in the highest reported scores on wsj evaluation set ( uas 93.50 % and las 92.41 % respectively ) without the aid of additional resources.
we present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. <eos> the model is trained on a set of high frequency relations extracted from the penn discourse treebank and achieves an accuracy of 86.6 %. <eos> analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. <eos> we also present results of an experiment that provides insight into the nature and difficulty of the task.
in japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. <eos> however, previous studies have focused on only zero endophora, in which a referent explicitly appears. <eos> we present a zero reference resolution model considering zero exophora and author/reader of a document. <eos> to deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. <eos> in addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. <eos> we represent their particular behavior in a discourse as a feature vector of a machine learning model. <eos> the experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora.
natural language conversation is widely regarded as a highly difficult problem, which is usually attacked with either rule-based or learning-based models. <eos> in this paper we propose a retrieval-based automatic response model for short-text conversation, to exploit the vast amount of short conversation instances available on social media. <eos> for this purpose we introduce a dataset of short-text conversation based on the real-world instances from sina weibo ( a popular chinese microblog service ), which will be soon released to public. <eos> this dataset provides rich collection of instances for the research on finding natural and relevant short responses to a given short text, and useful for both training and testing of conversation models. <eos> this dataset consists of both naturally formed conversations, manually labeled data, and a large repository of candidate responses. <eos> our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset.
explanatory sentences are employed to clarify reasons, details, facts, and so on. <eos> high quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. <eos> these explanations can help readers get easily comprehensible information of the discussed products and aspects. <eos> moreover, explanatory relations can also benefit sentiment analysis applications. <eos> in this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. <eos> we propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. <eos> experimental results demonstrate the effectiveness of the proposed method.
we present an approach for building multidocument event threads from a large corpus of newswire articles. <eos> an event thread is basically a succession of events belonging to the same story. <eos> it helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. <eos> a specific effort is also made on the detection of reactions to a particular event. <eos> in order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. <eos> we also share interesting comments concerning our manual annotation procedure for building a training and testing set1.
scope detection is a key task in information extraction. <eos> this paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. <eos> in addition, we have explored the way of selecting compatible features for different part-of-speech cues. <eos> experiments on the bioscope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. <eos> compared with the state of the art scope detection systems, our system achieves substantial improvement. <eos> *
temporal variations of text are usually ignored in nlp applications. <eos> however, text use changes with time, which can affect many applications. <eos> in this paper we model periodic distributions of words over time. <eos> focusing on hashtag frequency in twitter, we first automatically identify the periodic patterns. <eos> we use this for regression in order to forecast the volume of a hashtag based on past data. <eos> we use gaussian processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. <eos> we demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text. <eos> this method shows significant improvements over competitive baselines.
direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy. <eos> however, simply focusing on direct quotations ignores around half of all reported speech, which is in the form of indirect or mixed speech. <eos> this work presents the first large-scale experiments in indirect and mixed quotation extraction and attribution. <eos> we propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. <eos> we further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution.
web search users frequently modify their queries in hope of receiving better results. <eos> this process is referred to as ? query reformulation ?. <eos> previous research has mainly focused on proposing query reformulations in the form of suggested queries for users. <eos> some research has studied the problem of predicting whether the current query is a reformulation of the previous query or not. <eos> however, this work has been limited to bag-of-words models where the main signals being used are word overlap, character level edit distance and word level edit distance. <eos> in this work, we show that relying solely on surface level text similarity results in many false positives where queries with different intents yet similar topics are mistakenly predicted as query reformulations. <eos> we propose a new representation for web search queries based on identifying the concepts in queries and show that we can significantly improve query reformulation performance using features of query concepts.
passage retrieval is a crucial first step of automatic question answering ( qa ). <eos> while existing passage retrieval algorithms are effective at selecting document passages most similar to the question, or those that contain the expected answer types, they do not take into account which parts of the document the searchers actually found useful. <eos> we propose, to the best of our knowledge, the first successful attempt to incorporate searcher examination data into passage retrieval for question answering. <eos> specifically, we exploit detailed examination data, such as mouse cursor movements and scrolling, to infer the parts of the document the searcher found interesting, and then incorporate this signal into passage retrieval for qa. <eos> our extensive experiments and analysis demonstrate that our method significantly improves passage retrieval, compared to using textual features alone. <eos> as an additional contribution, we make available to the research community the code and the search behavior data used in this study, with the hope of encouraging further research in this area.
this paper presents the kazakh language corpus ( klc ), which is one of the first attempts made within a local research community to assemble a kazakh corpus. <eos> klc is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres : literary, publicistic, official, scientific and informal. <eos> along with its primary part klc comprises such parts as : ( i ) annotated sub-corpus, containing segmented documents encoded in the extensible markup language ( xml ) that marks complete morphological, syntactic, and structural characteristics of texts ; ( ii ) as well as a sub-corpus with the annotated speech data. <eos> klc has a web-based corpus management system that helps to navigate the data and retrieve necessary information. <eos> klc is also open for contributors, who are willing to make suggestions, donate texts and help with annotation of existing materials.
we present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. <eos> the method consists of a core languageindependent algorithm, which can be optimized for specific languages. <eos> the method is demonstrated on egyptian arabic and german, two morphologically rich languages. <eos> our best method for egyptian arabic provides an error reduction of 55.6 % over a simple baseline ; our best method for german achieves a 66.7 % error reduction.
we present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. <eos> the weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. <eos> we tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. <eos> our joint model builds on a well known recurrent neural network language model ( mikolov, 2012 ) augmented by a layer of additional inputs from the source language. <eos> we show competitive accuracy compared to the traditional channel model features. <eos> our best results improve the output of a system trained on wmt 2012 french-english data by up to 1.5 bleu, and by 1.1 bleu on average across several test sets.
domain adaptation for smt usually adapts models to an individual specific domain. <eos> however, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. <eos> in this paper, we propose a novel multi-domain adaptation approach for smt using multi-task learning ( mtl ), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. <eos> the parameters of these models are tuned jointly via mtl so that they can learn general knowledge more accurately and exploit domain knowledge better. <eos> our experiments on a largescale english-to-chinese translation task validate that the mtl-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. <eos> furthermore, it also outperforms the individual adaptation of each specific domain.
we present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. <eos> we take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. <eos> our rules hold the property of long distance reorderings and the compatibility with phrases. <eos> large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string ( +2.45 bleu on average ) and dependencyto-string ( +0.91 bleu on average ) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model ( +1.12 bleu on average ), on three chinese-english nist test sets.
when using a machine translation ( mt ) model trained on old-domain parallel data to translate new-domain text, one major challenge is the large number of out-of-vocabulary ( oov ) and new-translation-sense words. <eos> we present a method to identify new translations of both known and unknown source language words that uses new-domain comparable document pairs. <eos> starting with a joint distribution of source-target word pairs derived from the old-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the new-domain comparable document pairs, while minimizing the divergence from the old-domain distribution. <eos> adding learned translations to our french-english mt model results in gains of about 2 bleu points over strong baselines.
left-to-right ( lr ) decoding ( watanabe et al, 2006b ) is a promising decoding algorithm for hierarchical phrase-based translation ( hiero ). <eos> it generates the target sentence by extending the hypotheses only on the right edge. <eos> lr decoding has complexity o ( n2b ) for input of n words and beam size b, compared too ( n3 ) for the cky algorithm. <eos> it requires a single language model ( lm ) history for each target hypothesis rather than two lm histories per hypothesis as in cky. <eos> in this paper we present an augmented lr decoding algorithm that builds on the original algorithm in ( watanabe et al, 2006b ). <eos> unlike that algorithm, using experiments over multiple language pairs we show two new results : our lr decoding algorithm provides demonstrably more efficient decoding than cky hiero, four times faster ; and by introducing new distortion and reordering features for lr decoding, it maintains the same translation quality ( as in bleu scores ) obtained phrase-based and cky hiero with the same translation model.
this paper addresses the problem of producing a diverse set of plausible translations. <eos> we present a simple procedure that can be used with any statistical machine translation ( mt ) system. <eos> we explore three ways of using diverse translations : ( 1 ) system combination, ( 2 ) discriminative reranking with rich features, and ( 3 ) a novel post-editing scenario in which multiple translations are presented to users. <eos> we find that diversity can improve performance on these tasks, especially for sentences that are difficult for mt.
while large-scale discriminative training has triumphed in many nlp problems, its definite success on machine translation has been largely elusive. <eos> most recent efforts along this line are not scalable ( training on the small dev set with features from top ? 100 most frequent words ) and overly complicated. <eos> we instead present a very simple yet theoretically motivated approach by extending the recent framework of ? violation-fixing perceptron ?, using forced decoding to compute the target derivations. <eos> extensive phrase-based translation experiments on both chinese-to-english and spanish-to-english tasks show substantial gains in bleu by up to +2.3/+2.0 on dev/test over mert, thanks to 20m+ sparse features. <eos> this is the first successful effort of large-scale online discriminative training for mt.
this paper studies the problem of identifying users who use multiple userids to post in social media. <eos> since multiple userids may belong to the same author, it is hard to directly apply supervised learning to solve the problem. <eos> this paper proposes a new method, which still uses supervised learning but does not require training documents from the involved userids. <eos> instead, it uses documents from other userids for classifier building. <eos> the classifier can be applied to documents of the involved userids. <eos> this is possible because we transform the document space to a similarity space and learning is performed in this new space. <eos> our evaluation is done in the online review domain. <eos> the experimental results using a large number of userids and their reviews show that the proposed method is highly effective.
while much work has considered the problem of latent attribute inference for users of social media such as twitter, little has been done on non-english-based content and users. <eos> here, we conduct the first assessment of latent attribute inference in languages beyond english, focusing on gender inference. <eos> we find that the gender inference problem in quite diverse languages can be addressed using existing machinery. <eos> further, accuracy gains can be made by taking language-specific features into account. <eos> we identify languages with complex orthography, such as japanese, as difficult for existing methods, suggesting a valuable direction for future research.
recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. <eos> in this work, we improve a two-dimensional multimodal version of latent dirichlet allocation ( andrews et al, 2009 ) in various ways. <eos> ( 1 ) we outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. <eos> ( 2 ) we present a novel way to integrate visual features into the lda model using unsupervised clusters of images. <eos> the clusters are directly interpretable and improve on our evaluation tasks. <eos> ( 3 ) we provide two novel ways to extend the bimodal models to support three or more modalities. <eos> we find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that can not be forced into a shared, latent structure.
it has recently been shown that different nlp models can be effectively combined using dual decomposition. <eos> in this paper we demonstrate that pcfg-la parsing models are suitable for combination in this way. <eos> we experiment with the different models which result from alternative methods of extracting a grammar from a treebank ( retaining or discarding function labels, left binarization versus right binarization ) and achieve a labeled parseval f-score of 92.4 on wall street journal section 23 ? <eos> this represents an absolute improvement of 0.7 and an error reduction rate of 7 % over a strong pcfg-la product-model baseline. <eos> although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies.
nlp models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting. <eos> a recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data. <eos> we reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. <eos> we show how to apply this method to structured prediction using multinomial logistic regression and linear-chain crfs. <eos> we tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently. <eos> the regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. <eos> applied to text classification and ner, our method provides a > 1 % absolute performance gain over use of standard l2 regularization.
one of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. <eos> we advance the previous study of the bayesian topic language model by wallach ( 2006 ) in two directions : one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel gibbs sampler that enables moving multiple n-grams across different documents to another topic. <eos> our blocked sampler can efficiently search for higher probability space even with higher order n-grams. <eos> in terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.
in this paper we report an empirical study on semi-supervised chinese word segmentation using co-training. <eos> we utilize two segmenters : 1 ) a word-based segmenter leveraging a word-level language model, and 2 ) a character-based segmenter using characterlevel features within a crf-based sequence labeler. <eos> these two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data. <eos> our experimental results show that co-training captures 20 % and 31 % of the performance improvement achieved by supervised training with an order of magnitude more data for the sighan bakeoff 2005 pku and cu corpora respectively.
a precise syntacto-semantic analysis of english requires a large detailed lexicon with the possibility of treating multiple tokens as a single meaning-bearing unit, a word-with-spaces. <eos> however parsing with such a lexicon, as included in the english resource grammar, can be very slow. <eos> we show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging. <eos> our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy.
we present a method for automatically acquiring knowledge for case alternation between the passive and active voices in japanese. <eos> by leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. <eos> we then apply the acquired knowledge to a case alternation task and prove its usefulness.
hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other. <eos> most previous methods are based on lexical patterns but perform badly on opendomain data. <eos> other work extracts hypernym relations from encyclopedias but has limited coverage. <eos> this paper proposes a simple yet effective distant supervision framework for chinese open-domain hypernym discovery. <eos> given an entity name, we try to discover its hypernyms by leveraging knowledge from multiple sources, i.e., search engine results, encyclopedias, and morphology of the entity name. <eos> first, we extract candidate hypernyms from the above sources. <eos> then, we apply a statistical ranking model to select correct hypernyms. <eos> a set of novel features is proposed for the ranking model. <eos> we also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation. <eos> experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset.
this paper presents a novel approach to determine textual similarity. <eos> a layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. <eos> experimental results show that incorporating the semantic structure of sentences is beneficial. <eos> when training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.
why do certain combinations of words such as ? disadvantageous peace ? <eos> or ? metal to the petal ? <eos> appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as ? quiet teenager ?, or ? geometrical base ? <eos> not as much ? <eos> we present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. <eos> we first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.
assigning a positive or negative score to a word out of context ( i.e. <eos> a word ? s prior polarity ) is a challenging task for sentiment analysis. <eos> in the literature, various approaches based on sentiwordnet have been proposed. <eos> in this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. <eos> using two different versions of sentiwordnet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words ? <eos> prior polarity for sentiment analysis. <eos> we conclude our investigation showing interesting biases in calculated prior polarity scores when word part of speech and annotator gender are considered.
building search engines that can respond to spoken queries with spoken content requires that the system not just be able to find useful responses, but also that it know when it has heard enough about what the user wants to be able to do so. <eos> this paper describes a simulation study with queries spoken by non-native speakers that suggests that indicates that finding relevant content is often possible within a half minute, and that combining features based on automatically recognized words with features designed for automated prediction of query difficulty can serve as a useful basis for predicting when that useful content has been found.
the rapid development of web2.0 leads to significant information redundancy. <eos> especially for a complex news event, it is difficult to understand its general idea within a single coherent picture. <eos> a complex event often contains branches, intertwining narratives and side news which are all called storylines. <eos> in this paper, we propose a novel solution to tackle the challenging problem of storylines extraction and reconstruction. <eos> specifically, we first investigate two requisite properties of an ideal storyline. <eos> then a unified algorithm is devised to extract all effective storylines by optimizing these properties at the same time. <eos> finally, we reconstruct all extracted lines and generate the high-quality story map. <eos> experiments on real-world datasets show that our method is quite efficient and highly competitive, which can bring about quicker, clearer and deeper comprehension to readers.
describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. <eos> previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. <eos> in this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. <eos> we test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. <eos> we describe two template-based description generation models that operate over visual dependency representations. <eos> in an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.
in current dependency parsing models, conventional features ( i.e. <eos> base features ) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data. <eos> in this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features ( i.e. <eos> meta features ) with the help of a large amount of automatically parsed data. <eos> the meta features are used together with base features in our final parser. <eos> our studies indicate that our proposed approach is very effective in processing unseen data and features. <eos> experiments on chinese and english data sets show that the final parser achieves the best-reported accuracy on the chinese data and comparable accuracy with the best known parsers on the english data.
topic segmentation classically relies on one of two criteria, either finding areas with coherent vocabulary use or detecting discontinuities. <eos> in this paper, we propose a segmentation criterion combining both lexical cohesion and disruption, enabling a trade-off between the two. <eos> we provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation. <eos> experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination. <eos> gains were observed in all conditions, with segments of either regular or varying length and abrupt or smooth topic shifts. <eos> long segments benefit more than short segments. <eos> however the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences.
we propose a laplacian structured sparsity model to study computational branding analytics. <eos> to do this, we collected customer reviews from starbucks, dunkin ? <eos> donuts, and other coffee shops across 38 major cities in the midwest and northeastern regions of usa. <eos> we study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. <eos> in particular, we perform three tasks : automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. <eos> this work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. <eos> our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. <eos> in addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.
hot trends are likely to bring new business opportunities. <eos> for example, ? air pollution ? <eos> might lead to a significant increase of the sales of related products, e.g., mouth mask. <eos> for ecommerce companies, it is very important to make rapid and correct response to these hot trends in order to improve product sales. <eos> in this paper, we take the initiative to study the task of how to identify trend related products. <eos> the major novelty of our work is that we automatically learn commercial intents revealed from microblogs. <eos> we carefully construct a data collection for this task and present quite a few insightful findings. <eos> in order to solve this problem, we further propose a graph based method, which jointly models relevance and associativity. <eos> we perform extensive experiments and the results showed that our methods are very effective.
we investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. <eos> using pennebaker ? s linguistic inquiry and word count ( liwc ) lexicon to provide baseline features, we show that straightforward topic modeling using latent dirichlet allocation ( lda ) yields interpretable, psychologically relevant ? themes ? <eos> that add value in prediction of clinical assessments.
we present a statistical model for predicting how the user of an interactive, situated nlp system resolved a referring expression. <eos> the model makes an initial prediction based on the meaning of the utterance, and revises it continuously based on the user ? s behavior. <eos> the combined model outperforms its components in predicting reference resolution and when to give feedback.
we extend zhao and ng 's ( 2007 ) chinese anaphoric zero pronoun resolver by ( 1 ) using a richer set of features and ( 2 ) exploiting the coreference links between zero pronouns during resolution. <eos> results on ontonotes show that our approach significantly outperforms two state-of-the-art anaphoric zero pronoun resolvers. <eos> to our knowledge, this is the first work to report results obtained by an end-toend chinese zero pronoun resolver.
this paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. <eos> our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. <eos> we empirically show on new york times articles aligned with freebase relations that our approach is able to efficiently use the extra information provided by a large subset of freebase data ( 4m entities, 23k relationships ) to improve over methods that rely on text features alone.
in this paper, we present a recursive neural network ( rnn ) model that works on a syntactic tree. <eos> our model differs from previous rnn models in that the model allows for an explicit weighting of important phrases for the target task. <eos> we also propose to average parameters in training. <eos> our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. <eos> we also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. <eos> the proposed model marks scores competitive with state-of-the-art rnn-based models.
automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. <eos> we present a very simple and easy to implement method for using these word classes to improve translation quality. <eos> it can be applied across different machine translation paradigms and with arbitrary types of models. <eos> we show its efficacy on a small german ? english and a larger french ? german translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. <eos> our results show that with word class models, the baseline can be improved by up to 1.4 % bleu and 1.0 % ter on the french ? german task and 0.3 % bleu and 1.1 % ter on the german ? english task.
this paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. <eos> our model uses rich syntax parsing features for word reordering and runs in linear time. <eos> we apply it to postordering of phrase-based machine translation ( pbmt ) for japanese-to-english patent tasks. <eos> our experimental results show that our method achieves a significant improvement of +3.1 bleu scores against 30.15 bleu scores of the baseline pbmt system.
we explore the application of neural language models to machine translation. <eos> we develop a new model that combines the neural probabilistic language model of bengio et al, rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. <eos> our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 bleu.
we introduce bilingual word embeddings : semantic embeddings associated across two languages in the context of neural language models. <eos> we propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing mt word alignments to constrain translational equivalence. <eos> the new embeddings significantly out-perform baselines in word semantic similarity. <eos> a single semantic similarity feature induced with bilingual embeddings adds near half a bleu point to the results of nist08 chinese-english machine translation task.
in this paper we present a novel approach to automatic creation of anchor texts for hyperlinks in a document pointing to similar documents. <eos> methods used in this approach rank parts of a document based on the similarity to a presumably related document. <eos> ranks are then used to automatically construct the best anchor text for a link inside original document to the compared document. <eos> a number of different methods from information retrieval and natural language processing are adapted for this task. <eos> automatically constructed anchor texts are manually evaluated in terms of relatedness to linked documents and compared to baseline consisting of originally inserted anchor texts. <eos> additionally we use crowdsourcing for evaluation of original anchors and automatically constructed anchors. <eos> results show that our best adapted methods rival the precision of the baseline method.
sentence completion is a challenging semantic modeling task in which models must choose the most appropriate word from a given set to complete a sentence. <eos> although a variety of language models have been applied to this task in previous work, none of the existing approaches incorporate syntactic information. <eos> in this paper we propose to tackle this task using a pair of simple language models in which the probability of a sentence is estimated as the probability of the lexicalisation of a given syntactic dependency tree. <eos> we apply our approach to the microsoft research sentence completion challenge and show that it improves on n-gram language models by 8.7 percentage points, achieving the highest accuracy reported to date apart from neural language models that are more complex and expensive to train.
in this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. <eos> our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. <eos> using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them ( encoded as the structure of the parse tree ). <eos> we show an efficient formulation to compute this kernel using simple matrix operations. <eos> we present our results on three diverse nlp tasks, showing state-of-the-art results.
online resources, such as wiktionary, provide an accurate but incomplete source of idiomatic phrases. <eos> in this paper, we study the problem of automatically identifying idiomatic dictionary entries with such resources. <eos> we train an idiom classifier on a newly gathered corpus of over 60,000 wiktionary multi-word definitions, incorporating features that model whether phrase meanings are constructed compositionally. <eos> experiments demonstrate that the learned classifier can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65 %. <eos> these gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to match phrases to their definitions. <eos> in a set of wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points.
we present a novel unsupervised approach to detecting the compositionality of multi-word expressions. <eos> we compute the compositionality of a phrase through substituting the constituent words with their ? neighbours ? <eos> in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. <eos> several methods of obtaining neighbours are presented. <eos> the results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.
we introduce an extended naive bayes model for word sense induction ( wsi ) and apply it to a wsi task. <eos> the extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. <eos> the proposed model is very simple yet effective when evaluated on semeval-2010 wsi data.
this research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in verbnet, a lexicon of about 6300 english verbs. <eos> the current semantic predicates can be thought of semantic primitives, into which the concepts denoted by a verb can be decomposed. <eos> for example, the verb spray ( of the spray class ), involves the predicates motion, not, and location, where the event can be decomposed into an agent causing a theme that was originally not in a particular location to now be in that location. <eos> although verbnet ? s predicates are theoretically well-motivated, systematic empirical data is scarce. <eos> this paper describes a recently-launched attempt to address this issue with a series of human judgment tasks, posed to subjects in the form of games.
this paper offers an approach for governments to harness the information contained in social media in order to make public inspections and disclosure more efficient. <eos> as a case study, we turn to restaurant hygiene inspections ? <eos> which are done for restaurants throughout the united states and in most of the world and are a frequently cited example of public inspections and disclosure. <eos> we present the first empirical study that shows the viability of statistical models that learn the mapping between textual signals in restaurant reviews and the hygiene inspection records from the department of public health. <eos> the learned model achieves over 82 % accuracy in discriminating severe offenders from places with no violation, and provides insights into salient cues in reviews that are indicative of the restaurant ? s sanitary conditions. <eos> our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers.
the identification of pseudepigraphic texts ? <eos> texts not written by the authors to which they are attributed ? <eos> has important historical, forensic and commercial applications. <eos> we introduce an unsupervised technique for identifying pseudepigrapha. <eos> the idea is to identify textual outliers in a corpus based on the pairwise similarities of all documents in the corpus. <eos> the crucial point is that document similarity not be measured in any of the standard ways but rather be based on the output of a recently introduced algorithm for authorship verification. <eos> the proposed method strongly outperforms existing techniques in systematic experiments on a blog corpus.
feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. <eos> we propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. <eos> we model this as a sequential decision-making problem and solve it by imitation learning techniques. <eos> we test our method on 7 languages. <eos> our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30 % of the feature templates.
cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. <eos> an effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. <eos> in this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. <eos> specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. <eos> we conduct extensive experiments on cross-lingual sentiment classification tasks of amazon product reviews. <eos> our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach.
often the bottleneck in document classification is finding good representations that zoom in on the most important aspects of the documents. <eos> most research uses n-gram representations, but relevant features often occur discontinuously, e.g., not. <eos> . <eos> . <eos> good in sentiment analysis. <eos> in this paper we present experiments getting experts to provide regular expressions, as well as crowdsourced annotation tasks from which regular expressions can be derived. <eos> somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41 % over n-gram representations.
a major challenge in supervised sentence compression is making use of rich feature representations because of very scarce parallel data. <eos> we address this problem and present a method to automatically build a compression corpus with hundreds of thousands of instances on which deletion-based algorithms can be trained. <eos> in our corpus, the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence supervised systems which require a structural alignment between the input and output can be successfully trained. <eos> we also extend an existing unsupervised compression method with a learning module. <eos> the new system uses structured prediction to learn from lexical, syntactic and other features. <eos> an evaluation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality. <eos> also, the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting, significantly outperforming a strong baseline.
extractive summarization typically uses sentences as summarization units. <eos> in contrast, joint compression and summarization can use smaller units such as words and phrases, resulting in summaries containing more information. <eos> the goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint. <eos> we propose an efficient decoding algorithm for fast compressive summarization using graph cuts. <eos> our approach first relaxes the length constraint using lagrangian relaxation. <eos> then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem, which can be solved efficiently using graph max-flow/min-cut. <eos> since finding the tightest lower bound suffers from local optimality, we use convex relaxation for initialization. <eos> experimental results on tac2008 dataset demonstrate our method achieves competitive rouge score and has good readability, while is much faster than the integer linear programming ( ilp ) method.
in a language generation system, a content planner selects which elements must be included in the output text and the ordering between them. <eos> recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent. <eos> in this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering. <eos> content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data. <eos> we develop two approaches : the first one is inspired from rhetorical structure theory and represents the document as a tree of discourse relations between database records ; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. <eos> experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches.
recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a knapsack problem, a maximum coverage problem or a budgeted median problem. <eos> these methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. <eos> thus, summaries generated by these methods may lack logical coherence. <eos> this paper proposes a single document summarization method based on the trimming of a discourse tree. <eos> this is a two-fold process. <eos> first, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. <eos> second, we formulate the problem of trimming a dependency-based discourse tree as a tree knapsack problem, then solve it with integer linear programming ( ilp ). <eos> evaluation results showed that our method improved rouge scores.
social media like forums and microblogs have accumulated a huge amount of user generated content ( ugc ) containing human knowledge. <eos> currently, most of ugc is listed as a whole or in pre-defined categories. <eos> this ? list-based ? <eos> approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. <eos> to address this problem, we propose a hierarchical entity-based approach for structuralizing ugc in social media. <eos> by using a large-scale entity repository, we design a three-step framework to organize ugc in a novel hierarchical structure called ? cluster entity tree ( cet ) ?. <eos> with yahoo ! <eos> answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing cet. <eos> we further evaluate the performance of cet on ugc organization in both user and system aspects. <eos> from a user aspect, our user study demonstrates that, with cet-based structure, users perform significantly better in knowledge learning than using traditional list-based approach. <eos> from a system aspect, cet substantially boosts the performance of two information retrieval models ( i.e., vector space model and query likelihood language model ).
in this paper, we train a semantic parser that scales up to freebase. <eos> instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. <eos> the main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. <eos> we tackle this problem in two ways : first, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. <eos> second, we use a bridging operation to generate additional predicates based on neighboring predicates. <eos> on the dataset of cai and yates ( 2013 ), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. <eos> additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.
we consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with freebase. <eos> in such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. <eos> for example, even simple phrases such as ? daughter ? <eos> and ? number of people living in ? <eos> can not be directly represented in freebase, whose ontology instead encodes facts about gender, parenthood, and population. <eos> in this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. <eos> the parser is learned from question-answer pairs, uses a probabilistic ccg to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. <eos> experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent freebase qa corpus.
the goal of our research is to distinguish veterinary message board posts that describe a case involving a specific patient from posts that ask a general question. <eos> we create a text classifier that incorporates automatically generated attribute lists for veterinary patients to tackle this problem. <eos> using a small amount of annotated data, we train an information extraction ( ie ) system to identify veterinary patient attributes. <eos> we then apply the ie system to a large collection of unannotated texts to produce a lexicon of veterinary patient attribute terms. <eos> our experimental results show that using the learned attribute lists to encode patient information in the text classifier yields improved performance on this task.
lexical chains provide a representation of the lexical cohesion structure of a text. <eos> in this paper, we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation : 1 ) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis, 2 ) and a probability cohesion model that further takes chain word translation probabilities into account. <eos> we compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers. <eos> we then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models. <eos> we verify the effectiveness of the two models using a hierarchical phrase-based translation system. <eos> experiments on large-scale training data show that they can substantially improve translation quality in terms of bleu and that the probability cohesion model outperforms previous models based on lexical cohesion devices.
the ibm translation models have been hugely influential in statistical machine translation ; they are the basis of the alignment models used in modern translation systems. <eos> excluding ibm model 1, the ibm translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are non-convex, and hence have multiple local optima. <eos> in this paper we introduce a convex relaxation of ibm model 2, and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates. <eos> our approach gives the same level of alignment accuracy as ibm model 2.
pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. <eos> translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. <eos> we examine finitestate and smt-based methods for these related tasks, and demonstrate that the tasks have different characteristics ? <eos> finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not. <eos> we also show that we can increase accuracy by modeling syllable stress.
recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step. <eos> the purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps ( higher order tensors ) acting on a number of arguments ( vectors ). <eos> we propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. <eos> the results show that disambiguation can provide better compositional representation even for the case of tensor-based models. <eos> furthermore, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.
we present multi-relational latent semantic analysis ( mrlsa ) which generalizes latent semantic analysis ( lsa ). <eos> mrlsa provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. <eos> similar to lsa, a lowrank approximation of the tensor is derived using a tensor decomposition. <eos> each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. <eos> the degree of two words having a specific relation can then be measured through simple linear algebraic operations. <eos> we demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, mrlsa achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a.
we present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. <eos> the paper systematically introduces and describes all key elements of the bootstrapping procedure : ( 1 ) starting point or seed lexicon, ( 2 ) the confidence estimation and selection of new dimensions of the space, and ( 3 ) convergence. <eos> we test the quality of the induced bilingual vector spaces, and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction ( ble ) for two language pairs. <eos> results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. <eos> we also show that our approach outperforms the best performing fully corpus-based ble methods on these test sets.
continuous space word representations extracted from neural network language models have been used effectively for natural language processing, but until recently it was not clear whether the spatial relationships of such representations were interpretable. <eos> mikolov et al ( 2013 ) show that these representations do capture syntactic and semantic regularities. <eos> here, we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales ( e.g., okay < good < excellent ). <eos> we evaluate the scales on the indirect answers to yes/no questions corpus ( de marneffe et al, 2010 ). <eos> we obtain 72.8 % accuracy, which outperforms previous results ( ? 60 % ) on this corpus and highlights the quality of the scales extracted, providing further support that the continuous space word representations are meaningful.
semantic word spaces have been very useful but can not express the meaning of longer phrases in a principled way. <eos> further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. <eos> to remedy this, we introduce a sentiment treebank. <eos> it includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. <eos> to address them, we introduce the recursive neural tensor network. <eos> when trained on the new treebank, this model outperforms all previous methods on several metrics. <eos> it pushes the state of the art in single sentence positive/negative classification from 80 % up to 85.4 %. <eos> the accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7 %, an improvement of 9.7 % over bag of features baselines. <eos> lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.
we propose a novel approach to sentiment analysis for a low resource setting. <eos> the intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. <eos> this representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. <eos> we compare performance in both spanish and english on microblog data, using only a sentiment lexicon as an external resource. <eos> by leveraging linguisticallyinformed features within conditional random fields ( crfs ) trained to minimize empirical risk, our best models in spanish significantly outperform a strong baseline, and reach around 90 % accuracy on the combined task of named entity recognition and sentiment prediction. <eos> our models in english, trained on a much smaller dataset, are not yet statistically significant against their baselines.
aspect extraction is one of the key tasks in sentiment analysis. <eos> in recent years, statistical models have been used for the task. <eos> however, such models without any domain knowledge often produce aspects that are not interpretable in applications. <eos> to tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. <eos> however, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the can not -link type of knowledge or to automatically adjust the number of topics based on domain knowledge. <eos> this paper proposes a more advanced topic model, called mc-lda ( lda with m-set and c-set ), to address these problems, which is based on an extended generalized p ? lya urn ( e-gpu ) model ( which is also proposed in this paper ). <eos> experiments on real-life product reviews from a variety of domains show that mclda outperforms the existing state-of-the-art models markedly.
we introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500 %. <eos> we learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. <eos> in experiments, we observe bleu gains of 1.2 to 1.8 across three different test sets.
translation into morphologically rich languages is an important but recalcitrant problem in mt. <eos> we present a simple and effective approach that deals with the problem in two phases. <eos> first, a discriminative model is learned to predict inflections of target words from rich source-side annotations. <eos> then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as ? synthetic ? <eos> phrases. <eos> our approach relies on morphological analysis of the target language, but we show that an unsupervised bayesian model of morphology can successfully be used in place of a supervised analyzer. <eos> we report significant improvements in translation quality when translating from english to russian, hebrew and swahili.
we present an approach to learning bilingual n-gram correspondences from relevance rankings of english documents for japanese queries. <eos> we show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval ( clir ). <eos> we propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences. <eos> we show in an experimental evaluation on patent prior art search that our approach, and in particular a consensus-based combination of boosting and translation-based approaches, yields substantial improvements in clir performance. <eos> our training and test data are made publicly available.
we introduce a class of probabilistic continuous translation models called recurrent continuous translation models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. <eos> the models have a generation and a conditioning aspect. <eos> the generation of the translation is modelled with a target recurrent language model, whereas the conditioning on the source sentence is modelled with a convolutional sentence model. <eos> through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43 % lower than that of stateof-the-art alignment-based translation models. <eos> secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. <eos> finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.
biological processes are complex phenomena involving a series of events that are related to one another through various relationships. <eos> systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering ( qa ) ? <eos> specifically ? how ? ? <eos> and ? why ? ? <eos> questions. <eos> in this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. <eos> we represent processes by graphs whose edges describe a set of temporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs ( e.g., the graphs are connected ). <eos> then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint inference over the set of extracted relations. <eos> on a novel dataset containing 148 descriptions of biological processes ( released with this paper ), we show significant improvement comparing to baselines that disregard process structure.
chambers and jurafsky ( 2009 ) demonstrated that event schemas can be automatically induced from text corpora. <eos> however, our analysis of their schemas identifies several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. <eos> it is due in part to their pair-wise representation that treats subjectverb independently from verb-object. <eos> this often leads to subject-verb-object triples that are not meaningful in the real-world. <eos> we present a novel approach to inducing open-domain event schemas that overcomes these limitations. <eos> our approach uses cooccurrence statistics of semantically typed relational triples, which we call rel-grams ( relational n-grams ). <eos> in a human evaluation, our schemas outperform chambers ? s schemas by wide margins on several evaluation criteria. <eos> both rel-grams and event schemas are freely available to the research community.
cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. <eos> multilingual extensions of approaches based on latent ( lsi ), generative ( lda, plsi ) as well as explicit ( esa ) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. <eos> in this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. <eos> thus, the method combines the benefits of both explicit and latent topic modelling approaches. <eos> we show that on a crosslingual mate retrieval task, our model significantly outperforms lda, lsi, and esa, as well as a baseline that translates every word in a document into the target language.
previous approaches for automated essay scoring ( aes ) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used. <eos> in this paper, we argue that the current aes systems can be further improved by taking into account the agreement between human and machine raters. <eos> to this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. <eos> various linguistic and statistical features are utilized to facilitate the learning algorithms. <eos> experiments on the publicly available english essay dataset, automated student assessment prize ( asap ), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.
predicting the success of literary works is a curious question among publishers and aspiring writers alike. <eos> we examine the quantitative connection, if any, between writing style and successful literature. <eos> based on novels over several different genres, we probe the predictive power of statistical stylometry in discriminating successful literary works, and identify characteristic stylistic elements that are more prominent in successful writings. <eos> our study reports for the first time that statistical stylometry can be surprisingly effective in discriminating highly successful literature from less successful counterpart, achieving accuracy up to 84 %. <eos> closer analyses lead to several new insights into characteristics of the writing style in successful literature, including findings that are contrary to the conventional wisdom with respect to good writing style and readability.
we develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. <eos> our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts ( and their interactions ). <eos> we apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.
the distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings, has inspired several web mining algorithms for paraphrasing semantically equivalent phrases. <eos> unfortunately, these methods have several drawbacks, such as confusing synonyms with antonyms and causes with effects. <eos> this paper introduces three temporal correspondence heuristics, that characterize regularities in parallel news streams, and shows how they may be used to generate high precision paraphrases for event relations. <eos> we encode the heuristics in a probabilistic graphical model to create the newsspike algorithm for mining news streams. <eos> we present experiments demonstrating that newsspike significantly outperforms several competitive baselines. <eos> in order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by newsspike.
wikification, commonly referred to as disambiguation to wikipedia ( d2w ), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding wikipedia pages. <eos> previous approaches to d2w focused on the use of local and global statistics over the given text, wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. <eos> however, these methods fail ( often, embarrassingly ), when some level of text understanding is needed to support wikification. <eos> in this paper we introduce a novel approach to wikification by incorporating, along with statistical methods, richer relational analysis of the text. <eos> we provide an extensible, efficient and modular integer linear programming ( ilp ) formulation of wikification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candidate generation and ranking wikipedia titles considerably. <eos> our results show significant improvements in both wikification and the tac entity linking task.
event schema induction is the task of learning high-level representations of complex events ( e.g., a bombing ) and their entity roles ( e.g., perpetrator and victim ) from unlabeled text. <eos> event schemas have important connections to early nlp research on frames and scripts, as well as modern applications like template extraction. <eos> recent research suggests event schemas can be learned from raw text. <eos> inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. <eos> our generative model is conceptually simpler than the pipelined approach and requires far less training data. <eos> it also provides an interesting contrast with a recent hmm-based model. <eos> we evaluate on a common dataset for template schema extraction. <eos> our generative model matches the pipeline ? s performance, and outperforms the hmm by 7 f1 points ( 20 % ).
this paper introduces iqps ( integer quadratic programs ) as a way to model joint inference for the task of concept recognition in clinical domain. <eos> iqps make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. <eos> we show that soft constraints give statistically significant performance improvements when compared to hard constraints.
different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. <eos> in this paper we focus on learning gender differences in the use of subjective language in english, spanish, and russian twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. <eos> we show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for spanish and russian. <eos> our results show statistically significant relative f-measure improvement over the gender-independent baseline 1.5 % and 1 % for russian, 2 % and 0.5 % for spanish, and 2.5 % and 5 % for english for polarity and subjectivity classification.
a very valuable piece of information in newspaper articles is the tonality of extracted statements. <eos> for the analysis of tonality of newspaper articles either a big human effort is needed, when it is carried out by media analysts, or an automated approach which has to be as accurate as possible for a media response analysis ( mra ). <eos> to this end, we will compare several state-of-the-art approaches for opinion mining in newspaper articles in this paper. <eos> furthermore, we will introduce a new technique to extract entropy-based word connections which identifies the word combinations which create a tonality. <eos> in the evaluation, we use two different corpora consisting of news articles, by which we show that the new approach achieves better results than the four state-of-the-art methods.
microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. <eos> in this paper, we study the problem of extracting opinion targets of chinese microblog messages. <eos> such fine-grained word-level task has not been well investigated in microblogs yet. <eos> we propose an unsupervised label propagation algorithm to address the problem. <eos> the opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. <eos> topics in microblogs are identified by hashtags or using clustering algorithms. <eos> experimental results on chinese microblogs show the effectiveness of our framework and algorithms.
this paper presents an approach for detecting promotional content in wikipedia. <eos> by incorporating stylometric features, including features based on n-gram and pcfg language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures.
we explore debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. <eos> we introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. <eos> we evaluate the resulting representation ? s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.
with the rapid growth of social media, twitter has become one of the most widely adopted platforms for people to post short and instant message. <eos> on the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. <eos> moreover, people ? s posting behaviors on events are often closely tied to their personal interests. <eos> in this paper, we try to model topics, events and users on twitter in a unified way. <eos> we propose a model which combines an lda-like topic model and the recurrent chinese restaurant process to capture topics and events. <eos> we further propose a duration-based regularization component to find bursty events. <eos> we also propose to use event-topic affinity vectors to model the association between events and topics. <eos> our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.
work on authorship attribution has traditionally focused on long texts. <eos> in this work, we tackle the question of whether the author of a very short text can be successfully identified. <eos> we use twitter as an experimental testbed. <eos> we introduce the concept of an author ? s unique ? signature ?, and show that such signatures are typical of many authors when writing very short texts. <eos> we also present a new authorship attribution feature ( ? flexible patterns ? ) <eos> and demonstrate a significant improvement over our baselines. <eos> our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.
this short paper presents a pilot study investigating the training of a standard semantic role labeling ( srl ) system on product reviews for the new task of detecting comparisons. <eos> an ( opinionated ) comparison consists of a comparative ? predicate ? <eos> and up to three ? arguments ? <eos> : the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. <eos> in user-generated product reviews, the ? predicate ? <eos> and ? arguments ? <eos> are expressed in highly heterogeneous ways ; but since the elements are textually annotated in existing datasets, srl is technically applicable. <eos> we address the interesting question how well training an outof-the-box srl model works for english data. <eos> we observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps ( predicate identification, argument identification and argument classification ) and in three different datasets.
constituency parsing with rich grammars remains a computational challenge. <eos> graphics processing units ( gpus ) have previously been used to accelerate cky chart evaluation, but gains over cpu parsers were modest. <eos> in this paper, we describe a collection of new techniques that enable chart evaluation at close to the gpu ? s practical maximum speed ( a teraflop ), or around a half-trillion rule evaluations per second. <eos> net parser performance on a 4-gpu system is over 1 thousand length30 sentences/second ( 1 trillion rules/sec ), and 400 general sentences/second for the berkeley parser grammar. <eos> the techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.
in this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. <eos> we exploit this idea to choose the correct parsing of nps ( e.g., ( live fish ) transporter rather than live ( fish transporter ) ). <eos> we show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.
we present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. <eos> despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. <eos> this learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. <eos> we train and evaluate the approach on a new corpus of references to sets of visual objects. <eos> experiments show the approach is able to learn accurate models, which generate over 87 % of the expressions people used. <eos> additionally, on the previously studied special case of single object reference, we show a 35 % relative error reduction over previous state of the art.
the problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task. <eos> in this paper, we tackle this task as a supervised ranking problem. <eos> given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness. <eos> as a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches. <eos> on two datasets widely used for lexical substitution, our best models significantly advance the state-of-the-art.
recent work has developed supervised methods for detecting deceptive opinion spam ? <eos> fake reviews written to sound authentic and deliberately mislead readers. <eos> and whereas past work has focused on identifying individual fake reviews, this paper aims to identify offerings ( e.g., hotels ) that contain fake reviews. <eos> we introduce a semi-supervised manifold ranking algorithm for this task, which relies on a small set of labeled individual reviews for training. <eos> then, in the absence of gold standard labels ( at an offering level ), we introduce a novel evaluation procedure that ranks artificial instances of real offerings, where each artificial offering contains a known number of injected deceptive reviews. <eos> experiments on a novel dataset of hotel reviews show that the proposed method outperforms state-of-art learning baselines.
recommendation systems ( rs ) take advantage of products and users information in order to propose items to consumers. <eos> collaborative, content-based and a few hybrid rs have been developed in the past. <eos> in contrast, we propose a new domain-independent semantic rs. <eos> by providing textually well-argued recommendations, we aim to give more responsibility to the end user in his decision. <eos> the system includes a new similarity measure keeping up both the accuracy of rating predictions and coverage. <eos> we propose an innovative way to apply a fast adaptation scheme at a semantic level, providing recommendations and arguments in phase with the very recent past. <eos> we have performed several experiments on films data, providing textually well-argued recommendations.
minimum error rate training ( mert ) remains one of the preferred methods for tuning linear parameters in machine translation systems, yet it faces significant issues. <eos> first, mert is an unregularized learner and is therefore prone to overfitting. <eos> second, it is commonly used on a noisy, non-convex loss function that becomes more difficult to optimize as the number of parameters increases. <eos> to address these issues, we study the addition of a regularization term to the mert objective function. <eos> since standard regularizers such as `2 are inapplicable to mert due to the scale invariance of its objective function, we turn to two regularizers ? `0 and a modification of `2 ? <eos> and present methods for efficiently integrating them during search. <eos> to improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected bleu to orient mert ? s exact line searches. <eos> experiments with up to 3600 features show that these extensions of mert yield results comparable to pro, a learner often used with large feature sets.
traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. <eos> recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. <eos> these image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. <eos> in this study, we test whether image-based models capture the semantic patterns that emerge from fmri recordings of the neural signal. <eos> our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. <eos> despite some unsatisfactory, but explained outcomes ( in particular, failure to detect differential association of models with brain areas ), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.
classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. <eos> in contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. <eos> surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win ? easy victories ? <eos> without crafted heuristics. <eos> these features are successful on syntax and discourse ; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an ? uphill battle. ? <eos> nonetheless, our final system1 outperforms the stanford system ( lee et al ( 2011 ), the winner of the conll 2011 shared task ) by 3.5 % absolute on the conll metric and outperforms the ims system ( bjo ? rkelund and farkas ( 2012 ), the best publicly available english coreference system ) by 1.9 % absolute.
many statistical learning problems in nlp call for local model search methods. <eos> but accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly : hill-climbers can get stuck in local optima, whereas samplers may be inefficient. <eos> we propose to arrange individual local optimizers into organized networks. <eos> our building blocks are operators of two types : ( i ) transform, which suggests new places to search, via non-random restarts from already-found local optima ; and ( ii ) join, which merges candidate solutions to find better optima. <eos> experiments on grammar induction show that pursuing different transforms ( e.g., discarding parts of a learned model or ignoring portions of training data ) results in improvements. <eos> groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. <eos> using these tools, we designed several modular dependency grammar induction networks of increasing complexity. <eos> our complete system achieves 48.6 % accuracy ( directed dependency macro-average over all 19 languages in the 2006/7 conll data ) ? <eos> more than 5 % higher than the previous state-of-the-art.
we present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. <eos> to this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. <eos> our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. <eos> we show improvements over strong baselines for two tasks : part-of-speech tagging and namedentity segmentation.
we describe a novel method that extracts paraphrases from a bitext, for both the source and target languages. <eos> in order to reduce the search space, we decompose the phrase-table into sub-phrase-tables and construct separate clusters for source and target phrases. <eos> we convert the clusters into graphs, add smoothing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a random walk-based measure, the commute time. <eos> the resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial cooccurrence counts with a novel technique. <eos> the co-occurrence count distribution belongs to the power-law family.
we introduce two bayesian models for unsupervised semantic role labeling ( srl ) task. <eos> the models treat srl as clustering of syntactic signatures of arguments with clusters corresponding to semantic roles. <eos> the first model induces these clusterings independently for each predicate, exploiting the chinese restaurant process ( crp ) as a prior. <eos> in a more refined hierarchical model, we inject the intuition that the clusterings are similar across different predicates, even though they are not necessarily identical. <eos> this intuition is encoded as a distance-dependent crp with a distance between two syntactic signatures indicating how likely they are to correspond to a single semantic role. <eos> these distances are automatically induced within the model and shared across predicates. <eos> both models achieve state-of-the-art results when evaluated on propbank, with the coupled model consistently outperforming the factored counterpart in all experimental set-ups.
we introduce two ways to detect entailment using distributional semantic representations of phrases. <eos> our first experiment shows that the entailment relation between adjective-noun constructions and their head nouns ( big cat |= cat ), once represented as semantic vector pairs, generalizes to lexical entailment among nouns ( dog |= animal ). <eos> our second experiment shows that a classifier fed semantic vector pairs can similarly generalize the entailment relation among quantifier phrases ( many dogs|=some dogs ) to entailment involving unseen quantifiers ( all cats|=several cats ). <eos> moreover, nominal and quantifier phrase entailment appears to be cued by different distributional correlates, as predicted by the type-based view of entailment in formal semantics.
a major focus of current work in distributional models of semantics is to construct phrase representations compositionally from word representations. <eos> however, the syntactic contexts which are modelled are usually severely limited, a fact which is reflected in the lexical-level wsd-like evaluation methods used. <eos> in this paper, we broaden the scope of these models to build sentence-level representations, and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. <eos> we propose two evaluation methods in relation classification and qa which reflect these goals, and apply several recent compositional distributional models to the tasks. <eos> we find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference.
a serious bottleneck of comparative parser evaluation is the fact that different parsers subscribe to different formal frameworks and theoretical assumptions. <eos> converting outputs from one framework to another is less than optimal as it easily introduces noise into the process. <eos> here we present a principled protocol for evaluating parsing results across frameworks based on function trees, tree generalization and edit distance metrics. <eos> this extends a previously proposed framework for cross-theory evaluation and allows us to compare a wider class of parsers. <eos> we demonstrate the usefulness and language independence of our procedure by evaluating constituency and dependency parsers on english and swedish.
hungarian is a stereotype of morphologically rich and non-configurational languages. <eos> here, we introduce results on dependency parsing of hungarian that employ a 80k, multi-domain, fully manually annotated corpus, the szeged dependency treebank. <eos> we show that the results achieved by state-of-the-art data-driven parsers on hungarian and english ( which is at the other end of the configurational-nonconfigurational spectrum ) are quite similar to each other in terms of attachment scores. <eos> we reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages. <eos> this analysis highlights that addressing the language-specific phenomena is required for a further remarkable error reduction.
we introduce a new approach to transitionbased dependency parsing in which the parser does not directly construct a dependency structure, but rather an undirected graph, which is then converted into a directed dependency tree in a post-processing step. <eos> this alleviates error propagation, since undirected parsers do not need to observe the single-head constraint. <eos> undirected parsers can be obtained by simplifying existing transition-based parsers satisfying certain conditions. <eos> we apply this approach to obtain undirected variants of the planar and 2-planar parsers and of covington ? s non-projective parser. <eos> we perform experiments on several datasets from the conll-x shared task, showing that these variants outperform the original directed algorithms in most of the cases.
transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available. <eos> in this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining the advantages of transition-based and graph-based approaches. <eos> we also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. <eos> we apply the new transition-based parser on typologically different languages such as english, chinese, czech, and german and report competitive labeled and unlabeled attachment scores.
in information retrieval ( ir ) in general and question answering ( qa ) in particular, queries and relevant textual content often significantly differ in their properties and are therefore difficult to relate with traditional ir methods, e.g. <eos> key-word matching. <eos> in this paper we describe an algorithm that addresses this problem, but rather than looking at it on a term matching/term reformulation level, we focus on the syntactic differences between questions and relevant text passages. <eos> to this end we propose a novel algorithm that analyzes dependency structures of queries and known relevant text passages and acquires transformational patterns that can be used to retrieve relevant textual content. <eos> we evaluate our algorithm in a qa setting, and show that it outperforms a baseline that uses only dependency information contained in the questions by 300 % and that it also improves performance of a state of the art qa system significantly.
in this paper, we examined click patterns produced by users of yahoo ! <eos> search engine when prompting definition questions. <eos> regularities across these click patterns are then utilized for constructing a large and heterogeneous training corpus for answer ranking. <eos> in a nutshell, answers are extracted from clicked web-snippets originating from any class of web-site, including knowledge bases ( kbs ). <eos> on the other hand, nonanswers are acquired from redundant pieces of text across web-snippets. <eos> the effectiveness of this corpus was assessed via training two state-of-the-art models, wherewith answers to unseen queries were distinguished. <eos> these testing queries were also submitted by search engine users, and their answer candidates were taken from their respective returned web-snippets. <eos> this corpus helped both techniques to finish with an accuracy higher than 70 %, and to predict over 85 % of the answers clicked by users. <eos> in particular, our results underline the importance of non-kb training data.
this work proposes to adapt an existing general smt model for the task of translating queries that are subsequently going to be used to retrieve information from a target language collection. <eos> in the scenario that we focus on access to the document collection itself is not available and changes to the ir model are not possible. <eos> we propose two ways to achieve the adaptation effect and both of them are aimed at tuning parameter weights on a set of parallel queries. <eos> the first approach is via a standard tuning procedure optimizing for bleu score and the second one is via a reranking approach optimizing for map score. <eos> we also extend the second approach by using syntax-based features. <eos> our experiments show improvements of 1-2.5 in terms of map score over the retrieval with the non-adapted translation. <eos> we show that these improvements are due both to the integration of the adaptation and syntax-features for the query translation task.
the search space of phrase-based statistical machine translation ( pbsmt ) systems can be represented under the form of a directed acyclic graph ( lattice ). <eos> the quality of this search space can thus be evaluated by computing the best achievable hypothesis in the lattice, the so-called oracle hypothesis. <eos> for common smt metrics, this problem is however np-hard and can only be solved using heuristics. <eos> in this work, we present two new methods for efficiently computing bleu oracles on lattices : the first one is based on a linear approximation of the corpus bleu score and is solved using the fst formalism ; the second one relies on integer linear programming formulation and is solved directly and using the lagrangian relaxation framework. <eos> these new decoders are positively evaluated and compared with several alternatives from the literature for three language pairs, using lattices produced by two pbsmt systems.
we estimate the parameters of a phrasebased statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus. <eos> we extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for mt-scale phrasetables. <eos> we propose a novel algorithm to estimate reordering probabilities from monolingual data. <eos> we report translation results for an end-to-end translation system using these monolingual features alone. <eos> our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. <eos> in this paper, we examine an idealization where a phrase-table is given. <eos> we examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80 % + of the loss can be recovered with monolingually estimated features alone. <eos> we further show that our monolingual features add 1.5 bleu points when combined with standard bilingually estimated phrase table features.
in this paper we investigate the use of character-level translation models to support the translation from and to underresourced languages and textual domains via closely related pivot languages. <eos> our experiments show that these low-level models can be successful even with tiny amounts of training data. <eos> we test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task. <eos> our pivot translations outperform the baselines by a large margin.
nowadays, there are large amounts of data available to train statistical machine translation systems. <eos> however, it is not clear whether all the training data actually help or not. <eos> a system trained on a subset of such huge bilingual corpora might outperform the use of all the bilingual data. <eos> this paper studies such issues by analysing two training data selection techniques : one based on approximating the probability of an indomain corpus ; and another based on infrequent n-gram occurrence. <eos> experimental results not only report significant improvements over random sentence selection but also an improvement over a system trained with the whole available data. <eos> surprisingly, the improvements are obtained with just a small fraction of the data that accounts for less than 0.5 % of the sentences. <eos> afterwards, we show that a much larger room for improvement exists, although this is done under non-realistic conditions.
we consider the problem of ner in arabic wikipedia, a semisupervised domain adaptation setting for which we have no labeled training data in the target domain. <eos> to facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-specific entity types in addition to standard categories. <eos> standard supervised learning on newswire text leads to poor target-domain recall. <eos> we train a sequence model and show that a simple modification to the online learner ? a loss function encouraging it to ? arrogantly ? <eos> favor recall over precision ? <eos> substantially improves recall and f1. <eos> we then adapt our model with self-training on unlabeled target-domain data ; enforcing the same recall-oriented bias in the selftraining stage yields marginal gains.1
in this paper we deal with named entity recognition ( ner ) on transcriptions of french broadcast data. <eos> two aspects make the task more difficult with respect to previous ner tasks : i ) named entities annotated used in this work have a tree structure, thus the task can not be tackled as a sequence labelling task ; ii ) the data used are more noisy than data used for previous ner tasks. <eos> we approach the task in two steps, involving conditional random fields and probabilistic context-free grammars, integrated in a single parsing algorithm. <eos> we analyse the effect of using several tree representations. <eos> our system outperforms the best system of the evaluation campaign by a significant margin.
we present work on linking events and fluents ( i.e., relations that hold for certain periods of time ) to temporal information in text, which is an important enabler for many applications such as timelines and reasoning. <eos> previous research has mainly focused on temporal links for events, and we extend that work to include fluents as well, presenting a common methodology for linking both events and relations to timestamps within the same sentence. <eos> our approach combines tree kernels with classical feature-based learning to exploit context and achieves competitive f1-scores on event-time linking, and comparable f1scores for fluents. <eos> our best systems achieve f1-scores of 0.76 on events and 0.72 on fluents.
topic models have great potential for helping users understand document corpora. <eos> this potential is stymied by their purely unsupervised nature, which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks ( chang et al 2009 ). <eos> we propose a simple and effective way to guide topic models to learn topics of specific interest to a user. <eos> we achieve this by providing sets of seed words that a user believes are representative of the underlying topics in a corpus. <eos> our model uses these seeds to improve both topicword distributions ( by biasing topics to produce appropriate seed words ) and to improve document-topic distributions ( by biasing documents to select topics related to the seed words they contain ). <eos> extrinsic evaluation on a document clustering task reveals a significant improvement when using seed information, even over other models that use seed information na ? <eos> ? vely.
update summarization is a new challenge in multi-document summarization focusing on summarizing a set of recent documents relatively to another set of earlier documents. <eos> we present an unsupervised probabilistic approach to model novelty in a document collection and apply it to the generation of update summaries. <eos> the new model, called dualsum, results in the second or third position in terms of the rouge metrics when tuned for previous tac competitions and tested on tac-2011, being statistically indistinguishable from the winning system. <eos> a manual evaluation of the generated summaries shows state-of-the art results for dualsum with respect to focus, coherence and overall responsiveness.
in this paper, we present a supervised learning approach to training submodular scoring functions for extractive multidocument summarization. <eos> by taking a structured prediction approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. <eos> the learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. <eos> compared to state-of-theart functions that were tuned manually, our method significantly improves performance and enables high-fidelity models with number of parameters well beyond what could reasonably be tuned by hand.
this paper presents an incremental probabilistic learner that models the acquistion of syntax and semantics from a corpus of child-directed utterances paired with possible representations of their meanings. <eos> these meaning representations approximate the contextual input available to the child ; they do not specify the meanings of individual words or syntactic derivations. <eos> the learner then has to infer the meanings and syntactic properties of the words in the input along with a parsing model. <eos> we use the ccg grammatical framework and train a non-parametric bayesian model of parse structure with online variational bayesian expectation maximization. <eos> when tested on utterances from the childes corpus, our learner outperforms a state-of-the-art semantic parser. <eos> in addition, it models such aspects of child acquisition as ? fast mapping, ? <eos> while also countering previous criticisms of statistical syntactic learners.
translation needs have greatly increased during the last years. <eos> in many situations, text to be translated constitutes an unbounded stream of data that grows continually with time. <eos> an effective approach to translate text documents is to follow an interactive-predictive paradigm in which both the system is guided by the user and the user is assisted by the system to generate error-free translations. <eos> unfortunately, when processing such unbounded data streams even this approach requires an overwhelming amount of manpower. <eos> is in this scenario where the use of active learning techniques is compelling. <eos> in this work, we propose different active learning techniques for interactive machine translation. <eos> results show that for a given translation quality the use of active learning allows us to greatly reduce the human effort required to translate the sentences in the stream.
translation models used for statistical machine translation are compiled from parallel corpora ; such corpora are manually translated, but the direction of translation is usually unknown, and is consequently ignored. <eos> however, much research in translation studies indicates that the direction of translation matters, as translated language ( translationese ) has many unique properties. <eos> specifically, phrase tables constructed from parallel corpora translated in the same direction as the translation task perform better than ones constructed from corpora translated in the opposite direction. <eos> we reconfirm that this is indeed the case, but emphasize the importance of using also texts translated in the ? wrong ? <eos> direction. <eos> we take advantage of information pertaining to the direction of translation in constructing phrase tables, by adapting the translation model to the special properties of translationese. <eos> we define entropybased measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation. <eos> we show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically significant improvement in the quality of the translation.
in this paper we investigate the relevance of aspectual type for the problem of temporal information processing, i.e. <eos> the problems of the recent tempeval challenges. <eos> for a large list of verbs, we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with specific aspectual types. <eos> we then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way. <eos> the improved performance of the resulting models shows that ( i ) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that ( ii ) temporal information processing can profit from information about aspectual type.
in this paper, we define a new type of summary for sentiment analysis : a singlesentence summary that consists of a supporting sentence that conveys the overall sentiment of a review as well as a convincing reason for this sentiment. <eos> we present a system for extracting supporting sentences from online product reviews, based on a simple and unsupervised method. <eos> we design a novel comparative evaluation method for summarization, using a crowdsourcing service. <eos> the evaluation shows that our sentence extraction method performs better than a baseline of taking the sentence with the strongest sentiment.
most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. <eos> due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. <eos> in this paper, we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision. <eos> we aim to rapidly train a state-of-the-art event extraction system using a small set of ? seed nouns ? <eos> for each event role, a collection of relevant ( in-domain ) and irrelevant ( outof-domain ) texts, and a semantic dictionary. <eos> the experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the muc-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents.
in this paper, we describe a new approach to semi-supervised adaptive learning of event extraction from text. <eos> given a set of exam-ples and an un-annotated text corpus, the bear system ( bootstrapping events and relations ) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text, such as events involving multiple entities and their roles. <eos> for example, given a series of descriptions of bombing and shooting inci-dents ( e.g., in newswire ) the system will learn to extract, with a high degree of accu-racy, other attack-type events mentioned elsewhere in text, irrespective of the form of description. <eos> a series of evaluations using the ace data and event set show a signifi-cant performance improvement over our baseline system.
existing concept-color-emotion lexicons limit themselves to small sets of basic emotions and colors, which can not capture the rich pallet of color terms that humans use in communication. <eos> in this paper we begin to address this problem by building a novel, color-emotion-concept association lexicon via crowdsourcing. <eos> this lexicon, which we call clex, has over 2,300 color terms, over 3,000 affect terms and almost 2,000 concepts. <eos> we investigate the relation between color and concept, and color and emotion, reinforcing results from previous studies, as well as discovering new associations. <eos> we also investigate cross-cultural differences in color-emotion associations between us and india-based annotators.
we extend the original entity-based coherence model ( barzilay and lapata, 2008 ) by learning from more fine-grained coherence preferences in training data. <eos> we associate multiple ranks with the set of permutations originating from the same source document, as opposed to the original pairwise rankings. <eos> we also study the effect of the permutations used in training, and the effect of the coreference component used in entity extraction. <eos> with no additional manual annotations required, our extended model is able to outperform the original model on two tasks : sentence ordering and summary coherence rating.
in this paper, we compare three different generalization methods for in-domain and cross-domain opinion holder extraction being simple unsupervised word clustering, an induction method inspired by distant supervision and the usage of lexical resources. <eos> the generalization methods are incorporated into diverse classifiers. <eos> we show that generalization causes significant improvements and that the impact of improvement depends on the type of classifier and on how much training and test data differ from each other. <eos> we also address the less common case of opinion holders being realized in patient position and suggest approaches including a novel ( linguisticallyinformed ) extraction method how to detect those opinion holders without labeled training data as standard datasets contain too few instances of this type.
in this paper, we extend current state-of-theart research on unsupervised acquisition of scripts, that is, stereotypical and frequently observed sequences of events. <eos> we design, evaluate and compare different methods for constructing models for script event prediction : given a partial chain of events in a script, predict other events that are likely to belong to the script. <eos> our work aims to answer key questions about how best to ( 1 ) identify representative event chains from a source text, ( 2 ) gather statistics from the event chains, and ( 3 ) choose ranking functions for predicting new script events. <eos> we make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models.
document revision histories are a useful and abundant source of data for natural language processing, but selecting relevant data for the task at hand is not trivial. <eos> in this paper we introduce a scalable approach for automatically distinguishing between factual and fluency edits in document revision histories. <eos> the approach is based on supervised machine learning using language model probabilities, string similarity measured over different representations of user edits, comparison of part-of-speech tags and named entities, and a set of adaptive features extracted from large amounts of unlabeled user edits. <eos> applied to contiguous edit segments, our method achieves statistically significant improvements over a simple yet effective edit-distance baseline. <eos> it reaches high classification accuracy ( 88 % ) and is shown to generalize to additional sets of unseen data.
online community is an important source for latest news and information. <eos> accurate prediction of a user ? s interest can help provide better user experience. <eos> in this paper, we develop a recommendation system for online forums. <eos> there are a lot of differences between online forums and formal media. <eos> for example, content generated by users in online forums contains more noise compared to formal documents. <eos> content topics in the same forum are more focused than sources like news websites. <eos> some of these differences present challenges to traditional word-based user profiling and recommendation systems, but some also provide opportunities for better recommendation performance. <eos> in our recommendation system, we propose to ( a ) use latent topics to interpolate with content-based recommendation ; ( b ) model latent user groups to utilize information from other users. <eos> we have collected three types of forum data sets. <eos> our experimental results demonstrate that our proposed hybrid approach works well in all three types of forums.
we present the pong method to compute selectional preferences using part-of-speech ( pos ) n-grams. <eos> from a corpus labeled with grammatical dependencies, pong learns the distribution of word relations for each pos n-gram. <eos> from the much larger but unlabeled google n-grams corpus, pong learns the distribution of pos n-grams for a given pair of words. <eos> we derive the probability that one word has a given grammatical relation to the other. <eos> pong estimates this probability by combining both distributions, whether or not either word occurs in the labeled corpus. <eos> pong achieves higher average precision on 16 relations than a state-of-the-art baseline in a pseudo-disambiguation task, but lower coverage and recall.
probabilistic accounts of language processing can be psychologically tested by comparing word-reading times ( rt ) to the conditional word probabilities estimated by language models. <eos> using surprisal as a linking function, a significant correlation between unlexicalized surprisal and rt has been reported ( e.g., demberg and keller, 2008 ), but success using lexicalized models has been limited. <eos> in this study, phrase structure grammars and recurrent neural networks estimated both lexicalized and unlexicalized surprisal for words of independent sentences from narrative sources. <eos> these same sentences were used as stimuli in a self-paced reading experiment to obtain rts. <eos> the results show that lexicalized surprisal according to both models is a significant predictor of rt, outperforming its unlexicalized counterparts.
in this paper we study spectral learning methods for non-deterministic split headautomata grammars, a powerful hiddenstate formalism for dependency parsing. <eos> we present a learning algorithm that, like other spectral methods, is efficient and nonsusceptible to local minima. <eos> we show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions. <eos> furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.
kernel based methods dominate the current trend for various relation extraction tasks including protein-protein interaction ( ppi ) extraction. <eos> ppi information is critical in understanding biological processes. <eos> despite considerable efforts, previously reported ppi extraction results show that none of the approaches already known in the literature is consistently better than other approaches when evaluated on different benchmark ppi corpora. <eos> in this paper, we propose a novel hybrid kernel that combines ( automatically collected ) dependency patterns, trigger words, negative cues, walk features and regular expression patterns along with tree kernel and shallow linguistic kernel. <eos> the proposed kernel outperforms the exiting state-of-the-art approaches on the bioinfer corpus, the largest ppi benchmark corpus available. <eos> on the other four smaller benchmark corpora, it performs either better or almost as good as the existing approaches. <eos> moreover, empirical results show that the proposed hybrid kernel attains considerably higher precision than the existing approaches, which indicates its capability of learning more accurate models. <eos> this also demonstrates that the different types of information that we use are able to complement each other for relation extraction.
coordination disambiguation remains a difficult sub-problem in parsing despite the frequency and importance of coordination structures. <eos> we propose a method for disambiguating coordination structures. <eos> in this method, dual decomposition is used as a framework to take advantage of both hpsg parsing and coordinate structure analysis with alignment-based local features. <eos> we evaluate the performance of the proposed method on the genia corpus and the wall street journal portion of the penn treebank. <eos> results show it increases the percentage of sentences in which coordination structures are detected correctly, compared with each of the two algorithms alone.
in this paper, we address statistical machine translation of public conference talks. <eos> modeling the style of this genre can be very challenging given the shortage of available in-domain training data. <eos> we investigate the use of a hybrid lm, where infrequent words are mapped into classes. <eos> hybrid lms are used to complement word-based lms with statistics about the language style of the talks. <eos> extensive experiments comparing different settings of the hybrid lm are reported on publicly available benchmarks based on ted talks, from arabic to english and from english to french. <eos> the proposed models show to better exploit in-domain data than conventional word-based lms for the target language modeling component of a phrase-based statistical machine translation system.
in this paper, we extend the work on using latent cross-language topic models for identifying word translations across comparable corpora. <eos> we present a novel precisionoriented algorithm that relies on per-topic word distributions obtained by the bilingual lda ( bilda ) latent topic model. <eos> the algorithm aims at harvesting only the most probable word translations across languages in a greedy fashion, without any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint. <eos> we report our results for italian-english and dutch-english language pairs that outperform the current state-of-the-art results by a significant margin. <eos> in addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations.
previous work on treebank parsing with discontinuous constituents using linear context-free rewriting systems ( lcfrs ) has been limited to sentences of up to 30 words, for reasons of computational complexity. <eos> there have been some results on binarizing an lcfrs in a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. <eos> instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy. <eos> the resulting parser has been applied to a discontinuous treebank with favorable results.
it is not always clear how the differences in intrinsic evaluation metrics for a parser or classifier will affect the performance of the system that uses it. <eos> we investigate the relationship between the intrinsic evaluation scores of an interpretation component in a tutorial dialogue system and the learning outcomes in an experiment with human users. <eos> following the paradise methodology, we use multiple linear regression to build predictive models of learning gain, an important objective outcome metric in tutorial dialogue. <eos> we show that standard intrinsic metrics such as f-score alone do not predict the outcomes well. <eos> however, we can build predictive performance functions that account for up to 50 % of the variance in learning gain by combining features based on standard evaluation scores and on the confusion matrix entries. <eos> we argue that building such predictive models can help us better evaluate performance of nlp components that can not be distinguished based on f-score alone, and illustrate our approach by comparing the current interpretation component in the system to a new classifier trained on the evaluation data.
we describe a set of experiments using automatically labelled data to train supervised classifiers for multi-class emotion detection in twitter messages with no manual intervention. <eos> by cross-validating between models trained on different labellings for the same six basic emotion classes, and testing on manually labelled data, we conclude that the method is suitable for some emotions ( happiness, sadness and anger ) but less able to distinguish others ; and that different labelling conventions are more suitable for some emotions than others.
we present experiments with part-ofspeech tagging for bulgarian, a slavic language with rich inflectional and derivational morphology. <eos> unlike most previous work, which has used a small number of grammatical categories, we work with 680 morpho-syntactic tags. <eos> we combine a large morphological lexicon with prior linguistic knowledge and guided learning from a pos-annotated corpus, achieving accuracy of 97.98 %, which is a significant improvement over the state-of-the-art for bulgarian.
whether automatically extracted or human generated, open-domain factual knowledge is often available in the form of semantic annotations ( e.g., composed-by ) that take one or more specific instances ( e.g., rhapsody in blue, george gershwin ) as their arguments. <eos> this paper introduces a method for converting flat sets of instance-level annotations into hierarchically organized, concept-level annotations, which capture not only the broad semantics of the desired arguments ( e.g., ? people ? <eos> rather than ? locations ? <eos> ), but also the correct level of generality ( e.g., ? composers ? <eos> rather than ? people ?, or ? jazz composers ? ). <eos> the method refrains from encoding features specific to a particular domain or annotation, to ensure immediate applicability to new, previously unseen annotations. <eos> over a gold standard of semantic annotations and concepts that best capture their arguments, the method substantially outperforms three baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts.
we present a model of semantic processing of spoken language that ( a ) is robust against ill-formed input, such as can be expected from automatic speech recognisers, ( b ) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, ( c ) uses a principled, expressive semantic representation formalism ( rmrs ) with a well-defined model theory, and ( d ) works continuously ( producing meaning representations on a wordby-word basis, rather than only for full utterances ) and incrementally ( computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far ). <eos> we show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the nlu component ( around 10 % absolute, over a syntax-only baseline ).
in this paper we extend our work described in ( dinu et al 2011 ) by adding more conjugational rules to the labelling system introduced there, in an attempt to capture the entire dataset of romanian verbs extracted from ( barbu, 2007 ), and we employ machine learning techniques to predict a verb ? s correct label ( which says what conjugational pattern it follows ) when only the infinitive form is given. <eos>
we evaluate measures of contextual fitness on the task of detecting real-word spelling errors. <eos> for that purpose, we extract naturally occurring errors and their contexts from the wikipedia revision history. <eos> we show that such natural errors are better suited for evaluation than the previously used artificially created errors. <eos> in particular, the precision of statistical methods has been largely over-estimated, while the precision of knowledge-based approaches has been under-estimated. <eos> additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. <eos> finally, we show that statistical and knowledgebased methods can be combined for increased performance.
we investigate the problem of domain adaptation for parallel data in statistical machine translation ( smt ). <eos> while techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. <eos> we also explore adapting multiple ( 4 ? 10 ) data sets with no a priori distinction between in-domain and out-of-domain data except for an in-domain development set.
this paper describes subcat-lmf, an isolmf compliant lexicon representation format featuring a uniform representation of subcategorization frames ( scfs ) for the two languages english and german. <eos> subcat-lmf is able to represent scfs at a very fine-grained level. <eos> we utilized subcatlmf to standardize lexicons with largescale scf information : the english verbnet and two german lexicons, i.e., a subset of imslex and germanet verbs. <eos> to evaluate our lmf-model, we performed a crosslingual comparison of scf coverage and overlap for the standardized versions of the english and german lexicons. <eos> the subcatlmf dtd, the conversion tools and the standardized versions of verbnet and imslex subset are publicly available.1
text prediction is the task of suggesting text while the user is typing. <eos> its main aim is to reduce the number of keystrokes that are needed to type a text. <eos> in this paper, we address the influence of text type and domain differences on text prediction quality. <eos> by training and testing our text prediction algorithm on four different text types ( wikipedia, twitter, transcriptions of conversational speech and faq ) with equal corpus sizes, we found that there is a clear effect of text type on text prediction quality : training and testing on the same text type gave percentages of saved keystrokes between 27 and 34 % ; training on a different text type caused the scores to drop to percentages between 16 and 28 %. <eos> in our case study, we compared a number of training corpora for a specific data set for which training data is sparse : questions about neurological issues. <eos> we found that both text type and topic domain play a role in text prediction quality. <eos> the best performing training corpus was a set of medical pages from wikipedia. <eos> the second-best result was obtained by leaveone-out experiments on the test questions, even though this training corpus was much smaller ( 2,672 words ) than the other corpora ( 1.5 million words ).
the search in patent databases is a risky business compared to the search in other domains. <eos> a single document that is relevant but overlooked during a patent search can turn into an expensive proposition. <eos> while recent research engages in specialized models and algorithms to improve the effectiveness of patent retrieval, we bring another aspect into focus : the detection and exploitation of patent inconsistencies. <eos> in particular, we analyze spelling errors in the assignee field of patents granted by the united states patent & trademark office. <eos> we introduce technology in order to improve retrieval effectiveness despite the presence of typographical ambiguities. <eos> in this regard, we ( 1 ) quantify spelling errors in terms of edit distance and phonological dissimilarity and ( 2 ) render error detection as a learning problem that combines word dissimilarities with patent meta-features. <eos> for the task of finding all patents of a company, our approach improves recall from 96.7 % ( when using a state-of-the-art patent search engine ) to 99.5 %, while precision is compromised by only 3.7 %.
we present uby, a large-scale lexicalsemantic resource combining a wide range of information from expert-constructed and collaboratively constructed resources for english and german. <eos> it currently contains nine resources in two languages : english wordnet, wiktionary, wikipedia, framenet and verbnet, german wikipedia, wiktionary and germanet, and multilingual omegawiki modeled according to the lmf standard. <eos> for framenet, verbnet and all collaboratively constructed resources, this is done for the first time. <eos> our lmf model captures lexical information at a fine-grained level by employing a large number of data categories from isocat and is designed to be directly extensible by new languages and resources. <eos> all resources in uby can be accessed with an easy to use publicly available api.
we apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. <eos> we start by exploring the utility of standard topic models for word sense induction ( wsi ), with a pre-determined number of topics ( =senses ). <eos> we next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the wsi task. <eos> we go on to establish state-of-the-art results over two wsi datasets, and apply the proposed model to a novel sense detection task.
microblogging websites such as twitter offer a wealth of insight into a population ? s current mood. <eos> automated approaches to identify general sentiment toward a particular topic often perform two steps : topic identification and sentiment analysis. <eos> topic identification first identifies tweets that are relevant to a desired topic ( e.g., a politician or event ), and sentiment analysis extracts each tweet ? s attitude toward the topic. <eos> many techniques for topic identification simply involve selecting tweets using a keyword search. <eos> here, we present an approach that instead uses distant supervision to train a classifier on the tweets returned by the search. <eos> we show that distant supervision leads to improved performance in the topic identification task as well in the downstream sentiment analysis stage. <eos> we then use a system that incorporates distant supervision into both stages to analyze the sentiment toward president obama expressed in a dataset of tweets. <eos> our results better correlate with gallup ? s presidential job approval polls than previous work. <eos> finally, we discover a surprising baseline that outperforms previous work without a topic identification stage.
open issue trackers are a type of social media that has received relatively little attention from the text-mining community. <eos> we investigate the problems inherent in learning to triage bug reports from time-varying data. <eos> we demonstrate that concept drift is an important consideration. <eos> we show the effectiveness of online learning algorithms by evaluating them on several bug report datasets collected from open issue trackers associated with large open-source projects. <eos> we make this collection of data publicly available.
informal and formal ( ? t/v ? ) <eos> address in dialogue is not distinguished overtly in modern english, e.g. <eos> by pronoun choice like in many other languages such as french ( ? tu ? / ? vous ? ). <eos> our study investigates the status of the t/v distinction in english literary texts. <eos> our main findings are : ( a ) human raters can label monolingual english utterances as t or v fairly well, given sufficient context ; ( b ), a bilingual corpus can be exploited to induce a supervised classifier for t/v without human annotation. <eos> it assigns t/v at sentence level with up to 68 % accuracy, relying mainly on lexical features ; ( c ), there is a marked asymmetry between lexical features for formal speech ( which are conventionalized and therefore general ) and informal speech ( which are text-specific ).
better representations of plot structure could greatly improve computational methods for summarizing and generating stories. <eos> current representations lack abstraction, focusing too closely on events. <eos> we present a kernel for comparing novelistic plots at a higher level, in terms of the cast of characters they depict and the social relationships between them. <eos> our kernel compares the characters of different novels to one another by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them. <eos> given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure.
morphological lexica are often implemented on top of morphological paradigms, corresponding to different ways of building the full inflection table of a word. <eos> computationally precise lexica may use hundreds of paradigms, and it can be hard for a lexicographer to choose among them. <eos> to automate this task, this paper introduces the notion of a smart paradigm. <eos> it is a metaparadigm, which inspects the base form and tries to infer which low-level paradigm applies. <eos> if the result is uncertain, more forms are given for discrimination. <eos> the number of forms needed in average is a measure of predictability of an inflection system. <eos> the overall complexity of the system also has to take into account the code size of the paradigms definition itself. <eos> this paper evaluates the smart paradigms implemented in the open-source gf resource grammar library. <eos> predictability and complexity are estimated for four different languages : english, french, swedish, and finnish. <eos> the main result is that predictability does not decrease when the complexity of morphology grows, which means that smart paradigms provide an efficient tool for the manual construction and/or automatically bootstrapping of lexica.
we propose a novel method for learning morphological paradigms that are structured within a hierarchy. <eos> the hierarchical structuring of paradigms groups morphologically similar words close to each other in a tree structure. <eos> this allows detecting morphological similarities easily leading to improved morphological segmentation. <eos> our evaluation using ( kurimo et al., 2011a ; kurimo et al 2011b ) dataset shows that our method performs competitively when compared with current state-ofart systems.
the current state-of-the-art in statistical machine translation ( smt ) suffers from issues of sparsity and inadequate modeling power when translating into morphologically rich languages. <eos> we model both inflection and word-formation for the task of translating into german. <eos> we translate from english words to an underspecified german representation and then use linearchain crfs to predict the fully specified german representation. <eos> we show that improved modeling of inflection and wordformation leads to improved smt.
arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals ( which resemble singular nouns ), and nouns with irregular gender ( feminine nouns that look masculine and vice versa ). <eos> in addition, arabic morphosyntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. <eos> in this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in arabic. <eos> we compare two techniques, using simple maximum likelihood ( mle ) with back-off and a support vector machine based sequence tagger ( yamcha ). <eos> we study a number of orthographic, morphological and syntactic learning features. <eos> our results show that the mle technique is preferred for words seen in the training data, while the yamcha technique is optimal for unseen words, which are our real target. <eos> furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. <eos> a combination of the two techniques improves overall performance even further.
widely accepted resources for semantic parsing, such as propbank and framenet, are not perfect as a semantic role labeling framework. <eos> their semantic roles are not strictly defined ; therefore, their meanings and semantic characteristics are unclear. <eos> in addition, it is presupposed that a single semantic role is assigned to each syntactic argument. <eos> this is not necessarily true when we consider internal structures of verb semantics. <eos> we propose a new framework for semantic role annotation which solves these problems by extending the theory of lexical conceptual structure ( lcs ). <eos> by comparing our framework with that of existing resources, including verbnet and framenet, we demonstrate that our extended lcs framework can give a formal definition of semantic role labels, and that multiple roles of arguments can be represented strictly and naturally.
we propose an unsupervised, iterative method for detecting downward-entailing operators ( deos ), which are important for deducing entailment relations between sentences. <eos> like the distillation algorithm of danescu-niculescu-mizil et al ( 2009 ), the initialization of our method depends on the correlation between deos and negative polarity items ( npis ). <eos> however, our method trusts the initialization more and aggressively separates likely deos from spurious distractors and other words, unlike distillation, which we show to be equivalent to one iteration of em prior re-estimation. <eos> our method is also amenable to a bootstrapping method that co-learns deos and npis, and achieves the best results in identifying deos in two corpora.
in pro-drop languages, the detection of explicit subjects, zero subjects and nonreferential impersonal constructions is crucial for anaphora and co-reference resolution. <eos> while the identification of explicit and zero subjects has attracted the attention of researchers in the past, the automatic identification of impersonal constructions in spanish has not been addressed yet and this work is the first such study. <eos> in this paper we present a corpus to underpin research on the automatic detection of these linguistic phenomena in spanish and a novel machine learning-based methodology for their computational treatment. <eos> this study also provides an analysis of the features, discusses performance across two different genres and offers error analysis. <eos> the evaluation results show that our system performs better in detecting explicit subjects than alternative systems.
the task of paraphrase acquisition from related sentences can be tackled by a variety of techniques making use of various types of knowledge. <eos> in this work, we make the hypothesis that their performance can be increased if candidate paraphrases can be validated using information that characterizes paraphrases independently of the set of techniques that proposed them. <eos> we implement this as a bi-class classification problem ( i.e. <eos> paraphrase vs. not paraphrase ), allowing any paraphrase acquisition technique to be easily integrated into the combination system. <eos> we report experiments on two languages, english and french, with 5 individual techniques on parallel monolingual parallel corpora obtained via multiple translation, and a large set of classification features including surface to contextual similarity measures. <eos> relative improvements in f-measure close to 18 % are obtained on both languages over the best performing techniques.
when translating english to german, existing reordering models often can not model the long-range reorderings needed to generate german translations with verbs in the correct position. <eos> we reorder english as a preprocessing step for english-to-german smt. <eos> we use a sequence of hand-crafted reordering rules applied to english parse trees. <eos> the reordering rules place english verbal elements in the positions within the clause they will have in the german translation. <eos> this is a difficult problem, as german verbal elements can appear in different positions within a clause ( in contrast with english verbal elements, whose positions do not vary as much ). <eos> we obtain a significant improvement in translation performance.
a fundamental problem in text generation is word ordering. <eos> word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. <eos> there have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words ( wan et al 2009 ; zhang and clark, 2011 ). <eos> by using ccg and learning guided search, zhang and clark reported the highest scores on this task. <eos> one limitation of their system is the absence of an n-gram language model, which has been used by text generation systems to improve fluency. <eos> we take the zhang and clark system as the baseline, and incorporate an n-gram model by applying online large-margin training. <eos> our system significantly improved on the baseline by 3.7 bleu points.
this paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. <eos> by leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. <eos> results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.
we present a system for the real-time generation of car navigation instructions with landmarks. <eos> our system relies exclusively on freely available map data from openstreetmap, organizes its output to fit into the available time until the next driving maneuver, and reacts in real time to driving errors. <eos> we show that female users spend significantly less time looking away from the road when using our system compared to a baseline system.
we compare the impact of sentenceinternal vs. sentence-external features on word order prediction in two generation settings : starting out from a discriminative surface realisation ranking model for an lfg grammar of german, we enrich the feature set with lexical chain features from the discourse context which can be robustly detected and reflect rough grammatical correlates of notions from theoretical approaches to discourse coherence. <eos> in a more controlled setting, we develop a constituent ordering classifier that is trained on a german treebank with gold coreference annotation. <eos> surprisingly, in both settings, the sentence-external features perform poorly compared to the sentenceinternal ones, and do not improve over a baseline model capturing the syntactic functions of the constituents.
in this paper, we propose an annotation schema for the discourse analysis of wikipedia talk pages aimed at the coordination efforts for article improvement. <eos> we apply the annotation schema to a corpus of 100 talk pages from the simple english wikipedia and make the resulting dataset freely available for download1. <eos> furthermore, we perform automatic dialog act classification on wikipedia discussions and achieve an average f1-score of 0.82 with our classification pipeline.
speech style accommodation refers to shifts in style that are used to achieve strategic goals within interactions. <eos> models of stylistic shift that focus on specific features are limited in terms of the contexts to which they can be applied if the goal of the analysis is to model socially motivated speech style accommodation. <eos> in this paper, we present an unsupervised dynamic bayesian model that allows us to model stylistic style accommodation in a way that is agnostic to which specific speech style features will shift in a way that resembles socially motivated stylistic variation. <eos> this greatly expands the applicability of the model across contexts. <eos> our hypothesis is that stylistic shifts that occur as a result of social processes are likely to display some consistency over time, and if we leverage this insight in our model, we will achieve a model that better captures inherent structure within speech.
while information status ( is ) plays a crucial role in discourse processing, there have only been a handful of attempts to automatically determine the is of discourse entities. <eos> we examine a related but more challenging task, fine-grained is determination, which involves classifying a discourse entity as one of 16 is subtypes. <eos> we investigate the use of rich knowledge sources for this task in combination with a rule-based approach and a learning-based approach. <eos> in experiments with a set of switchboard dialogues, the learning-based approach achieves an accuracy of 78.7 %, outperforming the rulebased approach by 21.3 %.
a composition procedure for linear and nondeleting extended top-down tree transducers is presented. <eos> it is demonstrated that the new procedure is more widely applicable than the existing methods. <eos> in general, the result of the composition is an extended top-down tree transducer that is no longer linear or nondeleting, but in a number of cases these properties can easily be recovered by a post-processing step.
patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. <eos> in this paper we analyze patents along the orthogonal dimensions of topic and textual structure. <eos> we view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. <eos> we study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. <eos> we find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. <eos> a by-product of our work is a parallel patent corpus of 23 million german-english sentence pairs.
german case syncretism is often assumed to be the accidental by-product of historical development. <eos> this paper contradicts this claim and argues that the evolution of german case is driven by the need to optimize the cognitive effort and memory required for processing and interpretation. <eos> this hypothesis is supported by a novel kind of computational experiments that reconstruct and compare attested variations of the german definite article paradigm. <eos> the experiments show how the intricate interaction between those variations and the rest of the german ? linguistic landscape ? <eos> may direct language change.
low interannotator agreement ( iaa ) is a well-known issue in manual semantic tagging ( sense tagging ). <eos> iaa correlates with the granularity of word senses and they both correlate with the amount of information they give as well as with its reliability. <eos> we compare different approaches to semantic tagging in wordnet, framenet, propbank and ontonotes with a small tagged data sample based on the corpus pattern analysis to present the reliable information gain ( rg ), a measure used to optimize the semantic granularity of a sense inventory with respect to its reliability indicated by the iaa in the given data set. <eos> rg can also be used as feedback for lexicographers, and as a supporting component of automatic semantic classifiers, especially when dealing with a very fine-grained set of semantic categories.
this paper demonstrates a novel distributed architecture to facilitate the acquisition of language resources. <eos> we build a factory that automates the stages involved in the acquisition, production, updating and maintenance of these resources. <eos> the factory is designed as a platform where functionalities are deployed as web services, which can be combined in complex acquisition chains using workflows. <eos> we show a case study, which acquires a translation memory for a given pair of languages and a domain using web services for crawling, sentence alignment and conversion to tmx.
french researchers are required to frequently translate into french the description of their work published in english. <eos> at the same time, the need for french people to access articles in english, or to international researchers to access theses or papers in french, is incorrectly resolved via the use of generic translation tools. <eos> we propose the demonstration of an end-to-end tool integrated in the hal open archive for enabling efficient translation for scientific texts. <eos> this tool can give translation suggestions adapted to the scientific domain, improving by more than 10 points the bleu score of a generic system. <eos> it also provides a post-edition service which captures user post-editing data that can be used to incrementally improve the translations engines. <eos> thus it is helpful for users which need to translate or to access scientific texts.
difficulty of reading scholarly papers is significantly reduced by reader-friendly writing principles. <eos> writing reader-friendly text, however, is challenging due to difficulty in recognizing problems in one ? s own writing. <eos> to help scholars identify and correct potential writing problems, we introduce swan ( scientific writing assistant ) tool. <eos> swan is a rule-based system that gives feedback based on various quality metrics based on years of experience from scientific writing classes including 960 scientists of various backgrounds : life sciences, engineering sciences and economics. <eos> according to our first experiences, users have perceived swan as helpful in identifying problematic sections in text and increasing overall clarity of manuscripts.
we propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from arabic, czech, danish, farsi, french, german, italian, polish, portuguese, spanish and turkish into english. <eos> the moses-based system was optimised for the news domain and differs from other available systems in four ways : ( 1 ) news items are automatically categorised on the source side, before translation ; ( 2 ) named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language, making use of a separate entity repository ; ( 3 ) news titles are translated with a separate translation system which is optimised for the specific style of news titles ; ( 4 ) the system was optimised for speed in order to cope with the large volume of daily news articles.
this paper deals with an application of automatic titling. <eos> the aim of such application is to attribute a title for a given text. <eos> so, our application relies on three very different automatic titling methods. <eos> the first one extracts relevant noun phrases for their use as a heading, the second one automatically constructs headings by selecting words appearing in the text, and, finally, the third one uses nominalization in order to propose informative and catchy titles. <eos> experiments based on 1048 titles have shown that our methods provide relevant titles.
this paper presents folheador, an online service for browsing through portuguese semantic relations, acquired from different sources. <eos> besides facilitating the exploration of portuguese lexical knowledge bases, folheador is connected to services that access portuguese corpora, which provide authentic examples of the semantic relations in context.
current automatic speech transcription systems can achieve a high accuracy although they still make mistakes. <eos> in some scenarios, high quality transcriptions are needed and, therefore, fully automatic systems are not suitable for them. <eos> these high accuracy tasks require a human transcriber. <eos> however, we consider that automatic techniques could improve the transcriber ? s efficiency. <eos> with this idea we present an interactive speech recognition system integrated with a word processor in order to assists users when transcribing speech. <eos> this system automatically recognizes speech while allowing the user to interactively modify the transcription.
this paper presents the first demonstration of a statistical spoken dialogue system that uses automatic belief compression to reason over complex user goal sets. <eos> reasoning over the power set of possible user goals allows complex sets of user goals to be represented, which leads to more natural dialogues. <eos> the use of the power set results in a massive expansion in the number of belief states maintained by the partially observable markov decision process ( pomdp ) spoken dialogue manager. <eos> a modified form of value directed compression ( vdc ) is applied to the pomdp belief states producing a near-lossless compression which reduces the number of bases required to represent the belief distribution.
the aim of our software presentation is to demonstrate that corpus-driven bilingual dictionaries generated fully by automatic means are suitable for human use. <eos> previous experiments have proven that bilingual lexicons can be created by applying word alignment on parallel corpora. <eos> such an approach, especially the corpus-driven nature of it, yields several advantages over more traditional approaches. <eos> most importantly, automatically attained translation probabilities are able to guarantee that the most frequently used translations come first within an entry. <eos> however, the proposed technique have to face some difficulties, as well. <eos> in particular, the scarce availability of parallel texts for medium density languages imposes limitations on the size of the resulting dictionary. <eos> our objective is to design and implement a dictionary building workflow and a query system that is apt to exploit the additional benefits of the method and overcome the disadvantages of it.
data-driven systems for natural language processing have the advantage that they can easily be ported to any language or domain for which appropriate training data can be found. <eos> however, many data-driven systems require careful tuning in order to achieve optimal performance, which may require specialized knowledge of the system. <eos> we present maltoptimizer, a tool developed to facilitate optimization of parsers developed using maltparser, a data-driven dependency parser generator. <eos> maltoptimizer performs an analysis of the training data and guides the user through a three-phase optimization process, but it can also be used to perform completely automatic optimization. <eos> experiments show that maltoptimizer can improve parsing accuracy by up to 9 percent absolute ( labeled attachment score ) compared to default settings. <eos> during the demo session, we will run maltoptimizer on different data sets ( user-supplied if possible ) and show how the user can interact with the system and track the improvement in parsing accuracy.
cognitive linguistics has reached a stage of maturity where many researchers are looking for an explicit formal grounding of their work. <eos> unfortunately, most current models of deep language processing incorporate assumptions from generative grammar that are at odds with the cognitive movement in linguistics. <eos> this demonstration shows how fluid construction grammar ( fcg ), a fully operational and bidirectional unification-based grammar formalism, caters for this increasing demand. <eos> fcg features many of the tools that were pioneered in computational linguistics in the 70s-90s, but combines them in an innovative way. <eos> this demonstration highlights the main differences between fcg and related formalisms.
this paper describes a system designed to support event detection over twitter. <eos> the system operates by querying the data stream with a user-specified set of keywords, filtering out non-english messages, and probabilistically geolocating each message. <eos> the user can dynamically set a probability threshold over the geolocation predictions, and also the time interval to present data for.
named entity extraction is a mature task in the nlp field that has yielded numerous services gaining popularity in the semantic web community for extracting knowledge from web documents. <eos> these services are generally organized as pipelines, using dedicated apis and different taxonomy for extracting, classifying and disambiguating named entities. <eos> integrating one of these services in a particular application requires to implement an appropriate driver. <eos> furthermore, the results of these services are not comparable due to different formats. <eos> this prevents the comparison of the performance of these services as well as their possible combination. <eos> we address this problem by proposing nerd, a framework which unifies 10 popular named entity extractors available on the web, and the nerd ontology which provides a rich set of axioms aligning the taxonomies of these tools.
this demo presents information extraction from discharge letters in bulgarian language. <eos> the patient history section is automatically split into episodes ( clauses between two temporal markers ) ; then drugs, diagnoses and conditions are recognised within the episodes with accuracy higher than 90 %. <eos> the temporal markers, which refer to absolute or relative moments of time, are identified with precision 87 % and recall 68 %. <eos> the direction of time for the episode starting point : backwards or forward ( with respect to certain moment orienting the episode ) is recognised with precision 74.4 %.
we present a web tool that allows users to explore news stories concerning the 2012 us presidential elections via an interactive interface. <eos> the tool is based on concepts of ? narrative analysis ?, where the key actors of a narration are identified, along with their relations, in what are sometimes called ? semantic triplets ? <eos> ( one example of a triplet of this kind is ? romney criticised obama ? ). <eos> the network of actors and their relations can be mined for insights about the structure of the narration, including the identification of the key players, of the network of political support of each of them, a representation of the similarity of their political positions, and other information concerning their role in the media narration of events. <eos> the interactive interface allows the users to retrieve news report supporting the relations of interest.
this article describes galateas langlog, a system performing search log analysis. <eos> langlog illustrates how nlp technologies can be a powerful support tool for market research even when the source of information is a collection of queries each one consisting of few words. <eos> we push the standard search log analysis forward taking into account the semantics of the queries. <eos> the main innovation of langlog is the implementation of two highly customizable components that cluster and classify the queries in the log.
data-driven approaches in computational semantics are not common because there are only few semantically annotated resources available. <eos> we are building a large corpus of public-domain english texts and annotate them semi-automatically with syntactic structures ( derivations in combinatory categorial grammar ) and semantic representations ( discourse representation structures ), including events, thematic roles, named entities, anaphora, scope, and rhetorical structure. <eos> we have created a wiki-like web-based platform on which a crowd of expert annotators ( i.e. <eos> linguists ) can log in and adjust linguistic analyses in real time, at various levels of analysis, such as boundaries ( tokens, sentences ) and tags ( part of speech, lexical categories ). <eos> the demo will illustrate the different features of the platform, including navigation, visualization and editing.
we propose a set of open-source software modules to perform structured perceptron training, prediction and evaluation within the hadoop framework. <eos> apache hadoop is a freely available environment for running distributed applications on a computer cluster. <eos> the software is designed within the map-reduce paradigm. <eos> thanks to distributed computing, the proposed software reduces substantially execution times while handling huge data-sets. <eos> the distributed perceptron training algorithm preserves convergence properties, thus guaranties same accuracy performances as the serial perceptron. <eos> the presented modules can be executed as stand-alone software or easily extended or integrated in complex systems. <eos> the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the mapreduce jobs.
we introduce the brat rapid annotation tool ( brat ), an intuitive web-based tool for text annotation supported by natural language processing ( nlp ) technology. <eos> brat has been developed for rich structured annotation for a variety of nlp tasks and aims to support manual curation efforts and increase annotator productivity using nlp techniques. <eos> we discuss several case studies of real-world annotation projects using pre-release versions of brat and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15 % decrease in total annotation time. <eos> brat is available under an opensource license from : http : //brat.nlplab.org
machine translation is a well ? established field, yet the majority of current systems translate sentences in isolation, losing valuable contextual information from previously translated sentences in the discourse. <eos> one important type of contextual information concerns who or what a coreferring pronoun corefers to ( i.e., its antecedent ). <eos> languages differ significantly in how they achieve coreference, and awareness of antecedents is important in choosing the correct pronoun. <eos> disregarding a pronoun ? s antecedent in translation can lead to inappropriate coreferring forms in the target text, seriously degrading a reader ? s ability to understand it. <eos> this work assesses the extent to which source-language annotation of coreferring pronouns can improve english ? czech statistical machine translation ( smt ). <eos> as with previous attempts that use this method, the results show little improvement. <eos> this paper attempts to explain why and to provide insight into the factors affecting performance.
classifying text genres across languages can bring the benefits of genre classification to the target language without the costs of manual annotation. <eos> this article introduces the first approach to this task, which exploits text features that can be considered stable genre predictors across languages. <eos> my experiments show this method to perform equally well or better than full text translation combined with monolingual classification, while requiring fewer resources.
adaptive dialogue systems are rapidly becoming part of our everyday lives. <eos> as they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment. <eos> research in this field is currently focused on how to achieve adaptation, and particularly on applying reinforcement learning ( rl ) techniques, so a comparative study of the related methods, such as this, is necessary. <eos> in this work we compare several standard and state of the art online rl algorithms that are used to train the dialogue manager in a dynamic environment, aiming to aid researchers / developers choose the appropriate rl algorithm for their system. <eos> this is the first work, to the best of our knowledge, to evaluate online rl algorithms on the dialogue problem and in a dynamic environment.
myanmar language and script are unique and complex. <eos> up to our knowledge, considerable amount of work has not yet been done in describing myanmar script using formal language theory. <eos> this paper presents manually constructed context free grammar ( cfg ) with ? 111 ? <eos> productions to describe the myanmar syllable structure. <eos> we make our cfg in conformity with the properties of ll ( 1 ) grammar so that we can apply conventional parsing technique called predictive top-down parsing to identify myanmar syllables. <eos> we present myanmar syllable structure according to orthographic rules. <eos> we also discuss the preprocessing step called contraction for vowels and consonant conjuncts. <eos> we make ll ( 1 ) grammar in which ? 1 ? <eos> does not mean exactly one character of lookahead for parsing because of the above mentioned contracted forms. <eos> we use five basic sub syllabic elements to construct cfg and found that all possible syllable combinations in myanmar orthography can be parsed correctly using the proposed grammar.
there are lexical, syntactic, semantic and discourse variations amongst the languages used in various biomedical subdomains. <eos> it is important to recognise such differences and understand that biomedical tools that work well on some subdomains may not work as well on others. <eos> we report here on the semantic variations that occur in the sublanguages of two biomedical subdomains, i.e. <eos> cell biology and pharmacology, at the level of named entity information. <eos> by building a classifier using ratios of named entities as features, we show that named entity information can discriminate between documents from each subdomain. <eos> more specifically, our classifier can distinguish between documents belonging to each subdomain with an accuracy of 91.1 % f-score.
language identification of written text has been studied for several decades. <eos> despite this fact, most of the research is focused on a few most spoken languages, whereas the minor ones are ignored. <eos> the identification of a larger number of languages brings new difficulties that do not occur for a few languages. <eos> these difficulties are causing decreased accuracy. <eos> the objective of this paper is to investigate the sources of such degradation. <eos> in order to isolate the impact of individual factors, 5 different algorithms and 3 different number of languages are used. <eos> the support vector machine algorithm achieved an accuracy of 98 % for 90 languages and the yali algorithm based on a scoring function had an accuracy of 95.4 %. <eos> the yali algorithm has slightly lower accuracy but classifies around 17 times faster and its training is more than 4000 times faster. <eos> three different data sets with various number of languages and sample sizes were prepared to overcome the lack of standardized data sets. <eos> these data sets are now publicly available.
to cluster textual sequence types ( discourse types/modes ) in french texts, k-means algorithm with high-dimensional embeddings and fuzzy clustering algorithm were applied on clauses whose pos ( part-ofspeech ) n-gram profiles were previously extracted. <eos> uni-, bi- and trigrams were used on four 19th century french short stories by maupassant. <eos> for high-dimensional embeddings, power transformations on the chisquared distances between clauses were explored. <eos> preliminary results show that highdimensional embeddings improve the quality of clustering, contrasting the use of biand trigrams whose performance is disappointing, possibly because of feature space sparsity.
in this work i address the challenge of augmenting n-gram language models according to prior linguistic intuitions. <eos> i argue that the family of hierarchical pitman-yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for german compounds. <eos> in an empirical evaluation, the model outperforms the kneser-ney model in terms of perplexity, and achieves preliminary improvements in english-german translation.
this paper is focused on one aspect of sopmi, an unsupervised approach to sentiment vocabulary acquisition proposed by turney ( turney and littman, 2003 ). <eos> the method, originally applied and evaluated for english, is often used in bootstrapping sentiment lexicons for european languages where no such resources typically exist. <eos> in general, so-pmi values are computed from word co-occurrence frequencies in the neighbourhoods of two small sets of paradigm words. <eos> the goal of this work is to investigate how lexeme selection affects the quality of obtained sentiment estimations. <eos> this has been achieved by comparing ad hoc random lexeme selection with two alternative heuristics, based on clustering and svd decomposition of a word co-occurrence matrix, demonstrating superiority of the latter methods. <eos> the work can be also interpreted as sensitivity analysis on so-pmi with regard to paradigm word selection. <eos> the experiments were carried out for polish.
null subjects are non overtly expressed subject pronouns found in pro-drop languages such as italian and spanish. <eos> in this study we quantify and compare the occurrence of this phenomenon in these two languages. <eos> next, we evaluate null subjects ? <eos> translation into french, a ? non prodrop ? <eos> language. <eos> we use the europarl corpus to evaluate two mt systems on their performance regarding null subject translation : its-2, a rule-based system developed at latl, and a statistical system built using the moses toolkit. <eos> then we add a rule-based preprocessor and a statistical post-editor to the its-2 translation pipeline. <eos> a second evaluation of the improved its-2 system shows an average increase of 15.46 % in correct pro-drop translations for italian-french and 12.80 % for spanish-french.
many works ( of both fiction and non-fiction ) span multiple, intersecting narratives, each of which constitutes a story in its own right. <eos> in this work i introduce the task of multiple narrative disentanglement ( mnd ), in which the aim is to tease these narratives apart by assigning passages from a text to the sub-narratives to which they belong. <eos> the motivating example i use is david foster wallace ? s fictional text infinite jest. <eos> i selected this book because it contains multiple, interweaving narratives within its sprawling 1,000-plus pages. <eos> i propose and evaluate a novel unsupervised approach to mnd that is motivated by the theory of narratology. <eos> this method achieves strong empirical results, successfully disentangling the threads in infinite jest and significantly outperforming baseline strategies in doing so.
in conversation, speakers have been shown to entrain, or become more similar to each other, in various ways. <eos> we measure entrainment on eight acoustic features extracted from the speech of subjects playing a cooperative computer game and associate the degree of entrainment with a number of manually-labeled social variables acquired using amazon mechanical turk, as well as objective measures of dialogue success. <eos> we find that male-female pairs entrain on all features, while male-male pairs entrain only on particular acoustic features ( intensity mean, intensity maximum and syllables per second ). <eos> we further determine that entrainment is more important to the perception of female-male social behavior than it is for same-gender pairs, and it is more important to the smoothness and flow of male-male dialogue than it is for female-female or mixedgender pairs. <eos> finally, we find that entrainment is more pronounced when intensity or speaking rate is especially high or low.
argumentative discourse contains not only language expressing claims and evidence, but also language used to organize these claims and pieces of evidence. <eos> differentiating between the two may be useful for many applications, such as those that focus on the content ( e.g., relation extraction ) of arguments and those that focus on the structure of arguments ( e.g., automated essay scoring ). <eos> we propose an automated approach to detecting high-level organizational elements in argumentative discourse that combines a rule-based system and a probabilistic sequence model in a principled manner. <eos> we present quantitative results on a dataset of human-annotated persuasive essays, and qualitative analyses of performance on essays and on political debates.
modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. <eos> for example, the model of denero and klein ( 2010 ) uses an itg constraint and beam-based viterbi decoding for tractability, but is still slow. <eos> we first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. <eos> we then consider a more flexible, non-itg matching constraint which is less efficient for exact inference but more efficient for bp. <eos> with this new constraint, we achieve a relative error reduction of 40 % in f5 and a 5.5x speed-up.
the use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. <eos> for lack of sufficient training data, most models only consider a small amount of context. <eos> as a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. <eos> in order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a soul architecture. <eos> in small scale and large scale english to french experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.
arabic dialects present many challenges for machine translation, not least of which is the lack of data resources. <eos> we use crowdsourcing to cheaply and quickly build levantineenglish and egyptian-english parallel corpora, consisting of 1.1m words and 380k words, respectively. <eos> the dialectal sentences are selected from a large corpus of arabic web text, and translated using amazon ? s mechanical turk. <eos> we use this data to build dialectal arabic mt systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. <eos> when translating egyptian and levantine test sets, our dialectal arabic mt system performs 6.3 and 7.0 bleu points higher than a modern standard arabic mt system trained on a 150m-word arabic-english parallel corpus.
standard entity clustering systems commonly rely on mention ( string ) matching, syntactic features, and linguistic resources like english wordnet. <eos> when co-referent text mentions appear in different languages, these techniques can not be easily applied. <eos> consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters. <eos> our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures. <eos> crucially, we do not assume a pre-existing entity list ( knowledge base ), so entity characteristics are unknown. <eos> on an arabic-english corpus that contains seven different text genres, our best model yields a 24.3 % f1 gain over the baseline.
this paper addresses the extraction of event records from documents that describe multiple events. <eos> specifically, we aim to identify the fields of information contained in a document and aggregate together those fields that describe the same event. <eos> to exploit the inherent connections between field extraction and event identification, we propose to model them jointly. <eos> our model is novel in that it integrates information from separate sequential models, using global potentials that encourage the extracted event records to have desired properties. <eos> while the model contains high-order potentials, efficient approximate inference can be performed with dualdecomposition. <eos> we experiment with two data sets that consist of newspaper articles describing multiple terrorism events, and show that our model substantially outperforms traditional pipeline models.
a citing sentence is one that appears in a scientific article and cites previous work. <eos> citing sentences have been studied and used in many applications. <eos> for example, they have been used in scientific paper summarization, automatic survey generation, paraphrase identification, and citation function classification. <eos> citing sentences that cite multiple papers are common in scientific writing. <eos> this observation should be taken into consideration when using citing sentences in applications. <eos> for instance, when a citing sentence is used in a summary of a scientific paper, only the fragments of the sentence that are relevant to the summarized paper should be included in the summary. <eos> in this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. <eos> our methods are : word classification, sequence labeling, and segment classification. <eos> our experiments show that segment classification achieves the best results.
we present a model for detecting user disengagement during spoken dialogue interactions. <eos> intrinsic evaluation of our model ( i.e., with respect to a gold standard ) yields results on par with prior work. <eos> however, since our goal is immediate implementation in a system that already detects and adapts to user uncertainty, we go further than prior work and present an extrinsic evaluation of our model ( i.e., with respect to the real-world task ). <eos> correlation analyses show crucially that our automatic disengagement labels correlate with system performance in the same way as the gold standard ( manual ) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. <eos> our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely.
most previous research on automated speech scoring has focused on restricted, predictable speech. <eos> for automated scoring of unrestricted spontaneous speech, speech proficiency has been evaluated primarily on aspects of pronunciation, fluency, vocabulary and language usage but not on aspects of content and topicality. <eos> in this paper, we explore features representing the accuracy of the content of a spoken response. <eos> content features are generated using three similarity measures, including a lexical matching method ( vector space model ) and two semantic similarity measures ( latent semantic analysis and pointwise mutual information ). <eos> all of the features exhibit moderately high correlations with human proficiency scores on human speech transcriptions. <eos> the correlations decrease somewhat due to recognition errors when evaluated on the output of an automatic speech recognition system ; however, the additional use of word confidence scores can achieve correlations at a similar level as for human transcriptions.
this study aims to infer the social nature of conversations from their content automatically. <eos> to place this work in context, our motivation stems from the need to understand how social disengagement affects cognitive decline or depression among older adults. <eos> for this purpose, we collected a comprehensive and naturalistic corpus comprising of all the incoming and outgoing telephone calls from 10 subjects over the duration of a year. <eos> as a first step, we learned a binary classifier to filter out business related conversation, achieving an accuracy of about 85 %. <eos> this classification task provides a convenient tool to probe the nature of telephone conversations. <eos> we evaluated the utility of openings and closing in differentiating personal calls, and find that empirical results on a large corpus do not support the hypotheses by schegloff and sacks that personal conversations are marked by unique closing structures. <eos> for classifying different types of social relationships such as family vs other, we investigated features related to language use ( entropy ), hand-crafted dictionary ( liwc ) and topics learned using unsupervised latent dirichlet models ( lda ). <eos> our results show that the posteriors over topics from lda provide consistently higher accuracy ( 60-81 % ) compared to liwc or language use features in distinguishing different types of conversations.
conditional random fields ( crfs ) are a popular formalism for structured prediction in nlp. <eos> it is well known how to train crfs with certain topologies that admit exact inference, such as linear-chain crfs. <eos> some nlp phenomena, however, suggest crfs with more complex topologies. <eos> should such models be used, considering that they make exact inference intractable ? <eos> stoyanov et al ( 2011 ) recently argued for training parameters to minimize the task-specific loss of whatever approximate inference and decoding methods will be used at test time. <eos> we apply their method to three nlp problems, showing that ( i ) using more complex crfs leads to improved performance, and that ( ii ) minimumrisk training learns more accurate models.
most existing theory of structured prediction assumes exact inference, which is often intractable in many practical problems. <eos> this leads to the routine use of approximate inference such as beam search but there is not much theory behind it. <eos> based on the structured perceptron, we propose a general framework of ? violation-fixing ? <eos> perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions. <eos> this framework subsumes and justifies the popular heuristic ? early-update ? <eos> for perceptron with beam search ( collins and roark, 2004 ). <eos> we also propose several new update methods within this framework, among which the ? max-violation ? <eos> method dramatically reduces training time ( by 3 fold as compared to earlyupdate ) on state-of-the-art part-of-speech tagging and incremental parsing systems.
we propose a new segmentation evaluation metric, called segmentation similarity ( s ), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. <eos> we propose several adapted inter-annotator agreement coefficients which use s that are suitable for segmentation. <eos> we show that s is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. <eos> we also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.
we examine evaluation methods for systems that automatically annotate images using cooccurring text. <eos> we compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. <eos> some of our baselines match or exceed the best published scores for those datasets. <eos> these results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. <eos> we conclude with a list of recommended practices for future research combining language and vision processing techniques.
we propose to re-examine the hypothesis that automated metrics developed for mt evaluation can prove useful for paraphrase identification in light of the significant work on the development of new mt metrics over the last 4 years. <eos> we show that a meta-classifier trained using nothing but recent mt metrics outperforms all previous paraphrase identification approaches on the microsoft research paraphrase corpus. <eos> in addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. <eos> finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on mt metrics. <eos> we release both the new dataset and the error analysis annotations for use by the community.
as interest grows in the use of linguistically annotated corpora in research and teaching of foreign languages and literature, treebanks of various historical texts have been developed. <eos> we introduce the first large-scale dependency treebank for classical chinese literature. <eos> derived from the stanford dependency types, it consists of over 32k characters drawn from a collection of poems written in the 8th century ce. <eos> we report on the design of new dependency relations, discuss aspects of the annotation process and evaluation, and illustrate its use in a study of parallelism in classical chinese poetry.
we propose a new shared task on grading student answers with the goal of enabling welltargeted and flexible feedback in a tutorial dialogue setting. <eos> we provide an annotated corpus designed for the purpose, a precise specification for a prediction task and an associated evaluation methodology. <eos> the task is feasible but non-trivial, which is demonstrated by creating and comparing three alternative baseline systems. <eos> we believe that this corpus will be of interest to the researchers working in textual entailment and will stimulate new developments both in natural language processing in tutorial dialogue systems and textual entailment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks.
in a large-scale study of how people find topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel. <eos> we analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns. <eos> the results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks ? <eos> places where most prominent topic shifts occur. <eos> we recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks. <eos> we propose to account for this in a simple modification of the windowdiff metric. <eos> we discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis.
this paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. <eos> we review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation. <eos> instead, most have implicit connections to particular forms of ramp loss. <eos> we propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. <eos> most notably, our structured ramp loss minimization algorithm, rampion, is less sensitive to initialization and random seeds than standard approaches.
we propose an algorithm to find the best path through an intersection of arbitrarily many weighted automata, without actually performing the intersection. <eos> the algorithm is based on dual decomposition : the automata attempt to agree on a string by communicating about features of the string. <eos> we demonstrate the algorithm on the steiner consensus string problem, both on synthetic data and on consensus decoding for speech recognition. <eos> this involves implicitly intersecting up to 100 automata.
we present an online learning algorithm for statistical machine translation ( smt ) based on stochastic gradient descent ( sgd ). <eos> under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that can not be linearly decomposed into a sentence-wise loss, such as bleu. <eos> we propose a variant of sgd with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. <eos> learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. <eos> experiments on the nist chinese-to-english open mt task indicate significantly better translation results.
a tree transformation is sensible if the size of each output tree is uniformly bounded by a linear function in the size of the corresponding input tree. <eos> every sensible tree transformation computed by an arbitrary weighted extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer. <eos> this further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation.
the important mass of textual documents is in perpetual growth and requires strong applications to automatically process information. <eos> automatic titling is an essential task for several applications : ? no subject ? <eos> e-mails titling, text generation, summarization, and so forth. <eos> this study presents an original approach consisting in titling journalistic articles by nominalizing. <eos> in particular, morphological and semantic processing are employed to obtain a nominalized form which has to respect titles characteristics ( in particular, relevance and catchiness ). <eos> the evaluation of the approach, described in the paper, indicates that titles stemming from this method are informative and/or catchy.
while the field of grammatical error detection has progressed over the past few years, one area of particular difficulty for both native and non-native learners of english, comma placement, has been largely ignored. <eos> we present a system for comma error correction in english that achieves an average of 89 % precision and 25 % recall on two corpora of unedited student essays. <eos> this system also achieves state-of-theart performance in the sister task of restoring commas in well-formed text. <eos> for both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work.
we apply combinatory categorial grammar to wide-coverage parsing in chinese with the new chinese ccgbank, bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent. <eos> we train two state-of-the-art english ? ? ? <eos> parsers : the parser of petrov and klein ( p & k ), and the clark and curran ( c & c ) parser, uncovering a surprising performance gap between them not observed in english ? <eos> 72.73 ( p & k ) and 67.09 ( c & c ) f -score on ? ? ? ? <eos> 6. <eos> we explore the challenges of chinese ? ? ? <eos> parsing through three novel ideas : developing corpus variants rather than treating the corpus as fixed ; controlling noun/verb and other ? ? ? <eos> ambiguities ; and quantifying the impact of constructions like pro-drop.
we investigate the problem of automatically converting from a dependency representation to a phrase structure representation, a key aspect of understanding the relationship between these two representations for nlp work. <eos> we implement a new approach to this problem, based on a small number of supertags, along with an encoding of some of the underlying principles of the penn treebank guidelines. <eos> the resulting system significantly outperforms previous work in such automatic conversion. <eos> we also achieve comparable results to a system using a phrase-structure parser for the conversion. <eos> a comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing.
we propose a linguistically motivated set of features to capture morphological agreement and add them to the mstparser dependency parser. <eos> compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. <eos> we find increases in accuracy of up to 5.3 % absolute. <eos> while some of this results from the feature set capturing information unrelated to morphology, there is still significant improvement, up to 4.6 % absolute, due to the agreement model.
we present an approach to automatically recover hidden attributes of scientific articles, such as whether the author is a native english speaker, whether the author is a male or a female, and whether the paper was published in a conference or workshop proceedings. <eos> we train classifiers to predict these attributes in computational linguistics papers. <eos> the classifiers perform well in this challenging domain, identifying non-native writing with 95 % accuracy ( over a baseline of 67 % ). <eos> we show the benefits of using syntactic features in stylometry ; syntax leads to significant improvements over bag-of-words models on all three tasks, achieving 10 % to 25 % relative error reduction. <eos> we give a detailed analysis of which words and syntax most predict a particular attribute, and we show a strong correlation between our predictions and a paper ? s number of citations.
first story detection ( fsd ) involves identifying first stories about events from a continuous stream of documents. <eos> a major problem in this task is the high degree of lexical variation in documents which makes it very difficult to detect stories that talk about the same event but expressed using different words. <eos> we suggest using paraphrases to alleviate this problem, making this the first work to use paraphrases for fsd. <eos> we show a novel way of integrating paraphrases with locality sensitive hashing ( lsh ) in order to obtain an efficient fsd system that can scale to very large datasets. <eos> our system achieves state-of-the-art results on the first story detection task, beating both the best supervised and unsupervised systems. <eos> to test our approach on large data, we construct a corpus of events for twitter, consisting of 50 million documents, and show that paraphrasing is also beneficial in this domain.
we introduce a method for learning to predict text completion given a source text and partial translation. <eos> in our approach, predictions are offered aimed at alleviating users ? <eos> burden on lexical and grammar choices, and improving productivity. <eos> the method involves learning syntax-based phraseology and translation equivalents. <eos> at run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. <eos> we present a prototype writing assistant, transahead, that applies the method to computer-assisted translation and language learning. <eos> the preliminary results show that the method has great potentials in cat and call with significant improvement in translation quality across users.
we present a classifier that discriminates between types of corrections made by teachers of english in student essays. <eos> we define a set of linguistically motivated feature templates for a log-linear classification model, train this classifier on sentence pairs extracted from the cambridge learner corpus, and achieve 89 % accuracy improving upon a 33 % baseline. <eos> furthermore, we incorporate our classifier into a novel application that takes as input a set of corrected essays that have been sentence aligned with their originals and outputs the individual corrections classified by error type. <eos> we report the f-score of our implementation on this task.
we introduce a new segmentation evaluation measure, winpr, which resolves some of the limitations of windowdiff. <eos> winpr distinguishes between false positive and false negative errors ; produces more intuitive measures, such as precision, recall, and f-measure ; is insensitive to window size, which allows us to customize near miss sensitivity ; and is based on counting errors not windows, but still provides partial reward for near misses.
motivated by the fact that the pronunciation of a name may be influenced by its language of origin, we present methods to improve pronunciation prediction of proper names using word origin information. <eos> we train grapheme-to-phoneme ( g2p ) models on language-specific data sets and interpolate the outputs. <eos> we perform experiments on us surnames, a data set where word origin variation occurs naturally. <eos> our methods can be used with any g2p algorithm that outputs posterior probabilities of phoneme sequences for a given word.
we evaluate the performance of an morphological analyser for inuktitut across a mediumsized corpus, where it produces a useful analysis for two out of every three types. <eos> we then compare its segmentation to that of simpler approaches to morphology, and use these as a pre-processing step to a word alignment task. <eos> our observations show that the richer approaches provide little as compared to simply finding the head, which is more in line with the particularities of the task.
this paper proposes an improved approach to extractive summarization of spoken multi-party interaction, in which integrated random walk is performed on a graph constructed on topical/ lexical relations. <eos> each utterance is represented as a node of the graph, and the edges ? <eos> weights are computed from the topical similarity between the utterances, evaluated using probabilistic latent semantic analysis ( plsa ), and from word overlap. <eos> we model intra-speaker topics by partially sharing the topics from the same speaker in the graph. <eos> in this paper, we perform experiments on automatically and manually generated transcripts. <eos> for automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances.
we report on a pilot experiment to improve the performance of an automatic speech recognizer ( asr ) by using a single-channel eeg signal to classify the speaker ? s mental state as reading easy or hard text. <eos> we use a previously published method ( mostow et al., 2011 ) to train the eeg classifier. <eos> we use its probabilistic output to control weighted interpolation of separate language models for easy and difficult reading. <eos> the eeg-adapted asr achieves higher accuracy than two baselines. <eos> we analyze how its performance depends on eeg classification accuracy. <eos> this pilot result is a step towards improving asr more generally by using eeg to distinguish mental states.
we investigate a language model that combines morphological and shape features with a kneser-ney model and test it in a large crosslingual study of european languages. <eos> even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the europarl corpus, ranging from 3 % to 11 %. <eos> we show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency.
sequential transduction tasks, such as grapheme-to-phoneme conversion and machine transliteration, are usually addressed by inducing models from sets of input-output pairs. <eos> supplemental representations offer valuable additional information, but incorporating that information is not straightforward. <eos> we apply a unified reranking approach to both grapheme-to-phoneme conversion and machine transliteration demonstrating substantial accuracy improvements by utilizing heterogeneous transliterations and transcriptions of the input word. <eos> we describe several experiments that involve a variety of supplemental data and two state-of-the-art transduction systems, yielding error rate reductions ranging from 12 % to 43 %. <eos> we further apply our approach to system combination, with error rate reductions between 4 % and 9 %.
in this paper we present a fully unsupervised nonparametric bayesian model that jointly induces pos tags and morphological segmentations. <eos> the model is essentially an infinite hmm that infers the number of states from data. <eos> incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step. <eos> we show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. <eos> the evaluation on multilingual data shows that the model produces state-of-the-art results on pos induction.
it has long been observed that monolingual text exhibits a tendency toward ? one sense per discourse, ? <eos> and it has been argued that a related ? one translation per discourse ? <eos> constraint is operative in bilingual contexts as well. <eos> in this paper, we introduce a novel method using forced decoding to confirm the validity of this constraint, and we demonstrate that it can be exploited in order to improve machine translation quality. <eos> three ways of incorporating such a preference into a hierarchical phrase-based mt model are proposed, and the approach where all three are combined yields the greatest improvements for both arabic-english and chineseenglish translation experiments.
there has been a proliferation of recent work on smt tuning algorithms capable of handling larger feature sets than the traditional mert approach. <eos> we analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a structured svm. <eos> we perform empirical comparisons of eight different tuning strategies, including mert, in a variety of settings. <eos> among other results, we find that a simple and efficient batch version of mira performs at least as well as training online, and consistently outperforms other options.
in a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. <eos> in this work, we address the problem of incremental speech-to-speech translation ( s2s ) that enables cross-lingual communication between two remote participants over a telephone. <eos> we investigate the problem in a novel real-time session initiation protocol ( sip ) based s2s framework. <eos> the speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. <eos> we describe the statistical models comprising the s2s system and the sip architecture for enabling real-time two-way cross-lingual dialog. <eos> we present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation. <eos> experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach.
we present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. <eos> while most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a compositional grammar of time expressions. <eos> this grammar is used to construct a latent parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. <eos> in this way, we can employ a loosely supervised em-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework. <eos> we achieve an accuracy of 72 % on an adapted tempeval-2 task ? <eos> comparable to state of the art systems.
negated statements often carry positive implicit meaning. <eos> regardless of the semantic representation one adopts, pinpointing the positive concepts within a negated statement is needed in order to encode the statement ? s meaning. <eos> in this paper, novel ideas to reveal positive implicit meaning using focus of negation are presented. <eos> the concept of granularity of focus is introduced and justified. <eos> new annotation and features to detect fine-grained focus are discussed and results reported.
this paper presents a novel approach for inducing lexical taxonomies automatically from text. <eos> we recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. <eos> our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a monte carlo sampling algorithm. <eos> essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. <eos> we use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms.
it has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. <eos> while previous work has focused primarily on english, we extend these results to other languages along two dimensions. <eos> first, we show that these results hold true for a number of languages across families. <eos> second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. <eos> specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on english treebanks and transferred to foreign languages, can be reduced by up to 13 %. <eos> when applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26 %.
we introduce lightly supervised learning for dependency parsing. <eos> in this paradigm, the algorithm is initiated with a parser, such as one that was built based on a very limited amount of fully annotated training data. <eos> then, the algorithm iterates over unlabeled sentences and asks only for a single bit of feedback, rather than a full parse tree. <eos> specifically, given an example the algorithm outputs two possible parse trees and receives only a single bit indicating which of the two alternatives has more correct edges. <eos> there is no direct information about the correctness of any edge. <eos> we show on dependency parsing tasks in 14 languages that with only 1 % of fully labeled data, and light-feedback on the remaining 99 % of the training data, our algorithm achieves, on average, only 5 % lower performance than when training with fully annotated training set. <eos> we also evaluate the algorithm in different feedback settings and show its robustness to noise.
coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. <eos> we propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. <eos> our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. <eos> we observe speed-ups of up to two orders of magnitude compared to exhaustive search. <eos> our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.
we present an active learning method for coreference resolution that is novel in three respects. <eos> ( i ) it uses bootstrapped neighborhood pooling, which ensures a class-balanced pool even though gold labels are not available. <eos> ( ii ) it employs neighborhood selection, a selection strategy that ensures coverage of both positive and negative links for selected markables. <eos> ( iii ) it is based on a query-by-committee selection strategy in contrast to earlier uncertainty sampling work. <eos> experiments show that this new method outperforms random sampling in terms of both annotation effort and peak performance.
recent exploratory efforts in discourse-level language modeling have relied heavily on calculating pointwise mutual information ( pmi ), which involves significant computation when done over large collections. <eos> prior work has required aggressive pruning or independence assumptions to compute scores on large collections. <eos> we show the method of conditional random sampling, thus far an underutilized technique, to be a space-efficient means of representing the sufficient statistics in discourse that underly recent pmi-based work. <eos> this is demonstrated in the context of inducing shankian script-like structures over news articles.
we analyze overt displays of power ( odps ) in written dialogs. <eos> we present an email corpus with utterances annotated for odp and present a supervised learning system to predict it. <eos> we obtain a best cross validation f-measure of 65.8 using gold dialog act features and 55.6 without using them.
this paper describes our ongoing work on resolving third person pronouns and deictic words in a multi-modal corpus. <eos> we show that about two thirds of these referring expressions have antecedents that are introduced by pointing gestures or by haptic-ostensive actions ( actions that involve manipulating an object ). <eos> after describing our annotation scheme, we discuss the co-reference models we learn from multi-modal features. <eos> the usage of hapticostensive actions in a co-reference model is a novel contribution of our work.
in the area of machine translation ( mt ) system combination, previous work on generating input hypotheses has focused on varying a core aspect of the mt system, such as the decoding algorithm or alignment algorithm. <eos> in this paper, we propose a new method for generating diverse hypotheses from a single mt system using traits. <eos> these traits are simple properties of the mt output such as ? average output length ? <eos> and ? average rule length. ? <eos> our method is designed to select hypotheses which vary in trait value but do not significantly degrade in bleu score. <eos> these hypotheses can be combined using standard system combination techniques to produce a 1.21.5 bleu gain on the arabic-english nist mt06/mt08 translation task.
shallow-n grammars ( de gispert et al, 2010 ) were introduced to reduce over-generation in the hiero translation model ( chiang, 2005 ) resulting in much faster decoding and restricting reordering to a desired level for specific language pairs. <eos> however, shallow-n grammars require parameters which can not be directly optimized using minimum error-rate tuning by the decoder. <eos> this paper introduces some novel improvements to the translation model for shallow-n grammars. <eos> we introduce two rules : a bitg-style reordering glue rule and a simpler monotonic concatenation rule. <eos> we use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. <eos> we show this formulation of shallow-n hierarchical phrasebased translation is comparable in translation quality to full hiero-style decoding ( without shallow rules ) while at the same time being considerably faster.
we present a novel method to detect parallel fragments within noisy parallel corpora. <eos> isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction. <eos> we do this with existing machinery, making use of an existing word alignment model for this task. <eos> we evaluate the quality and utility of the extracted data on large-scale chinese-english and arabic-english translation tasks and show significant improvements over a state-of-the-art baseline.
we propose a tuning method for statistical machine translation, based on the pairwise ranking approach. <eos> hopkins and may ( 2011 ) presented a method that uses a binary classifier. <eos> in this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster.
determining the reading level of children ? s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. <eos> automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. <eos> however, guided reading levels developed by educators operate at a more fine-grained level, with multiple levels corresponding to each grade. <eos> we find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level ; including both sets of features, we find that we can predict the reading level up to 83 % of the time on a small corpus of children ? s books.
this paper introduces a general method to incorporate the lda topic model into text segmentation algorithms. <eos> we show that semantic information added by topic models significantly improves the performance of two wordbased algorithms, namely texttiling and c99. <eos> additionally, we introduce the new topictiling algorithm that is designed to take better advantage of topic information. <eos> we show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.
parallel corpora have applications in many areas of natural language processing, but are very expensive to produce. <eos> much information can be gained from comparable texts, and we present an algorithm which, given any bodies of text in multiple languages, uses existing named entity recognition software and topic detection algorithm to generate pairs of comparable texts without requiring a parallel corpus training phase. <eos> we evaluate the system ? s performance firstly on data from the online newspaper domain, and secondly on wikipedia cross-language links.
this paper describes a user study where humans interactively train automatic text classifiers. <eos> we attempt to replicate previous results using multiple ? average ? <eos> internet users instead of a few domain experts as annotators. <eos> we also analyze user annotation behaviors to find that certain labeling actions have an impact on classifier accuracy, drawing attention to the important role these behavioral factors play in interactive learning systems.
we present a novel method for evaluating grammatical error correction. <eos> the core of our method, which we call maxmatch ( m2 ), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation. <eos> this optimal edit sequence is subsequently scored using f1 measure. <eos> we test our m2 scorer on the helping our own ( hoo ) shared task data and show that our method results in more accurate evaluation for grammatical error correction.
we describe and evaluate several methods for estimating the confidence in the per-edge correctness of a predicted dependency parse. <eos> we show empirically that the confidence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efficiently. <eos> we evaluate our methods on parsing text in 14 languages.
we investigate models for unsupervised learning with concave log-likelihood functions. <eos> we begin with the most well-known example, ibm model 1 for word alignment ( brown et al, 1993 ) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. <eos> we then present concave models for dependency grammar induction and validate them experimentally. <eos> we find our concave models to be effective initializers for the dependency model of klein and manning ( 2004 ) and show that we can encode linguistic knowledge in them for improved performance.
this paper reports on an implementation of a multimodal grammar of speech and co-speech gesture within the lkb/pet grammar engineering environment. <eos> the implementation extends the english resource grammar ( erg, flickinger ( 2000 ) ) with hpsg types and rules that capture the form of the linguistic signal, the form of the gestural signal and their relative timing to constrain the meaning of the multimodal action. <eos> the grammar yields a single parse tree that integrates the spoken and gestural modality thereby drawing on standard semantic composition techniques to derive the multimodal meaning representation. <eos> using the current machinery, the main challenge for the grammar engineer is the nonlinear input : the modalities can overlap temporally. <eos> we capture this by identical speech and gesture token edges. <eos> further, the semantic contribution of gestures is encoded by lexical rules transforming a speech phrase into a multimodal entity of conjoined spoken and gestural semantics.
are word-level affect lexicons useful in detecting emotions at sentence level ? <eos> some prior research finds no gain over and above what is obtained with ngram features ? arguably the most widely used features in text classification. <eos> here, we experiment with two very different emotion lexicons and show that even in supervised settings, an affect lexicon can provide significant gains. <eos> we further show that while ngram features tend to be accurate, they are often unsuitable for use in new domains. <eos> on the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain.
public debate functions as a forum for both expressing and forming opinions, an important aspect of public life. <eos> we present results for automatically classifying posts in online debate as to the position, or stance that the speaker takes on an issue, such as pro or con. <eos> we show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parentpost context alone.
sentiment analysis of citations in scientific papers and articles is a new and interesting problem which can open up many exciting new applications in bibliographic search and bibliometrics. <eos> current work on citation sentiment detection focuses on only the citation sentence. <eos> in this paper, we address the problem of context-enhanced citation sentiment detection. <eos> we present a new citation sentiment corpus which has been annotated to take the dominant sentiment in the entire citation context into account. <eos> we believe that this gold standard is closer to the truth than annotation that looks only at the citation sentence itself. <eos> we then explore the effect of context windows of different lengths on the performance of a stateof-the-art citation sentiment detection system when using this context-enhanced gold standard definition.
microblogging networks serve as vehicles for reaching and influencing users. <eos> predicting whether a message will elicit a user response opens the possibility of maximizing the virality, reach and effectiveness of messages and ad campaigns on these networks. <eos> we propose a discriminative model for predicting the likelihood of a response or a retweet on the twitter network. <eos> the approach uses features derived from various sources, such as the language used in the tweet, the user ? s social network and history. <eos> the feature design process leverages aggregate statistics over the entire social network to balance sparsity and informativeness. <eos> we use real-world tweets to train models and empirically show that they are capable of generating accurate predictions for a large number of tweets.
although first names and nicknames in the united states have been well documented, there has been almost no quantitative analysis on the usage and association of these names amongst themselves. <eos> in this paper we introduce the intelius nickname collection, a quantitative compilation of millions of namenickname associations based on information gathered from billions of public records. <eos> to the best of our knowledge, this is the largest collection of its kind, making it a natural resource for tasks such as coreference resolution, record linkage, named entity recognition, people and expert search, information extraction, demographic and sociological studies, etc. <eos> the collection will be made freely available.
this paper compares a number of recently proposed models for computing context sensitive word similarity. <eos> we clarify the connections between these models, simplify their formulation and evaluate them in a unified setting. <eos> we show that the models are essentially equivalent if syntactic information is ignored, and that the substantial performance differences previously reported disappear to a large extent when these simplified variants are evaluated under identical conditions. <eos> furthermore, our reformulation allows for the design of a straightforward and fast implementation.
noticing that different information sources often provide complementary coverage of word sense and meaning, we propose a simple and yet effective strategy for measuring lexical semantics. <eos> our model consists of a committee of vector space models built on a text corpus, web search results and thesauruses, and measures the semantic word relatedness using the averaged cosine similarity scores. <eos> despite its simplicity, our system correlates with human judgements better or similarly compared to existing methods on several benchmark datasets, including wordsim353.
given a parallel corpus, if two distinct words in language a, a1 and a2, are aligned to the same word b1 in language b, then this might signal that b1 is polysemous, or it might signal a1 and a2 are synonyms. <eos> both assumptions with successful work have been put forward in the literature. <eos> we investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in english, asking how often they mean the same thing when they are : 1. aligned to the same foreign type ; and 2. aligned to different foreign types. <eos> results for french-english and chinese-english parallel corpora show similar behavior : synonymy is only very weakly the more prevalent scenario, where both cases regularly occur.
it is well known that the output quality of statistical machine translation ( smt ) systems increases with more training data. <eos> to obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. <eos> in contrast, we confront this challenge head on using the mapreduce framework. <eos> on a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from english and german wikipedia. <eos> augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline ( 2.39 bleu points in the best case ).
in this paper, we investigate the use of temporal information for improving extractive summarization of historical articles. <eos> our method clusters sentences based on their timestamps and temporal similarity. <eos> each resulting cluster is assigned an importance score which can then be used as a weight in traditional sentence ranking techniques. <eos> temporal importance weighting offers consistent improvements over baseline systems.
natural language generation ( nlg ) systems often use a pipeline architecture for sequential decision making. <eos> recent studies however have shown that treating nlg decisions jointly rather than in isolation can improve the overall performance of systems. <eos> we present a joint learning framework based on hierarchical reinforcement learning ( hrl ) which uses graphical models for surface realisation. <eos> our focus will be on a comparison of bayesian networks and hmms in terms of user satisfaction and naturalness. <eos> while the former perform best in isolation, the latter present a scalable alternative within joint systems.
generating referring expressions has received considerable attention in natural language generation. <eos> in recent years we start seeing deployments of referring expression generators moving away from limited domains with custom-made ontologies. <eos> in this work, we explore the feasibility of using large scale noisy ontologies ( folksonomies ) for open domain referring expression generation, an important task for summarization by re-generation. <eos> our experiments on a fully annotated anaphora resolution training set and a larger, volunteersubmitted news corpus show that existing algorithms are efficient enough to deal with large scale ontologies but need to be extended to deal with undefined values and some measure for information salience.
microblog streams often contain a considerable amount of information about local, regional, national, and global events. <eos> most existing microblog search capabilities are focused on recent happenings and do not provide the ability to search and explore past events. <eos> this paper proposes the problem of structured retrieval of historical event information over microblog archives. <eos> rather than retrieving individual microblog messages in response to an event query, we propose retrieving a ranked list of historical event summaries by distilling high quality event representations using a novel temporal query expansion technique. <eos> the results of an exploratory study carried out over a large archive of twitter messages demonstrates both the value of the microblog event retrieval task and the effectiveness of our proposed search methodologies.
we introduce the social study of bullying to the nlp community. <eos> bullying, in both physical and cyber worlds ( the latter known as cyberbullying ), has been recognized as a serious national health issue among adolescents. <eos> however, previous social studies of bullying are handicapped by data scarcity, while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes. <eos> our main contribution is to present evidence that social media, with appropriate natural language processing techniques, can be a valuable and abundant data source for the study of bullying in both worlds. <eos> we identify several key problems in using such data sources and formulate them as nlp tasks, including text classification, role labeling, sentiment analysis, and topic modeling. <eos> since this is an introductory paper, we present baseline results on these tasks using off-the-shelf nlp solutions, and encourage the nlp community to contribute better models in the future.
existing work in fine-grained sentiment analysis focuses on sentences and phrases but ignores the contribution of individual words and their grammatical connections. <eos> this is because of a lack of both ( 1 ) annotated data at the word level and ( 2 ) algorithms that can leverage syntactic information in a principled way. <eos> we address the first need by annotating articles from the information technology business press via crowdsourcing to provide training and testing data. <eos> to address the second need, we propose a suffix-tree data structure to represent syntactic relationships between opinion targets and words in a sentence that are opinion-bearing. <eos> we show that a factor graph derived from this data structure acquires these relationships with a small number of word-level features. <eos> we demonstrate that our supervised model performs better than baselines that ignore syntactic features and constraints.
we present novel methods to construct compact natural language lexicons within a graphbased semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data. <eos> to achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in gaussian and entropic pairwise markov networks constructed from labeled and unlabeled data. <eos> sparse measures are desirable for high-dimensional multi-class learning problems such as the induction of labels on natural language types, which typically associate with only a few labels. <eos> compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance.
we present a general framework containing a graded spectrum of expectation maximization ( em ) algorithms called unified expectation maximization ( uem. ) <eos> uem is parameterized by a single parameter and covers existing algorithms like standard em and hard em, constrained versions of em such as constraintdriven learning ( chang et al, 2007 ) and posterior regularization ( ganchev et al, 2010 ), along with a range of new em algorithms. <eos> for the constrained inference step in uem we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and lagrange relaxation algorithms popularized recently in the nlp literature ( ganchev et al, 2008 ; koo et al, 2010 ; rush and collins, 2011 ). <eos> uem is as efficient and easy to implement as standard em. <eos> furthermore, experiments on pos tagging, information extraction, and word-alignment show that often the best performing algorithm in the uem family is a new algorithm that wasn ? t available earlier, exhibiting the benefits of the uem framework.
the accuracy of many natural language processing tasks can be improved by a reranking step, which involves selecting a single output from a list of candidate outputs generated by a baseline system. <eos> we propose a novel family of reranking algorithms based on learning separate low-dimensional embeddings of the task ? s input and output spaces. <eos> this embedding is learned in such a way that prediction becomes a low-dimensional nearest-neighbor search, which can be done computationally efficiently. <eos> a key quality of our approach is that feature engineering can be done separately on the input and output spaces ; the relationship between inputs and outputs is learned automatically. <eos> experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches.
text input aids such as automatic correction systems play an increasingly important role in facilitating fast text entry and efficient communication between text message users. <eos> although these tools are beneficial when they work correctly, they can cause significant communication problems when they fail. <eos> to improve its autocorrection performance, it is important for the system to have the capability to assess its own performance and learn from its mistakes. <eos> to address this, this paper presents a novel task of self-assessment of autocorrection performance based on interactions between text message users. <eos> as part of this investigation, we collected a dataset of autocorrection mistakes from true text message users and experimented with a rich set of features in our self-assessment task. <eos> our experimental results indicate that there are salient cues from the text message discourse that allow systems to assess their own behaviors with high precision.
to build a coreference resolver for a new language, the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques. <eos> however, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages. <eos> to alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution. <eos> experimental results on two target languages demonstrate the promise of our approach.
we investigate the task of medical concept coreference resolution in clinical text using two semi-supervised methods, co-training and multi-view learning with posterior regularization. <eos> by extracting semantic and temporal features of medical concepts found in clinical text, we create conditionally independent data views ; co-training maxent classifiers on this data works almost as well as supervised learning for the task of pairwise coreference resolution of medical concepts. <eos> we also train maxent models with expectation constraints, using posterior regularization, and find that posterior regularization performs comparably to or slightly better than co-training. <eos> we describe the process of semantic and temporal feature extraction and demonstrate our methods on a corpus of case reports from the new england journal of medicine and a corpus of patient narratives obtained from the ohio state university wexner medical center.
not all learning takes place in an educational setting : more and more self-motivated learners are turning to on-line text to learn about new topics. <eos> our goal is to provide such learners with the well-known benefits of testing by automatically generating quiz questions for online text. <eos> prior work on question generation has focused on the grammaticality of generated questions and generating effective multiple-choice distractors for individual question targets, both key parts of this problem. <eos> our work focuses on the complementary aspect of determining what part of a sentence we should be asking about in the first place ; we call this ? gap selection. ? <eos> we address this problem by asking human judges about the quality of questions generated from a wikipedia-based corpus, and then training a model to effectively replicate these judgments. <eos> our data shows that good gaps are of variable length and span all semantic roles, i.e., nouns as well as verbs, and that a majority of good questions do not focus on named entities. <eos> our resulting system can generate fill-in-the-blank ( cloze ) questions from generic source materials.
concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. <eos> we present a joint model that captures content selection ( ? what to say ? ) <eos> and surface realization ( ? how to say ? ) <eos> in an unsupervised domain-independent fashion. <eos> rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input ( a corpus of database records and text describing some of them ). <eos> we represent our grammar compactly as a weighted hypergraph and recast generation as the task of finding the best derivation tree for a given input. <eos> experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.
when people describe a scene, they often include information that is not visually apparent ; sometimes based on background knowledge, sometimes to tell a story. <eos> we aim to separate visual text ? descriptions of what is being seen ? from non-visual text in natural images and their descriptions. <eos> to do so, we first concretely define what it means to be visual, annotate visual text and then develop algorithms to automatically classify noun phrases as visual or non-visual. <eos> we find that using text alone, we are able to achieve high accuracies at this task, and that incorporating features derived from computer vision algorithms improves performance. <eos> finally, we show that we can reliably mine visual nouns and adjectives from large corpora and that we can use these effectively in the classification task.
we propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. <eos> words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the softk-means algorithm. <eos> in addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages wordnet for evaluation. <eos> by comparing our induced clusters to reference clusters generated from wordnet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. <eos> finally, we describe a method for annotating clusters with usage examples.
with a few exceptions, extensions to latent dirichlet alocation ( lda ) have focused on the distribution over topics for each document. <eos> much less attention has been given to the underlying structure of the topics themselves. <eos> as a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in the form of multinomial distributions over the vocabulary. <eos> in this paper, we introduce the shared components topic model ( sctm ), in which each topic is a normalized product of a smaller number of underlying component distributions. <eos> our model learns these component distributions and the structure of how to combine subsets of them into topics. <eos> the sctm can represent topics in a much more compact representation than lda and achieves better perplexity with fewer parameters.
a u.s. congressional bill is a textual artifact that must pass through a series of hurdles to become a law. <eos> in this paper, we focus on one of the most precarious and least understood stages in a bill ? s life : its consideration, behind closed doors, by a congressional committee. <eos> we construct predictive models of whether a bill will survive committee, starting with a strong, novel baseline that uses features of the bill ? s sponsor and the committee it is referred to. <eos> we augment the model with information from the contents of bills, comparing different hypotheses about how a committee decides a bill ? s fate. <eos> these models give significant reductions in prediction error and highlight the importance of bill substance in explanations of policy-making and agenda-setting.
effective knowledge management is a key factor in the development and success of any organisation. <eos> many different methods have been devised to address this need. <eos> applying these methods to identify the experts within an organisation has attracted a lot of attention. <eos> we look at one such problem that arises within universities on a daily basis but has attracted little attention in the literature, namely the problem of a searcher who is trying to identify a potential phd supervisor, or, from the perspective of the university ? s research office, to allocate a phd application to a suitable supervisor. <eos> we reduce this problem to identifying a ranked list of experts for a given query ( representing a research area ). <eos> we report on experiments to find experts in a university domain using two different methods to extract a ranked list of candidates : a database-driven method and a data-driven method. <eos> the first one is based on a fixed list of experts ( e.g. <eos> all members of academic staff ) while the second method is based on automatic named-entity recognition ( ner ). <eos> we use a graded weighting based on proximity between query and candidate name to rank the list of candidates. <eos> as a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents.
we introduce the automatic annotation of noun phrases in parsed sentences with tags from a fine-grained semantic animacy hierarchy. <eos> this information is of interest within lexical semantics and has potential value as a feature in several nlp tasks. <eos> we train a discriminative classifier on an annotated corpus of spoken english, with features capturing each noun phrase ? s constituent words, its internal structure, and its syntactic relations with other key words in the sentence. <eos> only the first two of these three feature sets have a substantial impact on performance, but the resulting model is able to fairly accurately classify new data from that corpus, and shows promise for binary animacy classification and for use on automatically parsed text.
the preferred order of pre-nominal adjectives in english is determined primarily by semantics. <eos> nevertheless, adjective ordering ( ao ) systems do not generally exploit semantic features. <eos> this paper describes a system that orders adjectives with significantly abovechance accuracy ( 73.0 % ) solely on the basis of semantic features pertaining to the cognitive-semantic dimension of subjectivity. <eos> the results indicate that combining such semantic approaches with current methods could result in more accurate and robust ao systems.
this paper discusses a method for identifying diabetes symptoms and conditions in free text electronic health records in bulgarian. <eos> the main challenge is to automatically recognise phrases and paraphrases for which no ? canonical forms ? <eos> exist in any dictionary. <eos> the focus is on extracting blood sugar level and body weight change which are some of the dominant factors when diagnosing diabetes. <eos> a combined machine-learning and rule-based approach is applied. <eos> the experiment is performed on 2031 sentences of diabetes case history. <eos> the f-measure varies between 60 and 96 % in the separate processing phases.
this paper seeks to quantitatively evaluate the degree to which a number of popular metrics provide overlapping information to parser designers. <eos> two routine tasks are considered : optimizing a machine learning regularization parameter and selecting an optimal machine learning feature set. <eos> the main result is that the choice of evaluation metric used to optimize these problems ( with one exception among popular metrics ) has little effect on the solution to the optimization.
semantic relatedness, or its inverse, semantic distance, measures the degree of closeness between two pieces of text determined by their meaning. <eos> related work typically measures semantics based on a sparse knowledge base such as wordnet1 or cyc that requires intensive manual efforts to build and maintain. <eos> other work is based on the brown corpus, or more recently, wikipedia. <eos> wikipediabased measures, however, typically do not take into account the rapid growth of that resource, which exponentially increases the time to prepare and query the knowledge base. <eos> furthermore, the generalized knowledge domain may be difficult to adapt to a specific domain. <eos> to address these problems, this paper proposes a domain-specific semantic relatedness measure based on part of wikipedia that analyzes course descriptions to suggest whether a course can be transferred from one institution to another. <eos> we show that our results perform well when compared to previous work.
ology-based approaches to representing speech transcripts for automated speech scoring miao chen school of information studies syracuse university syracuse, ny 13244, usa mchen14 @ syr.edu abstract this paper presents a thesis proposal on ap-proaches to automatically scoring non-native speech from second language tests. <eos> current speech scoring systems assess speech by pri-marily using acoustic features such as fluency and pronunciation ; however content features are barely involved. <eos> motivated by this limita-tion, the study aims to investigate the use of content features in speech scoring systems. <eos> for content features, a central question is how speech content can be represented in appro-priate means to facilitate automated speech scoring. <eos> the study proposes using ontology-based representation to perform concept level representation on speech transcripts, and fur-thermore the content features computed from ontology-based representation may facilitate speech scoring. <eos> one baseline and two ontolo-gy-based representations are compared in ex-periments. <eos> preliminary results show that ontology-based representation slightly im-proves performance of one content feature for automated scoring over the baseline system.
statistical natural language processing ( nlp ) builds models of language based on statistical features extracted from the input text. <eos> we investigate deep learning methods for unsupervised feature learning for nlp tasks. <eos> recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results. <eos> in this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classifiers. <eos> we also hypothesise that modelling long-range dependencies in the input and ( separately ) in the output layers would further improve performance. <eos> we suggest methods for overcoming these limitations, which will form part of our final thesis work.
to date, researchers have proposed different ways to compute the readability and coherence of a text using a variety of lexical, syntax, entity and discourse properties. <eos> but these metrics have not been defined with special relevance to any particular genre but rather proposed as general indicators of writing quality. <eos> in this thesis, we propose and evaluate novel text quality metrics that utilize the unique properties of different genres. <eos> we focus on three genres : academic publications, news articles about science, and machine generated text, in particular the output from automatic text summarization systems.
we study1 the problem of extracting all possible relations among named entities from unstructured text, a task known as open information extraction ( open ie ). <eos> a state-of-theart open ie system consists of natural language processing tools to identify entities and extract sentences that relate such entities, followed by using text clustering to identify the relations among co-occurring entity pairs. <eos> in particular, we study how the current weighting scheme used for open ie affects the clustering results and propose a term weighting scheme that significantly improves on the state-of-theart in the task of relation extraction both when used in conjunction with the standard tf ? <eos> idf scheme, and also when used as a pruning filter.
much has been written about humor and even sarcasm automatic recognition on twitter. <eos> the task of classifying humorous tweets according to the type of humor has not been confronted so far, as far as we know. <eos> this research is aimed at applying classification and other nlp algorithms to the challenging task of automatically identifying the type and topic of humorous messages on twitter. <eos> to achieve this goal, we will extend the related work surveyed hereinafter, adding different types of humor and characteristics to distinguish between them, including stylistic, syntactic, semantic and pragmatic ones. <eos> we will keep in mind the complex nature of the task at hand, which emanates from the informal language applied in tweets and variety of humor types and styles. <eos> these tend to be remarkably different from the type specific ones recognized in related works. <eos> we will use semi-supervised classifiers on a dataset of humorous tweets driven from different twitter humor groups or funny tweet sites. <eos> using a mechanical turk we will create a gold standard in which each tweet will be tagged by several annotators, in order to achieve an agreement between them, although the nature of the humor might allow one tweet to be classified under more than one class and topic of humor.
source code re-use has become an important problem in academia. <eos> the amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re-use. <eos> we present the desocore tool based on techniques of natural language processing ( nlp ) applied to detect source code re-use. <eos> desocore compares two source codes at the level of methods or functions even when written in different programming languages. <eos> the system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re-used.
in this paper, we present xopin, a graphical user interface that have been developed to provide a smart access to the results of a feature-based opinion detection system, build on top of a parser. <eos>
comment threads contain fascinating and use-ful insights into public reactions, but are chal-lenging to read and understand without computational assistance. <eos> we present a tool for exploring large, community-created com-ments threads in an efficient manner.
at present, online shopping is typically a search-oriented activity where a user gains access to products which best match their query. <eos> instead, we propose a surf-oriented online shopping paradigm, which links associated products allowing users to ? wander around ? <eos> the online store and enjoy browsing a variety of items. <eos> as an initial step in creating this experience, we constructed a prototype of an online shopping interface which combines product ontology information with topic model results to allow users to explore items from the food and kitchen domain. <eos> as a novel task for topic model application, we also discuss possible approaches to the task of selecting the best product categories to illustrate the hidden topics discovered for our product domain.
we demonstrate a conversational humanoid robot that allows users to follow their own dialogue structures. <eos> our system uses a hierarchy of reinforcement learning dialogue agents, which support transitions across sub-dialogues in order to relax the strictness of hierarchical control and therefore support flexible interactions. <eos> we demonstrate our system with the nao robot playing two versions of a quiz game. <eos> whilst language input and dialogue control is autonomous or wizarded, language output is provided by the robot combining verbal and non-verbal contributions. <eos> the novel features in our system are ( a ) the flexibility given to users to navigate flexibly in the interaction ; and ( b ) a framework for investigating adaptive and flexible dialogues.
we describe msr splat, a toolkit for language analysis that allows easy access to the linguistic analysis tools produced by the nlp group at microsoft research. <eos> the tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. <eos> as we expand the tools we develop for our own research, the set of tools available in msr splat will be extended. <eos> the toolkit is accessible as a web service, which can be used from a broad set of programming languages.
this paper presents a demonstration of a temporal reasoning system that addresses three fundamental tasks related to temporal expressions in text : extraction, normalization to time intervals and comparison. <eos> our system makes use of an existing state-of-the-art temporal extraction system, on top of which we add several important novel contributions. <eos> in addition, we demonstrate that our system can perform temporal reasoning by comparing normalized temporal expressions with respect to several temporal relations. <eos> experimental study shows that the system achieves excellent performance on all the tasks we address.
this demonstration presents attitudeminer, a system for mining attitude from online discussions. <eos> attitudeminer uses linguistic techniques to analyze the text exchanged between participants of online discussion threads at different levels of granularity : the word level, the sentence level, the post level, and the thread level. <eos> the goal of this analysis is to identify the polarity of the attitude the discussants carry towards one another. <eos> attitude predictions are used to construct a signed network representation of the discussion thread. <eos> in this network, each discussant is represented by a node. <eos> an edge connects two discussants if they exchanged posts. <eos> the sign ( positive or negative ) of the edge is set based on the polarity of the attitude identified in the text associated with the edge. <eos> the system can be used in different applications such as : word polarity identification, identifying attitudinal sentences and their signs, signed social network extraction from text, subgroup detect in discussion. <eos> the system is publicly available for download and has an online demonstration at http : //clair.eecs.umich.edu/attitudeminer/.
n-gram-based models co-exist with their phrase-based counterparts as an alternative smt framework. <eos> both techniques have pros and cons. <eos> while the n-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation, the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units. <eos> in this paper we combine n-grambased modeling with phrase-based decoding, and obtain the benefits of both approaches. <eos> our experiments show that using this combination not only improves the search accuracy of the n-gram model but that it also improves the bleu scores. <eos> our system outperforms state-of-the-art phrase-based systems ( moses and phrasal ) and n-gram-based systems by a significant margin on german, french and spanish to english translation tasks.
standard phrase-based translation models do not explicitly model context dependence between translation units. <eos> as a result, they rely on large phrase pairs and target language models to recover contextual effects in translation. <eos> in this work, we explore n-gram models over minimal translation units ( mtus ) to explicitly capture contextual dependencies across phrase boundaries in the channel model. <eos> as there is no single best direction in which contextual information should flow, we explore multiple decomposition structures as well as dynamic bidirectional decomposition. <eos> the resulting models are evaluated in an intrinsic task of lexical selection for mt as well as a full mt system, through n-best reranking. <eos> these experiments demonstrate that additional contextual modeling does indeed benefit a phrase-based system and that the direction of conditioning is important. <eos> integrating multiple conditioning orders provides consistent benefit, and the most important directions differ by language pair.
there have been many recent investigations into methods to tune smt systems using large numbers of sparse features. <eos> however, there have not been nearly so many examples of helpful sparse features, especially for phrasebased systems. <eos> we use sparse features to address reordering, which is often considered a weak point of phrase-based translation. <eos> using a hierarchical reordering model as our baseline, we show that simple features coupling phrase orientation to frequent words or wordclusters can improve translation quality, with boosts of up to 1.2 bleu points in chineseenglish and 1.8 in arabic-english. <eos> we compare this solution to a more traditional maximum entropy approach, where a probability model with similar features is trained on wordaligned bitext. <eos> we show that sparse decoder features outperform maximum entropy handily, indicating that there are major advantages to optimizing reordering features directly for bleu with the decoder in the loop.
current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. <eos> we present a two-level alignment model that distinguishes between words and morphemes, in which we embed an ibm model 1 inside an hmm based word alignment model. <eos> the model jointly induces word and morpheme alignments using an em algorithm. <eos> we evaluated our model on turkish-english parallel data. <eos> we obtained significant improvement of bleu scores over ibm model 4. <eos> our results indicate that utilizing information from morphology improves the quality of word alignments.
identifying documents that describe a specific type of event is challenging due to the high complexity and variety of event descriptions. <eos> we propose a multi-faceted event recognition approach, which identifies documents about an event using event phrases as well as defining characteristics of the event. <eos> our research focuses on civil unrest events and learns civil unrest expressions as well as phrases corresponding to potential agents and reasons for civil unrest. <eos> we present a bootstrapping algorithm that automatically acquires event phrases, agent terms, and purpose ( reason ) phrases from unannotated texts. <eos> we use the bootstrapped dictionaries to identify civil unrest documents and show that multi-faceted event recognition can yield high accuracy.
different languages contain complementary cues about entities, which can be used to improve named entity recognition ( ner ) systems. <eos> we propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple integer linear program, which encourages entity tags to agree via bilingual constraints. <eos> bilingual ner experiments on the large ontonotes 4.0 chinese-english corpus show that the proposed method can improve strong baselines for both chinese and english. <eos> in particular, chinese performance improves by over 5 % absolute f1 score. <eos> we can then annotate a large amount of bilingual text ( 80k sentence pairs ) using our method, and add it as uptraining data to the original monolingual ner training corpus. <eos> the chinese model retrained on this new combined dataset outperforms the strong baseline by over 3 % f1 score.
we propose a minimally supervised method for multilingual paraphrase extraction from definition sentences on the web. <eos> hashimoto et al ( 2011 ) extracted paraphrases from japanese definition sentences on the web, assuming that definition sentences defining the same concept tend to contain paraphrases. <eos> however, their method requires manually annotated data and is language dependent. <eos> we extend their framework and develop a minimally supervised method applicable to multiple languages. <eos> our experiments show that our method is comparable to hashimoto et al ? s for japanese and outperforms previous unsupervised methods for english, japanese, and chinese, and that our method extracts 10,000 paraphrases with 92 % precision for english, 82.5 % precision for japanese, and 82 % precision for chinese.
traditional relation extraction predicts relations within some fixed and finite target schema. <eos> machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. <eos> the need for existing datasets can be avoided by using a universal schema : the union of all involved schemas ( surface form predicates as in openie, and relations in the schemas of preexisting databases ). <eos> this schema has an almost unlimited set of relations ( due to surface forms ), and supports integration with existing structured data ( through the relation types of existing databases ). <eos> to populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations. <eos> we show that such latent models achieve substantially higher accuracy than a traditional classification approach. <eos> more importantly, by operating simultaneously on relations observed in text and in pre-existing structured dbs such as freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. <eos> by doing so our approach outperforms stateof-the-art distant supervision.
we develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author. <eos> this method uses multiple corpora to allow for the removal of data set specific patterns, and addresses both feature relevancy and redundancy. <eos> we evaluate different relevancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features. <eos> our feature set is a broad class of syntactic patterns, and to better capture the signal we extend the bayesian tree substitution grammar induction algorithm to a supervised mixture of latent grammars. <eos> we show that this extension can be used to extract a larger set of relevant features.
the frequency of words and syntactic constructions has been observed to have a substantial effect on language processing. <eos> this begs the question of what causes certain constructions to be more or less frequent. <eos> a theory of grounding ( phillips, 2010 ) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. <eos> this paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. <eos> measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing.
we propose a new approach to identifying semantically similar words across languages. <eos> the approach is based on an idea that two words in different languages are similar if they are likely to generate similar words ( which includes both source and target language words ) as their top semantic word responses. <eos> semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word. <eos> the method consists of two main steps : ( 1 ) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, ( 2 ) it provides ranked lists of similar words according to the similarity of their semantic word response vectors. <eos> we evaluate our approach in the task of bilingual lexicon extraction ( ble ) for a variety of language pairs. <eos> we show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics.
humans identify word boundaries in continuous speech by combining multiple cues ; existing state-of-the-art models, though, look at a single cue. <eos> we extend the generative model of goldwater et al ( 2006 ) to segment using syllable stress as well as phonemic form. <eos> our new model treats identification of word boundaries and prevalent stress patterns in the language as a joint inference task. <eos> we show that this model improves segmentation accuracy over purely segmental input representations, and recovers the dominant stress pattern of the data. <eos> additionally, our model retains high performance even without single-word utterances. <eos> we also demonstrate a discrepancy in the performance of our model and human infants on an artificial-language task in which stress cues and transition-probability information are pitted against one another. <eos> we argue that this discrepancy indicates a bound on rationality in the mechanisms of human segmentation.
we consider the problem of training a statistical parser in the situation when there are multiple treebanks available, and these treebanks are annotated according to different linguistic conventions. <eos> to address this problem, we present two simple adaptation methods : the first method is based on the idea of using a shared feature representation when parsing multiple treebanks, and the second method on guided parsing where the output of one parser provides features for a second one. <eos> to evaluate and analyze the adaptation methods, we train parsers on treebank pairs in four languages : german, swedish, italian, and english. <eos> we see significant improvements for all eight treebanks when training on the full training sets. <eos> however, the clearest benefits are seen when we consider smaller training sets. <eos> our experiments were carried out with unlabeled dependency parsers, but the methods can easily be generalized to other featurebased parsers.
most work on weakly-supervised learning for part-of-speech taggers has been based on unrealistic assumptions about the amount and quality of training data. <eos> for this paper, we attempt to create true low-resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages kinyarwanda and malagasy. <eos> given these severely limited amounts of either type supervision ( tag dictionaries ) or token supervision ( labeled sentences ), we are able to dramatically improve the learning of a hidden markov model through our method of automatically generalizing the annotations, reducing noise, and inducing word-tag frequency information.
latent-variable pcfgs ( l-pcfgs ) are a highly successful model for natural language parsing. <eos> recent work ( cohen et al, 2012 ) has introduced a spectral algorithm for parameter estimation of l-pcfgs, which ? unlike the em algorithm ? is guaranteed to give consistent parameter estimates ( it has pac-style guarantees of sample complexity ). <eos> this paper describes experiments using the spectral algorithm. <eos> we show that the algorithm provides models with the same accuracy as em, but is an order of magnitude more efficient. <eos> we describe a number of key steps used to obtain this level of performance ; these should be relevant to other work on the application of spectral learning algorithms. <eos> we view our results as strong empirical evidence for the viability of spectral methods as an alternative to em.
topics generated automatically, e.g. <eos> using lda, are now widely used in computational linguistics. <eos> topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. <eos> we introduce an alternative approach in which topics are represented using images. <eos> candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. <eos> the most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves. <eos> we show that the proposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic.
multi-dimensional latent text models, such as factorial lda ( f-lda ), capture multiple factors of corpora, creating structured output for researchers to better understand the contents of a corpus. <eos> we consider such models for clinical research of new recreational drugs and trends, an important application for mining current information for healthcare workers. <eos> we use a ? three-dimensional ? <eos> f-lda variant to jointly model combinations of drug ( marijuana, salvia, etc. <eos> ), aspect ( effects, chemistry, etc. ) <eos> and route of administration ( smoking, oral, etc. ) <eos> since a purely unsupervised topic model is unlikely to discover these specific factors of interest, we develop a novel method of incorporating prior knowledge by leveraging user generated tags as priors in our model. <eos> we demonstrate that this model can be used as an exploratory tool for learning about these drugs from the web by applying it to the task of extractive summarization. <eos> in addition to providing useful output for this important public health task, our prior-enriched model provides a framework for the application of flda to other tasks.
we propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. <eos> we build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. <eos> we then aggregate those selected phrases by means of phrase generalization and merging. <eos> we motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.
we present a new hierarchical bayesian model for unsupervised topic segmentation. <eos> this new model integrates a point-wise boundary sampling algorithm used in bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents. <eos> we develop an mcmc inference algorithm to split/merge segment ( s ). <eos> experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on choi ? s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets.
the primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. <eos> recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. <eos> in this paper, we describe an improved method for combining partial captions into a final output based on weighted a ? <eos> search and multiple sequence alignment ( msa ). <eos> in contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. <eos> our method outperforms the current state-of-the-art on word error rate ( wer ) ( 29.6 % ), bleu score ( 41.4 % ), and f-measure ( 36.9 % ). <eos> the end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.
automatically assessing the fidelity of a retelling to the original narrative ? <eos> a task of growing clinical importance ? <eos> is challenging, given extensive paraphrasing during retelling along with cascading automatic speech recognition ( asr ) errors. <eos> we present a word tagging approach using conditional random fields ( crfs ) that allows a diversity of features to be considered during inference, including some capturing acoustic confusions encoded in word confusion networks. <eos> we evaluate the approach under several scenarios, including both supervised and unsupervised training, the latter achieved by training on the output of a baseline automatic word-alignment model. <eos> we also adapt the asr models to the domain, and evaluate the impact of error rate on performance. <eos> we find strong robustness to asr errors, even using just the 1-best system output. <eos> a hybrid approach making use of both automatic alignment and crfs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone.
addressee detection ( ad ) is an important problem for dialog systems in human-humancomputer scenarios ( contexts involving multiple people and a system ) because systemdirected speech must be distinguished from human-directed speech. <eos> recent work on ad ( shriberg et al, 2012 ) showed good results using prosodic and lexical features trained on in-domain data. <eos> in-domain data, however, is expensive to collect for each new domain. <eos> in this study we focus on lexical models and investigate how well out-of-domain data ( either outside the domain, or from single-user scenarios ) can fill in for matched in-domain data. <eos> we find that human-addressed speech can be modeled using out-of-domain conversational speech transcripts, and that human-computer utterances can be modeled using single-user data : the resulting ad system outperforms a system trained only on matched in-domain data. <eos> further gains ( up to a 4 % reduction in equal error rate ) are obtained when in-domain and out-of-domain models are interpolated. <eos> finally, we examine which parts of an utterance are most useful. <eos> we find that the first 1.5 seconds of an utterance contain most of the lexical information for ad, and analyze which lexical items convey this. <eos> overall, we conclude that the h-h-c scenario can be approximated by combining data from h-c and h-h scenarios only. <eos> ? work done while first author was an intern with microsoft.
the study presented in this work is a first effort at real-time speech translation of ted talks, a compendium of public talks with different speakers addressing a variety of topics. <eos> we address the goal of achieving a system that balances translation accuracy and latency. <eos> in order to improve asr performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. <eos> in order to improve machine translation ( mt ) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. <eos> we also experiment with inserting text segmenters of various types between asr and mt in a series of real-time translation experiments. <eos> among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. <eos> it was also found to be important to synchronize various pipeline components in order to minimize latency.
treebanks are not large enough to adequately model subcategorization frames of predicative lexemes, which is an important source of lexico-syntactic constraints for parsing. <eos> as a consequence, parsers trained on such treebanks usually make mistakes when selecting the arguments of predicative lexemes. <eos> in this paper, we propose an original way to correct subcategorization errors by combining subparses of a sentence s that appear in the list of the n-best parses of s. the subcategorization information comes from three different resources, the first one is extracted from a treebank, the second one is computed on a large corpora and the third one is an existing syntactic lexicon. <eos> experiments on the french treebank showed a 15.24 % reduction of erroneous subcategorization frames ( sf ) selections for verbs as well as a relative decrease of the error rate of 4 % labeled accuracy score on the state of the art parser on this treebank.
we introduce a new large-scale discriminative learning algorithm for machine translation that is capable of learning parameters in models with extremely sparse features. <eos> to ensure their reliable estimation and to prevent overfitting, we use a two-phase learning algorithm. <eos> first, the contribution of individual sparse features is estimated using large amounts of parallel data. <eos> second, a small development corpus is used to determine the relative contributions of the sparse features and standard dense features. <eos> not only does this two-phase learning approach prevent overfitting, the second pass optimizes corpus-level bleu of the viterbi translation of the decoder. <eos> we demonstrate significant improvements using sparse rule indicator features in three different translation tasks. <eos> to our knowledge, this is the first large-scale discriminative training algorithm capable of showing improvements over the mert baseline with only rule indicator features in addition to the standard mert features.
measuring term informativeness is a fundamental nlp task. <eos> existing methods, mostly based on statistical information in corpora, do not actually measure informativeness of a term with regard to its semantic context. <eos> this paper proposes a new lightweight feature-free approach to encode term informativeness in context by leveraging web knowledge. <eos> given a term and its context, we model contextaware term informativeness based on semantic similarity between the context and the term ? s most featured context in a knowledge base, wikipedia. <eos> we apply our method to three applications : core term extraction from snippets ( text segment ), scientific keywords extraction ( paper ), and back-of-the-book index generation ( book ). <eos> the performance is state-of-theart or close to it for each application, demonstrating its effectiveness and generality.
we here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages. <eos> we evaluate the two approaches in two different information extraction settings : monolingual and cross-lingual information extraction. <eos> the extraction systems are trained on auto-annotated summaries ( containing the induced concepts ) and evaluated on humanannotated documents. <eos> extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries.
language variations are generally known to have a severe impact on the performance of human language technology systems. <eos> in order to predict or improve system performance, a thorough investigation into these variations, similarities and dissimilarities, is required. <eos> distance measures have been used in several applications of speech processing to analyze different varying speech attributes. <eos> however, not much work has been done on language distance measures, and even less work has been done involving south african languages. <eos> this study explores two methods for measuring the linguistic distance of six south african languages. <eos> it concerns a text based method, ( the levenshtein distance ), and an acoustic approach using extracted mean pitch values. <eos> the levenshtein distance uses parallel word transcriptions from all six languages with as little as 144 words, whereas the pitch method is text-independent and compares mean language pitch differences. <eos> cluster analysis resulting from the distance matrices from both methods correlates closely with human perceptual distances and existing literature about the six languages.
we present a new variant of the syntaxaugmented machine translation ( samt ) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars. <eos> we induce bilingual labels into the samt grammar, use them for category coarsening, then project back to monolingual labeling as in standard samt. <eos> the result is a ? collapsed ? <eos> grammar with the same expressive power and format as the original, but many fewer nonterminal labels. <eos> we show that the smaller label set provides improved translation scores by 1.14 bleu on two chinese ? <eos> english test sets while reducing the occurrence of sparsity and ambiguity problems common to large label sets.
multi-sentence compression ( msc ) is the task of generating a short single sentence summary from a cluster of related sentences. <eos> this paper presents an n-best reranking method based on keyphrase extraction. <eos> compression candidates generated by a word graph-based msc approach are reranked according to the number and relevance of keyphrases they contain. <eos> both manual and automatic evaluations were performed using a dataset made of clusters of newswire sentences. <eos> results show that the proposed method significantly improves the informativity of the generated compressions.
this paper describes the annotation process and linguistic properties of the persian syntactic dependency treebank. <eos> the treebank consists of approximately 30,000 sentences annotated with syntactic roles in addition to morpho-syntactic features. <eos> one of the unique features of this treebank is that there are almost 4800 distinct verb lemmas in its sentences making it a valuable resource for educational goals. <eos> the treebank is constructed with a bootstrapping approach by means of available tagging and parsing tools and manually correcting the annotations. <eos> the data is splitted into standard train, development and test set in the conll dependency format and is freely available to researchers.
recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. <eos> this model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the traveling salesman problem ( tsp ). <eos> however, for efficiently solving the tsp, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. <eos> in this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the tsp model using higher order and structural features which help in capturing longer range dependencies. <eos> in addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. <eos> our experiments, involving urdu-english, show that the proposed approach outperforms a state-of-theart pbsmt system which uses the tsp model for reordering by 1.3 bleu points, and a publicly available state-of-the-art mt system, hiero, by 3 bleu points.
translation models in statistical machine translation can be scaled to large corpora and arbitrarily-long phrases by looking up translations of source phrases ? on the fly ? <eos> in an indexed parallel corpus using suffix arrays. <eos> however, this can be slow because on-demand extraction of phrase tables is computationally expensive. <eos> we address this problem by developing novel algorithms for general purpose graphics processing units ( gpus ), which enable suffix array queries for phrase lookup and phrase extraction to be massively parallelized. <eos> compared to a highly-optimized, state-of-the-art serial cpu-based implementation, our techniques achieve at least an order of magnitude improvement in terms of throughput. <eos> this work demonstrates the promise of massively parallel architectures and the potential of gpus for tackling computationallydemanding problems in statistical machine translation and language processing.
until recently, the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters. <eos> in this paper, we propose to scale up discriminative training of ( he and deng, 2012 ) to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. <eos> the experimental results confirm the effectiveness of our proposals on nist mt06 set over a strong baseline.
in statistical machine translation we often have to combine different sources of parallel training data to build a good system. <eos> one way of doing this is to build separate translation models from each data set and linearly interpolate them, and to date the main method for optimising the interpolation weights is to minimise the model perplexity on a heldout set. <eos> in this work, rather than optimising for this indirect measure, we directly optimise for bleu on the tuning set and show improvements in average performance over two data sets and 8 language pairs.
modern standard arabic ( msa ) has a wealth of natural language processing ( nlp ) tools and resources. <eos> in comparison, resources for dialectal arabic ( da ), the unstandardized spoken varieties of arabic, are still lacking. <eos> we present elissa, a machine translation ( mt ) system for da to msa. <eos> elissa employs a rule-based approach that relies on morphological analysis, transfer rules and dictionaries in addition to language models to produce msa paraphrases of da sentences. <eos> elissa can be employed as a general preprocessor for da when using msa nlp tools. <eos> a manual error analysis of elissa ? s output shows that it produces correct msa translations over 93 % of the time. <eos> using elissa to produce msa versions of da sentences as part of an msa-pivoting da-to-english mt solution, improves bleu scores on multiple blind test sets between 0.6 % and 1.4 %.
the rise of social media has brought computational linguistics in ever-closer contact with bad language : text that defies our expectations about vocabulary, spelling, and syntax. <eos> this paper surveys the landscape of bad language, and offers a critical review of the nlp community ? s response, which has largely followed two paths : normalization and domain adaptation. <eos> each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication. <eos> in addition, the paper presents a quantitative analysis of the lexical diversity of social media text, and its relationship to other corpora.
online learning algorithms such as perceptron and mira have become popular for many nlp tasks thanks to their simpler architecture and faster convergence over batch learning methods. <eos> however, while batch learning such as crf is easily parallelizable, online learning is much harder to parallelize : previous efforts often witness a decrease in the converged accuracy, and the speedup is typically very small ( ? 3 ) even with many ( 10+ ) processors. <eos> we instead present a much simpler architecture based on ? mini-batches ?, which is trivially parallelizable. <eos> we show that, unlike previous methods, minibatch learning ( in serial mode ) actually improves the converged accuracy for both perceptron and mira learning, and when combined with simple parallelization, minibatch leads to very significant speedups ( up to 9x on 12 processors ) on stateof-the-art parsing and tagging systems.
we consider the problem of part-of-speech tagging for informal, online conversational text. <eos> we systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. <eos> with these features, our system achieves state-of-the-art tagging results on both twitter and irc pos tagging tasks ; twitter tagging is improved from 90 % to 93 % accuracy ( more than 3 % absolute ). <eos> qualitative analysis of these word clusters yields insights about nlp and linguistic phenomena in this genre. <eos> additionally, we contribute the first pos annotation guidelines for such text and release a new dataset of english language tweets annotated using these guidelines. <eos> tagging software, annotation guidelines, and large-scale word clusters are available at : http : //www.ark.cs.cmu.edu/tweetnlp this paper describes release 0.3 of the ? cmu twitter part-of-speech tagger ? <eos> and annotated data.
we describe a new self-learning framework for parser lexicalisation that requires only a plain-text corpus of in-domain text. <eos> the method first creates augmented versions of dependency graphs by applying a series of modifications designed to directly capture higherorder lexical path dependencies. <eos> scores are assigned to each edge in the graph using statistics from an automatically parsed background corpus. <eos> as bilexical dependencies are sparse, a novel directed distributional word similarity measure is used to smooth edge score estimates. <eos> edge scores are then combined into graph scores and used for reranking the topn analyses found by the unlexicalised parser. <eos> the approach achieves significant improvements on wsj and biomedical text over the unlexicalised baseline parser, which is originally trained on a subset of the brown corpus.
advances in sentiment analysis have enabled extraction of user relations implied in online textual exchanges such as forum posts. <eos> however, recent studies in this direction only consider direct relation extraction from text. <eos> as user interactions can be sparse in online discussions, we propose to apply collaborative filtering through probabilistic matrix factorization to generalize and improve the opinion matrices extracted from forum posts. <eos> experiments with two tasks show that the learned latent factor representation can give good performance on a relation polarity prediction task and improve the performance of a subgroup detection task.
feature and context aggregation play a large role in current ner systems, allowing significant opportunities for research into optimizing these features to cater to different domains. <eos> this work strives to reduce the noise introduced into aggregated features from disparate and generic training data in order to allow for contextual features that more closely model the entities in the target data. <eos> the proposed approach trains models based on only a part of the training set that is more similar to the target domain. <eos> to this end, models are trained for an existing ner system using the top documents from the training set that are similar to the target document in order to demonstrate that this technique can be applied to improve any pre-built ner system. <eos> initial results show an improvement over the university of illinois ne tagger with a weighted average f1 score of 91.67 compared to the illinois tagger ? s score of 91.32. <eos> this research serves as a proof-of-concept for future planned work to cluster the training documents to produce a number of more focused models from a given training set, thereby reducing noise and extracting a more representative feature set.
language can describe our visual world at many levels, including not only what is literally there but also the sentiment that it invokes. <eos> in this paper, we study visual language, both literal and sentimental, that describes the overall appearance and style of virtual characters. <eos> sentimental properties, including labels such as ? youthful ? <eos> or ? country western, ? <eos> must be inferred from descriptions of the more literal properties, such as facial features and clothing selection. <eos> we present a new dataset, collected to describe xbox avatars, as well as models for learning the relationships between these avatars and their literal and sentimental descriptions. <eos> in a series of experiments, we demonstrate that such learned models can be used for a range of tasks, including predicting sentimental words and using them to rank and build avatars. <eos> together, these results demonstrate that sentimental language provides a concise ( though noisy ) means of specifying low-level visual properties.
the many differences between dialectal arabic and modern standard arabic ( msa ) pose a challenge to the majority of arabic natural language processing tools, which are designed for msa. <eos> in this paper, we retarget an existing state-of-the-art msa morphological tagger to egyptian arabic ( arz ). <eos> our evaluation demonstrates that our arz morphology tagger outperforms its msa variant on arz input in terms of accuracy in part-of-speech tagging, diacritization, lemmatization and tokenization ; and in terms of utility for arz-toenglish statistical machine translation.
we present a novel, structured language model - supertagged dependency language model to model the syntactic dependencies between words. <eos> the goal is to identify ungrammatical hypotheses from a set of candidate translations in a mt system combination framework and help select the best translation candidates using a variety of sentence-level features. <eos> we use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. <eos> our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination.
we report the results of our work on automating the transliteration decision of named entities for english to arabic machine translation. <eos> we construct a classification-based framework to automate this decision, evaluate our classifier both in the limited news and the diverse wikipedia domains, and achieve promising accuracy. <eos> moreover, we demonstrate a reduction of translation error and an improvement in the performance of an english-to-arabic machine translation system.
this paper describes an approach to improve summaries for a collection of twitter posts created using the phrase reinforcement ( pr ) algorithm ( sharifi et al, 2010a ). <eos> the pr algorithm often generates summaries with excess text and noisy speech. <eos> we parse these summaries using a dependency parser and use the dependencies to eliminate some of the excess text and build better-formed summaries. <eos> we compare the results to those obtained using the pr algorithm.
this paper presents a general, statistical framework for modeling phrase translation via markov random fields. <eos> the model allows for arbituary features extracted from a phrase pair to be incorporated as evidence. <eos> the parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an n-best list based expected bleu as the objective function. <eos> the model is easy to be incoporated into a standard phrase-based statistical machine translation system, requiring no code change in the runtime engine. <eos> evaluation is performed on two europarl translation tasks, germanenglish and french-english. <eos> results show that incoporating the markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system, leading to a gain of 0.8-1.3 bleu points.
in this paper, we study the problem of automatic enrichment of a morphologically underspecified treebank for arabic, a morphologically rich language. <eos> we show that we can map from a tagset of size six to one with 485 tags at an accuracy rate of 94 % -95 %. <eos> we can also identify the unspecified lemmas in the treebank with an accuracy over 97 %. <eos> furthermore, we demonstrate that using our automatic annotations improves the performance of a state-of-the-art arabic morphological tagger. <eos> our approach combines a variety of techniques from corpus-based statistical models to linguistic rules that target specific phenomena. <eos> these results suggest that the cost of treebanking can be reduced by designing underspecified treebanks that can be subsequently enriched automatically.
social media texts are written in an informal style, which hinders other natural language processing ( nlp ) applications such as machine translation. <eos> text normalization is thus important for processing of social media text. <eos> previous work mostly focused on normalizing words by replacing an informal word with its formal form. <eos> in this paper, to further improve other downstream nlp applications, we argue that other normalization operations should also be performed, e.g., missing word recovery and punctuation correction. <eos> a novel beam-search decoder is proposed to effectively integrate various normalization operations. <eos> empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks, for both chinese and english.
lda-frames is an unsupervised approach for identifying semantic frames from semantically unlabeled text corpora, and seems to be a useful competitor for manually created databases of selectional preferences. <eos> the most limiting property of the algorithm is such that the number of frames and roles must be predefined. <eos> in this paper we present a modification of the lda-frames algorithm allowing the number of frames and roles to be determined automatically, based on the character and size of training data.
we provide an approximation algorithm for pcfg parsing, which asymptotically improves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. <eos> we test our algorithm on two treebanks, and get significant improvements in parsing speed.
the rising influence of user-generated online reviews ( cone, 2011 ) has led to growing incentive for businesses to solicit and manufacture deceptive opinion spam ? fictitious reviews that have been deliberately written to sound authentic and deceive the reader. <eos> recently, ott et al ( 2011 ) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. <eos> however, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. <eos> following an approach similar to ott et al ( 2011 ), in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews. <eos> based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. <eos> finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship.
we present a method for improving the perceived naturalness of corpus-based speech synthesizers. <eos> it consists in removing pronounced pitch peaks in the original recordings, which typically lead to noticeable discontinuities in the synthesized speech. <eos> we perceptually evaluated this method using two concatenative and two hmm-based synthesis systems, and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility.
we show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. <eos> we present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. <eos> this new corpus is automatically extracted from wikipedia revisions and contains over one million instances of preposition corrections.
y of pennsylvaniaabstract prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. <eos> although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. <eos> even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. <eos> our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50 % over an unsupervised baseline. <eos> large gains in accuracy hold for all 22 languages ( low and high resource ) that we investigate.
bilingual dictionaries are expensive resources and not many are available when one of the languages is resource-poor. <eos> in this paper, we propose algorithms for creation of new reverse bilingual dictionaries from existing bilingual dictionaries in which english is one of the two languages. <eos> our algorithms exploit the similarity between word-concept pairs using the english wordnet to produce reverse dictionary entries. <eos> since our algorithms rely on available bilingual dictionaries, they are applicable to any bilingual dictionary as long as one of the two languages has wordnet type lexical ontology.
this paper examines the efficacy of the application of a pre-existing technique in the area of event-event temporal relationship identification. <eos> we attempt to both reproduce the results of said technique, as well as extend the previous work with application to a newlycreated domain of biographical data. <eos> we find that initially the simpler feature sets perform as expected, but that the final improvement to the feature set underperforms. <eos> in response, we provide an analysis of the individual features and identify differences existing between two corpora.
we examine predicative adjectives as an unsupervised criterion to extract subjective adjectives. <eos> we do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives, i.e. <eos> another highly subjective subset of adjectives that can be extracted in an unsupervised fashion. <eos> in order to prove the robustness of this extraction method, we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons ( as a gold standard ).
incorporating semantic structure into a linguistics-free translation model is challenging, since semantic structures are closely tied to syntax. <eos> in this paper, we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model. <eos> first, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. <eos> second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. <eos> experiments on chinese-to-english translation demonstrate that both advances significantly improve translation accuracy.
this paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement. <eos> this system makes for a more informative iaa evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types. <eos> we evaluate the system on two corpora - ( 1 ) a corpus of english web text, and ( 2 ) a corpus of modern british english.
word sense disambiguation aims to identify which meaning of a word is present in a given usage. <eos> gathering word sense annotations is a laborious and difficult task. <eos> several methods have been proposed to gather sense annotations using large numbers of untrained annotators, with mixed results. <eos> we propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses. <eos> our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling.
embedding features for semi-supervised learning mo yu1, tiejun zhao1, daxiang dong2, hao tian2 and dianhai yu2 harbin institute of technology, harbin, china baidu inc., beijing, china { yumo, tjzhao } @ mtlab.hit.edu.cn { dongdaxiang, tianhao, yudianhai } @ baidu.com abstract to solve data sparsity problem, recently there has been a trend in discriminative methods of nlp to use representations of lexical items learned from unlabeled data as features. <eos> in this paper, we investigated the usage of word representations learned by neural language models, i.e. <eos> word embeddings. <eos> the direct us-age has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability. <eos> to overcome these problems, we instead built compound features from continuous word embeddings based on clustering. <eos> experiments showed that the com-pound features not only improved the perfor-mances on several nlp tasks, but also ran faster, suggesting the potential of embeddings.
in the field of intelligent user interfaces, spoken dialogue systems ( sdss ) play a key role as speech represents a true intuitive means of human communication. <eos> deriving information about its quality can help rendering sdss more user-adaptive. <eos> work on automatic estimation of subjective quality usually relies on statistical models. <eos> to create those, manual data annotation is required, which may be performed by actual users or by experts. <eos> here, both variants have their advantages and drawbacks. <eos> in this paper, we analyze the relationship between user and expert ratings by investigating models which combine the advantages of both types of ratings. <eos> we explore two novel approaches using statistical classification methods and evaluate those with a preexisting corpus providing user and expert ratings. <eos> after analyzing the results, we eventually recommend to use expert ratings instead of user ratings in general.
large unsupervised latent variable models ( lvms ) of text, such as latent dirichlet allocation models or hidden markov models ( hmms ), are constructed using parallel training algorithms on computational clusters. <eos> the memory required to hold lvm parameters forms a bottleneck in training more powerful models. <eos> in this paper, we show how the memory required for parallel lvm training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. <eos> we present a greedy document partitioning technique for the task. <eos> for large corpora, our approach reduces memory consumption by over 50 %, and trains the same models up to three times faster, when compared with existing approaches for parallel lvm training.
in cases in which there is no standard orthography for a language or language variant, written texts will display a variety of orthographic choices. <eos> this is problematic for natural language processing ( nlp ) because it creates spurious data sparseness. <eos> we study the transformation of spontaneously spelled egyptian arabic into a conventionalized orthography which we have previously proposed for nlp purposes. <eos> we show that a two-stage process can reduce divergences from this standard by 69 %, making subsequent processing of egyptian arabic easier.
bibliometric measures are commonly used to estimate the popularity and the impact of published research. <eos> existing bibliometric measures provide ? quantitative ? <eos> indicators of how good a published paper is. <eos> this does not necessarily reflect the ? quality ? <eos> of the work presented in the paper. <eos> for example, when hindex is computed for a researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative. <eos> in this paper, we propose using nlp to add a ? qualitative ? <eos> aspect to biblometrics. <eos> we analyze the text that accompanies citations in scientific articles ( which we term citation context ). <eos> we propose supervised methods for identifying citation text and analyzing it to determine the purpose ( i.e. <eos> author intention ) and the polarity ( i.e. <eos> author sentiment ) of citation.
most nlp tools are applied to text that is different from the kind of text they were evaluated on. <eos> common evaluation practice prescribes significance testing across data points in available test data, but typically we only have a single test sample. <eos> this short paper argues that in order to assess the robustness of nlp tools we need to evaluate them on diverse samples, and we consider the problem of finding the most appropriate way to estimate the true effect size across datasets of our systems over their baselines. <eos> we apply meta-analysis and show experimentally ? <eos> by comparing estimated error reduction over observed error reduction on held-out datasets ? <eos> that this method is significantly more predictive of success than the usual practice of using macro- or micro-averages. <eos> finally, we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis.
we present a systematic study of the effect of crowdsourced translations on machine translation performance. <eos> we compare machine translation systems trained on the same data but with translations obtained using amazon ? s mechanical turk vs. professional translations, and show that the same performance is obtained from mechanical turk translations at 1/5th the cost. <eos> we also show that adding a mechanical turk reference translation of the development set improves parameter tuning and output evaluation.
dependency analysis relies on morphosyntactic evidence, as well as semantic evidence. <eos> in some cases, however, morphosyntactic evidence seems to be in conflict with semantic evidence. <eos> for this reason dependency grammar theories, annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions. <eos> most experiments for which constituent-based treebanks such as the penn treebank are converted into dependency treebanks rely blindly on one of four-five widely used tree-to-dependency conversion schemes. <eos> this paper evaluates the down-stream effect of choice of conversion scheme, showing that it has dramatic impact on end results.
a discourse typically involves numerous entities, but few are mentioned more than once. <eos> distinguishing discourse entities that die out after just one mention ( singletons ) from those that lead longer lives ( coreferent ) would benefit nlp applications such as coreference resolution, protagonist identification, topic modeling, and discourse coherence. <eos> we build a logistic regression model for predicting the singleton/coreferent distinction, drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features. <eos> the model is effective in its own right ( 78 % accuracy ), and incorporating it into a state-of-the-art coreference resolution system yields a significant improvement.
a respelling is an alternative spelling of a word in the same writing system, intended to clarify pronunciation. <eos> we introduce the task of automatic generation of a respelling from the word ? s phonemic representation. <eos> our approach combines machine learning with linguistic constraints and electronic resources. <eos> we evaluate our system both intrinsically through a human judgment experiment, and extrinsically by passing its output to a letterto-phoneme converter. <eos> the results show that the respellings generated by our system are better on average than those found on the web, and approach the quality of respellings designed by an expert.
we present a simple log-linear reparameterization of ibm model 2 that overcomes problems arising from model 1 ? s strong assumptions and model 2 ? s overparameterization. <eos> efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. <eos> training the model is consistently ten times faster than model 4. <eos> on three large-scale translation tasks, systems built using our alignment model outperform ibm model 4. <eos> an open-source implementation of the alignment model described in this paper is available from http : //github.com/clab/fast align.
we present a novel approach for translation model ( tm ) adaptation using phrase training. <eos> the proposed adaptation procedure is initialized with a standard general-domain tm, which is then used to perform phrase training on a smaller in-domain set. <eos> this way, we bias the probabilities of the general tm towards the in-domain distribution. <eos> experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones. <eos> additionally, we compare our results to mixture modeling, where we report gains when using the suggested phrase training adaptation method.
we propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora. <eos> the motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms. <eos> using the resulting set of query terms has the advantage that we can overcome the problem that a single query term ? s context vector does not always reliably represent a terms meaning due to the context vector ? s sparsity. <eos> our proposed method uses a weighted average of the synonyms ? <eos> context vectors, that is derived by inferring the mean vector of the von mises-fisher distribution. <eos> we evaluate our method, using the synsets from the cross-lingually aligned japanese and english wordnet. <eos> the experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.
we consider the task of tagging arabic nouns with wordnet supersenses. <eos> three approaches are evaluated. <eos> the first uses an expertcrafted but limited-coverage lexicon, arabic wordnet, and heuristics. <eos> the second uses unsupervised sequence modeling. <eos> the third and most successful approach uses machine translation to translate the arabic into english, which is automatically tagged with english supersenses, the results of which are then projected back into arabic. <eos> analysis shows gains and remaining obstacles in four wikipedia topical domains.
inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech ( pos ) tagging that is less sensitive to domain shifts. <eos> the objective of our method is to minimize average loss under random distribution shifts. <eos> we restrict the possible target distributions to mixtures of the source distribution and random zipfian distributions. <eos> our algorithm is used for pos tagging and evaluated on the english web treebank and the danish dependency treebank with an average 4.4 % error reduction in tagging accuracy.
we adapt the popular lda topic model ( blei et al, 2003 ) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. <eos> we show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions.
the topic of a document can prove to be useful information for word sense disambiguation ( wsd ) since certain meanings tend to be associated with particular topics. <eos> this paper presents an lda-based approach for wsd, which is trained using any available wsd system to establish a sense per ( latent dirichlet alocation based ) topic. <eos> the technique is tested using three unsupervised and one supervised wsd algorithms within the sport and finance domains giving a performance increase each time, suggesting that the technique may be useful to improve the performance of any available wsd system.
multi-domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. <eos> however, real-world datasets often have multiple metadata attributes that can divide the data into domains. <eos> it is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. <eos> we propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. <eos> experimentally, they outperform the multi-domain learning baseline, even when it selects the single ? best ? <eos> attribute.
this opinion piece proposes that recent advances in opinion detection are limited in the extent to which they can detect important categories of opinion because they are not designed to capture some of the pragmatic aspects of opinion. <eos> a component of these is the perspective of the user of an opinion-mining system as to what an opinion really is, which is in itself a matter of opinion ( metasubjectivity ). <eos> we propose a way to define this component of opinion and describe the challenges it poses for corpus development and sentence-level detection technologies. <eos> finally, we suggest that investment in techniques to handle metasubjectivity will likely bear costs but bring benefits in the longer term.
social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. <eos> in this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. <eos> we then conduct exploratory analysis in order to isolate factors associated with deleted posts. <eos> finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret.
we investigate two systems for automatic disfluency detection on english and mandarin conversational speech data. <eos> the first system combines various lexical and prosodic features in a conditional random field model for detecting edit disfluencies. <eos> the second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition. <eos> we compare the contributions of different knowledge sources to detection performance between these two languages.
atypical semantic and pragmatic expression is frequently reported in the language of children with autism. <eos> although this atypicality often manifests itself in the use of unusual or unexpected words and phrases, the rate of use of such unexpected words is rarely directly measured or quantified. <eos> in this paper, we use distributional semantic models to automatically identify unexpected words in narrative retellings by children with autism. <eos> the classification of unexpected words is sufficiently accurate to distinguish the retellings of children with autism from those with typical development. <eos> these techniques demonstrate the potential of applying automated language analysis techniques to clinically elicited language data for diagnostic purposes.
automatic assessment of reading ability builds on applying speech recognition tools to oral reading, measuring words correct per minute. <eos> this work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study. <eos> experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase, i.e. <eos> in less appropriate locations. <eos> the results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions.
in this paper we leverage methods from submodular function optimization developed for document summarization and apply them to the problem of subselecting acoustic data. <eos> we evaluate our results on data subset selection for a phone recognition task. <eos> our framework shows significant improvements over random selection and previously proposed methods using a similar amount of resources.
one way to improve the accuracy of automatic speech recognition ( asr ) is to use discriminative language modeling ( dlm ), which enhances discrimination by learning where the asr hypotheses deviate from the uttered sentences. <eos> however, dlm requires large amounts of asr output to train. <eos> instead, we can simulate the output of an asr system, in which case the training becomes semisupervised. <eos> the advantage of using simulated hypotheses is that we can generate as many hypotheses as we want provided that we have enough text material. <eos> in typical scenarios, transcribed in-domain data is limited but large amounts of out-of-domain ( ood ) data is available. <eos> in this study, we investigate how semi-supervised training performs with ood data. <eos> we find out that ood data can yield improvements comparable to in-domain data.
word sense disambiguation ( wsd ) approaches have reported good accuracies in recent years. <eos> however, these approaches can be classified as weak ai systems. <eos> according to the classical definition, a strong ai based wsd system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings. <eos> in order to accomplish this, a detailed understanding of the human techniques employed for sense disambiguation is necessary. <eos> instead of building yet another wsd system that uses contextual evidence for sense disambiguation, as has been done before, we have taken a step back - we have endeavored to discover the cognitive faculties that lie at the very core of the human sense disambiguation technique. <eos> in this paper, we present a hypothesis regarding the cognitive sub-processes involved in the task ofwsd.we support our hypothesis using the experiments conducted through the means of an eye-tracking device. <eos> we also strive to find the levels of difficulties in annotating various classes of words, with senses. <eos> we believe, once such an in-depth analysis is performed, numerous insights can be gained to develop a robust wsd system that conforms to the principle of strong ai.
sentence similarity computes a similarity score between two sentences. <eos> the ss task differs from document level semantics tasks in that it features the sparsity of words in a data unit, i.e. <eos> a sentence. <eos> accordingly it is crucial to robustly model each word in a sentence to capture the complete semantic picture of the sentence. <eos> in this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. <eos> we incorporate both corpus-based ( selectional preference information ) and knowledge-based ( similar words extracted in a dictionary ) lexical semantics into a latent variable model. <eos> the experiments show state-of-the-art performance among unsupervised systems on two ss datasets.
continuous space language models have recently demonstrated outstanding results across a variety of tasks. <eos> in this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. <eos> we find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. <eos> this allows vector-oriented reasoning based on the offsets between words. <eos> for example, the male/female relationship is automatically learned, and with the induced vector representations, ? king man + woman ? <eos> results in a vector very close to ? queen. ? <eos> we demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions ( provided with this paper ), and are able to correctly answer almost 40 % of the questions. <eos> we demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer semeval-2012 task 2 questions. <eos> remarkably, this method outperforms the best previous systems.
we propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present truthteller, a standalone publiclyavailable tool that produces such annotations. <eos> truthteller integrates a range of semantic phenomena, such as negation, modality, presupposition, implicativity, and more, which were dealt only partly in previous works. <eos> empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for nlp.
we present the 1.0 release of our paraphrase database, ppdb. <eos> its english portion, ppdb : eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. <eos> the paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion english words. <eos> we also release ppdb : spa, a collection of 196 million spanish paraphrases. <eos> each paraphrase pair in ppdb contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the google n-grams and the annotated gigaword corpus. <eos> our release includes pruning tools that allow users to determine their own precision/recall tradeoff.
this paper presents an approach that exploits the scope of negation cues for relation extraction ( re ) without the need of using any specifically annotated dataset for building a separate negation scope detection classifier. <eos> new features are proposed which are used in two different stages. <eos> these also include non-target entity specific features. <eos> the proposed re approach outperforms the previous state of the art for drug-drug interaction ( ddi ) extraction.
iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision. <eos> unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions. <eos> this paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph. <eos> in contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, and it empirically results in better extractions than iterative methods.
distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. <eos> in this paper, we show that a significant number of ? negative ? <eos> examples generated by the labeling process are false negatives because the knowledge base is incomplete. <eos> therefore the heuristic for generating negative examples has a serious flaw. <eos> building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. <eos> experimental results demonstrate its advantage over existing algorithms.
in this paper, we propose a novel rhetorical structure index ( rsi ) to measure the structural importance of a word or a phrase. <eos> unlike tf-idf and other content-driven measurements, rsi identifies words or phrases that are structural cues in an unstructured document. <eos> we show structurally motivated features with high rsi values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments. <eos> experiments show that using rsi significantly improves the segmentation accuracy compared to tf-idf, a traditional content-based feature weighting scheme.
twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza. <eos> however, previous work has relied on simple content analysis, which conflates flu tweets that report infection with those that express concerned awareness of the flu. <eos> by discriminating these categories, as well as tweets about the authors versus about others, we demonstrate significant improvements on influenza surveillance using twitter.
wizard-of-oz experimental setup in a dialogue system is commonly used to gather data for informing an automated version of that system. <eos> previous work has exposed dependencies between user behavior towards systems and user belief about whether the system is automated or human-controlled. <eos> this work examines whether user behavior changes when user belief is held constant and the system ? s operator is varied. <eos> we perform a posthoc experiment using generalizable prosodic and lexical features of user responses to a dialogue system backed with and without a human wizard. <eos> our results suggest that user responses are different when communicating with a wizarded and an automated system, indicating that wizard data may be less reliable for informing automated systems than generally assumed.
we present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the class based not on confidence, but on a cascade of classifiers. <eos> we show that it gives a minority class f-measure error reduction of 22.8 %, while also reducing the error for other classes and the overall error by about 10 %.
document-level sentiment analysis can benefit from fine-grained subjectivity, so that sentiment polarity judgments are based on the relevant parts of the document. <eos> while finegrained subjectivity annotations are rarely available, encouraging results have been obtained by modeling subjectivity as a latent variable. <eos> however, latent variable models fail to capitalize on our linguistic knowledge about discourse structure. <eos> we present a new method for injecting linguistic knowledge into latent variable subjectivity modeling, using discourse connectors. <eos> connector-augmented transition features allow the latent variable model to learn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. <eos> this yields significantly improved performance on documentlevel sentiment analysis in english and spanish. <eos> we also describe a simple heuristic for automatically identifying connectors when no predefined list is available.
this study focuses on modeling discourse coherence in the context of automated assessment of spontaneous speech from non-native speakers. <eos> discourse coherence has always been used as a key metric in human scoring rubrics for various assessments of spoken language. <eos> however, very little research has been done to assess a speaker 's coherence in automated speech scoring systems. <eos> to address this, we present a corpus of spoken responses that has been annotated for discourse coherence quality. <eos> then, we investigate the use of several features originally developed for essays to model coherence in spoken responses. <eos> an analysis on the annotated corpus shows that the prediction accuracy for human holistic scores of an automated speech scoring system can be improved by around 10 % relative after the addition of the coherence features. <eos> further experiments indicate that a weighted fmeasure of 73 % can be achieved for the automated prediction of the coherence scores.
in this paper, we propose a multi-step stacked learning model for disfluency detection. <eos> our method incorporates refined n-gram features step by step from different word sequences. <eos> first, we detect filler words. <eos> second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. <eos> in the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. <eos> we usemax-marginmarkov networks ( m3ns ) as the classifier with the weighted hamming loss to balance precision and recall. <eos> experiments on the switchboard corpus show that the refined n-gram features from multiple steps and m3ns with weighted hamming loss can significantly improve the performance. <eos> our method for disfluency detection achieves the best reported f-score 0.841 without the use of additional resources.1
we consider the problem of translating natural language text queries into regular expressions which represent their meaning. <eos> the mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. <eos> however, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. <eos> we evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from amazon mechanical turk. <eos> our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29 % absolute improvement in accuracy.1
in natural-language discourse, related events tend to appear near each other to describe a larger scenario. <eos> such structures can be formalized by the notion of a frame ( a.k.a. <eos> template ), which comprises a set of related events and prototypical participants and event transitions. <eos> identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. <eos> methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. <eos> in this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. <eos> the number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. <eos> in end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.
in this paper we explore the potential of quantum theory as a formal framework for capturing lexical meaning. <eos> we present a novel semantic space model that is syntactically aware, takes word order into account, and features key quantum aspects such as superposition and entanglement. <eos> we define a dependency-based hilbert space and show how to represent the meaning of words by density matrices that encode dependency neighborhoods. <eos> experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models.
our goal is to extract answers from preretrieved sentences for question answering ( qa ). <eos> we construct a linear-chain conditional random field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. <eos> this casts answer extraction as an answer sequence tagging problem for the first time, where knowledge of shared structure between question and source sentence is incorporated through features based on tree edit distance ( ted ). <eos> our model is free of manually created question and answer templates, fast to run ( processing 200 qa pairs per second excluding parsing time ), and yields an f1 of 63.3 % on a new public dataset based on prior trec qa evaluations. <eos> the developed system is open-source, and includes an implementation of the ted model that is state of the art in the task of ranking qa pairs.
traditional relation extraction seeks to identify pre-specified semantic relations within natural language text, while open information extraction ( open ie ) takes a more general approach, and looks for a variety of relations without restriction to a fixed relation set. <eos> with this generalization comes the question, what is a relation ? <eos> for example, should the more general task be restricted to relations mediated by verbs, nouns, or both ? <eos> to help answer this question, we propose two levels of subtasks for open ie. <eos> one task is to determine if a sentence potentially contains a relation between two entities ? <eos> the other task looks to confirm explicit relation words for two entities. <eos> we propose multiple svm models with dependency tree kernels for both tasks. <eos> for explicit relation extraction, our system can extract both noun and verb relations. <eos> our results on three datasets show that our system is superior when compared to state-of-the-art systems like reverb and ollie for both tasks. <eos> for example, in some experiments our system achieves 33 % improvement on nominal relation extraction over ollie. <eos> in addition we propose an unsupervised rule-based approach which can serve as a strong baseline for open ie systems.
in natural language question answering ( qa ) systems, questions often contain terms and phrases that are critically important for retrieving or finding answers from documents. <eos> we present a learnable system that can extract and rank these terms and phrases ( dubbed mandatory matching phrases or mmps ), and demonstrate their utility in a qa system on internet discussion forum data sets. <eos> the system relies on deep syntactic and semantic analysis of questions only and is independent of relevant documents. <eos> our proposed model can predict mmps with high accuracy. <eos> when used in a qa system features derived from the mmp model improve performance significantly over a state-of-the-art baseline. <eos> the final qa system was the best performing system in the darpa bolt-ir evaluation.
in a meeting, it is often desirable to extract keywords from each utterance as soon as it is spoken. <eos> thus, this paper proposes a just-intime keyword extraction from meeting transcripts. <eos> the proposed method considers two major factors that make it different from keyword extraction from normal texts. <eos> the first factor is the temporal history of preceding utterances that grants higher importance to recent utterances than old ones, and the second is topic relevance that forces only the preceding utterances relevant to the current utterance to be considered in keyword extraction. <eos> our experiments on two data sets in english and korean show that the consideration of the factors results in performance improvement in keyword extraction from meeting transcripts.
coreference resolution systems rely heavily on string overlap ( e.g., google inc. and google ), performing badly on mentions with very different words ( opaque mentions ) like google and the search giant. <eos> yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall. <eos> we present a new unsupervised method for mining opaque pairs. <eos> our intuition is to restrict distributional semantics to articles about the same event, thus promoting referential match. <eos> using an english comparable corpus of tech news, we built a dictionary of opaque coreferent mentions ( only 3 % are in wordnet ). <eos> our dictionary can be integrated into any coreference system ( it increases the performance of a state-of-the-art system by 1 % f1 on all measures ) and is easily extendable by using news aggregators.
we present the first work on antecedent selection for bridging resolution without restrictions on anaphor or relation types. <eos> our model integrates global constraints on top of a rich local feature set in the framework of markov logic networks. <eos> the global model improves over the local one and both strongly outperform a reimplementation of prior work.
we examine the task of temporal relation classification. <eos> unlike existing approaches to this task, we ( 1 ) classify an event-event or eventtime pair as one of the 14 temporal relations defined in the timebank corpus, rather than as one of the six relations collapsed from the original 14 ; ( 2 ) employ sophisticated linguistic knowledge derived from a variety of semantic and discourse relations, rather than focusing on morpho-syntactic knowledge ; and ( 3 ) leverage a novel combination of rule-based and learning-based approaches, rather than relying solely on one or the other. <eos> experiments with the timebank corpus demonstrate that our knowledge-rich, hybrid approach yields a 15 ? 16 % relative reduction in error over a state-of-the-art learning-based baseline system.
inferring the information structure of scientific documents is useful for many downstream applications. <eos> existing feature-based machine learning approaches to this task require substantial training data and suffer from limited performance. <eos> our idea is to guide feature-based models with declarative domain knowledge encoded as posterior distribution constraints. <eos> we explore a rich set of discourse and lexical constraints which we incorporate through the generalized expectation ( ge ) criterion. <eos> our constrained model improves the performance of existing fully and lightly supervised models. <eos> even a fully unsupervised version of this model outperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available.
previous research on domain adaptation ( da ) for statistical machine translation ( smt ) has mainly focused on the translation model ( tm ) and the language model ( lm ). <eos> to the best of our knowledge, there is no previous work on reordering model ( rm ) adaptation for phrasebased smt. <eos> in this paper, we demonstrate that mixture model adaptation of a lexicalized rm can significantly improve smt performance, even when the system already contains a domain-adapted tm and lm. <eos> we find that, surprisingly, different training corpora can vary widely in their reordering characteristics for particular phrase pairs. <eos> furthermore, particular training corpora may be highly suitable for training the tm or the lm, but unsuitable for training the rm, or vice versa, so mixture weights for these models should be estimated separately. <eos> an additional contribution of the paper is to propose two improvements to mixture model adaptation : smoothing the in-domain sample, and weighting instances by document frequency. <eos> applied to mixture rms in our experiments, these techniques ( especially smoothing ) yield significant performance improvements.
this paper examines tuning for statistical machine translation ( smt ) with respect to multiple evaluation metrics. <eos> we propose several novel methods for tuning towards multiple objectives, including some based on ensemble decoding methods. <eos> pareto-optimality is a natural way to think about multi-metric optimization ( mmo ) and our methods can effectively combine several pareto-optimal solutions, obviating the need to choose one. <eos> our best performing ensemble tuning method is a new algorithm for multi-metric optimization that searches for pareto-optimal ensemble models. <eos> we study the effectiveness of our methods through experiments on multiple as well as single reference ( s ) datasets. <eos> our experiments show simultaneous gains across several metrics ( bleu, ribes ), without any significant reduction in other metrics. <eos> this contrasts the traditional tuning where gains are usually limited to a single metric. <eos> our human evaluation results confirm that in order to produce better mt output, optimizing multiple metrics is better than optimizing only one.
we propose a new algorithm to approximately extract top-scoring hypotheses from a hypergraph when the score includes an n ? gram language model. <eos> in the popular cube pruning algorithm, every hypothesis is annotated with boundary words and permitted to recombine only if all boundary words are equal. <eos> however, many hypotheses share some, but not all, boundary words. <eos> we use these common boundary words to group hypotheses and do so recursively, resulting in a tree of hypotheses. <eos> this tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand. <eos> machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases.
the dominant yet ageing ibm and hmm word alignment models underpin most popular statistical machine translation implementations in use today. <eos> though beset by the limitations of implausible independence assumptions, intractable optimisation problems, and an excess of tunable parameters, these models provide a scalable and reliable starting point for inducing translation systems. <eos> in this paper we build upon this venerable base by recasting these models in the non-parametric bayesian framework. <eos> by replacing the categorical distributions at their core with hierarchical pitman-yor processes, and through the use of collapsed gibbs sampling, we provide a more flexible formulation and sidestep the original heuristic optimisation techniques. <eos> the resulting models are highly extendible, naturally permitting the introduction of phrasal dependencies. <eos> we present extensive experimental results showing improvements in both aer and bleu when benchmarked against giza++, including significant improvements over ibm model 4.
we present a novel approach to automatic metaphor identification, that discovers both metaphorical associations and metaphorical expressions in unrestricted text. <eos> our system first performs hierarchical graph factorization clustering ( hgfc ) of nouns and then searches the resulting graph for metaphorical connections between concepts. <eos> it then makes use of the salient features of the metaphorically connected clusters to identify the actual metaphorical expressions. <eos> in contrast to previous work, our method is fully unsupervised. <eos> despite this fact, it operates with an encouraging precision ( 0.69 ) and recall ( 0.61 ). <eos> our approach is also the first one in nlp to exploit the cognitive findings on the differences in organisation of abstract and concrete concepts in the human brain.
we present three approaches to lexical chaining based on the lda topic model and evaluate them intrinsically on a manually annotated set of german documents. <eos> after motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony. <eos> also, we propose a new measure for direct evaluation of lexical chains. <eos> our three lda-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. <eos> subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within nlp applications.
in this work, we study the problem of measuring relational similarity between two word pairs ( e.g., silverware : fork and clothing : shirt ). <eos> due to the large number of possible relations, we argue that it is important to combine multiple models based on heterogeneous information sources. <eos> our overall system consists of two novel general-purpose relational similarity models and three specific word relation models. <eos> when evaluated in the setting of a recently proposed semeval-2012 task, our approach outperforms the previous best system substantially, achieving a 54.1 % relative increase in spearman ? s rank correlation.
hidden properties of social media users, such as their ethnicity, gender, and location, are often reflected in their observed attributes, such as their first and last names. <eos> furthermore, users who communicate with each other often have similar hidden properties. <eos> we propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of twitter users. <eos> attributes such as user names are grouped together if users with those names communicate with other similar users. <eos> we separately cluster millions of unique first names, last names, and userprovided locations. <eos> the efficacy of these clusters is then evaluated on a diverse set of classification tasks that predict hidden users properties such as ethnicity, geographic location, gender, language, and race, using only profile names and locations when appropriate. <eos> our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile, substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied.
information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. <eos> as the core component of information extraction, we consider the task of twitter entity linking in this paper. <eos> in the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. <eos> however, in our task, we find that mention detection is often the performance bottleneck. <eos> the reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. <eos> to rigorously address the twitter entity linking problem, we propose a structural svm algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. <eos> by combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15 % f1.
threaded discussion forums provide an important social media platform. <eos> its rich user generated content has served as an important source of public feedback. <eos> to automatically discover the viewpoints or stances on hot issues from forum threads is an important and useful task. <eos> in this paper, we propose a novel latent variable model for viewpoint discovery from threaded forum posts. <eos> our model is a principled generative latent variable model which captures three important factors : viewpoint specific topic preference, user identity and user interactions. <eos> evaluation results show that our model clearly outperforms a number of baseline models in terms of both clustering posts based on viewpoints and clustering users with different viewpoints.
this paper proposes to study the problem of identifying intention posts in online discussion forums. <eos> for example, in a discussion forum, a user wrote ? i plan to buy a camera, ? <eos> which indicates a buying intention. <eos> this intention can be easily exploited by advertisers. <eos> to the best of our knowledge, there is still no reported study of this problem. <eos> our research found that this problem is particularly suited to transfer learning because in different domains, people express the same intention in similar ways. <eos> we then propose a new transfer learning method which, unlike a general transfer learning algorithm, exploits several special characteristics of the problem. <eos> experimental results show that the proposed method outperforms several strong baselines, including supervised learning in the target domain and a recent transfer learning method.
we describe a novel approach to detecting empty categories ( ec ) as represented in dependency trees as well as a new metric for measuring ec detection accuracy. <eos> the new metric takes into account not only the position and type of an ec, but also the head it is a dependent of in a dependency tree. <eos> we also introduce a variety of new features that are more suited for this approach. <eos> tested on a subset of the chinese treebank, our system improved significantly over the best previously reported results even when evaluated with this more stringent metric.
we study multi-source transfer parsing for resource-poor target languages ; specifically methods for target language adaptation of delexicalized discriminative graph-based dependency parsers. <eos> we first show how recent insights on selective parameter sharing, based on typological and language-family features, can be applied to a discriminative parser by carefully decomposing its model features. <eos> we then show how the parser can be relexicalized and adapted using unlabeled target language data and a learning method that can incorporate diverse knowledge sources through ambiguous labelings. <eos> in the latter scenario, we exploit two sources of knowledge : arc marginals derived from the base parser in a self-training algorithm, and arc predictions from multiple transfer parsers in an ensemble-training algorithm. <eos> our final model outperforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages.
grice characterized communication in terms of the cooperative principle, which enjoins speakers to make only contributions that serve the evolving conversational goals. <eos> we show that the cooperative principle and the associated maxims of relevance, quality, and quantity emerge from multi-agent decision theory. <eos> we utilize the decentralized partially observable markov decision process ( dec-pomdp ) model of multi-agent decision making which relies only on basic definitions of rationality and the ability of agents to reason about each other ? s beliefs in maximizing joint utility. <eos> our model uses cognitively-inspired heuristics to simplify the otherwise intractable task of reasoning jointly about actions, the environment, and the nested beliefs of other actors. <eos> our experiments on a cooperative language task show that reasoning about others ? <eos> belief states, and the resulting emergent gricean communicative behavior, leads to significantly improved task performance.
this paper explores the relationship between explicit and predictive models of incremental speech understanding in a dialogue system that supports a finite set of user utterance meanings. <eos> we present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. <eos> we show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches.
in this paper we propose a new approach to the generation of pseudowords, i.e., artificial words which model real polysemous words. <eos> our approach simultaneously addresses the two important issues that hamper the generation of large pseudosense-annotated datasets : semantic awareness and coverage. <eos> we evaluate these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts.
in this paper we consider the problem of labeling the languages of words in mixed-language documents. <eos> this problem is approached in a weakly supervised fashion, as a sequence labeling problem with monolingual text samples for training data. <eos> among the approaches evaluated, a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied.
non-expert annotation services like amazon ? s mechanical turk ( amt ) are cheap and fast ways to evaluate systems and provide categorical annotations for training data. <eos> unfortunately, some annotators choose bad labels in order to maximize their pay. <eos> manual identification is tedious, so we experiment with an item-response model. <eos> it learns in an unsupervised fashion to a ) identify which annotators are trustworthy and b ) predict the correct underlying labels. <eos> we match performance of more complex state-of-the-art systems and perform well even under adversarial conditions. <eos> we show considerable improvements over standard baselines, both for predicted label accuracy and trustworthiness estimates. <eos> the latter can be further improved by introducing a prior on model parameters and using variational bayes inference. <eos> additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in ( trading in some recall ), and by incorporating annotated control instances. <eos> our system, mace ( multi-annotator competence estimation ), is available for download1.
we propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary. <eos> instead of learning word-specific substitution patterns, a global model for lexical substitution is trained on delexicalized ( i.e., non lexical ) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. <eos> this way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. <eos> using features from lexical resources, as well as a variety of features computed from large corpora ( n-gram counts, distributional similarity ) and a ranking method based on the posterior probabilities obtained from a maximum entropy classifier, we improve over the state of the art in the lexsub best-precision metric and the generalized average precision measure. <eos> robustness of our approach is demonstrated by evaluating it successfully on two different datasets.
in this paper, we present a novel method for the computation of compositionality within a distributional framework. <eos> the key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. <eos> we use our method to model the composition of subject verb object triples. <eos> the method consists of two steps. <eos> first, we compute a latent factor model for nouns from standard co-occurrence data. <eos> next, the latent factors are used to induce a latent model of three-way subject verb object interactions. <eos> our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art.
twitter offers an unprecedented advantage on live reporting of the events happening around the world. <eos> however, summarizing the twitter event has been a challenging task that was not fully explored in the past. <eos> in this paper, we propose a participant-based event summarization approach that ? zooms-in ? <eos> the twitter event streams to the participant level, detects the important sub-events associated with each participant using a novel mixture model that combines the ? burstiness ? <eos> and ? cohesiveness ? <eos> properties of the event tweets, and generates the event summaries progressively. <eos> we evaluate the proposed approach on different event types. <eos> results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art.
this paper presents g-flow, a novel system for coherent extractive multi-document summarization ( mds ).1 where previous work on mds considered sentence selection and ordering separately, g-flow introduces a joint model for selection and ordering that balances coherence and salience. <eos> g-flow ? s core representation is a graph that approximates the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference, and more. <eos> this graph enables g-flow to estimate the coherence of a candidate summary. <eos> we evaluate g-flow on mechanical turk, and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joint model.
we introduce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. <eos> our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. <eos> expressions generated using this method are often overspecified and may be underspecified, akin to expressions produced by people. <eos> we call such expressions identifying descriptions. <eos> the algorithm outperforms the well-known incremental algorithm ( dale and reiter, 1995 ) and the graphbased algorithm ( krahmer et al, 2003 ; viethen et al, 2008 ) across a variety of images in two domains. <eos> we additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm ? s non-determinism into account.
we describe a supervised approach to predicting the set of all inflected forms of a lexical item. <eos> our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples, and then learns the contexts in which those transformations apply using a discriminative sequence model. <eos> because our approach is completely data-driven and the model is trained on examples extracted from wiktionary, our method can extend to new languages without change. <eos> our end-to-end system is able to predict complete paradigms with 86.1 % accuracy and individual inflected forms with 94.9 % accuracy, averaged across three languages and two parts of speech.
in this paper we introduce the task of unlabeled, optimal, data set selection. <eos> given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set. <eos> our first proposed method, based on the rank-revealing qr matrix factorization, selects a subset of words which span the entire word-space effectively. <eos> for our second method, we develop the concept of feature coverage which we optimize with a greedy algorithm. <eos> we apply these methods to the task of grapheme-to-phoneme prediction. <eos> experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples. <eos> when fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20 % over randomly selected examples.
we present a morphology-aware nonparametric bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages. <eos> this relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts. <eos> our model can be used in virtually any scenario where multinomial distributions over words would be used. <eos> we obtain state-of-the-art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages.
in this paper we revisit the task of quantitative evaluation of coreference resolution systems. <eos> we review the most commonly used metrics ( muc, b3, ceaf and blanc ) on the basis of their evaluation of coreference resolution in five texts from the ontonotes corpus. <eos> we examine both the correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics. <eos> in conclusion we claim that loss of information value is an essential factor, insufficiently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. <eos> we thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation.
annotated corpora play a significant role in many nlp applications. <eos> however, annotation by humans is time-consuming and costly. <eos> in this paper, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes. <eos> we demonstrate the effectiveness of our approach in the context of one form of unbalanced task : annotation of transcribed human-human dialogues for presence/absence of uncertainty. <eos> in two data sets, our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy. <eos> the method is able to reduce human annotation effort by about 80 % without a significant loss in data quality, as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations.
in this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms. <eos> in our preliminary experiments, we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction. <eos> we achieved state of the art results for unigram extraction in brazilian portuguese.
we present our work in generating karmina, an old malay poetic form for indonesian language. <eos> karmina is a poem with two lines that consists of a hook ( sampiran ) on the first line and a message on the second line. <eos> one of the unique aspects of karmina is in the absence of discourse relation between its hook and message. <eos> we approached the problem by generating the hooks and the messages in separate processes using predefined schemas and a manually built knowledge base. <eos> the karminas were produced by randomly pairing the messages with the hooks, subject to the constraints imposed on the rhymes and on the structure similarity. <eos> syllabifications were performed on the cue words of the hooks and messages to ensure the generated pairs have matching rhymes. <eos> we were able to generate a number of positive examples while still leaving room for improvement, particularly in the generation of the messages, which currently are still limited, and in filtering the negative results.
revealing an anonymous author ? s traits from text is a well-researched area. <eos> in this paper we aim to identify the native language and language family of a non-native english author, given his/her english writings. <eos> we extract features from the text based on prior work, and extend or modify it to construct different feature sets, and use support vector machines for classification. <eos> we show that native language identification accuracy can be improved by up to 6.43 % for a 9-class task, depending on the feature set, by introducing a novel method to incorporate language family information. <eos> in addition we show that introducing grammarbased features improves accuracy of both native language and language family identification.
our research investigates the translation of ontology labels, which has applications in multilingual knowledge access. <eos> ontologies are often defined only in one language, mostly english. <eos> to enable knowledge access across languages, such monolingual ontologies need to be translated into other languages. <eos> the primary challenge in ontology label translation is the lack of context, which makes this task rather different than document translation. <eos> the core objective therefore, is to provide statistical machine translation ( smt ) systems with additional context information. <eos> in our approach, we first extend standard smt by enhancing a translation model with context information that keeps track of surrounding words for each translation. <eos> we compute a semantic similarity between the phrase pair context vector from the parallel corpus and a vector of noun phrases that occur in surrounding ontology labels. <eos> we applied our approach to the translation of a financial ontology, translating from english to german, using europarl as parallel corpus. <eos> this experiment showed that our approach can provide a slight improvement over standard smt for this task, without exploiting any additional domain-specific resources.
morphological tokenization has been used in machine translation for morphologically complex languages to reduce lexical sparsity. <eos> unfortunately, when translating into a morphologically complex language, recombining segmented tokens to generate original word forms is not a trivial task, due to morphological, phonological and orthographic adjustments that occur during tokenization. <eos> we review a number of detokenization schemes for arabic, such as rule-based and table-based approaches and show their limitations. <eos> we then propose a novel detokenization scheme that uses a character-level discriminative string transducer to predict the original form of a segmented word. <eos> in a comparison to a stateof-the-art approach, we demonstrate slightly better detokenization error rates, without the need for any hand-crafted rules. <eos> we also demonstrate the effectiveness of our approach in an english-to-arabic translation task.
my thesis will explore ways to improve the performance of statistical machine translation ( smt ) in low resource conditions. <eos> specifically, it aims to reduce the dependence of modern smt systems on expensive parallel data. <eos> we define low resource settings as having only small amounts of parallel data available, which is the case for many language pairs. <eos> all current smt models use parallel data during training for extracting translation rules and estimating translation probabilities. <eos> the theme of our approach is the integration of information from alternate data sources, other than parallel corpora, into the statistical model. <eos> in particular, we focus on making use of large monolingual and comparable corpora. <eos> by augmenting components of the smt framework, we hope to extend its applicability beyond the small handful of language pairs with large amounts of available parallel text.
we examine the application of data-driven paraphrasing to natural language understanding. <eos> we leverage bilingual parallel corpora to extract a large collection of syntactic paraphrase pairs, and introduce an adaptation scheme that allows us to tackle a variety of text transformation tasks via paraphrasing. <eos> we evaluate our system on the sentence compression task. <eos> further, we use distributional similarity measures based on context vectors derived from large monolingual corpora to annotate our paraphrases with an orthogonal source of information. <eos> this yields significant improvements in our compression system ? s output quality, achieving state-of-the-art performance. <eos> finally, we propose a refinement of our paraphrases by classifying them into natural logic entailment relations. <eos> by extending the synchronous parsing paradigm towards these entailment relations, we will enable our system to perform recognition of textual entailment.
automatically describing visual content is an extremely difficult task, with hard ai problems in computer vision ( cv ) and natural language processing ( nlp ) at its core. <eos> previous work relies on supervised visual recognition systems to determine the content of images. <eos> these systems require massive amounts of hand-labeled data for training, so the number of visual classes that can be recognized is typically very small. <eos> we argue that these approaches place unrealistic limits on the kinds of images that can be captioned, and are unlikely to produce captions which reflect human interpretations. <eos> we present a framework for image caption generation that does not rely on visual recognition systems, which we have implemented on a dataset of online shopping images and product descriptions. <eos> we propose future work to improve this method, and extensions for other domains of images and natural text.
review mining and summarization has been a hot topic for the past decade. <eos> a lot of effort has been devoted to aspect detection and sentiment analysis under the assumption that every review has the same utility for related tasks. <eos> however, reviews are not equally helpful as indicated by user-provided helpfulness assessment associated with the reviews. <eos> in this thesis, we propose a novel review summarization framework which summarizes review content under the supervision of automated assessment of review helpfulness. <eos> this helpfulness-guided framework can be easily adapted to traditional review summarization tasks, for a wide range of domains.
entrainment is the phenomenon of the speech of conversational partners becoming more similar to each other. <eos> this thesis proposal presents a comprehensive look at entrainment in human conversations and how entrainment may be incorporated into the design of spoken dialogue systems in order to improve system performance and user satisfaction. <eos> we compare different kinds of entrainment in both classic and novel dimensions, provide experimental results on the utility of entrainment, and show that entrainment can be used to improve a system ? s asr performance and turntaking decisions.
in this paper, a maximum entropy markov model ( memm ) for dialog state tracking is proposed to efficiently handle user goal evolvement in two steps. <eos> the system first predicts the occurrence of a user goal change based on linguistic features and dialog context for each dialog turn, and then the proposed model could utilize this user goal change information to infer the most probable dialog state sequence which underlies the evolvement of user goal during the dialog. <eos> it is believed that with the suggested various domain independent feature functions, the proposed model could better exploit not only the intra-dependencies within long asr n-best lists but also the inter-dependencies of the observations across dialog turns, which leads to more efficient and accurate dialog state inference.
automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings. <eos> these ambiguities can still be found when language is restricted to a particular domain, such as biomedicine. <eos> word sense disambiguation ( wsd ) systems attempt to resolve these ambiguities but are often only able to identify the meanings for a small set of ambiguous terms. <eos> dale ( disambiguation using automatically labeled examples ) is a supervised wsd system that can disambiguate a wide range of ambiguities found in biomedical documents. <eos> dale uses the umls metathesaurus as both a sense inventory and as a source of information for automatically generating labeled training examples. <eos> dale is able to disambiguate biomedical documents with the coverage of unsupervised approaches and accuracy of supervised methods.
effectively exploring and analyzing large text corpora requires visualizations that provide a high level summary. <eos> past work has relied on faceted browsing of document metadata or on natural language processing of document text. <eos> in this paper, we present a new web-based tool that integrates topics learned from an unsupervised topic model in a faceted browsing experience. <eos> the user can manage topics, filter documents by topic and summarize views with metadata and topic graphs. <eos> we report a user study of the usefulness of topics in our tool.
tmtprime is a recommender system that facilitates the effective use of both translation memory ( tm ) and machine translation ( mt ) technology within industrial language service providers ( lsps ) localization workflows. <eos> lsps have long used translation memory ( tm ) technology to assist the translation process. <eos> recent research shows how mt systems can be combined with tms in computer aided translation ( cat ) systems, selecting either tm or mt output based on sophisticated translation quality estimation without access to a reference. <eos> however, to date there are no commercially available frameworks for this. <eos> tmtprime takes confidence estimation out of the lab and provides a commercially viable platform that allows for the seamless integration of mt with legacy tm systems to provide the most effective ( least effort/cost ) translation options to human translators, based on the tmtprime confidence score.
anafora is a newly-developed open source web-based text annotation tool built to be lightweight, flexible, easy to use and capable of annotating with a variety of schemas, simple and complex. <eos> anafora allows secure web-based annotation of any plaintext file with both spanned ( e.g. <eos> named entity or markable ) and relation annotations, as well as adjudication for both types of annotation. <eos> anafora offers automatic set assignment and progress-tracking, centralized and humaneditable xml annotation schemas, and filebased storage and organization of data in a human-readable single-file xml format.
hand gesture-based input systems have been in active research, yet most of them focus only on single character recognition. <eos> we propose koosho : an environment for japanese input based on aerial hand gestures. <eos> the system provides an integrated experience of character input, kana-kanji conversion, and search result visualization. <eos> to achieve faster input, users only have to input consonant, which is then converted directly to kanji sequences by direct consonant decoding. <eos> the system also shows suggestions to complete the user input. <eos> the comparison with voice recognition and a screen keyboard showed that koosho can be a more practical solution compared to the existing system.
umls : :similarity is freely available open source software that allows a user to measure the semantic similarity or relatedness of biomedical terms found in the unified medical language system ( umls ). <eos> it is written in perl and can be used via a command line interface, an api, or a web interface.
we present kelvin, an automated system for processing a large text corpus and distilling a knowledge base about persons, organizations, and locations. <eos> we have tested the kelvin system on several corpora, including : ( a ) the tac kbp 2012 cold start corpus which consists of public web pages from the university of pennsylvania, and ( b ) a subset of 26k news articles taken from english gigaword 5th edition. <eos> our naacl hlt 2013 demonstration permits a user to interact with a set of searchable html pages, which are automatically generated from the knowledge base. <eos> each page contains information analogous to the semi-structured details about an entity that are present in wikipedia infoboxes, along with hyperlink citations to supporting text.
we introduce an efficient, interactive framework ? argviz ? for experts to analyze the dynamic topical structure of multi-party conversations. <eos> users inject their needs, expertise, and insights into models via iterative topic refinement. <eos> the refined topics feed into a segmentation model, whose outputs are shown to users via multiple coordinated views.
we introduce an approach to optimize a machine translation ( mt ) system on multiple metrics simultaneously. <eos> different metrics ( e.g. <eos> bleu, ter ) focus on different aspects of translation quality ; our multi-objective approach leverages these diverse aspects to improve overall quality. <eos> our approach is based on the theory of pareto optimality. <eos> it is simple to implement on top of existing single-objective optimization methods ( e.g. <eos> mert, pro ) and outperforms ad hoc alternatives based on linear-combination of metrics. <eos> we also discuss the issue of metric tunability and show that our pareto approach is more effective in incorporating new metrics from mt evaluation for mt optimization.
with a few exceptions, discriminative training in statistical machine translation ( smt ) has been content with tuning weights for large feature sets on small development data. <eos> evidence from machine learning indicates that increasing the training sample size results in better prediction. <eos> the goal of this paper is to show that this common wisdom can also be brought to bear upon smt. <eos> we deploy local features for scfg-based smt that can be read off from rules at runtime, and present a learning algorithm that applies `1/`2 regularization for joint feature selection over distributed stochastic learning processes. <eos> we present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.
parallel data in the domain of interest is the key resource when training a statistical machine translation ( smt ) system for a specific purpose. <eos> since ad-hoc manual translation can represent a significant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. <eos> in this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data. <eos> we consider two scenarios, 1 ) monolingual samples in the source and target languages are available and 2 ) an additional small amount of parallel corpus is also available. <eos> we propose methods for predicting learning curves in both these scenarios.
this paper presents a probabilistic framework that combines multiple knowledge sources for haptic voice recognition ( hvr ), a multimodal input method designed to provide efficient text entry on modern mobile devices. <eos> hvr extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. <eos> this paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. <eos> in addition to the acoustic and language models used in automatic speech recognition systems, hvr uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. <eos> experimental results show that both the word error rate and runtime factor can be reduced by a factor of two using hvr.
we investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. <eos> we present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units ( e.g., phones ) and learns a hidden markov model ( hmm ) for each induced acoustic unit. <eos> our approach is formulated as a dirichlet process mixture model in which each mixture is an hmm that represents a sub-word unit. <eos> we apply our model to the timit corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with english phones and also produces better segmentation than the state-of-the-art unsupervised baseline. <eos> we test the quality of the learned acoustic models on a spoken term detection task. <eos> compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model.
conventional automated essay scoring ( aes ) measures may cause severe problems when directly applied in scoring automatic speech recognition ( asr ) transcription as they are error sensitive and unsuitable for the characteristic of asr transcription. <eos> therefore, we introduce a framework of finite state transducer ( fst ) to avoid the shortcomings. <eos> compared with the latent semantic analysis with support vector regression ( lsa-svr ) method ( stands for the conventional measures ), our fst method shows better performance especially towards the asr transcription. <eos> in addition, we apply the synonyms similarity to expand the fst model. <eos> the final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation ( 0.87 ) between human raters.
in this paper, we develop an rst-style textlevel discourse parser, based on the hilda discourse parser ( hernault et al, 2010b ). <eos> we significantly improve its tree-building step by incorporating our own rich linguistic features. <eos> we also analyze the difficulty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourseparsing performance under different discourse conditions.
we describe a discourse annotation scheme for chinese and report on the preliminary results. <eos> our scheme, inspired by the penn discourse treebank ( pdtb ), adopts the lexically grounded approach ; at the same time, it makes adaptations based on the linguistic and statistical characteristics of chinese text. <eos> annotation results show that these adaptations work well in practice. <eos> our scheme, taken together with other pdtb-style schemes ( e.g. <eos> for english, turkish, hindi, and czech ), affords a broader perspective on how the generalized lexically grounded approach can flesh itself out in the context of cross-linguistic annotation of discourse relations.
we propose a new approach to characterizing the timeline of a text : temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like before, after, overlap and identity. <eos> we annotate a corpus of children ? s stories with temporal dependency trees, achieving agreement ( krippendorff ? s alpha ) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. <eos> we compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. <eos> our analysis of the dependency parser errors gives some insights into future research directions.
temporal reasoners for document understanding typically assume that a document ? s creation date is known. <eos> algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. <eos> unfortunately, the timestamp is not always known, particularly on the web. <eos> this paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. <eos> the first is a discriminative classifier with new features extracted from the text ? s time expressions ( e.g., ? since 1999 ? ). <eos> this model alone improves on previous generative models by 77 %. <eos> the second model learns probabilistic constraints between time expressions and the unknown document time. <eos> imposing these learned constraints on the discriminative model further improves its accuracy. <eos> finally, we present a new experiment design that facilitates easier comparison by future work.
although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. <eos> this paper proposes a methodological approach to temporally anchored relation extraction. <eos> our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. <eos> we use a rich graphbased document-level representation to generate novel features for this task. <eos> results show that our implementation for temporal anchoring is able to achieve a 69 % of the upper bound performance imposed by the relation extraction step. <eos> compared to the state of the art, the overall system achieves the highest precision reported.
learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. <eos> in this paper we address the problem of learning transitive graphs that describe entailment rules between predicates ( termed entailment graphs ). <eos> we first identify that entailment graphs exhibit a ? tree-like ? <eos> property and are very similar to a novel type of graph termed forest-reducible graph. <eos> we utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. <eos> we compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm.
comprehending action preconditions and effects is an essential step in modeling the dynamics of the world. <eos> in this paper, we express the semantics of precondition relations extracted from text in terms of planning operations. <eos> the challenge of modeling this connection is to ground language at the level of relations. <eos> this type of grounding enables us to create high-level plans based on language abstractions. <eos> our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations. <eos> we implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempts. <eos> when applied to a complex virtual world and text describing that world, our relation extraction technique performs on par with a supervised baseline, yielding an f-measure of 66 % compared to the baseline ? s 65 %. <eos> additionally, we show that a high-level planner utilizing these extracted relations significantly outperforms a strong, text unaware baseline ? <eos> successfully completing 80 % of planning tasks as compared to 69 % for the baseline.1
our research aims at building computational models of word meaning that are perceptually grounded. <eos> using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. <eos> our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks ( accounting for semantic relatedness ), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. <eos> moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.
when automatically translating from a weakly inflected source language like english to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. <eos> to address this issue, we present a target-side, class-based agreement model. <eos> agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. <eos> for english-to-arabic translation, our model yields a +1.04 bleu average improvement over a state-of-the-art baseline. <eos> the model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders.
in this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages. <eos> we present a modification of the method shown in ( ravi and knight, 2011 ) that is scalable to vocabulary sizes of several thousand words. <eos> on the task shown in ( ravi and knight, 2011 ) we obtain better results with only 5 % of the computational effort when running our method with an n-gram language model. <eos> the efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the verbmobil corpus. <eos> we also report results using data from the monolingual french and english gigaword corpora.
in this paper, we demonstrate that accurate machine translation is possible without the concept of ? words, ? <eos> treating mt as a problem of transformation between character strings. <eos> we achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based mt framework. <eos> we also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. <eos> in an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.
long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. <eos> however, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. <eos> in this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. <eos> we apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. <eos> when using these improved tools in a language model for speech recognition, we obtain significant speed improvements with bothn -best and hill climbing rescoring, and show that up-training leads to wer reduction.
during early language acquisition, infants must learn both a lexicon and a model of phonetics that explains how lexical items can vary in pronunciation ? for instance ? the ? <eos> might be realized as or. <eos> previous models of acquisition have generally tackled these problems in isolation, yet behavioral evidence suggests infants acquire lexical and phonetic knowledge simultaneously. <eos> we present a bayesian model that clusters together phonetic variants of the same lexical item while learning both a language model over lexical items and a log-linear model of pronunciation variability based on articulatory features. <eos> the model is trained on transcribed surface pronunciations, and learns by bootstrapping, without access to the true lexicon. <eos> we test the model using a corpus of child-directed speech with realistic phonetic variation and either gold standard or automatically induced word boundaries. <eos> in both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item has a unique pronunciation.
we address the problem of learning the mapping between words and their possible pronunciations in terms of sub-word units. <eos> most previous approaches have involved generative modeling of the distribution of pronunciations, usually trained to maximize likelihood. <eos> we propose a discriminative, feature-rich approach using large-margin learning. <eos> this approach allows us to optimize an objective closely related to a discriminative task, to incorporate a large number of complex features, and still do inference efficiently. <eos> we test the approach on the task of lexical access ; that is, the prediction of a word given a phonetic transcription. <eos> in experiments on a subset of the switchboard conversational speech corpus, our models thus far improve classification error rates from a previously published result of 29.1 % to about 15 %. <eos> we find that large-margin approaches outperform conditional random field learning, and that the passive-aggressive algorithm for largemargin learning is faster to converge than the pegasos algorithm.
the integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. <eos> this paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. <eos> firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. <eos> however, it has no statistically significant impact in terms of f-score as incorrect multiword expression recognition has important side effects on parsing. <eos> secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.
most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. <eos> in this paper, we present an approach to enriching high-order feature representations for graph-based dependency parsing models using a dependency language model and beam search. <eos> the dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. <eos> based on the dependency language model, we represent a set of features for the parsing model. <eos> finally, the features are efficiently integrated into the parsing model during decoding using beam search. <eos> our approach has two advantages. <eos> firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. <eos> secondly our approach does not increase the decoding complexity. <eos> we evaluate the proposed approach on english and chinese data. <eos> the experimental results show that our new parser achieves the best accuracy on the chinese data and comparable accuracy with the best known systems on the english data.
we introduce a spectral learning algorithm for latent-variable pcfgs ( petrov et al, 2006 ). <eos> under a separability ( singular value ) condition, we prove that the method provides consistent parameter estimates.
we address the issue of consuming heterogeneous annotation data for chinese word segmentation and part-of-speech tagging. <eos> we empirically analyze the diversity between two representative corpora, i.e. <eos> penn chinese treebank ( ctb ) and pku ? s people ? s daily ( ppd ), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. <eos> the analysis is further exploited to improve processing accuracy by ( 1 ) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and ( 2 ) re-training models with high quality automatically converted data to reduce the estimation error. <eos> evaluation on the ctb and ppd data shows that our novel model achieves a relative error reduction of 11 % over the best reported result in the literature.
from the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for chinese pos tagging, an important and challenging task for chinese language processing. <eos> paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. <eos> syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. <eos> experiments on the penn chinese treebank demonstrate the importance of both paradigmatic and syntagmatic relations. <eos> our linguistically motivated approaches yield a relative error reduction of 18 % in total over a stateof-the-art baseline.
we present a joint model for chinese word segmentation and new word detection. <eos> we present high dimensional new features, including word-based features and enriched edge ( label-transition ) features, for the joint modeling. <eos> as we know, training a word segmentation system on large-scale datasets is already costly. <eos> in our case, adding high dimensional new features will further slow down the training speed. <eos> to solve this problem, we propose a new training method, adaptive online gradient descent based on feature frequency information, for very fast online training of the parameters, even given large-scale datasets with high dimensional features. <eos> compared with existing training methods, our training method is an order magnitude faster in terms of training time, and can achieve equal or even higher accuracies. <eos> the proposed fast training method is a general purpose optimization method, and it is not limited in the specific task discussed in this paper.
in this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely verbnet and framenet. <eos> first, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. <eos> then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in support vector machines. <eos> the extensive empirical analysis on verbnet class and frame detection shows that our models capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art.
previous research has conflicting conclusions on whether word sense disambiguation ( wsd ) systems can improve information retrieval ( ir ) performance. <eos> in this paper, we propose a method to estimate sense distributions for short queries. <eos> together with the senses predicted for words in documents, we propose a novel approach to incorporate word senses into the language modeling approach to ir and also exploit the integration of synonym relations. <eos> our experimental results on standard trec collections show that using the word senses tagged by a supervised wsd system, we obtain significant improvements over a state-of-the-art ir system.
this paper addresses the search problem in textual inference, where systems need to infer one piece of text from another. <eos> a prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations, a.k.a. <eos> a proof, while estimating the proof ? s validity. <eos> this raises a search challenge of finding the best possible proof. <eos> we explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference : a gradient-style evaluation function, and a locallookahead node expansion method. <eos> evaluations, using the open-source system, biutee, show the contribution of these ideas to search efficiency and proof quality.
in this paper, we address the issue for learning better translation consensus in machine translation ( mt ) research, and explore the search of translation consensus from similar, rather than the same, source sentences or their spans. <eos> unlike previous work on this topic, we formulate the problem as structured labeling over a much smaller graph, and we propose a novel structured label propagation for the task. <eos> we convert such graph-based translation consensus from similar source strings into useful features both for n-best output reranking and for decoding algorithm. <eos> experimental results show that, our method can significantly improve machine translation performance on both iwslt and nist data, compared with a state-ofthe-art baseline.
two decades after their invention, the ibm word-based translation models, widely available in the giza++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. <eos> although many models have surpassed them in accuracy, none have supplanted them in practice. <eos> in this paper, we propose a simple extension to the ibm models : an `0 prior to encourage sparsity in the word-to-word translation model. <eos> we explain how to implement this extension efficiently for large-scale data ( also released as a modification to giza++ ) and demonstrate, in experiments on czech, arabic, chinese, and urdu to english translation, significant improvements over ibm model 4 in both word alignment ( up to +6.7 f1 ) and translation quality ( up to +1.4 bleu ).
we describe a joint model for understanding user actions in natural language utterances. <eos> our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance ? s target domain ( e.g. <eos> movies ), intention ( e.g., finding a movie ) along with other semantic units ( e.g., movie name ). <eos> we inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model. <eos> using utterances from five domains, our approach shows up to 4.5 % improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model ( which requires fully labeled data ).
aspect extraction is a central problem in sentiment analysis. <eos> current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. <eos> by categorizing, we mean the synonymous aspects should be clustered into the same category. <eos> in this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. <eos> this setting is important because categorizing aspects is a subjective task. <eos> for different application purposes, different categorizations may be needed. <eos> some form of user guidance is desired. <eos> in this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. <eos> our experimental results show that the two proposed models are indeed able to perform the task effectively.
most information extraction ( ie ) systems identify facts that are explicitly stated in text. <eos> however, in natural language, some facts are implicit, and identifying them requires ? reading between the lines ?. <eos> human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts. <eos> we propose an approach that uses bayesian logic programs ( blps ), a statistical relational model combining firstorder logic and bayesian networks, to infer additional implicit information from extracted facts. <eos> it involves learning uncertain commonsense knowledge ( in the form of probabilistic first-order rules ) from natural language text by mining a large corpus of automatically extracted facts. <eos> these rules are then used to derive additional facts from extracted information using blp inference. <eos> experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach.
we present a holistic data-driven approach to image description generation, exploiting the vast amount of ( noisy ) parallel image data and associated natural language descriptions available on the web. <eos> more specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. <eos> we cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. <eos> evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.
this paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. <eos> a key insight in our approach is to reduce the tasks of content selection ( ? what to say ? ) <eos> and surface realization ( ? how to say ? ) <eos> into a common parsing problem. <eos> we define a probabilistic context-free grammar that describes the structure of the input ( a corpus of database records and text describing some of them ) and represent it compactly as a weighted hypergraph. <eos> the hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. <eos> we propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting. <eos> experimental evaluation on the atis domain shows that our model outperforms a competitive discriminative system both using bleu and in a judgment elicitation study.
methods that measure compatibility between mention pairs are currently the dominant approach to coreference. <eos> however, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. <eos> as these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. <eos> in this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. <eos> these trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. <eos> we demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single cpu.
to address semantic ambiguities in coreference resolution, we use web n-gram features that capture a range of world knowledge in a diffuse but robust way. <eos> specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. <eos> when added to a state-of-the-art coreference baseline, our web features give significant gains on multiple datasets ( ace 2004 and ace 2005 ) and metrics ( muc and b3 ), resulting in the best results reported to date for the end-to-end task of coreference resolution.
the rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. <eos> many of these groups discuss ideological and political topics. <eos> it is not uncommon that the participants in such discussions split into two or more subgroups. <eos> the members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. <eos> in this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. <eos> we analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. <eos> we use attitude predictions to construct an attitude vector for each discussant. <eos> we use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. <eos> we compare our methods to text clustering and other baselines, and show that our method achieves promising results.
extracting sentiment and topic lexicons is important for opinion mining. <eos> previous works have showed that supervised learning methods are superior for this task. <eos> however, the performance of supervised methods highly relies on manually labeled training data. <eos> in this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. <eos> the framework is twofold. <eos> in the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. <eos> in the second step, we propose a novel relational adaptive bootstrapping ( rap ) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. <eos> experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.
we present a novel approach for building verb subcategorization lexicons using a simple graphical model. <eos> in contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. <eos> our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. <eos> this quantitative evaluation is complemented by a qualitative discussion of verbs and their frames. <eos> we discuss the advantages of graphical models for this task, in particular the ease of integrating semantic information about verbs and arguments in a principled fashion. <eos> we conclude with future work to augment the approach.
learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language. <eos> it is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context. <eos> recent work by chen and mooney ( 2011 ) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs. <eos> while the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets. <eos> in this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results. <eos> we show that by changing the grammar of the formal meaning representation language and training on additional data collected from amazon ? s mechanical turk we can further improve the results. <eos> we also include experimental results on a chinese translation of the training data to demonstrate the generality of our approach.
we propose symbol-refined tree substitution grammars ( sr-tsgs ) for syntactic parsing. <eos> an sr-tsg is an extension of the conventional tsg model where each nonterminal symbol can be refined ( subcategorized ) to fit the training data. <eos> we aim to provide a unified model where tsg rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. <eos> we present a novel probabilistic sr-tsg model based on the hierarchical pitman-yor process to encode backoff smoothing from a fine-grained sr-tsg to simpler cfg rules, and develop an efficient training method based on markov chain monte carlo ( mcmc ) sampling. <eos> our sr-tsg parser achieves an f1 score of 92.4 % in the wall street journal ( wsj ) english penn treebank parsing task, which is a 7.7 point improvement over a conventional bayesian tsg parser, and better than state-of-the-art discriminative reranking parsers.
learning for sentence re-writing is a fundamental task in natural language processing and information retrieval. <eos> in this paper, we propose a new class of kernel functions, referred to as string re-writing kernel, to address the problem. <eos> a string re-writing kernel measures the similarity between two pairs of strings, each pair representing re-writing of a string. <eos> it can capture the lexical and structural similarity between two pairs of sentences without the need of constructing syntactic trees. <eos> we further propose an instance of string rewriting kernel which can be computed efficiently. <eos> experimental results on benchmark datasets show that our method can achieve better results than state-of-the-art methods on two sentence re-writing learning tasks : paraphrase identification and recognizing textual entailment.
to adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. <eos> in this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. <eos> our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. <eos> experimental result on the nist chinese-english translation task shows that our approach significantly outperforms the baseline system.
we propose a novel model to automatically extract transliteration pairs from parallel corpora. <eos> our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. <eos> we model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. <eos> we evaluate on news 2010 shared task data and on parallel corpora with competitive results.
this paper presents a novel method to suggest long word reorderings to a phrase-based smt decoder. <eos> we address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. <eos> then we use reordered n-gram lms to rank the resulting permutations and select the n-best for translation. <eos> finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. <eos> in this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. <eos> the proposed techniques are tested on arabic-english and german-english using well-known smt benchmarks.
many semantic parsing models use tree transformations to map between natural language and meaning representation. <eos> however, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. <eos> this paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. <eos> in particular, this paper further introduces a variational bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.
optimising for one grammatical representation, but evaluating over a different one is a particular challenge for parsers and n-best ccg parsing. <eos> we find that this mismatch causes many n-best ccg parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best f-score by 0.7 % and reranking accuracy by 0.4 %. <eos> we also present a comprehensive analysis of errors made by the c & c ccg parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy.
recently, it was shown ( kuhlmann, satta : tree-adjoining grammars are not closed under strong lexicalization. <eos> comput. <eos> linguist., 2012 ) that finitely ambiguous tree adjoining grammars can not be transformed into a normal form ( preserving the generated tree language ), in which each production contains a lexical symbol. <eos> a more powerful model, the simple context-free tree grammar, admits such a normal form. <eos> it can be effectively constructed and the maximal rank of the nonterminals only increases by 1. <eos> thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.
as one of the most popular micro-blogging services, twitter attracts millions of users, producing millions of tweets daily. <eos> shared information through this service spreads faster than would have been possible with traditional sources, however the proliferation of user-generation content poses challenges to browsing and finding valuable information. <eos> in this paper we propose a graph-theoretic model for tweet recommendation that presents users with items they may have an interest in. <eos> our model ranks tweets and their authors simultaneously using several networks : the social network connecting the users, the network connecting the tweets, and a third network that ties the two together. <eos> tweet and author entities are ranked following a co-ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweets and their authors that could be reflected in the rankings. <eos> we show that this framework can be parametrized to take into account user preferences, the popularity of tweets and their authors, and diversity. <eos> experimental evaluation on a large dataset shows that our model outperforms competitive approaches by a large margin.
tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. <eos> we study the problem of named entity normalization ( nen ) for tweets. <eos> two main challenges are the errors propagated from named entity recognition ( ner ) and the dearth of information in a single tweet. <eos> we propose a novel graphical model to simultaneously conduct ner and nen on multiple tweets to address these challenges. <eos> particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. <eos> we evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the f1 from 80.2 % to 83.6 % for ner, and the accuracy from 79.4 % to 82.6 % for nen, respectively.
microblogs such as twitter reflect the general public ? s reactions to major events. <eos> bursty topics from microblogs reveal what events have attracted the most online attention. <eos> although bursty event detection from text streams has been studied before, previous work may not be suitable for microblogs because compared with other text streams such as news articles and scientific publications, microblog posts are particularly diverse and noisy. <eos> to find topics that have bursty patterns on microblogs, we propose a topic model that simultaneously captures two observations : ( 1 ) posts published around the same time are more likely to have the same topic, and ( 2 ) posts published by the same user are more likely to have the same topic. <eos> the former helps find eventdriven posts while the latter helps identify and filter out ? personal ? <eos> posts. <eos> our experiments on a large twitter dataset show that there are more meaningful and unique bursty topics in the top-ranked results returned by our model than an lda baseline and two degenerate variations of our model. <eos> we also show some case studies that demonstrate the importance of considering both the temporal information and users ? <eos> personal interests for bursty topic detection from microblogs.
there are a growing number of popular web sites where users submit and review instructions for completing tasks as varied as building a table and baking a pie. <eos> in addition to providing their subjective evaluation, reviewers often provide actionable refinements. <eos> these refinements clarify, correct, improve, or provide alternatives to the original instructions. <eos> however, identifying and reading all relevant reviews is a daunting task for a user. <eos> in this paper, we propose a generative model that jointly identifies user-proposed refinements in instruction reviews at multiple granularities, and aligns them to the appropriate steps in the original instructions. <eos> labeled data is not readily available for these tasks, so we focus on the unsupervised setting. <eos> in experiments in the recipe domain, our model provides 90.1 % f1 for predicting refinements at the review level, and 77.0 % f1 for predicting refinement segments within reviews.
online forums are becoming a popular resource in the state of the art question answering ( qa ) systems. <eos> because of its nature as an online community, it contains more updated knowledge than other places. <eos> however, going through tedious and redundant posts to look for answers could be very time consuming. <eos> most prior work focused on extracting only question answering sentences from user conversations. <eos> in this paper, we introduce the task of sentence dependency tagging. <eos> finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. <eos> we use linear-chain conditional random fields ( crf ) for sentence type tagging, and a 2d crf to label the dependency relation between sentences. <eos> our experimental results show that our proposed approach performs well for sentence dependency tagging. <eos> this dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.
we predict entity type distributions in web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated. <eos> we jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. <eos> we apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. <eos> our models are efficiently trained using maximum likelihood estimation over millions of real-world web search queries. <eos> we show that modeling user intent significantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance.
the amount of labeled sentiment data in english is much larger than that in other languages. <eos> such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language ( e.g. <eos> chinese ) using labeled data in the source language ( e.g. <eos> english ). <eos> most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language. <eos> this approach suffers from the limited coverage of vocabulary in the machine translation results. <eos> in this paper, we propose a generative cross-lingual mixture model ( clmm ) to leverage unlabeled bilingual parallel data. <eos> by fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly. <eos> experiments on multiple data sets show that clmm is consistently effective in two settings : ( 1 ) labeled data in the target language are unavailable ; and ( 2 ) labeled data in the target language are also available.
we present a novel answer summarization method for community question answering services ( cqas ) to address the problem of ? incomplete answer ?, i.e., the ? best answer ? <eos> of a complex multi-sentence question misses valuable information that is contained in other answers. <eos> in order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general conditional random field ( crf ) based answer summary method with group l1 regularization. <eos> various textual and non-textual qa features are explored. <eos> specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. <eos> to further unleash the potential of the abundant cqa features, we introduce the group l1 regularization for feature learning. <eos> experimental results on a yahoo ! <eos> answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cqa summarization task.
in recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. <eos> however the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. <eos> in this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. <eos> we show that this tree mining algorithm permits identifying not only errors in the generation system ( grammar, lexicon ) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.
this paper studies the problem of sentencelevel semantic coherence by answering satstyle sentence completion questions. <eos> these questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. <eos> we tackle the problem with two approaches : methods that use local lexical information, such as the n-grams of a classical language model ; and methods that evaluate global coherence, such as latent semantic analysis. <eos> we evaluate these methods on a suite of practice sat questions, and on a recently released sentence completion task based on data taken from five conan doyle novels. <eos> we find that by fusing local and global information, we can exceed 50 % on this task ( chance baseline is 20 % ), and we suggest some avenues for further research.
sequential modeling has been widely used in a variety of important applications including named entity recognition and shallow parsing. <eos> however, as more and more real time large-scale tagging applications arise, decoding speed has become a bottleneck for existing sequential tagging algorithms. <eos> in this paper we propose 1-best a*, 1-best iterative a*, k-best a* and k-best iterative viterbi a* algorithms for sequential decoding. <eos> we show the efficiency of these proposed algorithms for five nlp tagging tasks. <eos> in particular, we show that iterative viterbi a* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. <eos> this algorithm makes real-time large-scale tagging applications with thousands of labels feasible.
bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them. <eos> this paper introduces a novel variant of the yarowsky algorithm based on this view. <eos> it is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function. <eos> the experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.
we present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language. <eos> our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource-rich languages. <eos> the algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly. <eos> the model factorizes the process of generating a dependency tree into two steps : selection of syntactic dependents and their ordering. <eos> being largely languageuniversal, the selection component is learned in a supervised fashion from all the training languages. <eos> in contrast, the ordering decisions are only influenced by languages with similar properties. <eos> we systematically model this cross-lingual sharing using typological features. <eos> in our experiments, the model consistently outperforms a state-of-the-art multilingual parser. <eos> the largest improvement is achieved on the non indo-european languages yielding a gain of 14.4 %.1
metalanguage is an essential linguistic mechanism which allows us to communicate explicit information about language itself. <eos> however, it has been underexamined in research in language technologies, to the detriment of the performance of systems that could exploit it. <eos> this paper describes the creation of the first tagged and delineated corpus of english metalanguage, accompanied by an explicit definition and a rubric for identifying the phenomenon in text. <eos> this resource will provide a basis for further studies of metalanguage and enable its utilization in language technologies.
we argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. <eos> specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative bayesian non-parametric model. <eos> at inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. <eos> we propose a simple approximate learning algorithm for our set-up which results in efficient inference. <eos> when applied to german-english parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.
this paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. <eos> this algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. <eos> experiments on the english penn treebank data and the chinese conll-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.
the language mix consists of all strings over the three-letter alphabet { a, b, c } that contain an equal number of occurrences of each letter. <eos> we prove joshi ? s ( 1985 ) conjecture that mix is not a tree-adjoining language.
we present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing. <eos> several types of transformation patterns ( tp ) are designed to capture the systematic annotation inconsistencies among different treebanks. <eos> based on such tps, we design quasisynchronous grammar features to augment the baseline parsing models. <eos> our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target treebanks ( penn chinese treebank 5.1 and 6.0 ) using the chinese dependency treebank as the source treebank. <eos> the improvements are respectively 1.37 % and 1.10 % with automatic part-of-speech tags. <eos> moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion.
we present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes ( or parts of attributes ). <eos> the model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. <eos> transductive learning from a few seeds and a collection of mention tokens combines bayesian inference and conditional estimation. <eos> we evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.
in this paper we propose a method to automatically label multi-lingual data with named entity tags. <eos> we build on prior work utilizing wikipedia metadata and show how to effectively combine the weak annotations stemming from wikipedia metadata with information obtained through english-foreign language parallel wikipedia sentences. <eos> the combination is achieved using a novel semi-crf model for foreign sentence tagging in the context of a parallel english sentence. <eos> the model outperforms both standard annotation projection methods and methods based solely on wikipedia metadata.
in this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. <eos> we describe all the linguistic resources and natural language processing techniques that we have exploited for this task. <eos> then, we analyze the performance of the system that we have developed. <eos> the empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation of the naming process.
to discover relation types from text, most methods cluster shallow or syntactic patterns of relation mentions, but consider only one possible sense per pattern. <eos> in practice this assumption is often violated. <eos> in this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns. <eos> in particular, we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features. <eos> we merge these sense clusters into semantic relations using hierarchical agglomerative clustering. <eos> we compare against several baselines : a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. <eos> experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial.
in relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as freebase, as a source of supervision. <eos> when a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. <eos> however, this heuristic can fail with the result that some sentences are labeled wrongly. <eos> this noisy labeled data causes poor extraction performance. <eos> in this paper, we propose a method to reduce the number of wrong labels. <eos> we present a novel generative model that directly models the heuristic labeling process of distant supervision. <eos> the model predicts whether assigned labels are correct or wrong via its hidden variables. <eos> our experimental results show that this model detected wrong labels with higher performance than baseline methods. <eos> in the experiment, we also found that our wrong label reduction boosted the performance of relation extraction.
we present an approach for detecting salient ( important ) dates in texts in order to automatically build event timelines from a search query ( e.g. <eos> the name of an event or person, etc. ). <eos> this work was carried out on a corpus of newswire texts in english provided by the agence france presse ( afp ). <eos> in order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. <eos> we focused only on extracting the dates and not the events to which they are related.
we propose a latent variable model to enhance historical analysis of large corpora. <eos> this work extends prior work in topic modelling by incorporating metadata, and the interactions between the components in metadata, in a general way. <eos> to test this, we collect a corpus of slavery-related united states property law judgements sampled from the years 1730 to 1866. <eos> we study the language use in these legal cases, with a special focus on shifts in opinions on controversial topics across different regions. <eos> because this is a longitudinal data set, we are also interested in understanding how these opinions change over the course of decades. <eos> we show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identification tasks. <eos> experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting, and that these improvements are robust across different parameter settings.
previous work using topic model for statistical machine translation ( smt ) explore topic information at the word level. <eos> however, smt has been advanced from word-based paradigm to phrase/rule-based paradigm. <eos> we therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. <eos> we associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. <eos> we show that our model significantly improves the translation performance over the baseline on nist chinese-to-english translation experiments. <eos> our model also achieves a better performance and a faster speed than previous approaches that work at the word level.
in this paper, we encode topic dependencies in hierarchical multi-label text categorization ( tc ) by means of rerankers. <eos> we represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. <eos> additionally, to better investigate the role of category relationships, we consider two interesting cases : ( i ) traditional schemes in which node-fathers include all the documents of their child-categories ; and ( ii ) more general schemes, in which children can include documents not belonging to their fathers. <eos> the extensive experimentation on reuters corpus volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.
prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. <eos> across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. <eos> prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. <eos> as lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. <eos> by including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for english dependencies with 93.55 % correct attachments on the current standard. <eos> furthermore, conjunctions are attached with an accuracy of 90.8 %, and prepositions with an accuracy of 87.4 %.
treebanks are not large enough to reliably model precise lexical phenomena. <eos> this deficiency provokes attachment errors in the parsers trained on such data. <eos> we propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser. <eos> experiments on the french treebank showed a relative decrease of the error rate of 7.1 % labeled accuracy score yielding the best parsing results on this treebank.
the chinese comma signals the boundary of discourse units and also anchors discourse relations between adjacent text spans. <eos> in this work, we propose a discourse structureoriented classification of the comma that can be automatically extracted from the chinese treebank based on syntactic patterns. <eos> we then experimented with two supervised learning methods that automatically disambiguate the chinese comma based on this classification. <eos> the first method integrates comma classification into parsing, and the second method adopts a ? post-processing ? <eos> approach that extracts features from automatic parses to train a classifier. <eos> the experimental results show that the second approach compares favorably against the first approach.
previous work on classifying information status ( nissim, 2006 ; rahman and ng, 2011 ) is restricted to coarse-grained classification and focuses on conversational dialogue. <eos> we here introduce the task of classifying finegrained information status and work on written text. <eos> we add a fine-grained information status layer to the wall street journal portion of the ontonotes corpus. <eos> we claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. <eos> our approach strongly outperforms reimplementations of previous work.
large e-commerce enterprises feature millions of items entered daily by a large variety of sellers. <eos> while some sellers provide rich, structured descriptions of their items, a vast majority of them provide unstructured natural language descriptions. <eos> in the paper we present a 2 steps method for structuring items into descriptive properties. <eos> the first step consists in unsupervised property discovery and extraction. <eos> the second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm. <eos> we evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall.
the named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique realworld entity. <eos> this task can be modeled as a classification problem, provided that positive and negative examples are available for learning binary classifiers. <eos> high-quality senseannotated data, however, are hard to be obtained in streaming environments, since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream. <eos> on the other hand, few positive examples plus large amounts of unlabeled data may be easily acquired. <eos> producing binary classifiers directly from this data, however, leads to poor disambiguation performance. <eos> thus, we propose to enhance the quality of the classifiers using finer-grained variations of the well-known expectationmaximization ( em ) algorithm. <eos> we conducted a systematic evaluation using twitter streaming data and the results show that our classifiers are extremely effective, providing improvements ranging from 1 % to 20 %, when compared to the current state-of-the-art biased svms, being more than 120 times faster.
classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. <eos> to mitigate this cost, nlu researchers have considered two newly available sources of less expensive ( but potentially lower quality ) labeled data from distant supervision and crowd sourcing. <eos> there is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. <eos> to fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. <eos> we use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. <eos> our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality ( f1 score ). <eos> in contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.
this paper presents a novel sequence labeling model based on the latent-variable semimarkov conditional random fields for jointly extracting argument roles of events from texts. <eos> the model takes in coarse mention and type information and predicts argument roles for a given event template. <eos> this paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. <eos> our key contribution is a novel learning framework called structured preference modeling ( pm ), that allows arbitrary preference to be assigned to certain structures during the learning procedure. <eos> we establish and discuss connections between this framework and other existing works. <eos> we show empirically that the structured preferences are crucial to the success of our task. <eos> our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.
this paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. <eos> the approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. <eos> such an approach can, for example, learn likely event durations and the fact that start times should come before end times. <eos> while the joint inference space is large, we demonstrate effective learning with a perceptron-style approach that uses simple, greedy beam decoding. <eos> empirical results in two benchmark domains demonstrate consistently strong performance on both mention detection and template filling tasks.
we present a novel approach to the automatic acquisition of a verbnet like classification of french verbs which involves the use ( i ) of a neural clustering method which associates clusters with features, ( ii ) of several supervised and unsupervised evaluation metrics and ( iii ) of various existing syntactic and semantic lexical resources. <eos> we evaluate our approach on an established test set and show that it outperforms previous related work with an fmeasure of 0.70.
sentence similarity is the process of computing a similarity score between two sentences. <eos> previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences. <eos> in this paper, we show that by carefully handling words that are not in the sentences ( missing words ), we can train a reliable latent variable model on sentences. <eos> in the process, we propose a new evaluation framework for sentence similarity : concept definition retrieval. <eos> the new framework allows for large scale tuning and testing of sentence similarity models. <eos> experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. <eos> our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.
unsupervised word representations are very useful in nlp tasks both as inputs to learning algorithms and as extra word features in nlp systems. <eos> however, most of these models are built with only local context and one representation per word. <eos> this is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. <eos> we present a new neural network architecture which 1 ) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2 ) accounts for homonymy and polysemy by learning multiple embeddings per word. <eos> we introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. <eos> 1
this paper uses an unsupervised model of grounded language acquisition to study the role that social cues play in language acquisition. <eos> the input to the model consists of ( orthographically transcribed ) child-directed utterances accompanied by the set of objects present in the non-linguistic context. <eos> each object is annotated by social cues, indicating e.g., whether the caregiver is looking at or touching the object. <eos> we show how to model the task of inferring which objects are being talked about ( and which words refer to which objects ) as standard grammatical inference, and describe pcfg-based unigram models and adaptor grammar-based collocation models for the task. <eos> exploiting social cues improves the performance of all models. <eos> our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consistent with the idea that children could discover the importance of particular social information sources during word learning.
predicate-argument structure contains rich semantic information of which statistical machine translation hasn ? t taken full advantage. <eos> in this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation : 1 ) a predicate translation model and 2 ) an argument reordering model. <eos> the predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. <eos> the argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. <eos> the two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on chinese-to-english translation tasks with large-scale training data. <eos> experimental results demonstrate that the two models significantly improve translation accuracy.
long distance word reordering is a major challenge in statistical machine translation research. <eos> previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. <eos> in this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. <eos> the ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. <eos> we evaluated our approach on largescale japanese-english and english-japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased smt system.
in this work, we introduce the teslacelab metric ( translation evaluation of sentences with linear-programming-based analysis ? <eos> character-level evaluation for languages with ambiguous word boundaries ) for automatic machine translation evaluation. <eos> for languages such as chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, tesla-celab acknowledges the advantage of character-level evaluation over word-level evaluation. <eos> by reformulating the problem in the linear programming framework, teslacelab addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. <eos> we show empirically that teslacelab significantly outperforms characterlevel bleu in the english-chinese translation evaluation tasks.
many machine translation ( mt ) evaluation metrics have been shown to correlate better with human judgment than bleu. <eos> in principle, tuning on these metrics should yield better systems than tuning on bleu. <eos> however, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. <eos> this paper presents port 1, a new mt evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning mt systems. <eos> port does not require external resources and is quick to compute. <eos> it has a better correlation with human judgment than bleu. <eos> we compare port-tuned mt systems to bleu-tuned baselines in five experimental conditions involving four language pairs. <eos> port tuning achieves consistently better performance than bleu tuning, according to four automated metrics ( including bleu ) and to human evaluation : in comparisons of outputs from 300 source sentences, human judges preferred the port-tuned output 45.3 % of the time ( vs. 32.7 % bleu tuning preferences and 22.0 % ties ).
statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. <eos> we propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. <eos> in this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. <eos> our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.
we present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. <eos> with the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. <eos> under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. <eos> our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on english-chinese translation tasks.
we propose a simple generative, syntactic language model that conditions on overlapping windows of tree context ( or treelets ) in the same way that n-gram language models condition on overlapping windows of linear context. <eos> we estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. <eos> we evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. <eos> our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. <eos> we also show fluency improvements in a preliminary machine translation experiment.
the problem addressed in this paper is to segment a given multilingual document into segments for each language and then identify the language of each segment. <eos> the problem was motivated by an attempt to collect a large amount of linguistic data for non-major languages from the web. <eos> the problem is formulated in terms of obtaining the minimum description length of a text, and the proposed solution finds the segments and their languages through dynamic programming. <eos> empirical results demonstrating the potential of this approach are presented for experiments using texts taken from the universal declaration of human rights and wikipedia, covering more than 200 languages.
in recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for nlp tasks. <eos> in particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. <eos> in this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. <eos> the central idea is exploiting existing commercial tools for advertising on the web, such as google adwords, to measure message impact in an ecological setting. <eos> the paper includes a description of the approach, tips for how to use adwords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach.
polarity classification of words is important for applications such as opinion mining and sentiment analysis. <eos> a number of sentiment word/sense dictionaries have been manually or ( semi ) automatically constructed. <eos> the dictionaries have substantial inaccuracies. <eos> besides obvious instances, where the same word appears with different polarities in different dictionaries, the dictionaries exhibit complex cases, which can not be detected by mere manual inspection. <eos> we introduce the concept of polarity consistency of words/senses in sentiment dictionaries in this paper. <eos> we show that the consistency problem is np-complete. <eos> we reduce the polarity consistency problem to the satisfiability problem and utilize a fast sat solver to detect inconsistencies in a sentiment dictionary. <eos> we perform experiments on four sentiment dictionaries and wordnet.
an ideal summarization system should produce summaries that have high content coverage and linguistic quality. <eos> many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. <eos> a current research focus is to process these sentences so that they read fluently as a whole. <eos> the current aesop task encourages research on evaluating summaries on content, readability, and overall responsiveness. <eos> in this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. <eos> the results show significantly improved performance over aesop 2011 submitted metrics.
in this paper we describe a method for simplifying sentences using phrase based machine translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. <eos> we compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the english part of wikipedia and simple wikipedia. <eos> human test subjects judge the output of the different systems. <eos> analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. <eos> we also argue that text readability metrics such as the flesch-kincaid grade level should be used with caution when evaluating the output of simplification systems.
all types of part-of-speech ( pos ) tagging errors have been equally treated by existing taggers. <eos> however, the errors are not equally important, since some errors affect the performance of subsequent natural language processing ( nlp ) tasks seriously while others do not. <eos> this paper aims to minimize these serious errors while retaining the overall performance of pos tagging. <eos> two gradient loss functions are proposed to reflect the different types of errors. <eos> they are designed to assign a larger cost to serious errors and a smaller one to minor errors. <eos> through a set of pos tagging experiments, it is shown that the classifier trained with the proposed loss functions reduces serious errors compared to state-of-the-art pos taggers. <eos> in addition, the experimental result on text chunking shows that fewer serious errors help to improve the performance of subsequent nlp tasks.
social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. <eos> it is of crucial importance to normalize the noisy nonstandard tokens before applying other nlp techniques. <eos> a major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. <eos> in this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. <eos> the system was evaluated on both word- and messagelevel using four sms and twitter data sets. <eos> results show that our system achieves over 90 % word-coverage across all data sets ( a 10 % absolute increase compared to state-ofthe-art ) ; the broad word-coverage can also successfully translate into message-level performance gain, yielding 6 % absolute increase compared to the best prior approach.
we propose the first joint model for word segmentation, pos tagging, and dependency parsing for chinese. <eos> based on an extension of the incremental joint model for pos tagging and dependency parsing ( hatori et al, 2011 ), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, pos tagging, and dependency parsing models. <eos> we also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. <eos> in experiments using the chinese treebank ( ctb ), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6 % for pos tagging and 2.4 % for dependency parsing. <eos> we also perform comparison experiments with the partially joint models.
this paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. <eos> experiments on english and chinese treebanks confirm its advantage over its first-order version. <eos> it achieves its best f1 scores of 91.86 % and 85.58 % on the two languages, respectively, and further pushes them to 92.80 % and 85.60 % via combination with other highperformance parsers.
we present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. <eos> the protocol uses distance-based metrics defined for the space of trees over lattices. <eos> our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios ( assuming gold segmented and tagged input ) and realistic ones ( not assuming gold segmentation and tags ). <eos> our evaluation of segmentation and parsing for modern hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios.
stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by ( i ) converting the output of a constituent parser, or ( ii ) predicting dependencies directly. <eos> previous comparisons of the two approaches for english suggest that starting from constituents yields higher accuracies. <eos> in this paper, we re-evaluate both methods for chinese, using more accurate dependency parsers than in previous work. <eos> our comparison of performance and efficiency across seven popular open source parsers ( four constituent and three dependency ) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. <eos> we demonstrate also that n-way jackknifing is a useful technique for producing automatic ( rather than gold ) partof-speech tags to train chinese dependency parsers. <eos> finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.
we present llccm, a log-linear variant of the constituent context model ( ccm ) of grammar induction. <eos> llccm retains the simplicity of the original ccm but extends robustly to long sentences. <eos> on sentences of up to length 40, llccm outperforms ccm by 13.9 % bracketing f1 and outperforms a right-branching baseline in regimes where ccm does not.
some statistical machine translation systems never see the light because the owner of the appropriate training data can not release them, and the potential user of the system can not disclose what should be translated. <eos> we propose a simple and practical encryption-based method addressing this barrier.
in this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation ( smt ), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. <eos> our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. <eos> two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. <eos> we compare our approach with moses and observe the same performance, but a substantially better trade-off between translation quality and speed. <eos> at a speed of roughly 70 words per second, moses reaches 17.2 % bleu, whereas our approach yields 20.0 % with identical models.
this paper presents an extension of chiang ? s hierarchical phrase-based ( hpb ) model, called head-driven hpb ( hd-hpb ), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. <eos> experiments on chinese-english translation on four nist mt test sets show that the hd-hpb model significantly outperforms chiang ? s model with average gains of 1.91 points absolute in bleu.
smt has been used in paraphrase generation by translating a source sentence into another ( pivot ) language and then back into the source. <eos> the resulting sentences can be used as candidate paraphrases of the source sentence. <eos> existing work that uses two independently trained smt systems can not directly optimize the paraphrase results. <eos> paraphrase criteria especially the paraphrase rate is not able to be ensured in that way. <eos> in this paper, we propose a joint learning method of two smt systems to optimize the process of paraphrase generation. <eos> in addition, a revised bleu score ( called ibleu ) which measures the adequacy and diversity of the generated paraphrase sentence is proposed for tuning parameters in smt systems. <eos> our experiments on nist 2008 testing data with automatic evaluation as well as human judgments suggest that the proposed method is able to enhance the paraphrase quality by adjusting between semantic equivalency and surface dissimilarity.
mining retrospective events from text streams has been an important research topic. <eos> classic text representation model ( i.e., vector space model ) can not model temporal aspects of documents. <eos> to address it, we proposed a novel burst-based text representation model, denoted as burstvsm. <eos> burstvsm corresponds dimensions to bursty features instead of terms, which can capture semantic and temporal information. <eos> meanwhile, it significantly reduces the number of non-zero entries in the representation. <eos> we test it via scalable event detection, and experiments in a 10-year news archive show that our methods are both effective and efficient.
although researchers have conducted extensive studies on relation extraction in the last decade, supervised approaches are still limited because they require large amounts of training data to achieve high performances. <eos> to build a relation extractor without significant annotation effort, we can exploit cross-lingual annotation projection, which leverages parallel corpora as external resources for supervision. <eos> this paper proposes a novel graph-based projection approach and demonstrates the merits of it by using a korean relation extraction system based on projected dataset from an english-korean parallel corpus.
we describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. <eos> we leverage distant supervision using relations from the knowledge base freebase, but do not require any manual heuristic nor manual seed list selections. <eos> results show that the learned patterns can be used to extract new relations with good precision.
in social psychology, it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship. <eos> we present a computational framework for automatically analyzing such self-disclosure behavior in twitter conversations. <eos> our framework uses text mining techniques to discover topics, emotions, sentiments, lexical patterns, as well as personally identifiable information ( pii ) and personally embarrassing information ( pei ). <eos> our preliminary results illustrate that in relationships with high relationship strength, twitter users show significantly more frequent behaviors of self-disclosure.
we describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread. <eos> an intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion. <eos> sentiment tags play an important role in this detection, but we also note another dimension to the detection of people ? s attitudes in a discussion : if two persons share the same opinion, they tend to use similar language content. <eos> we consider the latter to be an implicit attitude. <eos> in this paper, we investigate the impact of implicit and explicit attitude in two genres of social media discussion data, more formal wikipedia discussions and a debate discussion forum that is much more informal. <eos> experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes ( expressed via sentiment ) and it can improve the sub-group detection performance independent of genre.
we investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. <eos> we represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. <eos> such a representation allows us to learn all of allen ? s temporal relations between medical events. <eos> interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the timebank corpus. <eos> this finding has important implications for styles of data representation and resources used for temporal relation learning : clinical narratives may have different language attributes corresponding to temporal ordering relative to timebank, implying that the field may need to look at a wider range of domains to fully understand the nature of temporal ordering.
-sensitive, multi-faceted model of lexico-conceptual affect tony veale web science and technology division, kaist, daejeon, south korea. <eos> tony.veale @ gmail.com abstract since we can ? spin ? <eos> words and concepts to suit our affective needs, context is a major determinant of the perceived affect of a word or concept. <eos> we view this re-profiling as a selective emphasis or de-emphasis of the qualities that underpin our shared stere-otype of a concept or a word meaning, and construct our model of the affective lexicon accordingly. <eos> we show how a large body of affective stereotypes can be acquired from the web, and also show how these are used to create and interpret affective metaphors.
there has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. <eos> we consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using gibbs sampling with a word language model. <eos> we evaluate our method on synthetic ciphertexts of different lengths, and find that it outperforms previous work that employs viterbi decoding with character-based models.
we present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in goldwater ( 2006 ). <eos> by adding rejuvenation to a particle filter, we are able to considerably improve its performance, both in terms of finding higher probability and higher accuracy solutions.
variants of naive bayes ( nb ) and support vector machines ( svm ) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset. <eos> we show that : ( i ) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks ; ( ii ) for short snippet sentiment tasks, nb actually does better than svms ( while for longer documents the opposite result holds ) ; ( iii ) a simple but novel svm variant using nb log-count ratios as feature values consistently performs well across tasks and datasets. <eos> based on these observations, we identify simple nb and svm variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.
we propose a new approach for the creation of child language development metrics. <eos> a set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. <eos> in the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. <eos> we then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. <eos> in both cases we compare our results with established measures of language development, showing improvements in age prediction performance.
this paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. <eos> our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. <eos> besides using traditional dependency parsers, we also use the dependency structures transformed from pcfg trees and predicate-argument structures ( pass ) which are generated by an hpsg parser and a ccg parser. <eos> the experiments on chinese-to-english translation show that the hpsg parser ? s pass achieved the best dependency and translation accuracies.
we propose an improved, bottom-up method for converting ccg derivations into ptb-style phrase structure trees. <eos> in contrast with past work ( clark and curran, 2009 ), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. <eos> our conversion preserves more sentences under round-trip conversion ( 51.1 % vs. 39.6 % ) and is more robust. <eos> in particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.
we present a bayesian nonparametric model for estimating tree insertion grammars ( tig ), building upon recent work in bayesian inference of tree substitution grammars ( tsg ) via dirichlet processes. <eos> under our general variant of tig, grammars are estimated via the metropolis-hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of cohn and blunsom ( 2010 ). <eos> we use the penn treebank for our experiments and find that our proposal bayesian tig model not only has competitive parsing performance but also finds compact yet linguistically rich tig representations of the data.
we propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts, where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation. <eos> we use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features. <eos> conditioning lexical probabilities on the topic biases translations toward topicrelevant output, resulting in significant improvements of up to 1 bleu and 3 ter on chinese to english translation over a strong baseline.
we address a core aspect of the multilingual content synchronization task : the identification of novel, more informative or semantically equivalent pieces of information in two documents about the same topic. <eos> this can be seen as an application-oriented variant of textual entailment recognition where : i ) t and h are in different languages, and ii ) entailment relations between t and h have to be checked in both directions. <eos> using a combination of lexical, syntactic, and semantic features to train a cross-lingual textual entailment system, we report promising results on different datasets.
we present a system for cross-lingual parse disambiguation, exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities. <eos> we simultaneously reduce ambiguity in multiple languages in a fully automatic way. <eos> evaluation shows that the system reliably discards dispreferred parses from the raw parser output, which results in a pre-selection that can speed up manual treebanking.
in this paper, we present a new method for learning to finding translations and transliterations on the web for a given term. <eos> the approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. <eos> at runtime, the model is used to extracting translation candidates for a given term. <eos> preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work.
we investigate how novel english-derived words ( anglicisms ) are used in a germanlanguage internet hip hop forum, and what factors contribute to their uptake. <eos>
in this paper we study unsupervised word sense disambiguation ( wsd ) based on sense definition. <eos> we learn low-dimensional latent semantic vectors of concept definitions to construct a more robust sense similarity measure wmfvec. <eos> experiments on four all-words wsd data sets show significant improvement over the baseline wsd systems and lda based similarity measures, achieving results comparable to state of the art wsd systems.
we propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. <eos> the role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. <eos> the global role ordering consists of the sequence of primary roles only, thus making it a partial ordering.
this work presents a first step to a general implementation of the semantic-script theory of humor ( ssth ). <eos> of the scarce amount of research in computational humor, no research had focused on humor generation beyond simple puns and punning riddles. <eos> we propose an algorithm for mining simple humorous scripts from a semantic network ( conceptnet ) by specifically searching for dual scripts that jointly maximize overlap and incongruity metrics in line with raskin ? s semantic-script theory of humor. <eos> initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles. <eos> we evaluate the said metrics through a user-assessed quality of the generated two-liners.
the importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. <eos> however, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. <eos> in this paper, we suggest a framework for evaluating inference-rule resources. <eos> our framework simplifies a previously proposed ? instance-based evaluation ? <eos> method that involved substantial annotator training, making it suitable for crowdsourcing. <eos> we show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.
many researchers have attempted to predict the enron corporate hierarchy from the data. <eos> this work, however, has been hampered by a lack of data. <eos> we present a new, large, and freely available gold-standard hierarchy. <eos> using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current nlp systems.
this paper presents a two-step approach to compress spontaneous spoken utterances. <eos> in the first step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences. <eos> in the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them. <eos> for evaluation, we compare our system output with multiple human references. <eos> our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references.
most previous studies in computerized deception detection have relied only on shallow lexico-syntactic patterns. <eos> this paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. <eos> over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from context free grammar ( cfg ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. <eos> our results improve the best published result on the hotel review data ( ott et al, 2011 ) reaching 91.2 % accuracy with 14 % error reduction.
previous approaches to instruction interpretation have required either extensive domain adaptation or manually annotated corpora. <eos> this paper presents a novel approach to instruction interpretation that leverages a large amount of unannotated, easy-to-collect data from humans interacting with a virtual world. <eos> we compare several algorithms for automatically segmenting and discretizing this data into ( utterance, reaction ) pairs and training a classifier to predict reactions given the next utterance. <eos> our empirical analysis shows that the best algorithm achieves 70 % accuracy on this task, with no manual annotation required.
natural language questions have become popular in web search. <eos> however, various questions can be formulated to convey the same information need, which poses a great challenge to search systems. <eos> in this paper, we automatically mined 5w1h question reformulation patterns from large scale search log data. <eos> the question reformulations generated from these patterns are further incorporated into the retrieval model. <eos> experiments show that using question reformulation patterns can significantly improve the search performance of natural language questions.
we investigate the potential of tree substitution grammars as a source of features for native language detection, the task of inferring an author ? s native language from text in a different language. <eos> we compare two state of the art methods for tree substitution grammar induction and show that features from both methods outperform previous state of the art results at native language detection. <eos> furthermore, we contrast these two induction algorithms and show that the bayesian approach produces superior classification results with a smaller feature set.
as the number of learners of english is constantly growing, automatic error correction of esl learners ? <eos> writing is an increasingly active area of research. <eos> however, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. <eos> one of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. <eos> previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. <eos> in order to take global information into account, we regard the task as sequence labeling : each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. <eos> our experiments show that the global context makes a moderate contribution to tense/aspect error correction.
this paper describes movie-dic a movie dialogue corpus recently collected for research and development purposes. <eos> the collected dataset comprises 132,229 dialogues containing a total of 764,146 turns that have been extracted from 753 movies. <eos> details on how the data collection has been created and how it is structured are provided along with its main statistics and characteristics.
blogs and forums are widely adopted by online communities to debate about various issues. <eos> however, a user that wants to cut in on a debate may experience some difficulties in extracting the current accepted positions, and can be discouraged from interacting through these applications. <eos> in our paper, we combine textual entailment with argumentation theory to automatically extract the arguments from debates and to evaluate their acceptability.
we seek to automatically estimate typical durations for events and habits described in twitter tweets. <eos> a corpus of more than 14 million tweets containing temporal duration information was collected. <eos> these tweets were classified as to their habituality status using a bootstrapped, decision tree. <eos> for each verb lemma, associated duration information was collected for episodic and habitual uses of the verb. <eos> summary statistics for 483 verb lemmas and their typical habit and episode durations has been compiled and made available. <eos> this automatically generated duration information is broadly comparable to hand-annotation.
interpreting news requires identifying its constituent events. <eos> events are complex linguistically and ontologically, so disambiguating their reference is challenging. <eos> we introduce event linking, which canonically labels an event reference with the article where it was first reported. <eos> this implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. <eos> we annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement.
the web and digitized text sources contain a wealth of information about named entities such as politicians, actors, companies, or cultural landmarks. <eos> extracting this information has enabled the automated construction of large knowledge bases, containing hundred millions of binary relationships or attribute values about these named entities. <eos> however, in reality most knowledge is transient, i.e. <eos> changes over time, requiring a temporal dimension in fact extraction. <eos> in this paper we develop a methodology that combines label propagation with constraint reasoning for temporal fact extraction. <eos> label propagation aggressively gathers fact candidates, and an integer linear program is used to clean out false hypotheses that violate temporal constraints. <eos> our method is able to improve on recall while keeping up with precision, which we demonstrate by experiments with biography-style wikipedia pages and a large corpus of news articles.
syntactic analysis of search queries is important for a variety of information-retrieval tasks ; however, the lack of annotated data makes training query analysis models difficult. <eos> we propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. <eos> unlike previous work, our final model does not require any additional resources at run-time. <eos> compared to a state-ofthe-art approach, we achieve more than 20 % relative error reduction. <eos> additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.
this paper presents the problem within hittite and ancient near eastern studies of fragmented and damaged cuneiform texts, and proposes to use well-known text classification metrics, in combination with some facts about the structure of hittite-language cuneiform texts, to help classify a number of fragments of clay cuneiform-script tablets into more complete texts. <eos> in particular, i propose using sumerian and akkadian ideogrammatic signs within hittite texts to improve the performance of naive bayes and maximum entropy classifiers. <eos> the performance in some cases is improved, and in some cases very much not, suggesting that the variable frequency of occurrence of these ideograms in individual fragments makes considerable difference in the ideal choice for a classification method. <eos> further, complexities of the writing system and the digital availability of hittite texts complicate the problem.
this paper describes the creation of the first large-scale corpus containing drafts and final versions of essays written by non-native speakers, with the sentences aligned across different versions. <eos> furthermore, the sentences in the drafts are annotated with comments from teachers. <eos> the corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. <eos> this corpus has been converted into an xml format conforming to the standards of the text encoding initiative ( tei ).
? lightweight ? <eos> semantic annotation of text calls for a simple representation, ideally without requiring a semantic lexicon to achieve good coverage in the language and domain. <eos> in this paper, we repurpose wordnet ? s supersense tags for annotation, developing specific guidelines for nominal expressions and applying them to arabic wikipedia articles in four topical domains. <eos> the resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement.
in this paper we introduce the novel task of ? word epoch disambiguation, ? <eos> defined as the problem of identifying changes in word usage over time. <eos> through experiments run using word usage examples collected from three major periods of time ( 1800, 1900, 2000 ), we show that the task is feasible, and significant differences can be observed between occurrences of words in different periods of time.
authorship attribution deals with identifying the authors of anonymous texts. <eos> building on our earlier finding that the latent dirichlet allocation ( lda ) topic model can be used to improve authorship attribution accuracy, we show that employing a previously-suggested author-topic ( at ) model outperforms lda when applied to scenarios with many authors. <eos> in addition, we define a model that combines lda and at by representing authors and documents over two disjoint topic sets, and show that our model outperforms lda, at and support vector machines on datasets with many authors.
we use multiple views for cross-domain document classification. <eos> the main idea is to strengthen the views ? <eos> consistency for target data with source training data by identifying the correlations of domain-specific features from different domains. <eos> we present an information-theoretic multi-view adaptation model ( imam ) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. <eos> experiments show that imam significantly outperforms state-of-the-art baselines.
topic modeling with a tree-based prior has been used for a variety of applications because it can encode correlations between words that traditional topic modeling can not. <eos> however, its expressive power comes at the cost of more complicated inference. <eos> we extend the sparselda ( yao et al, 2009 ) inference scheme for latent dirichlet alocation ( lda ) to tree-based topic models. <eos> this sampling scheme computes the exact conditional distribution for gibbs sampling much more quickly than enumerating all possible latent variable assignments. <eos> we further improve performance by iteratively refining the sampling distribution only when needed. <eos> experiments show that the proposed techniques dramatically improve the computation time.
this paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. <eos> experiments on chinese-english translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation.
the dominant practice of statistical machine translation ( smt ) uses the same chinese word segmentation specification in both alignment and translation rule induction steps in building chinese-english smt system, which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. <eos> to tackle this, we propose a framework that uses two different segmentation specifications for alignment and translation respectively : we use chinese character as the basic unit for alignment, and then convert this alignment to conventional word alignment for translation rule induction. <eos> experimentally, our approach outperformed two baselines : fully word-based system ( using word for both alignment and translation ) and fully character-based system, in terms of alignment quality and translation performance.
in this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. <eos> previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. <eos> on the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of smt decoders as much as possible. <eos> experimental results on chinese-toenglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.
we propose a novel heuristic algorithm for cube pruning running in linear time in the beam size. <eos> empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy.
we propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. <eos> we use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level bleu, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. <eos> the evaluation on macedonian-bulgarian movie subtitles shows an improvement of 2.84 bleu points over a phrase-based word-level baseline.
bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the em algorithm, by placing prior probabilities on the model parameters. <eos> we apply one such bayesian technique, variational bayes, to the ibm models of word alignment for statistical machine translation. <eos> we show that using variational bayes improves the performance of the widely used giza++ software, as well as improving the overall performance of the moses machine translation system in terms of bleu score.
reordering is a difficult task in translating between widely different languages such as japanese and english. <eos> we employ the postordering framework proposed by ( sudoh et al., 2011b ) for japanese to english translation and improve upon the reordering method. <eos> the existing post-ordering method reorders a sequence of target language words in a source language word order via smt, while our method reorders the sequence by : 1 ) parsing the sequence to obtain syntax structures similar to a source language structure, and 2 ) transferring the obtained syntax structures into the syntax structures of the target language.
syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. <eos> in this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. <eos> we find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the chinese treebank.
if unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving mt performance on low-resource inflected languages. <eos> in this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart arabic-to-english mt system. <eos> we apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for arabic, while also making segmentation output more stable. <eos> our approach gives an 18 % relative bleu gain for levantine dialectal arabic. <eos> furthermore, it gives higher gains for modern standard arabic ( msa ), as measured on nist mt-08, than does mada ( habash and rambow, 2005 ), a leading supervised msa segmenter.
in this paper, we present a structural learning model for joint sentiment classification and aspect analysis of text at various levels of granularity. <eos> our model aims to identify highly informative sentences that are aspect-specific in online custom reviews. <eos> the primary advantages of our model are two-fold : first, it performs document-level and sentence-level sentiment polarity classification jointly ; second, it is able to find informative sentences that are closely related to some respects in a review, which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization. <eos> the proposed method was evaluated with 9,000 chinese restaurant reviews. <eos> preliminary experiments demonstrate that our model obtains promising performance.
convolution kernels support the modeling of complex syntactic information in machinelearning tasks. <eos> however, such models are highly sensitive to the type and size of syntactic structure used. <eos> it is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. <eos> in this paper we present a systematic study investigating ( combinations of ) sequence and convolution kernels using different types of substructures in document-level sentiment classification. <eos> we show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus.
for sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. <eos> these constraints are based on semantic roles and superior to the constraints of syntactic dependencies. <eos> our empirical evaluation on the written news compression corpus ( clarke and lapata, 2008 ) demonstrates that our system achieves results comparable to other state-of-the-art techniques.
this paper shows that full abstraction can be accomplished in the context of guided summarization. <eos> we describe a work in progress that relies on information extraction, statistical content selection and natural language generation. <eos> early results already demonstrate the effectiveness of the approach.
we investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. <eos> using text analysis conference data, we measure annotator consistency based on human scoring of summaries for responsiveness, readability, and pyramid scoring. <eos> we identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. <eos> finally, we examine the stability of automatic metrics ( rouge and classy ) with respect to the inconsistent assessments.
this paper presents a novel way of improving pos tagging on heterogeneous data. <eos> first, two separate models are trained ( generalized and domain-specific ) from the same data set by controlling lexical items with different document frequencies. <eos> during decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. <eos> this dynamic model selection approach, coupled with a one-pass, leftto-right pos tagging algorithm, is evaluated on corpora from seven different genres. <eos> even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. <eos> furthermore, our system is able to tag about 32k tokens per second. <eos> we believe that this model selection approach can be applied to more sophisticated tagging algorithms and improve their robustness even further.
we present a novel approach to the task of word lemmatisation. <eos> we formalise lemmatisation as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a specific language. <eos> in this way, a lemmatisation system can be trained and tested using any supervised tagging model. <eos> in contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information. <eos> we test our approach on eight languages reaching a new state-of-the-art level for the lemmatisation task.
this paper presents a comparative study of spelling errors that are corrected as you type, vs. those that remain uncorrected. <eos> first, we generate naturally occurring online error correction data by logging users ? <eos> keystrokes, and by automatically deriving pre- and postcorrection strings from them. <eos> we then perform an analysis of this data against the errors that remain in the final text as well as across languages. <eos> our analysis shows a clear distinction between the types of errors that are generated and those that remain uncorrected, as well as across languages.
we examine some of the frequently disregarded subtleties of tokenization in penn treebank style, and present a new rule-based preprocessing toolkit that not only reproduces the treebank tokenization with unmatched accuracy, but also maintains exact stand-off pointers to the original text and allows flexible configuration to diverse use cases ( e.g. <eos> to genreor domain-specific idiosyncrasies ).
in this paper, we present an unsupervized segmentation system tested on mandarin chinese. <eos> following harris 's hypothesis in kempe ( 1999 ) and tanaka-ishii 's ( 2005 ) reformulation, we base our work on the variation of branching entropy. <eos> we improve on ( jin and tanaka-ishii, 2006 ) by adding normalization and viterbidecoding. <eos> this enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results ( wang et al, 2011 ) with a simpler system. <eos> we provide evaluation on different corpora available from the segmentation bake-off ii ( emerson, 2005 ) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf ( zhang and clark, 2010 ; zhao and kit, 2008 ; huang and zhao, 2007 )
this paper presents grammar error correction for japanese particles that uses discriminative sequence conversion, which corrects erroneous particles by substitution, insertion, and deletion. <eos> the error correction task is hindered by the difficulty of collecting large error corpora. <eos> we tackle this problem by using pseudoerror sentences generated automatically. <eos> furthermore, we apply domain adaptation, the pseudo-error sentences are from the source domain, and the real-error sentences are from the target domain. <eos> experiments show that stable improvement is achieved by using domain adaptation.
we demonstrate applications of psycholinguistic and sublexical information for learning chinese characters. <eos> the knowledge about the grapheme-phoneme conversion ( gpc ) rules of languages has been shown to be highly correlated to the ability of reading alphabetic languages and chinese. <eos> we build and will demo a game platform for strengthening the association of phonological components in chinese characters with the pronunciations of the characters. <eos> results of a preliminary evaluation of our games indicated significant improvement in learners ? <eos> response times in chinese naming tasks. <eos> in addition, we construct a webbased open system for teachers to prepare their own games to best meet their teaching goals. <eos> techniques for decomposing chinese characters and for comparing the similarity between chinese characters were employed to recommend lists of chinese characters for authoring the games. <eos> evaluation of the authoring environment with 20 subjects showed that our system made the authoring of games more effective and efficient.
g viewpoint and information need with affective metaphors a system demonstration of the metaphor magnet web app/service tony veale guofu li web science and technology division, school of computer science & informatics, kaist, daejeon, university college dublin, south korea. <eos> belfield, dublin d4, ireland. <eos> tony.veale @ gmail.com yanfen.hao @ ucd.ie abstract metaphors pervade our language because they are elastic enough to allow a speaker to express an affective viewpoint on a topic without committing to a specific meaning. <eos> this balance of expressiveness and inde-terminism means that metaphors are just as useful for eliciting information as they are for conveying information. <eos> we explore here, via a demonstration of a system for metaphor interpretation and generation called metaphor magnet, the practical uses of metaphor as a basis for formulating af-fective information queries. <eos> we also con-sider the kinds of deep and shallow stereotypical knowledge that are needed for such a system, and demonstrate how they can be acquired from corpora and the web.
tweets have become a comprehensive repository for real-time information. <eos> however, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. <eos> we present quickview, an nlp-based tweet search platform to tackle this issue. <eos> specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. <eos> then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.
we present a new open source toolkit for phrase-based and syntax-based machine translation. <eos> the toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. <eos> the key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. <eos> moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning.
we present langid.py, an off-the-shelf language identification tool. <eos> we discuss the design and implementation of langid.py, and provide an empirical comparison on 5 longdocument datasets, and 2 datasets from the microblog domain. <eos> we find that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data.
this paper describes the personalized normalization of a multilingual chat system that supports chatting in user defined short-forms or abbreviations. <eos> one of the major challenges for multilingual chat realized through machine translation technology is the normalization of non-standard, self-created short-forms in the chat message to standard words before translation. <eos> due to the lack of training data and the variations of short-forms used among different social communities, it is hard to normalize and translate chat messages if user uses vocabularies outside the training data and create short-forms freely. <eos> we develop a personalized chat normalizer for english and integrate it with a multilingual chat system, allowing user to create and use personalized short-forms in multilingual chat.
this system demonstration paper presents iris ( informal response interactive system ), a chat-oriented dialogue system based on the vector space model framework. <eos> the system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. <eos> additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed.
to facilitate the creation and usage of custom smt systems we have created a cloud-based platform for do-it-yourself mt. <eos> the platform is developed in the eu collaboration project letsmt !. <eos> this system demonstration paper presents the motivation in developing the letsmt ! <eos> platform, its main features, architecture, and an evaluation in a practical use case.
we demonstrate a web-based environment for development and testing of different pedestrian route instruction-giving systems. <eos> the environment contains a city model, a tts interface, a game-world, and a user gui including a simulated street-view. <eos> we describe the environment and components, the metrics that can be used for the evaluation of pedestrian route instruction-giving systems, and the shared challenge which is being organised using this environment.
in this paper, we propose a web-based bilingual concordancer, domcat 1, for domain-specific computer assisted translation. <eos> given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs ( translation spotting ) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. <eos> to provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. <eos> to ranking the retrieved sentence pairs, we propose a correlation function modified from the dice coefficient for assessing the correlation between the query and the translation equivalents. <eos> the performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively.
in this paper, we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules into finite-state transducers, and for n-gram language modeling. <eos> the opengrm libraries use the openfst library to provide an efficient encoding of grammars and general algorithms for building, modifying and applying models.
in this paper we present an api for programmatic access to babelnet ? <eos> a wide-coverage multilingual lexical knowledge base ? <eos> and multilingual knowledge-rich word sense disambiguation ( wsd ). <eos> our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction.
this paper introduces biutee1, an opensource system for recognizing textual entailment. <eos> its main advantages are its ability to utilize various types of knowledge resources, and its extensibility by which new knowledge resources and inference components can be easily integrated. <eos> these abilities make biutee an appealing rte system for two research communities : ( 1 ) researchers of end applications, that can benefit from generic textual inference, and ( 2 ) rte researchers, who can integrate their novel algorithms and knowledge resources into our system, saving the time and effort of developing a complete rte system from scratch. <eos> notable assistance for these researchers is provided by a visual tracing tool, by which researchers can refine and ? debug ? <eos> their knowledge resources and inference components.
we present a novel text exploration model, which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration. <eos> the proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process. <eos> a user of our system can explore the result space of a query by drilling down/up from one statement to another, according to entailment relations specified by an entailment graph and an optional concept taxonomy. <eos> as a prominent use case, we apply our exploration system and illustrate its benefit on the health-care domain. <eos> to the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation.
we present csniper ( corpus sniper ), a tool that implements ( i ) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and ( ii ) evaluation of annotation quality by measuring inter-rater agreement. <eos> this annotationby-query approach efficiently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools.
we present illume, a software tool pack which creates a personalized ambient using the music and lighting. <eos> illume includes an emotion analysis software, the small space ambient lighting, and a multimedia controller. <eos> the software analyzes emotional changes from instant message logs and corresponds the detected emotion to the best sound and light settings. <eos> the ambient lighting can sparkle with different forms of light and the smart phone can broadcast music respectively according to different atmosphere. <eos> all settings can be modified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software. <eos> the illume system, equipped with the learning function, provides a link between residential situation and personal emotion. <eos> it works in a chinese chatting environment to illustrate the language technology in life.
we present a component for incremental speech synthesis ( iss ) and a set of applications that demonstrate its capabilities. <eos> this component can be used to increase the responsivity and naturalness of spoken interactive systems. <eos> while iss can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities.
information extraction ( ie ) is becoming a critical building block in many enterprise applications. <eos> in order to satisfy the increasing text analytics demands of enterprise applications, it is crucial to enable developers with general computer science background to develop high quality ie extractors. <eos> in this demonstration, we present wizie, an ie development environment intended to reduce the development life cycle and enable developers with little or no linguistic background to write high quality ie rules. <eos> wizie provides an integrated wizard-like environment that guides ie developers step-by-step throughout the entire development process, based on best practices synthesized from the experience of expert developers. <eos> in addition, wizie reduces the manual effort involved in performing key ie development tasks by offering automatic result explanation and rule discovery functionality. <eos> preliminary results indicate that wizie is a step forward towards enabling extractor development for novice ie developers.
, dogan can**, abe kazemzadeh**, fran ? ois bar* and shrikanth narayanan** annenberg innovation laboratory ( ail ) * signal analysis and interpretation laboratory ( sail ) ** university of southern california, los angeles, ca { haowang @, dogancan @, kazemzad @, fbar @, shri @ sipi }.usc.edu abstract this paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 u.s. election as expressed on twitter, a micro-blogging service. <eos> twitter has become a central site where people express their opinions and views on political parties and candidates. <eos> emerging events or news are often followed almost instantly by a burst in twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. <eos> in addition, sentiment analysis can help explore how these events affect public opinion. <eos> while traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire twitter traffic about the election, delivering results instantly and continuously. <eos> it offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion.
argo is a web-based nlp and text mining workbench with a convenient graphical user interface for designing and executing processing workflows of various complexity. <eos> the workbench is intended for specialists and nontechnical audiences alike, and provides the ever expanding library of analytics compliant with the unstructured information management architecture, a widely adopted interoperability framework. <eos> we explore the flexibility of this framework by demonstrating workflows involving three processing components capable of performing self-contained machine learning-based tagging. <eos> the three components are responsible for the three distinct tasks of 1 ) generating observations or features, 2 ) training a statistical model based on the generated features, and 3 ) tagging unlabelled data with the model. <eos> the learning and tagging components are based on an implementation of conditional random fields ( crf ) ; whereas the feature generation component is an analytic capable of extending basic token information to a comprehensive set of features. <eos> users define the features of their choice directly from argo ? s graphical interface, without resorting to programming ( a commonly used approach to feature engineering ). <eos> the experimental results performed on two tagging tasks, chunking and named entity recognition, showed that a tagger with a generic set of features built in argo is capable of competing with taskspecific solutions.
we describe akamon, an open source toolkit for tree and forest-based statistical machine translation ( liu et al, 2006 ; mi et al, 2008 ; mi and huang, 2008 ). <eos> akamon implements all of the algorithms required for tree/forestto-string decoding using tree-to-string translation rules : multiple-thread forest-based decoding, n-gram language model integration, beam- and cube-pruning, k-best hypotheses extraction, and minimum error rate training. <eos> in terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using pcfg trees ( galley et al, 2004 ) and hpsg trees/forests ( wu et al, 2010 ).
we present subgroup detector, a system for analyzing threaded discussions and identifying the attitude of discussants towards one another and towards the discussion topic. <eos> the system uses attitude predictions to detect the split of discussants into subgroups of opposing views. <eos> the system uses an unsupervised approach based on rule-based opinion target detecting and unsupervised clustering techniques. <eos> the system is open source and is freely available for download. <eos> an online demo of the system is available at : http : //clair.eecs.umich.edu/subgroupdetector/
error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the mt systems under development and allow fair comparisons among them. <eos> this work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations. <eos> to do so, we have set up an online graphical interface for the asiya toolkit, a rich repository of evaluation measures working at different linguistic levels. <eos> the current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations, and word alignments. <eos> the intelligent visualization of the linguistic structures used by the metrics, as well as a set of navigational functionalities, may lead towards advanced methods for automatic error analysis.
we present uwn, a large multilingual lexical knowledge base that describes the meanings and relationships of words in over 200 languages. <eos> this paper explains how link prediction, information integration and taxonomy induction methods have been used to build uwn based on wordnet and extend it with millions of named entities from wikipedia. <eos> we additionally introduce extensions to cover lexical relationships, frame-semantic knowledge, and language data. <eos> an online interface provides human access to the data, while a software api enables applications to look up over 16 million words and names.
social event radar is a new social networking-based service platform, that aim to alert as well as monitor any merchandise flaws, food-safety related issues, unexpected eruption of diseases or campaign issues towards to the government, enterprises of any kind or election parties, through keyword expansion detection module, using bilingual sentiment opinion analysis tool kit to conclude the specific event social dashboard and deliver the outcome helping authorities to plan ? risk control ? <eos> strategy. <eos> with the rapid development of social network, people can now easily publish their opinions on the internet. <eos> on the other hand, people can also obtain various opinions from others in a few seconds even though they do not know each other. <eos> a typical approach to obtain required information is to use a search engine with some relevant keywords. <eos> we thus take the social media and forum as our major data source and aim at collecting specific issues efficiently and effectively in this work.
we present a new edition of the google books ngram corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages ; it reflects 6 % of all books ever published. <eos> this new edition introduces syntactic annotations : words are tagged with their part-of-speech, and headmodifier relationships are recorded. <eos> the annotations are produced automatically with statistical models that are specifically adapted to historical text. <eos> the corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.
we introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. <eos> as the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. <eos> to resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. <eos> as our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the nist chinese-english datasets.
since statistical machine translation ( smt ) and translation memory ( tm ) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate tm information into phrase-based smt. <eos> unlike previous multi-stage pipeline approaches, which directly merge tm result into the final output, the proposed models refer to the corresponding tm information associated with each phrase at smt decoding. <eos> on a chinese ? english tm database, our experiments show that the proposed integrated model-iii is significantly better than either the smt or the tm systems when the fuzzy match score is above 0.4. <eos> furthermore, integrated model-iii achieves overall 3.48 bleu points improvement and 2.62 ter points reduction in comparison with the pure smt system. <eos> besides, the proposed models also outperform previous approaches significantly.
we derive variants of the fertility based models ibm-3 and ibm-4 that, while maintaining their zero and first order parameters, are nondeficient. <eos> subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of em training. <eos> the arising m-step energies are non-trivial and handled via projected gradient ascent. <eos> our evaluation on gold alignments shows substantial improvements ( in weighted fmeasure ) for the ibm-3. <eos> for the ibm4 there are no consistent improvements. <eos> training the nondeficient ibm-5 in the regular way gives surprisingly good results. <eos> using the resulting alignments for phrasebased translation systems offers no clear insights w.r.t. <eos> bleu scores.
annotating linguistic data is often a complex, time consuming and expensive endeavour. <eos> even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. <eos> we present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. <eos> these techniques use multi-task gaussian processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. <eos> our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently outperform strong baselines.
we present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown kneser-ney ( 1995 ) smoothing. <eos> unlike kneser-ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. <eos> as a result, the algorithm avoids issues observed when pruning kneser-ney models ( siivola et al, 2007 ; chelba et al, 2010 ), while retaining the benefits of such marginal distribution constraints. <eos> we present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. <eos> an open-source version of the algorithm has been released as part of the opengrm ngram library.1
we present a method that learns representations for word meanings from short video clips paired with sentences. <eos> unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. <eos> unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. <eos> the correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. <eos> the learned word meanings can be subsequently used to automatically generate description of new video.
recent work on statistical quantifier scope disambiguation ( qsd ) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. <eos> no corpusbased method, however, has yet addressed qsd when incorporating the implicit universal of plurals and/or operators such as negation. <eos> in this paper we report early, though promising, results for automatic qsd when handling both phenomena. <eos> we also present a general model for learning to build partial orders from a set of pairwise preferences. <eos> we give an n log n algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. <eos> finally, we significantly improve the performance of the previous model using a rich set of automatically generated features.
traditional approaches to the task of ace event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. <eos> by contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. <eos> in addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. <eos> experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. <eos> our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.
temporal resolution systems are traditionally tuned to a particular language, requiring significant human effort to translate them to new languages. <eos> we present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference. <eos> we make use of a latent parse that encodes a language-flexible representation of time, and extract rich features over both the parse and associated temporal semantics. <eos> the parameters of the model are learned using a weakly supervised bootstrapping approach, without the need for manually tuned parameters or any other language expertise. <eos> we achieve state-of-the-art accuracy on all languages in the tempeval2 temporal normalization task, reporting a 4 % improvement in both english and spanish accuracy, and to our knowledge the first results for four other languages.
we propose a computationally efficient graph-based approach for local coherence modeling. <eos> we evaluate our system on three tasks : sentence ordering, summary coherence rating and readability assessment. <eos> the performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems.
automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data. <eos> important behavioral categories, though, are often sparse and often appear only in specific subsections of a conversation. <eos> this makes supervised machine learning difficult, through a combination of noisy features and unbalanced class distributions. <eos> we propose within-instance content selection, using cue features to selectively suppress sections of text and biasing the remaining representation towards minority classes. <eos> we show the effectiveness of this technique in automated annotation of empowerment language in online support group chatrooms. <eos> our technique is significantly more accurate than multiple baselines, especially when prioritizing high precision.
efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. <eos> we describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. <eos> this model can be represented as a factor graph for each document that admits efficient inference via belief propagation. <eos> we show that our method can use entity-level information to outperform a basic pairwise system.
characters play an important role in the chinese language, yet computational processing of chinese has been dominated by word-based approaches, with leaves in syntax trees being words. <eos> we investigate chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. <eos> we demonstrate the importance of character-level information to chinese processing by building a joint segmentation, part-of-speech ( pos ) tagging and phrase-structure parsing system that integrates character-structure features. <eos> our joint system significantly outperforms a state-of-the-art word-based baseline on the standard ctb5 test, and gives the best published results for chinese parsing.
we present a novel transition-based, greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies. <eos> the new strategy allows the parser to postpone difficult decisions until the relevant information becomes available. <eos> the novel parser has a ? 12 % error reduction in unlabeled attachment score over an arc-eager parser, with a slow-down factor of 2.8.
binarization of grammars is crucial for improving the complexity and performance of parsing and translation. <eos> we present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter. <eos> we apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation.
this paper proposes new distortion models for phrase-based smt. <eos> in decoding, a distortion model estimates the source word position to be translated next ( np ) given the last translated source word position ( cp ). <eos> we propose a distortion model that can consider the word at the cp, a word at an np candidate, and the context of the cp and the np candidate simultaneously. <eos> moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the cp to np candidates. <eos> it enables our model to learn the effect of relative word order among np candidates as well as to learn the effect of distances from the training data. <eos> in our experiments, our model improved 2.9 bleu points for japanese-english and 2.6 bleu points for chinese-english translation compared to the lexical reordering models.
in this paper, we explore a novel bilingual word alignment approach based on dnn ( deep neural network ), which has been proven to be very effective in various machine learning tasks ( collobert et al, 2011 ). <eos> we describe in detail how we adapt and extend the cd-dnnhmm ( dahl et al, 2012 ) method introduced in speech recognition to the hmmbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. <eos> while being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. <eos> experiments on a large scale englishchinese word alignment task show that the proposed method outperforms the hmm and ibm model 4 baselines by 2 points in f-score.
in the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text : some users create post multilingual messages targeting international audiences while others ? retweet ? <eos> translations. <eos> we present an efficient method for detecting these messages and extracting parallel segments from them. <eos> we have been able to extract over 1m chinese-english parallel segments from sina weibo ( the chinese counterpart of twitter ) using only their public apis. <eos> as a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. <eos> the resources in described in this paper are available at http : //www.cs.cmu.edu/ ? lingwang/utopia.
supervised topic models with a logistic likelihood have two issues that potentially limit their practical use : 1 ) response variables are usually over-weighted by document word counts ; and 2 ) existing variational inference methods make strict mean-field assumptions. <eos> we address these issues by : 1 ) introducing a regularization constant to better balance the two parts based on an optimization formulation of bayesian inference ; and 2 ) developing a simple gibbs sampling algorithm by introducing auxiliary polya-gamma variables and collapsing out dirichlet variables. <eos> our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. <eos> empirical results demonstrate significant improvements on prediction performance and time efficiency.
we present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. <eos> compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have : conciseness, informativeness, and grammaticality. <eos> in addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. <eos> experiments in the tac2008 dataset yield the highest published rouge scores to date, with runtimes that rival those of extractive summarizers.
we present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. <eos> by jointly modeling the text of the document and the noisy ( but regular ) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. <eos> overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31 % relative reduction in word error rate over the leading commercial system for historical transcription, and a 47 % relative reduction over tesseract, google ? s open source ocr system.
we adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. <eos> unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. <eos> instead, we show how the weak supervision of response feedback ( e.g. <eos> successful task completion ) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.
syntactic structures, by their nature, reflect first and foremost the formal constructions used for expressing meanings. <eos> this renders them sensitive to formal variation both within and across languages, and limits their value to semantic applications. <eos> we present ucca, a novel multi-layered framework for semantic representation that aims to accommodate the semantic distinctions expressed through linguistic utterances. <eos> we demonstrate ucca ? s portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. <eos> we also show that ucca can be effectively and quickly learned by annotators with no linguistic background, and describe the compilation of a uccaannotated corpus.
many current natural language processing techniques work well assuming a large context of text as input data. <eos> however they become ineffective when applied to short texts such as twitter feeds. <eos> to overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for nlp tasks. <eos> this requires robust modeling and understanding of the semantics of short texts. <eos> the contribution of the paper is two-fold : 1. we introduce the linking-tweets-tonews task as well as a dataset of linked tweet-news pairs, which can benefit many nlp applications ; 2. in contrast to previous research which focuses on lexical features within the short texts ( text-to-word information ), we propose a graph based latent variable model that models the inter short text correlations ( text-to-text information ). <eos> this is motivated by the observation that a tweet usually only covers one aspect of an event. <eos> we show that using tweet specific feature ( hashtag ) and news specific feature ( named entities ) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. <eos> our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task.
we propose a computational framework for identifying linguistic aspects of politeness. <eos> our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. <eos> these findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. <eos> our classifier achieves close to human performance and is effective across domains. <eos> we use our framework to study the relationship between politeness and social power, showing that polite wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. <eos> we see a similar negative correlation between politeness and power on stack exchange, where users at the top of the reputation scale are less polite than those at the bottom. <eos> finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community.
recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and relevance to prompt, but there is relatively little work on modeling thesis clarity. <eos> we present a new annotated corpus and propose a learning-based approach to scoring essays along the thesis clarity dimension. <eos> additionally, in order to provide more valuable feedback on why an essay is scored as it is, we propose a second learning-based approach to identifying what kinds of errors an essay has that may lower its thesis clarity score.
we present a corpus analysis of how italian connectives are translated into lis, the italian sign language. <eos> since corpus resources are scarce, we propose an alignment method between the syntactic trees of the italian sentence and of its lis translation. <eos> this method, and clustering applied to its outputs, highlight the different ways a connective can be rendered in lis : with a corresponding sign, by affecting the location or shape of other signs, or being omitted altogether. <eos> we translate these findings into a computational model that will be integrated into the pipeline of an existing italian-lis rendering system. <eos> initial experiments to learn the four possible translations with decision trees give promising results.
even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. <eos> in this paper, we exploit a prior knowledge of stop-probabilities ( whether a given word has any children in a given direction ), which is obtained from a large raw corpus using the reducibility principle. <eos> by incorporating this knowledge into dependency model with valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from conll 2006 and 2007 shared tasks.
in this paper, we consider the problem of cross-formalism transfer in parsing. <eos> we are interested in parsing constituencybased grammars such as hpsg and ccg using a small amount of data specific for the target formalism, and a large quantity of coarse cfg annotations from the penn treebank. <eos> while all of the target formalisms share a similar basic syntactic structure with penn treebank cfg, they also encode additional constraints and semantic features. <eos> to handle this apparent discrepancy, we design a probabilistic model that jointly generates cfg and target formalism parses. <eos> the model includes features of both parses, allowing transfer between the formalisms, while preserving parsing efficiency. <eos> we evaluate our approach on three constituency-based grammars ? <eos> ccg, hpsg, and lfg, augmented with the penn treebank-1. <eos> our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1
we propose a new variant of treeadjoining grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. <eos> we provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. <eos> this collapsed context-free form is used to implement efficient grammar estimation and parsing algorithms. <eos> we perform parsing experiments the penn treebank and draw comparisons to treesubstitution grammars and between different variations in probabilistic model design. <eos> examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.
we present a fast and scalable online method for tuning statistical machine translation models with large feature sets. <eos> the standard tuning algorithm ? mert ? only scales to tens of features. <eos> recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. <eos> our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. <eos> large-scale experiments on arabic-english and chinese-english show that our method produces significant translation quality gains by exploiting sparse features. <eos> equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation.
in this paper, we propose a novel reordering model based on sequence labeling techniques. <eos> our model converts the reordering problem into a sequence labeling problem, i.e. <eos> a tagging task. <eos> results on five chinese-english nist tasks show that our model improves the baseline system by 1.32 bleu and 1.53 ter on average. <eos> results of comparative study with other seven widely used reordering models will also be reported.
most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. <eos> however phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. <eos> we propose a new model to address this imbalance, based on a word-based markov model of translation which generates target translations left-to-right. <eos> our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical pitman-yor process prior to provide dynamic adaptive smoothing. <eos> this mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. <eos> our experiments on chinese to english and arabic to english translation show consistent improvements over competitive baselines, of up to +3.4 bleu.
semi-supervised learning ( ssl ) methods augment standard machine learning ( ml ) techniques to leverage unlabeled data. <eos> ssl techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. <eos> however, existing ssl techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. <eos> in this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. <eos> we present a novel learning algorithm, which optimizes a naive bayes model to accord with statistics calculated from the unlabeled corpus. <eos> in experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than ssl techniques from previous work.
we present two latent variable models for learning character types, or personas, in film, in which a persona is defined as a set of mixtures over latent lexical classes. <eos> these lexical classes capture the stereotypical actions of which a character is the agent and patient, as well as attributes by which they are described. <eos> as the first attempt to solve this problem explicitly, we also present a new dataset for the text-driven analysis of film, along with a benchmark testbed to help drive future work in this area.
in this paper, we propose a new bayesian inference method to train statistical machine translation systems using only nonparallel corpora. <eos> following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features ( besides simple bag-of-words ) as side-information used for estimating translation models. <eos> in order to perform fast, efficient bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of ahmed et al ( 2012 ). <eos> the new translation hash sampler enables us to scale elegantly to complex models ( for the first time ) and large vocabulary/corpora sizes. <eos> we show empirical results on the opus data ? our method yields the best bleu scores compared to existing approaches, while achieving significant computational speedups ( several orders faster ). <eos> we also report for the first time ? bleu score results for a largescale mt task using only non-parallel data ( emea corpus ).
the english ? s possessive construction occurs frequently in text and can encode several different semantic relations ; however, it has received limited attention from the computational linguistics community. <eos> this paper describes the creation of a semantic relation inventory covering the use of ? s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of possessives annotated according to the relations, and an accurate automatic annotation system for labeling new examples. <eos> our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic classification system, which achieves 87.4 % accuracy in our classification experiment, and our annotation data are publicly available.
this paper presents novel methods for modeling numerical common sense : the ability to infer whether a given number ( e.g., three billion ) is large, small, or normal for a given context ( e.g., number of people facing a water shortage ). <eos> we first discuss the necessity of numerical common sense in solving textual entailment problems. <eos> we explore two approaches for acquiring numerical common sense. <eos> both approaches start with extracting numerical expressions and their context from the web. <eos> one approach estimates the distribution of numbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distribution. <eos> another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. <eos> experimental results demonstrate the effectiveness of both approaches.
generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. <eos> in contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. <eos> we introduce distributional semantic hidden markov models, a novel variant of a hidden markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. <eos> experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. <eos> in a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization.
in this paper we present a method for extracting bilingual terminologies from comparable corpora. <eos> in our approach we treat bilingual term extraction as a classification problem. <eos> for classification we use an svm binary classifier and training data taken from the eurovoc thesaurus. <eos> we test our approach on a held-out test set from eurovoc and perform precision, recall and f-measure evaluations for 20 european language pairs. <eos> the performance of our classifier reaches the 100 % precision level for many language pairs. <eos> we also perform manual evaluation on bilingual terms extracted from english-german term-tagged comparable corpora. <eos> the results of this manual evaluation showed 60-83 % of the term pairs generated are exact translations and over 90 % exact or partial translations.
expensive feature engineering based on wordnet senses has been shown to be useful for document level sentiment classification. <eos> a plausible reason for such a performance improvement is the reduction in data sparsity. <eos> however, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. <eos> in this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. <eos> experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. <eos> similar idea is applied to cross lingual sentiment analysis ( clsa ), and it is shown that reduction in data sparsity ( after translation or bilingual-mapping ) produces accuracy higher than machine translation based clsa and sense based clsa.
supervised training procedures for semantic parsers produce high-quality semantic parsers, but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. <eos> we present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms, schema matching, and pattern learning. <eos> leveraging techniques from each of these areas, we develop a semantic parser for freebase that is capable of parsing questions with an f1 that improves by 0.42 over a purely-supervised learning algorithm.
shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. <eos> one important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. <eos> this turns out to have a large empirical impact on the framework of global training and beam search. <eos> we propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. <eos> our parser gives comparable accuracies to the state-of-the-art chart parsers. <eos> with linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.
many models in nlp involve latent variables, such as unknown parses, tags, or alignments. <eos> finding the optimal model parameters is then usually a difficult nonconvex optimization problem. <eos> the usual practice is to settle for local optimization methods such as em or gradient ascent. <eos> we explore how one might instead search for a global optimum in parameter space, using branch-and-bound. <eos> our method would eventually find the global maximum ( up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. <eos> as an illustrative case, we study a generative model for dependency parsing. <eos> we search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. <eos> we show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. <eos> we use the reformulation linearization technique to produce convex relaxations during branch-and-bound. <eos> although these techniques do not yet provide a practical solution to our instance of this np-hard problem, they sometimes find better solutions than viterbi em with random restarts, in the same time.
natural language parsing has typically been done with small sets of discrete categories such as np and vp, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. <eos> instead, we introduce a compositional vector grammar ( cvg ), which combines pcfgs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. <eos> the cvg improves the pcfg of the stanford parser by 3.8 % to obtain an f1 score of 90.4 %. <eos> it is fast to train and implemented approximately as an efficient reranker it is about 20 % faster than the current stanford factored parser. <eos> the cvg learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as pp attachments.
in spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. <eos> current approaches based on generative or discriminative models have different but important shortcomings that limit their accuracy. <eos> in this paper we discuss these limitations and introduce a new approach for discriminative state tracking that overcomes them by leveraging the problem structure. <eos> an offline evaluation with dialog data collected from real users shows improvements in both state tracking accuracy and the quality of the posterior probabilities. <eos> features that encode speech recognition error patterns are particularly helpful, and training requires relatively few dialogs.
to overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. <eos> however, a previous study ( sporleder and lascarides, 2008 ) showed that models trained on these synthetic data do not generalize very well to natural ( i.e. <eos> genuine ) implicit discourse data. <eos> in this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. <eos> results on pdtb data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged f1 improvement of 5.86 % over baseline models.
we propose a novel approach for developing a two-stage document-level discourse parser. <eos> our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two conditional random fields : one for intrasentential parsing and the other for multisentential parsing. <eos> we present two approaches to combine these two stages of discourse parsing effectively. <eos> a set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.
this paper proposes a new method for significantly improving the performance of pairwise coreference models. <eos> given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. <eos> in effect, our approach finds an optimal feature space ( derived from a base feature set and indicator set ) for discriminating coreferential mention pairs. <eos> although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. <eos> our experiments on the conll-2012 shared task english datasets ( gold mentions ) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. <eos> our best system obtains a competitive 67.2 of average f1 over muc, b3, and ceaf which, despite its simplicity, places it above the mean score of other systems on these datasets.
techniques that compare short text segments using dependency paths ( or simply, paths ) appear in a wide range of automated language processing applications including question answering ( qa ). <eos> however, few models in ad hoc information retrieval ( ir ) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection. <eos> in this paper, we introduce a flexible notion of paths that describe chains of words on a dependency path. <eos> these chains, or catenae, are readily applied in standard ir models. <eos> informative catenae are selected using supervised machine learning with linguistically informed features and compared to both non-linguistic terms and catenae selected heuristically with filters derived from work on paths. <eos> automatically selected catenae of 1-2 words deliver significant performance gains on three trec collections.
paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. <eos> this has painful consequences such as high frequency of parsing errors related to coordination. <eos> in other words, coordination is a pending problem in dependency analysis of natural languages. <eos> this paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. <eos> we introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. <eos> in addition, empirical observations on convertibility between selected styles of representations are shown too.
we present glossboot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. <eos> for each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the web for that domain by means of iteratively acquired term/gloss extraction patterns. <eos> our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages.
crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. <eos> yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. <eos> in this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. <eos> we formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. <eos> we test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.
this paper discusses the construction of a parallel treebank currently involving ten languages from six language families. <eos> the treebank is based on deep lfg ( lexicalfunctional grammar ) grammars that were developed within the framework of the pargram ( parallel grammar ) effort. <eos> the grammars produce output that is maximally parallelized across languages and language families. <eos> this output forms the basis of a parallel treebank covering a diverse set of phenomena. <eos> the treebank is publicly available via the iness treebanking environment, which also allows for the alignment of language pairs. <eos> we thus present a unique, multilayered parallel treebank that represents more and different types of languages than are available in other treebanks, that represents deep linguistic knowledge and that allows for the alignment of sentences at several levels : dependency structures, constituency structures and pos information.
distributional thesauri are now widely used in a large number of natural language processing tasks. <eos> however, they are far from containing only interesting semantic relations. <eos> as a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. <eos> in this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. <eos> this identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. <eos> its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. <eos> we evaluate the interest of this method for a large set of english nouns with various frequencies.
we consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. <eos> we create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688k images. <eos> we use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. <eos> we show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.
developing natural language processing tools for low-resource languages often requires creating resources from scratch. <eos> while a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. <eos> we discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. <eos> we obtain timed annotations from linguists for the low-resource languages kinyarwanda and malagasy ( as well as english ) and evaluate how the amounts of various kinds of data affect performance of a trained pos-tagger. <eos> our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. <eos> we also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.
this paper demonstrates the need and impact of subcategorization information for smt. <eos> we combine ( i ) features on sourceside syntactic subcategorization and ( ii ) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. <eos> a manual evaluation of an english-togerman translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case.
we propose a name-aware machine translation ( mt ) approach which can tightly integrate name processing into mt model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. <eos> additionally, we also propose a new mt metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. <eos> experiments on chinese-english translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality mt baseline1.
in this paper we show that even for the case of 1:1 substitution ciphers ? which encipher plaintext symbols by exchanging them with a unique substitute ? finding the optimal decipherment with respect to a bigram language model is np-hard. <eos> we show that in this case the decipherment problem is equivalent to the quadratic assignment problem ( qap ). <eos> to the best of our knowledge, this connection between the qap and the decipherment problem has not been known in the literature before.
this paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions : ( 1 ) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other ; and ( 2 ) initial alignment is readily available with existing alignment techniques. <eos> they are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. <eos> the evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.
this paper studies the problem of mining named entity translations from comparable corpora with some ? asymmetry ?. <eos> unlike the previous approaches relying on the ? symmetry ? <eos> found in parallel corpora, the proposed method is tolerant to asymmetry often found in comparable corpora, by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora. <eos> our experimental results on english-chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for organization names, and 0.14 in a low comparability case.
wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing. <eos> however, infobox information is very incomplete and imbalanced among the wikipedias in different languages. <eos> it is a promising but challenging problem to utilize the rich structured knowledge from a source language wikipedia to help complete the missing infoboxes for a target language. <eos> in this paper, we formulate the problem of cross-lingual knowledge extraction from multilingual wikipedia sources, and present a novel framework, called wikicike, to solve this problem. <eos> an instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors. <eos> our experimental results demonstrate that wikicike outperforms the monolingual knowledge extraction method and the translation-based method.
we propose the hypothesis that word etymology is useful for nlp applications as a bridge between languages. <eos> we support this hypothesis with experiments in crosslanguage ( english-italian ) document categorization. <eos> in a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on english data, on italian test data ( and viceversa ). <eos> the results show not only statistically significant, but a large improvement ? <eos> a jump of almost 40 points in f1-score ? <eos> over the raw ( vanilla bag-ofwords ) representation.
online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. <eos> in forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. <eos> existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. <eos> in this paper, we focus on identifying the nature of interactions among user pairs. <eos> the central questions are : how does each pair of users interact with each other ? <eos> does the pair of users mostly agree or disagree ? <eos> what is the lexicon that people often use to express agreement and disagreement ? <eos> we present a topic model based approach to answer these questions. <eos> since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. <eos> after modeling, we use the modeling results to classify the nature of interaction of each user pair. <eos> our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.
metaphor is an important way of conveying the affect of people, hence understanding how people use metaphors to convey affect is important for the communication between individuals and increases cohesion if the perceived affect of the concrete example is the same for the two individuals. <eos> therefore, building computational models that can automatically identify the affect in metaphor-rich texts like ? the team captain is a rock. <eos> ?, ? time is money. <eos> ?, ? my lawyer is a shark. ? <eos> is an important challenging problem, which has been of great interest to the research community. <eos> to solve this task, we have collected and manually annotated the affect of metaphor-rich texts for four languages. <eos> we present novel algorithms that integrate triggers for cognitive, affective, perceptual and social processes with stylistic and lexical information. <eos> by running evaluations on datasets in english, spanish, russian and farsi, we show that the developed affect polarity and valence prediction technology of metaphor-rich texts is portable and works equally well for different languages.
standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages ( which require large lexical tagset inventories ). <eos> for this reason, a number of alternative methods have been proposed over the years. <eos> one of the most successful methods used for this task, fdoohg7lhuhg7djjlqj 7xil, 1999 ), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. <eos> a second phase is aimed at recovering the full set of morpho-syntactic features. <eos> in this paper we present an alternative method to tiered tagging, based on local optimizations with neural networks and we show how, by properly encoding the input sequence in a general neural network architecture, we achieve results similar to the tiered tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method.
we present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem. <eos> the idea draws on the observation that the lemmatisation of almost all polish noun phrases may be decomposed into transformation of singular words ( tokens ) that make up each phrase. <eos> we perform evaluation, which shows results similar to those obtained earlier by a rule-based system, while our approach allows to separate chunking from lemmatisation.
we describe a novel approach for automatically predicting the hidden demographic properties of social media users. <eos> building on prior work in common-sense knowledge acquisition from third-person text, we first learn the distinguishing attributes of certain classes of people. <eos> for example, we learn that people in the female class tend to have maiden names and engagement rings. <eos> we then show that this knowledge can be used in the analysis of first-person communication ; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. <eos> our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20 % relative error reduction over the current state-of-the-art.
with the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. <eos> however, only a small amount of annotated data is available for training quality assessment systems. <eos> wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. <eos> we show that the distribution of these labels is topically biased, since they can not be applied freely to any arbitrary article. <eos> we argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. <eos> we factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. <eos> this approach better reflects the situation a classifier would face in a real-life application.
we address the problem of informal word recognition in chinese microblogs. <eos> a key problem is the lack of word delimiters in chinese. <eos> we exploit this reliance as an opportunity : recognizing the relation between informal word recognition and chinese word segmentation, we propose to model the two tasks jointly. <eos> our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially.
we introduce the novel task of automatically generating questions that are relevant to a text but do not appear in it. <eos> one motivating example of its application is for increasing user engagement around news articles by suggesting relevant comparable questions, such as ? is beyonce a better singer than madonna ? <eos> ?, for the user to answer. <eos> we present the first algorithm for the task, which consists of : ( a ) offline construction of a comparable question template database ; ( b ) ranking of relevant templates to a given article ; and ( c ) instantiation of templates only with entities in the article whose comparison under the template ? s relation makes sense. <eos> we tested the suggestions generated by our algorithm via a mechanical turk experiment, which showed a significant improvement over the strongest baseline of more than 45 % in all metrics.
punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. <eos> this paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. <eos> our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. <eos> it can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. <eos> the experimental results on the test data sets of iwslt and tdt4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text.
structural information in web text provides natural annotations for nlp problems such as word segmentation and parsing. <eos> in this paper we propose a discriminative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the internet. <eos> it utilizes the internet as an external corpus with massive ( although slight and sparse ) natural annotations, and enables a classifier to evolve on the large-scaled and real-time updated web text. <eos> with chinese word segmentation as a case study, experiments show that the segmenter enhanced with the chinese wikipedia achieves significant improvement on a series of testing sets from different domains, even with a single classifier and local features.
this paper introduces a graph-based semisupervised joint model of chinese word segmentation and part-of-speech tagging. <eos> the proposed approach is based on a graph-based label propagation technique. <eos> one constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. <eos> the derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields ( crfs ) on unlabeled data. <eos> an inductive character-based joint model is obtained eventually. <eos> empirical results on chinese tree bank ( ctb-7 ) and microsoft research corpora ( msr ) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised crfs in this task.
modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. <eos> this is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. <eos> this paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar ( itg ) using a recursive bayesian prior. <eos> overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units ( phrase-pairs ) recursively, terminating at word translations. <eos> our experiments on arabic, urdu and farsi to english demonstrate improvements over competitive baseline systems.
most statistical machine translation ( smt ) systems are modeled using a loglinear framework. <eos> although the log-linear model achieves success in smt, it still suffers from some limitations : ( 1 ) the features are required to be linear with respect to the model itself ; ( 2 ) features can not be further interpreted to reach their potential. <eos> a neural network is a reasonable method to address these pitfalls. <eos> however, modeling smt with a neural network is not trivial, especially when taking the decoding efficiency into consideration. <eos> in this paper, we propose a variant of a neural network, i.e. <eos> additive neural networks, for smt to go beyond the log-linear translation model. <eos> in addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. <eos> our model outperforms the log-linear translation models with/without embedding features on chinese-to-english and japanese-to-english translation tasks.
typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. <eos> with the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. <eos> in face of the problem, we propose an efficient phrase table combination method. <eos> in particular, we train a bayesian phrasal inversion transduction grammars for each domain separately. <eos> the learned phrase tables are hierarchically combined as if they are drawn from a hierarchical pitman-yor process. <eos> the performance measured by bleu is at least as comparable to the traditional batch training method. <eos> furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.
we present a new translation model integrating the shallow local multi bottomup tree transducer. <eos> we perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the wmt 2009 english ? german translation task. <eos> as an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation.
empty categories ( ec ) are artificial elements in penn treebanks motivated by the government-binding ( gb ) theory to explain certain language phenomena such as pro-drop. <eos> ecs are ubiquitous in languages like chinese, but they are tacitly ignored in most machine translation ( mt ) work because of their elusive nature. <eos> in this paper we present a comprehensive treatment of ecs by first recovering them with a structured maxent model with a rich set of syntactic and lexical features, and then incorporating the predicted ecs into a chinese-to-english machine translation task through multiple approaches, including the extraction of ec-specific sparse features. <eos> we show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic mt system.
while domain adaptation techniques for smt have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. <eos> we present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. <eos> we also describe a method for unsupervised adaptation with development and test data from multiple domains. <eos> experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 bleu over unadapted systems and single-domain adaptation.
this paper proposes a nonparametric bayesian method for inducing part-ofspeech ( pos ) tags in dependency trees to improve the performance of statistical machine translation ( smt ). <eos> in particular, we extend the monolingual infinite tree model ( finkel et al, 2007 ) to a bilingual scenario : each hidden state ( pos tag ) of a source-side dependency tree emits a source word together with its aligned target word, either jointly ( joint model ), or independently ( independent model ). <eos> evaluations of japanese-to-english translation on the ntcir-9 data show that our induced japanese pos tags for dependency trees improve the performance of a forestto-string smt system. <eos> our independent model gains over 1 point in bleu by resolving the sparseness problem introduced in the joint model.
community question answering ( cqa ) has become an increasingly popular research topic. <eos> in this paper, we focus on the problem of question retrieval. <eos> question retrieval in cqa can automatically find the most relevant and recent questions that have been solved by other users. <eos> however, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in cqa. <eos> state-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. <eos> while useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora ( e.g., question-answer pairs ) in the absence of which they are troubled by noise issue. <eos> in this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. <eos> our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization. <eos> experiments conducted on a real cqa data show that our proposed approach is promising.
subcategorization frames ( scfs ), selectional preferences ( sps ) and verb classes capture related aspects of the predicateargument structure. <eos> we present the first unified framework for unsupervised learning of these three types of information. <eos> we show how to utilize determinantal point processes ( dpps ), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. <eos> our novel clustering algorithm constructs a joint scf-dpp dpp kernel matrix and utilizes the efficient sampling algorithms of dpps to cluster together verbs with similar scfs and sps. <eos> we evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1.
semantic frames are a rich linguistic resource. <eos> there has been much work on semantic frame parsers, but less that applies them to general nlp problems. <eos> we address a task to predict change in stock price from financial news. <eos> semantic frames help to generalize from specific sentences to scenarios, and to detect the ( positive or negative ) roles of specific companies. <eos> we introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. <eos> our experiments test multiple text representations on two binary classification tasks, change of price and polarity. <eos> experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task.
this paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. <eos> by generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. <eos> through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. <eos> experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of semeval2 unsupervised systems reached.
modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of natural language processing. <eos> in this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by combinatory categorial grammar to introduce combinatory categorial autoencoders. <eos> this model leverages the ccg combinatory operators to guide a non-linear transformation of meaning within a sentence. <eos> we use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.
given that structured output prediction is typically performed over entire datasets, one natural question is whether it is possible to re-use computation from earlier inference instances to speed up inference for future instances. <eos> amortized inference has been proposed as a way to accomplish this. <eos> in this paper, first, we introduce a new amortized inference algorithm called the margin-based amortized inference, which uses the notion of structured margin to identify inference problems for which previous solutions are provably optimal. <eos> second, we introduce decomposed amortized inference, which is designed to address very large inference problems, where earlier amortization methods become less effective. <eos> this approach works by decomposing the output structure and applying amortization piece-wise, thus increasing the chance that we can re-use previous solutions for parts of the output structure. <eos> these parts are then combined to a global coherent solution using lagrangian relaxation. <eos> in our experiments, using the nlp tasks of semantic role labeling and entityrelation extraction, we demonstrate that with the margin-based algorithm, we need to call the inference engine only for a third of the test examples. <eos> further, we show that the decomposed variant of margin-based amortized inference achieves a greater reduction in the number of inference calls.
finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. <eos> furthermore, data mismatch issues, which arise when the expected test ( target ) data does not exactly match the training data, aggravate this scarcity problem. <eos> to deal with these issues, we describe an efficient semisupervised learning ( ssl ) approach which has two components : ( i ) markov topic regression is a new probabilistic model to cluster words into semantic tags ( concepts ). <eos> it can efficiently handle semantic ambiguity by extending standard topic models with two new features. <eos> first, it encodes word n-gram features from labeled source and unlabeled target data. <eos> second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. <eos> ( ii ) retrospective learner is a new learning technique that adapts to the unlabeled target data. <eos> our new ssl approach improves semantic tagging performance by 3 % absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.
hyperedge replacement grammar ( hrg ) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. <eos> a recognition algorithm due to lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. <eos> we present a more precise characterization of the algorithm ? s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. <eos> the algorithm is part of bolinas, a new software toolkit for hrg processing.
we present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. <eos> our gusp system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using em. <eos> to compensate for the lack of example annotations or question-answer pairs, gusp adopts a novel grounded-learning approach to leverage database for indirect supervision. <eos> on the challenging atis dataset, gusp attained an accuracy of 84 %, effectively tying with the best published results by supervised approaches.
it is important that the testimony of children be admissible in court, especially given allegations of abuse. <eos> unfortunately, children can be misled by interrogators or might offer false information, with dire consequences. <eos> in this work, we evaluate various parameterizations of five classifiers ( including support vector machines, neural networks, and random forests ) in deciphering truth from lies given transcripts of interviews with 198 victims of abuse between the ages of 4 and 7. <eos> these evaluations are performed using a novel set of syntactic features, including measures of complexity. <eos> our results show that sentence length, the mean number of clauses per utterance, and the stajnermitkov measure of complexity are highly informative syntactic features, that classification accuracy varies greatly by the age of the speaker, and that accuracy up to 91.7 % can be achieved by support vector machines given a sufficient amount of data.
a number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. <eos> we propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. <eos> we demonstrate experimentally that sentiment relevance and subjectivity are related, but different. <eos> since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers : a distant supervision approach that leverages structured information about the domain of the reviews ; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. <eos> we show that both methods learn sentiment relevance classifiers that perform well.
while there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have explored how her/his utterance affects the emotion of the addressee. <eos> this has motivated us to investigate two novel tasks : predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee ? s mind. <eos> we target japanese twitter posts as a source of dialogue data and automatically build training data for learning the predictors and generators. <eos> the feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by five human workers.
during real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. <eos> with the recent growth of social websites such as youtube, facebook, and amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. <eos> this paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. <eos> using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5 % as compared to the best performing individual modality.
sentiment similarity of word pairs reflects the distance between the words regarding their underlying sentiments. <eos> this paper aims to infer the sentiment similarity between word pairs with respect to their senses. <eos> to achieve this aim, we propose a probabilistic emotionbased approach that is built on a hidden emotional model. <eos> the model aims to predict a vector of basic human emotions for each sense of the words. <eos> the resultant emotional vectors are then employed to infer the sentiment similarity of word pairs. <eos> we apply the proposed approach to address two main nlp tasks, namely, indirect yes/no question answer pairs inference and sentiment orientation prediction. <eos> extensive experiments demonstrate the effectiveness of the proposed approach.
social media contain a multitude of user opinions which can be used to predict realworld phenomena in many domains including politics, finance and health. <eos> most existing methods treat these problems as linear regression, learning to relate word frequencies and other simple features to a known response variable ( e.g., voting intention polls or financial indicators ). <eos> these techniques require very careful filtering of the input texts, as most social media posts are irrelevant to the task. <eos> in this paper, we present a novel approach which performs high quality filtering automatically, through modelling not just words but also users, framed as a bilinear model with a sparse regulariser. <eos> we also consider the problem of modelling groups of related output variables, using a structured multi-task regularisation method. <eos> our experiments on voting intention prediction demonstrate strong performance over large-scale input from twitter on two distinct case studies, outperforming competitive baselines.
in this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming ( ilp ) framework. <eos> for each bigram, a regression model is used to estimate its frequency in the reference summary. <eos> the regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. <eos> during testing, the sentence selection problem is formulated as an ilp problem to maximize the bigram gains. <eos> we demonstrate that our system consistently outperforms the previous ilp method on different tac data sets, and performs competitively compared to the best results in the tac evaluations. <eos> we also conducted various analysis to show the impact of bigram selection, weight estimation, and ilp setup.
we propose a new optimization framework for summarization by generalizing the submodular framework of ( lin and bilmes, 2011 ). <eos> in our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion ; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. <eos> we consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. <eos> we conduct experiments on two corpora ? duc 2004 and user comments on news articles ? and show that the performance of our algorithm outperforms those that rely only on submodularity.
this study proposes a text summarization model that simultaneously performs sentence extraction and compression. <eos> we translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. <eos> we also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. <eos> in order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 12 ( 1 ? <eos> e ? 1 ). <eos> ourexperiments with the ntcir aclia test collections show that our approach outperforms a state-of-the-art algorithm.
probabilistic context-free grammars have the unusual property of not always defining tight distributions ( i.e., the sum of the ? probabilities ? <eos> of the trees the grammar generates can be less than one ). <eos> this paper reviews how this non-tightness can arise and discusses its impact on bayesian estimation of pcfgs. <eos> we begin by presenting the notion of ? almost everywhere tight grammars ? <eos> and show that linear cfgs follow it. <eos> we then propose three different ways of reinterpreting non-tight pcfgs to make them tight, show that the bayesian estimators in johnson et al ( 2007 ) are correct under one of them, and provide mcmc samplers for the other two. <eos> we conclude with a discussion of the impact of tightness empirically.
this paper describes a method of inducing wide-coverage ccg resources for japanese. <eos> while deep parsers with corpusinduced grammars have been emerging for some languages, those for japanese have not been widely studied, mainly because most japanese syntactic resources are dependency-based. <eos> our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into ccg derivations. <eos> the method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing.
we present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. <eos> selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. <eos> we also present a new transition-based dependency parsing algorithm that gives a complexity of o ( n ) for projective parsing and an expected linear time speed for non-projective parsing. <eos> with the standard setup, our parser shows an unlabeled attachment score of 92.96 % and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.
this paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. <eos> by moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. <eos> we induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed english translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5 % over the unsupervised baseline and 3.0 % over the best projection baseline on average.
translated bi-texts contain complementary language cues, and previous work on named entity recognition ( ner ) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. <eos> however, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. <eos> we observe that ner label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual ner tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. <eos> we introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. <eos> we design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and ner output space. <eos> experiments on the ontonotes dataset demonstrate that our method yields significant improvements in both ner and word alignment over state-of-the-art monolingual baselines.
in some societies, internet users have to create information morphs ( e.g. <eos> ? peace west king ? <eos> to refer to ? bo xilai ? ) <eos> to avoid active censorship or achieve other communication goals. <eos> in this paper we aim to solve a new problem of resolving entity morphs to their real targets. <eos> we exploit temporal constraints to collect crosssource comparable corpora relevant to any given morph query and identify target candidates. <eos> then we propose various novel similarity measurements including surface features, meta-path based semantic features and social correlation features and combine them in a learning-to-rank framework. <eos> experimental results on chinese sina weibo data demonstrate that our approach is promising and significantly outperforms baseline methods1.
we describe a new probabilistic model for extracting events between major political actors from news corpora. <eos> our unsupervised model brings together familiar components in natural language processing ( like parsers and topic models ) with contextual political information ? <eos> temporal and dyad dependence ? to infer latent event classes. <eos> we quantitatively evaluate the model ? s performance on political science benchmarks : recovering expert-assigned event class valences, and detecting real-world conflict. <eos> we also conduct a small case study based on our model ? s inferences. <eos> a supplementary appendix, and replication software/data are available online, at : http : //brenocon.com/irevents
out-of-vocabulary ( oov ) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. <eos> in this paper, we propose a novel approach to finding translations for oov words. <eos> we induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. <eos> our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step ( from oov directly to a source language phrase that has a translation ) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. <eos> experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.
recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. <eos> however, these solutions are impractical in complex structured prediction problems such as statistical machine translation. <eos> we present an online gradient-based algorithm for relative margin maximization, which bounds the spread of the projected data while maximizing the margin. <eos> we evaluate our optimizer on chinese-english and arabicenglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 bleu and 1.7-4.3 ter on average over state-of-the-art optimizers with the large feature set.
predicate-argument structure ( pas ) has been demonstrated to be very effective in improving smt performance. <eos> however, since a sourceside pas might correspond to multiple different target-side pass, there usually exist many pas ambiguities during translation. <eos> in this paper, we group pas ambiguities into two types : role ambiguity and gap ambiguity. <eos> then we propose two novel methods to handle the two pas ambiguities for smt accordingly : 1 ) inside context integration ; 2 ) a novel maximum entropy pas disambiguation ( mepd ) model. <eos> in this way, we incorporate rich context information of pas for disambiguation. <eos> then we integrate the two methods into a pasbased translation framework. <eos> experiments show that our approach helps to achieve significant improvements on translation quality.
mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. <eos> although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. <eos> to address these questions, this paper explores and visualizes mother tongue interference preserved in english texts written by indo-european language speakers. <eos> this paper further explores linguistic features that explain why certain relations are preserved in english writing, and which contribute to related tasks such as native language identification.
we describe a new representation of the content vocabulary of a text we call word association profile that captures the proportions of highly associated, mildly associated, unassociated, and dis-associated pairs of words that co-exist in the given text. <eos> we illustrate the shape of the distirbution and observe variation with genre and target audience. <eos> we present a study of the relationship between quality of writing and word association profiles. <eos> for a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly associated and dis-associated pairs, and lower percentages of mildly associated pairs of words. <eos> finally, we use word association profiles to improve a system for automated scoring of essays.
text normalization is an important first step towards enabling many natural language processing ( nlp ) tasks over informal text. <eos> while many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense ; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. <eos> in this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. <eos> to understand the real effect of normalization on the parser, we tie normalization performance directly to parser performance. <eos> additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. <eos> our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.
this paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. <eos> based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate ? s smoothed preferences. <eos> experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.
this paper presents a novel deterministic algorithm for implicit semantic role labeling. <eos> the system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. <eos> the algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. <eos> in addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.
derivational models are still an underresearched area in computational morphology. <eos> even for german, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. <eos> this paper describes a rule-based framework for inducing derivational families ( i.e., clusters of lemmas in derivational relationships ) and its application to create a highcoverage german resource, derivbase, mapping over 280k lemmas into more than 17k non-singleton clusters. <eos> we focus on the rule component and a qualitative and quantitative evaluation. <eos> our approach achieves up to 93 % precision and 71 % recall. <eos> we attribute the high precision to the fact that our rules are based on information from grammar books.
we report on the construction of the webis text reuse corpus 2012 for advanced research on text reuse. <eos> the corpus compiles manually written documents obtained from a completely controlled, yet representative environment that emulates the web. <eos> each of the 297 documents in the corpus is about one of the 150 topics used at the trec web tracks 2009 ? 2011, thus forming a strong connection with existing evaluation efforts. <eos> writers, hired at the crowdsourcing platform odesk, had to retrieve sources for a given topic and to reuse text from what they found. <eos> part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents. <eos> this will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third party ? a setting which has not been studied so far. <eos> in addition, the corpus provides an original resource for the evaluation of text reuse and plagiarism detectors, where currently only less realistic resources are employed.
we present spred, a novel method for the creation of large repositories of semantic predicates. <eos> we start from existing collocations to form lexical predicates ( e.g., break ? ) <eos> and learn the semantic classes that best fit the ? <eos> argument. <eos> to do this, we extract all the occurrences in wikipedia which match the predicate and abstract its arguments to general semantic classes ( e.g., break body part, break agreement, etc. ). <eos> our experiments show that we are able to create a large collection of semantic predicates from the oxford advanced learner ? s dictionary with high precision and recall, and perform well against the most similar approach.
in automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. <eos> current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. <eos> in this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. <eos> we conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. <eos> we show that model summaries ( 1 ) are more abstractive and make use of more sentence aggregation, ( 2 ) do not contain as many topical caseframes as system summaries, and ( 3 ) can not be reconstructed solely from the source text, but can be if texts from in-domain documents are added. <eos> these results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.
this paper presents heady : a novel, abstractive approach for headline generation from news collections. <eos> from a web-scale corpus of english news, we mine syntactic patterns that a noisy-or model generalizes into event descriptions. <eos> at inference time, we query the model with the patterns observed in an unseen news collection, identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a headline. <eos> heady improves over a state-of-theart open-domain title abstraction method, bridging half of the gap that separates it from extractive methods using humangenerated titles in manual evaluations, and performs comparably to human-generated headlines as evaluated with rouge.
surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. <eos> they need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. <eos> we formulate surface realisation as a sequence labelling task and combine the use of conditional random fields ( crfs ) with semantic trees. <eos> due to their extended notion of context, crfs are able to take the global utterance context into account and are less constrained by local features than other realisers. <eos> this leads to more natural and less repetitive surface realisation. <eos> it also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. <eos> results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher ( up to 14 % ) than those for taking only local context into account.
long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units. <eos> in this paper, we propose two-neighbor orientation ( tno ) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries. <eos> we explicitly model the longest span of such chunks, referred to as maximal orientation span, to serve as a global parameter that constrains underlying local decisions. <eos> we integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale chinese-to-english translation task. <eos> on nist mt08 set, our most advanced model brings around +2.0 bleu and -1.0 ter improvement.
preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. <eos> previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. <eos> in this paper, we focus on further improving the performance of the reordering model ( and thereby machine translation ) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. <eos> the main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy. <eos> to mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. <eos> this approach generates alignments that are 2.6 f-measure points better than a baseline supervised aligner. <eos> the data generated allows us to train a reordering model that gives an improvement of 1.8 bleu points on the nist mt-08 urdu-english evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 bleu points over a standard phrase-based baseline.
this paper proposes a new approach to domain adaptation in statistical machine translation ( smt ) based on a vector space model ( vsm ). <eos> the general idea is first to create a vector profile for the in-domain development ( ? dev ? ) <eos> set. <eos> this profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora ; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. <eos> then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. <eos> thus, we obtain a decoding feature whose value represents the phrase pair ? s closeness to the dev. <eos> this is a simple, computationally cheap form of instance weighting for phrase pairs. <eos> experiments on large scale nist evaluation data show improvements over strong baselines : +1.8 bleu on arabic to english and +1.4 bleu on chinese to english over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. <eos> an informal analysis suggests that vsm adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.
we present a method for automatically generating input parsers from english specifications of input file formats. <eos> we use a bayesian generative model to capture relevant natural language phenomena and translate the english specification into a specification tree, which is then translated into a c++ input parser. <eos> we model the problem as a joint dependency parsing and semantic role labeling task. <eos> our method is based on two sources of information : ( 1 ) the correlation between the text and the specification tree and ( 2 ) noisy supervision as determined by the success of the generated c++ parser in reading input examples. <eos> our results show that our approach achieves 80.0 % f-score accuracy compared to an f-score of 66.7 % produced by a state-of-the-art semantic parser on a dataset of input format specifications from the acm international collegiate programming contest ( which were written in english for humans with no intention of providing support for automated processing ).1
we study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. <eos> two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. <eos> to address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. <eos> particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. <eos> we evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.
speaker identification is the task of attributing utterances to characters in a literary narrative. <eos> it is challenging to automate because the speakers of the majority of utterances are not explicitly identified in novels. <eos> in this paper, we present a supervised machine learning approach for the task that incorporates several novel features. <eos> the experimental results show that our method is more accurate and general than previous approaches to the problem.
hierarchical bayesian models ( hbms ) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition. <eos> however, as is well known, hbms are ? ideal ? <eos> learning systems, assuming access to unlimited computational resources that may not be available to child language learners. <eos> consequently, it remains crucial to carefully assess the use of hbms along with alternative, possibly simpler, candidate models. <eos> this paper presents such an evaluation for a language acquisition domain where explicit hbms have been proposed : the acquisition of english dative constructions. <eos> in particular, we present a detailed, empiricallygrounded model-selection comparison of hbms vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning ( lcl ). <eos> our results demonstrate that lcl can match hbm model performance without incurring on the high computational costs associated with hbms.
automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. <eos> a recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. <eos> we propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. <eos> evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. <eos> we also release a first context-sensitive inference rule set.
semantic similarity is an essential component of many natural language processing applications. <eos> however, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. <eos> we present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. <eos> our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. <eos> this unified representation shows state-ofthe-art performance on three tasks : semantic textual similarity, word similarity, and word sense coarsening.
we create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. <eos> it is made by combining wordnets with open licences, data from wiktionary and the unicode common locale data repository. <eos> overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of languages.
we present a new bilingual framenet lexicon for english and german. <eos> it is created through a simple, but powerful approach to construct a framenet in any language using wiktionary as an interlingual representation. <eos> our approach is based on a sense alignment of framenet and wiktionary, and subsequent translation disambiguation into the target language. <eos> we perform a detailed evaluation of the created resource and a discussion of wiktionary as an interlingual connection for the cross-language transfer of lexicalsemantic resources. <eos> the created resource is publicly available at http : //www. <eos> ukp.tu-darmstadt.de/fnwkde/.
parallel text is the fuel that drives modern machine translation systems. <eos> the web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. <eos> we bring web-scale parallel text to the masses by mining the common crawl, a public web crawl hosted on amazon ? s elastic cloud. <eos> starting from nothing more than a set of common two-letter language codes, our open-source extension of the strand algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $ 500. <eos> our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. <eos> even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 bleu. <eos> we make our code and data available for other researchers seeking to mine this rich new data resource.1
we consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. <eos> we present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. <eos> an innovative beam search decoder is proposed to efficiently find highly probable compressions. <eos> under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. <eos> our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics ( e.g. <eos> 8.0 % and 5.4 % improvements in rouge-2 respectively ) for the duc 2006 and 2007 summarization task.
we address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. <eos> we apply multiple-sequence alignment to induce abstract generation templates that can be used for different domains. <eos> an overgenerateand-rank strategy is utilized to produce and rank candidate abstracts. <eos> experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. <eos> in addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.
we present a hybrid natural language generation ( nlg ) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. <eos> our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. <eos> first, we identify domain specific entity tags and discourse representation structures on a per sentence basis. <eos> each sentence is then organized into semantically similar groups ( representing a domain specific concept ) by k-means clustering. <eos> after this semi-automatic processing ( human review of cluster assignments ), a number of corpus ? level statistics are compiled and used as features by a ranking svm to develop model weights from a training corpus. <eos> at generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. <eos> our system is evaluated with automatic, non ? expert crowdsourced and expert evaluation metrics. <eos> we also introduce a novel automatic metric ? <eos> syntactic variability ? <eos> that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. <eos> the metrics for generated weather and biography texts fall within acceptable ranges. <eos> in sum, we argue that our statistical approach to nlg reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. <eos> ? <eos> *ravi kondadadi is now affiliated with nuance communications, inc.
currently, almost all of the statistical machine translation ( smt ) models are trained with the parallel corpora in some specific domains. <eos> however, when it comes to a language pair or a different domain without any bilingual resources, the traditional smt loses its power. <eos> recently, some research works study the unsupervised smt for inducing a simple word-based translation model from the monolingual corpora. <eos> it successfully bypasses the constraint of bitext for smt and obtains a relatively promising result. <eos> in this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. <eos> we apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality.
words often gain new senses in new domains. <eos> being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. <eos> we define a task, sensespotting, in which we build systems to spot tokens that have new senses in new domain text. <eos> instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. <eos> our system is able to achieve f-measures of as much as 80 %, when applied to word types it has never seen before. <eos> our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.
we present brainsup, an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain relatedness and phonetic properties. <eos> we evaluate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters.
we propose a joint inference algorithm for grammatical error correction. <eos> different from most previous work where different error types are corrected independently, our proposed inference process considers all possible errors in a unied framework. <eos> we use integer linear programming ( ilp ) to model the inference process, which can easily incorporate both the power of existing error classiers and prior knowledge on grammatical error correction. <eos> experimental results on the helping our own shared task show that our method is competitive with state-of-the-art systems.
toponym resolvers identify the specific locations referred to by ambiguous placenames in text. <eos> most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. <eos> this paper shows that text-driven disambiguation for toponyms is far more effective. <eos> we exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. <eos> results are given for both 19th century texts pertaining to the american civil war and 20th century newswire articles.
as a paratactic language, sentence-level argument extraction in chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. <eos> to resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as coreference, sequence and parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. <eos> evaluation on the ace 2005 chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline.
methods for information extraction ( ie ) and knowledge base ( kb ) construction have been intensively studied. <eos> however, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. <eos> in this paper, we present a method for discovering and semantically typing newly emerging out-ofkb entities, thus improving the freshness and recall of ontology-based ie and improving the precision and semantic rigor of open ie. <eos> our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. <eos> our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.
relation extraction ( re ) is the task of extracting semantic relationships between entities in text. <eos> recent studies on relation extraction are mostly supervised. <eos> the clear drawback of supervised methods is the need of training data : labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. <eos> this is the problem of domain adaptation. <eos> in this paper, we propose to combine ( i ) term generalization approaches such as word clustering and latent semantic analysis ( lsa ) and ( ii ) structured kernels to improve the adaptability of relation extractors to new text genres/domains. <eos> the empirical evaluation on ace 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation.
word-final /t/-deletion refers to a common phenomenon in spoken english where words such as /west/ ? west ? <eos> are pronounced as ? wes ? <eos> in certain contexts. <eos> phonological variation like this is common in naturally occurring speech. <eos> current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation. <eos> we extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model. <eos> we analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. <eos> we find that bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts.1
speakers of a language can construct an unlimited number of new words through morphological derivation. <eos> this is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. <eos> we adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. <eos> semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. <eos> our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.
in this paper, we present a solution to one aspect of the decipherment task : the prediction of consonants and vowels for an unknown language and alphabet. <eos> adopting a classical bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. <eos> we achieve average accuracy in the unsupervised consonant/vowel prediction task of 99 % across 503 languages. <eos> we further show that our methodology can be used to predict more fine-grained phonetic distinctions. <eos> on a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsupervised accuracy of 89 % across the same set of languages.
in this paper we examine language modeling for text simplification. <eos> unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. <eos> we explore the relationship between normal english and simplified english and compare language models trained on varying amounts of text from each. <eos> we evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from semeval 2012. <eos> we find that a combined model using both simplified and normal english data achieves a 23 % improvement in perplexity and a 24 % improvement on the lexical simplification task over a model trained only on simple data. <eos> post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams.
we suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. <eos> we present a data set of german articles annotated with deep syntax and referents, including some types of implicit referents. <eos> our experiments compare several architectures varying the order of a set of trainable modules. <eos> the results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.
some languages lack large knowledge bases and good discriminative features for name entity recognition ( ner ) that can generalize to previously unseen named entities. <eos> one such language is arabic, which : a ) lacks a capitalization feature ; and b ) has relatively small knowledge bases, such as wikipedia. <eos> in this work we address both problems by incorporating cross-lingual features and knowledge bases from english using cross-lingual links. <eos> we show that such features have a dramatic positive effect on recall. <eos> we show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. <eos> on the standard dataset, we achieved a 4.1 % relative improvement in fmeasure over the best reported result in the literature. <eos> the features led to improvements of 17.1 % and 20.5 % on the new news and microblogs test sets respectively.
in this paper we address the problem of solving substitution ciphers using a beam search approach. <eos> we present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. <eos> we show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6 % decipherment error, while our approach achieves 4.13 % decipherment error in a fraction of time by using a 6-gram language model. <eos> we also apply our approach to the famous zodiac-408 cipher and obtain slightly better ( and near to optimal ) results than previously published. <eos> unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. <eos> furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the gigaword corpus of 7.8 % to 6.0 % error rate.
we introduce a social media text normalization system that can be deployed as a preprocessing step for machine translation and various nlp applications to handle social media text. <eos> the proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. <eos> the proposed approach uses random walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. <eos> we show that the proposed approach has a very high precision of ( 92.43 ) and a reasonable recall of ( 56.4 ). <eos> when used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6 %. <eos> the proposed approach is domain and language independent and can be deployed as a preprocessing step for any nlp application to handle social media text.
hiero translation models have two limitations compared to phrase-based models : 1 ) limited hypothesis space ; 2 ) no lexicalized reordering model. <eos> we propose an extension of hiero called phrasalhiero to address hiero ? s second problem. <eos> phrasal-hiero still has the same hypothesis space as the original hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. <eos> the work consists of two parts : 1 ) for each hiero translation derivation, find its corresponding discontinuous phrase-based path. <eos> 2 ) extend the chart decoder to incorporate features from the phrase-based path. <eos> we achieve significant improvement over both hiero and phrase-based baselines for arabicenglish, chinese-english and germanenglish translation.
we propose a method for automatically detecting low-quality web-text translated by statistical machine translation ( smt ) systems. <eos> we focus on the phrase salad phenomenon that is observed in existing smt results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale web-mined text. <eos> unlike previous approaches that require bilingual data, our method uses only monolingual text as input ; therefore it is applicable for refining data produced by a variety of web-mining activities. <eos> evaluation results show that the proposed method achieves an accuracy of 95.8 % for sentences and 80.6 % for text in noisy web pages.
we study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. <eos> given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. <eos> our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. <eos> experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8 % loss in precision.
the 2011 great east japan earthquake caused a wide range of problems, and as countermeasures, many aid activities were carried out. <eos> many of these problems and aid activities were reported via twitter. <eos> however, most problem reports and corresponding aid messages were not successfully exchanged between victims and local governments or humanitarian organizations, overwhelmed by the vast amount of information. <eos> as a result, victims could not receive necessary aid and humanitarian organizations wasted resources on redundant efforts. <eos> in this paper, we propose a method for discovering matches between problem reports and aid messages. <eos> our system contributes to problem-solving in a large scale disaster situation by facilitating communication between victims and humanitarian organizations.
we propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. <eos> we deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task ; consequently, the induced discourse relations play the role of opinion and aspect shifters. <eos> the quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.
this paper addresses the task of finegrained opinion extraction ? <eos> the identification of opinion-related entities : the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. <eos> most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. <eos> we propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution. <eos> experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.
unbiased language is a requirement for reference sources like encyclopedias and scientific texts. <eos> bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. <eos> to this end we analyze real instances of human edits designed to remove bias from wikipedia articles. <eos> the analysis uncovers two classes of bias : framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity ; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. <eos> we identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensifiers. <eos> these insights help us develop features for a model to solve a new prediction task of practical importance : given a biased sentence, identify the bias-inducing word. <eos> our linguistically-informed model performs almost as well as humans tested on the same task.
we present a city navigation and tourist information mobile dialogue app with integrated question-answering ( qa ) and geographic information system ( gis ) modules that helps pedestrian users to navigate in and learn about urban environments. <eos> in contrast to existing mobile apps which treat these problems independently, our android app addresses the problem of navigation and touristic questionanswering in an integrated fashion using a shared dialogue context. <eos> we evaluated our system in comparison with samsung s-voice ( which interfaces to google navigation and google search ) with 17 users and found that users judged our system to be significantly more interesting to interact with and learn from. <eos> they also rated our system above google search ( with the samsung s-voice interface ) for tourist information tasks.
procedural dialog systems can help users achieve a wide range of goals. <eos> however, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowledge and dialog management strategies. <eos> in this paper, we demonstrate that it is possible to learn procedural dialog systems given only light supervision, of the type that can be provided by non-experts. <eos> we consider domains where the required task knowledge exists in textual form ( e.g., instructional web pages ) and where system builders have access to statements of user intent ( e.g., search query logs or dialog interactions ). <eos> to learn from such textual resources, we describe a novel approach that first automatically extracts task knowledge from instructions, then learns a dialog manager over this task knowledge to provide assistance. <eos> evaluation in a microsoft office domain shows that the individual components are highly accurate and can be integrated into a dialog system that provides effective help to users.
social media platforms have enabled people to freely express their views and discuss issues of interest with others. <eos> while it is important to discover the topics in discussions, it is equally useful to mine the nature of such discussions or debates and the behavior of the participants. <eos> there are many questions that can be asked. <eos> one key question is whether the participants give reasoned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies. <eos> the central idea of this question is tolerance, which is a key concept in the field of communications. <eos> in this work, we perform a computational study of tolerance in the context of online discussions. <eos> we aim to identify tolerant vs. intolerant participants and investigate how disagreement affects tolerance in discussions in a quantitative framework. <eos> to the best of our knowledge, this is the first such study. <eos> our experiments using real-life discussions demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions.
repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. <eos> we present two concrete use cases involving key techniques in the nlp domain for which we show that reproducing results is still difficult. <eos> we show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. <eos> moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. <eos> we identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. <eos> our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.
this work proposes a new segmentation evaluation metric, named boundary similarity ( b ), an inter-coder agreement coefficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in fournier and inkpen ( 2012 ). <eos> existing segmentation metrics such as pk, windowdiff, and segmentation similarity ( s ) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. <eos> despite s ? s improvements, its normalization also produces cosmetically high values that overestimate agreement & performance, leading this work to propose a solution.
query segmentation, like text chunking, is the first step towards query understanding. <eos> in this study, we explore the effectiveness of crowdsourcing for this task. <eos> through carefully designed control experiments and inter annotator agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal ( often only two ) parts. <eos> similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.
in community question answering ( qa ) sites, malicious users may provide deceptive answers to promote their products or services. <eos> it is important to identify and filter out these deceptive answers. <eos> in this paper, we first solve this problem with the traditional supervised learning methods. <eos> two kinds of features, including textual and contextual features, are investigated for this task. <eos> we further propose to exploit the user relationships to identify the deceptive answers, based on the hypothesis that similar users will have similar behaviors to post deceptive or authentic answers. <eos> to measure the user similarity, we propose a new user preference graph based on the answer preference expressed by users, such as ? helpful ? <eos> voting and ? best answer ? <eos> selection. <eos> the user preference graph is incorporated into traditional supervised learning framework with the graph regularization technique. <eos> the experiment results demonstrate that the user preference graph can indeed help improve the performance of deceptive answer prediction.
in this paper, we explore the utility of intra- and inter-sentential causal relations between terms or clauses as evidence for answering why-questions. <eos> to the best of our knowledge, this is the first work that uses both intra- and inter-sentential causal relations for why-qa. <eos> we also propose a method for assessing the appropriateness of causal relations as answers to a given question using the semantic orientation of excitation proposed by hashimoto et al ( 2012 ). <eos> by applying these ideas to japanese why-qa, we improved precision by 4.4 % against all the questions in our test set over the current state-of-theart system for japanese why-qa. <eos> in addition, unlike the state-of-the-art system, our system could achieve very high precision ( 83.2 % ) for 25 % of all the questions in the test set by restricting its output to the confident answers only.
in this paper, we study the answer sentence selection problem for question answering. <eos> unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. <eos> experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. <eos> when evaluated on a benchmark dataset, the map and mrr scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. <eos> moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.
mining opinion targets is a fundamental and important task for opinion mining from online reviews. <eos> to this end, there are usually two kinds of methods : syntax based and alignment based methods. <eos> syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. <eos> in contrast, alignment based methods used word alignment model to fulfill this task, which could avoid parsing errors without using parsing. <eos> however, there is no research focusing on which kind of method is more better when given a certain amount of reviews. <eos> to fill this gap, this paper empirically studies how the performance of these two kinds of methods vary when changing the size, domain and language of the corpus. <eos> we further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. <eos> in our experiments, we verify that our combination is effective on the corpus with small and medium size.
this paper proposes a novel two-stage method for mining opinion words and opinion targets. <eos> in the first stage, we propose a sentiment graph walking algorithm, which naturally incorporates syntactic patterns in a sentiment graph to extract opinion word/target candidates. <eos> then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confidence of patterns. <eos> in the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous methods. <eos> the experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods.
understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purposefully conjure emotion from the readers ? <eos> minds. <eos> the focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the surface, such as ? intelligence ?, ? human ?, and ? cheesecake ?. <eos> we propose induction algorithms encoding a diverse set of linguistic insights ( semantic prosody, distributional similarity, semantic parallelism of coordination ) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon.
we present a dialectal egyptian arabic to english statistical machine translation system that leverages dialectal to modern standard arabic ( msa ) adaptation. <eos> in contrast to previous work, we first narrow down the gap between egyptian and msa by applying an automatic characterlevel transformational model that changes egyptian to eg ?, which looks similar to msa. <eos> the transformations include morphological, phonological and spelling changes. <eos> the transformation reduces the out-of-vocabulary ( oov ) words from 5.2 % to 2.6 % and gives a gain of 1.87 bleu points. <eos> further, adapting large msa/english parallel data increases the lexical coverage, reduces oovs to 0.7 % and leads to an absolute bleu improvement of 2.73 points.
the notion of fertility in word alignment ( the number of words emitted by a single state ) is useful but difficult to model. <eos> initial attempts at modeling fertility used heuristic search methods. <eos> recent approaches instead use more principled approximate inference techniques such as gibbs sampling for parameter estimation. <eos> yet in practice we also need the single best alignment, which is difficult to find using gibbs. <eos> building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility hmm. <eos> finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.
this paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models. <eos> the main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique. <eos> experiments on two well-studied nlp tasks, dependency parsing and ner, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small, i.e., 8 or even 2. <eos> this significant benefit enables us to provide compact model representation, which is especially useful in actual use.
this paper proposes a technique to leverage topic based sentiments from twitter to help predict the stock market. <eos> we first utilize a continuous dirichlet process mixture model to learn the daily topic set. <eos> then, for each topic we derive its sentiment according to its opinion words distribution to build a sentiment time series. <eos> we then regress the stock index and the twitter sentiment time series to predict the market. <eos> experiments on real-life s & p100 index show that our approach is effective and performs better than existing state-of-the-art non-topic based methods.
we propose a novel entity disambiguation model, based on deep neural network ( dnn ). <eos> instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. <eos> stacked denoising auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. <eos> a supervised fine-tuning stage follows to optimize the representation towards the similarity measure. <eos> experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.
statistical language models have successfully been used to describe and analyze natural language documents. <eos> recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. <eos> in this work, we predict comments from java source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. <eos> we evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47 % of the comment typing.
mismatch between queries and documents is a key issue for the web search task. <eos> in order to narrow down such mismatch, in this paper, we present an in-depth investigation on adapting a paraphrasing technique to web search from three aspects : a search-oriented paraphrasing model ; an ndcg-based parameter optimization algorithm ; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries. <eos> experiments performed on the large scale query-document data set show that, the search performance can be significantly improved, with +3.28 % and +1.14 % ndcg gains on dev and test sets respectively.
semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. <eos> here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. <eos> in experiments on the multilingual geoquery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. <eos> these results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.
distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. <eos> we explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases ( dps ). <eos> we introduce a new publicly available dataset to test distributional representations of dps, and we evaluate state-of-the-art models on this set.
uncertainty text detection is important to many social-media-based applications since more and more users utilize social media platforms ( e.g., twitter, facebook, etc. ) <eos> as information source to produce or derive interpretations based on them. <eos> however, existing uncertainty cues are ineffective in social media context because of its specific characteristics. <eos> in this paper, we propose a variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets. <eos> we then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification.
we introduce parma, a system for crossdocument, semantic predicate and argument alignment. <eos> our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. <eos> parma achieves state of the art results on an existing and a new dataset. <eos> we suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17 % f1.
we present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the penn discourse treebank. <eos> our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. <eos> in addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.
conversational implicatures involve reasoning about multiply nested belief structures. <eos> this complexity poses significant challenges for computational models of conversation and cognition. <eos> we show that agents in the multi-agent decentralizedpomdp reach implicature-rich interpretations simply as a by-product of the way they reason about each other to maximize joint utility. <eos> our simulations involve a reference game of the sort studied in psychology and linguistics as well as a dynamic, interactional scenario involving implemented artificial agents.
most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. <eos> in this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. <eos> we show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.
ordering texts is an important task for many nlp applications. <eos> most previous works on summary sentence ordering rely on the contextual information ( e.g. <eos> adjacent sentences ) of each sentence in the source document. <eos> in this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. <eos> we introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. <eos> we also propose to use the genetic algorithm to determine the total order of all sentences. <eos> evaluation results on a news corpus show the effectiveness of our proposed method.
we present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : german, english, swedish, spanish, french and korean. <eos> to show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. <eos> this ? universal ? <eos> treebank is made freely available in order to facilitate research on multilingual dependency parsing.1
aspects of chinese syntax result in a distinctive mix of parsing challenges. <eos> however, the contribution of individual sources of error to overall difficulty is not well understood. <eos> we conduct a comprehensive automatic analysis of error types made by chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of chinese error types by their performance impact. <eos> we also investigate which error types are resolved by using gold part-of-speech tags, showing that improving chinese tagging only addresses certain error types, leaving substantial outstanding challenges.
this paper is concerned with the problem of heterogeneous dependency parsing. <eos> in this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. <eos> different from stacked learning methods ( nivre and mcdonald, 2008 ; martins et al, 2008 ), which process the dependency parsing in a pipelined way ( e.g., a second level uses the first level outputs ), in our method, multiple dependency parsing models are coordinated to exchange consensus information. <eos> we conduct experiments on chinese dependency treebank ( cdt ) and penn chinese treebank ( ctb ), experimental results show that joint inference can bring significant improvements to all state-of-the-art dependency parsers.
in this paper, we combine easy-first dependency parsing and pos tagging algorithms with beam search and structured perceptron. <eos> we propose a simple variant of ? early-update ? <eos> to ensure valid update in the training process. <eos> the proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. <eos> on ctb, we achieve 94.01 % tagging accuracy and 86.33 % unlabeled attachment score with a relatively small beam width. <eos> on ptb, we also achieve state-of-the-art performance.
we present a model for inducing sentential argument structure, which distinguishes arguments from optional modifiers. <eos> we use this model to study whether representing an argument/modifier distinction helps in learning argument structure, and whether a linguistically-natural argument/modifier distinction can be induced from distributional data alone. <eos> our results provide evidence for both hypotheses.
this paper presents an annotation scheme for events that negatively or positively affect entities ( benefactive/malefactive events ) and for the attitude of the writer toward their agents and objects. <eos> work on opinion and sentiment tends to focus on explicit expressions of opinions. <eos> however, many attitudes are conveyed implicitly, and benefactive/malefactive events are important for inferring implicit attitudes. <eos> we describe an annotation scheme and give the results of an inter-annotator agreement study. <eos> the annotated corpus is available online.
this paper attempts to use an off-the-shelf anaphora resolution ( ar ) system for bengali. <eos> the language specific preprocessing modules of guitar ( v3.0.3 ) are identified and suitably designed for bengali. <eos> anaphora resolution module is also modified or replaced in order to realize different configurations of guitar. <eos> performance of each configuration is evaluated and experiment shows that the off-the-shelf ar system can be effectively used for indic languages.
how good are automatic content metrics for news summary evaluation ? <eos> here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. <eos> using four years of data from the text analysis conference, we analyze the performance of eight rouge variants in terms of accuracy, precision and recall in finding significantly different systems. <eos> our experiments show that some of the neglected variants of rouge, based on higher order n-grams and syntactic dependencies, are most accurate across the years ; the commonly used rouge-1 scores find too many significant differences between systems which manual evaluation would deem comparable. <eos> we also test combinations of rouge variants and find that they considerably improve the accuracy of automatic prediction.
this paper tackles the problem of collecting reliable human assessments. <eos> we show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. <eos> to reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. <eos> even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example.
the pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students ? <eos> summaries. <eos> this motivates the development of a more accurate automated method to compute pyramid scores. <eos> of three methods tested here, the one that performs best relies on latent semantics.
the current topic modeling approaches for information retrieval do not allow to explicitly model query-oriented latent topics. <eos> more, the semantic coherence of the topics has never been considered in this field. <eos> we propose a model-based feedback approach that learns latent dirichlet allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. <eos> we perform a first experimental evaluation using two major trec test collections. <eos> results show that retrieval performances tend to be better when using topics with higher semantic coherence.
post-retrieval clustering is the task of clustering web search results. <eos> within this context, we propose a new methodology that adapts the classical k-means algorithm to a third-order similarity measure initially developed for nlp tasks. <eos> results obtained with the definition of a new stopping criterion over the odp-239 and the moresque gold standard datasets evidence that our proposal outperforms all reported text-based approaches.
information retrieval ( ir ) and answer extraction are often designed as isolated or loosely connected components in question answering ( qa ), with repeated overengineering on ir, and not necessarily performance gain for qa. <eos> we propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured ir model. <eos> our method is very quick to implement, and significantly improves ir for qa ( measured in mean average precision and mean reciprocal rank ) by 10 % -20 % against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20 % improvement in qa f1.
we study the mathematical properties of a recently proposed mdl-based unsupervised word segmentation algorithm, called regularized compression. <eos> our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information. <eos> the proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark.
this paper presents a semi-supervised chinese word segmentation ( cws ) approach that co-regularizes character-based and word-based models. <eos> similarly to multi-view learning, the ? segmentation agreements ? <eos> between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. <eos> the proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. <eos> then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. <eos> the agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. <eos> the segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. <eos> the evaluation on the chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature.
transliterated compound nouns not separated by whitespaces pose difficulty on word segmentation ( ws ). <eos> offline approaches have been proposed to split them using word statistics, but they rely on static lexicon, limiting their use. <eos> we propose an online approach, integrating source lm, and/or, back-transliteration and english lm. <eos> the experiments on japanese and chinese ws have shown that the proposed models achieve significant improvement over state-of-the-art, reducing 16 % errors in japanese.
we present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. <eos> the latent topic distribution estimated by latent dirichlet allocation ( lda ) is used to represent each text block. <eos> we employ laplacian eigenmaps ( le ) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. <eos> we evaluate two approaches employing lda and probabilistic latent semantic analysis ( plsa ) distributions respectively. <eos> the effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. <eos> experimental results show that our proposed lda-based approach can outperform the corresponding plsa-based approach. <eos> the proposed approach provides the best performance with the highest f1-measure of 0.7860.
in this paper, we relook at the problem of pronunciation of english words using native phone set. <eos> specifically, we investigate methods of pronouncing english words using telugu phoneset in the context of telugu text-to-speech. <eos> we compare phone-phone substitution and wordphone mapping for pronunciation of english words using telugu phones. <eos> we are not considering other than native language phoneset in all our experiments. <eos> this differentiates our approach from other works in polyglot speech synthesis.
this paper studies named entity translation and proposes ? selective temporality ? <eos> as a new feature, as using temporal features may be harmful for translating ? atemporal ? <eos> entities. <eos> our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1 %.
in this paper, we investigate the application of recurrent neural network language models ( rnnlm ) and factored language models ( flm ) to the task of language modeling for code-switching speech. <eos> we present a way to integrate partof-speech tags ( pos ) and language information ( lid ) into these models which leads to significant improvements in terms of perplexity. <eos> furthermore, a comparison between rnnlms and flms and a detailed analysis of perplexities on the different backoff levels are performed. <eos> finally, we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance. <eos> the final combined language model provides 37.8 % relative improvement in terms of perplexity on the seame development set and a relative improvement of 32.7 % on the evaluation set compared to the traditional n-gram language model. <eos> index terms : multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models
unsupervised object matching ( uom ) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. <eos> however, uom only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying uom to real-world cross-language natural language processing tasks. <eos> to alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. <eos> we demonstrate the effectiveness of our method on cross-language text categorization. <eos> the results show that our method outperforms conventional unsupervised object matching methods.
product reviews are now widely used by individuals and organizations for decision making ( litvin et al, 2008 ; jansen, 2010 ). <eos> and because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. <eos> as a result, the task of deceptive review detection has been gaining increasing attention. <eos> in this paper, we propose a generative lda-based topic modeling approach for fake review detection. <eos> our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95 % accuracy on review spam datasets, outperforming existing baselines by a large margin.
ambiguity preserving representations such as lattices are very useful in a number of nlp tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. <eos> lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. <eos> we advocate hypergraphs as compact representations for sets of utterances describing the same event or object. <eos> we present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. <eos> given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.
humor generation is a very hard problem. <eos> it is difficult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. <eos> we depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. <eos> instead we present a model that uses large amounts of unannotated data to generate i like my x like i like my y, z jokes, where x, y, and z are variables to be filled in. <eos> this is, to the best of our knowledge, the first fully unsupervised humor generation system. <eos> our model significantly outperforms a competitive baseline and generates funny jokes 16 % of the time, compared to 33 % for human-generated jokes.
in this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. <eos> we attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. <eos> evaluated on the wsj corpus, bigram and trigram model perplexity were reduced up to 23.5 % and 14.0 %, respectively. <eos> compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence.
we propose discriminative methods to generate semantic distractors of fill-in-theblank quiz for language learners using a large-scale language learners ? <eos> corpus. <eos> unlike previous studies, the proposed methods aim at satisfying both reliability and validity of generated distractors ; distractors should be exclusive against answers to avoid multiple answers in one quiz, and distractors should discriminate learners ? <eos> proficiency. <eos> detailed user evaluation with 3 native and 23 non-native speakers of english shows that our methods achieve better reliability and validity than previous methods.
we propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. <eos> we propose three types of lexical constraints as building blocks of humorous word substitution : constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. <eos> empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly.
in this paper, we study the problem of automatically annotating the factoids present in collective discourse. <eos> factoids are information units that are shared between instances of collective discourse and may have many different ways of being realized in words. <eos> our approach divides this problem into two steps, using a graph-based approach for each step : ( 1 ) factoid discovery, finding groups of words that correspond to the same factoid, and ( 2 ) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. <eos> we study this on two novel data sets : the new yorker caption contest data set, and the crossword clues data set.
here, we introduce a machine learningbased approach that allows us to identify light verb constructions ( lvcs ) in hungarian and english free texts. <eos> we also present the results of our experiments on the szegedparalellfx english ? hungarian parallel corpus where lvcs were manually annotated in both languages. <eos> with our approach, we were able to contrast the performance of our method and define language-specific features for these typologically different languages. <eos> our presented method proved to be sufficiently robust as it achieved approximately the same scores on the two typologically different languages.
this paper presents the settings and the results of the romip 2013 mt shared task for the english ? russian language direction. <eos> the quality of generated translations was assessed using automatic metrics and human evaluation. <eos> we also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations.
we present indonet, a multilingual lexical knowledge base for indian languages. <eos> it is a linked structure of wordnets of 18 different indian languages, universal word dictionary and the suggested upper merged ontology ( sumo ). <eos> we discuss various benefits of the network and challenges involved in the development. <eos> the system is encoded in lexical markup framework ( lmf ) and we propose modifications in lmf to accommodate universal word dictionary and sumo. <eos> this standardized version of lexical knowledge base of indian languages can now easily be linked to similar global resources.
this paper proposes a methodology for generating specialized japanese data sets for textual entailment, which consists of pairs decomposed into basic sentence relations. <eos> we experimented with our methodology over a number of pairs taken from the rite-2 data set. <eos> we compared our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy.
comparable corpora are important basic resources in cross-language information processing. <eos> however, the existing methods of building comparable corpora, which use intertranslate words and relative features, can not evaluate the topical relation between document pairs. <eos> this paper adopts the bilingual lda model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. <eos> experiments show that the novel method can obtain similar documents with consistent topics own better adaptability and stability performance.
automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words. <eos> in this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. <eos> to improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. <eos> our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines.
linking heterogeneous resources is a major research challenge in the semantic web. <eos> this paper studies the task of mining equivalent relations from linked data, which was insufficiently addressed before. <eos> we introduce an unsupervised method to measure equivalency of relation pairs and cluster equivalent relations. <eos> early experiments have shown encouraging results with an average of 0.75~0.87 precision in predicting relation pair equivalency and 0.78~0.98 precision in relation clustering.
current approaches for word sense disambiguation and translation selection typically require lexical resources or large bilingual corpora with rich information fields and annotations, which are often infeasible for under-resourced languages. <eos> we extract translation context knowledge from a bilingual comparable corpora of a richer-resourced language pair, and inject it into a multilingual lexicon. <eos> the multilingual lexicon can then be used to perform context-dependent lexical lookup on texts of any language, including under-resourced ones. <eos> evaluations on a prototype lookup tool, trained on a english ? malay bilingual wikipedia corpus, show a precision score of 0.65 ( baseline 0.55 ) and mean reciprocal rank score of 0.81 ( baseline 0.771 ). <eos> based on the early encouraging results, the context-dependent lexical lookup tool may be developed further into an intelligent reading aid, to help users grasp the gist of a second or foreign language text.
resource scarcity along with diversity ? <eos> both in dialect and script ? are the two primary challenges in kurdish language processing. <eos> in this paper we aim at addressing these two problems by ( i ) building a text corpus for sorani and kurmanji, the two main dialects of kurdish, and ( ii ) highlighting some of the orthographic, phonological, and morphological differences between these two dialects from statistical and rule-based perspectives.
as most of the world ? s languages are under-resourced, projection algorithms offer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment. <eos> these algorithms, however, make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another. <eos> this assumption is useful but often leads to errors in projection. <eos> in this paper, we will address this weakness by using trees created from instances of interlinear glossed text ( igt ) to discover patterns of divergence between the languages. <eos> we will show that this method improves the performance of projection algorithms significantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees.
cross-lingual projection methods can benefit from resource-rich languages to improve performances of nlp tasks in resources-scarce languages. <eos> however, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. <eos> to make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. <eos> experiments showed that our methods improve the performances greatly on projections between english and chinese.
mapping phrases between languages as translation of each other by using an intermediate language ( pivot language ) may generate translation pairs that are wrong. <eos> since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. <eos> we propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. <eos> we use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. <eos> using the proposed method a relative improvement of 2.8 percent in terms of bleu score is achieved.
we present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval ( clir ). <eos> by iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. <eos> adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. <eos> our setup is time- and memory-efficient and of similar quality as clir-based adaptation on millions of parallel sentences.
this paper explores the use of propositional dynamic logic ( pdl ) as a suitable formal framework for describing sign language ( sl ), the language of deaf people, in the context of natural language processing. <eos> sls are visual, complete, standalone languages which are just as expressive as oral languages. <eos> signs in sl usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. <eos> here we propose a formal representation of sl signs, that will help us with the analysis of automatically-collected hand tracking data from french sign language ( fsl ) video corpora. <eos> we further show how such a representation could help us with the design of computer aided sl verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages.
we propose the use of stacking, an ensemble learning technique, to the statistical machine translation ( smt ) models. <eos> a diverse ensemble of weak learners is created using the same smt engine ( a hierarchical phrase-based system ) by manipulating the training data and a strong model is created by combining the weak models on-the-fly. <eos> experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 bleu points over a conventionally trained smt model.
the quality of bilingual data is a key factor in statistical machine translation ( smt ). <eos> low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. <eos> previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. <eos> to reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. <eos> the method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. <eos> end-to-end experiments show that the proposed method substantially improves the performance in largescale chinese-to-english translation tasks.
in this paper we introduce translation difficulty index ( tdi ), a measure of difficulty in text translation. <eos> we first define and quantify translation difficulty in terms of tdi. <eos> we realize that any measure of tdi based on direct input by translators is fraught with subjectivity and adhocism. <eos> we, rather, rely on cognitive evidences from eye tracking. <eos> tdi is measured as the sum of fixation ( gaze ) and saccade ( rapid eye movement ) times of the eye. <eos> we then establish that tdi is correlated with three properties of the input sentence, viz. <eos> length ( l ), degree of polysemy ( dp ) and structural complexity ( sc ). <eos> we train a support vector regression ( svr ) system to predict tdis for new sentences using these features as input. <eos> the prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of < l, dp, sc > and tdi pairs for a set of sentences. <eos> the primary use of our work is a way of ? binning ? <eos> sentences ( to be translated ) in ? easy ?, ? medium ? <eos> and ? hard ? <eos> categories as per their predicted tdi. <eos> this can decide pricing of any translation task, especially useful in a scenario where parallel corpora for machine translation are built through translation crowdsourcing/outsourcing. <eos> this can also provide a way of monitoring progress of second language learners.
we present a context-sensitive chart pruning method for cky-style mt decoding. <eos> source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. <eos> the proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. <eos> on a full-scale englishto-german experiment with a string-totree model, we obtain a speed-up of more than 60 % over a strong baseline, with no loss in bleu.
in this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. <eos> however, estimating the probabilities of rules extracted from hypergraphs is an np-complete problem, which is computationally infeasible. <eos> therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. <eos> the experiments show that our approach outperforms both 1-best and n-best alignments.
current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. <eos> in this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. <eos> we employ stem as the atomic translation unit to alleviate data spareness. <eos> in addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. <eos> experimental results show that our approach significantly improves the translation performance on tasks of translating from three turkic languages to chinese.
rhetorical structure theory ( rst ) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. <eos> in this paper, we propose a novel translation framework with the help of rst. <eos> in our framework, the translation process mainly includes three steps : 1 ) source rst-tree acquisition : a source sentence is parsed into an rst tree ; 2 ) rule extraction : translation rules are extracted from the source tree and the target string via bilingual word alignment ; 3 ) rst-based translation : the source rst-tree is translated with translation rules. <eos> experiments on chinese-to-english show that our rst-based approach achieves improvements of 2.3/0.77/1.43 bleu points on nist04/nist05/cwmt2008 respectively.
we present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, meant, produces more robustly adequate translations than tuning against bleu or ter as measured across commonly used metrics and human subjective evaluation. <eos> moreover, for informal web forum data, human evaluators preferredmeant-tuned systems over bleu- or ter-tuned systems by a significantly wider margin than that for formal newswire ? even though automatic semantic parsing might be expected to fare worse on informal language. <eos> we argue that by preserving themeaning of the translations as captured by semantic frames right in the training process, an mt system is constrained to make more accurate choices of both lexical and reordering rules. <eos> as a result, mt systems tuned against semantic frame based mt evaluation metrics produce output that is more adequate. <eos> tuning a machine translation system against a semantic frame based objective function is independent of the translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.
in this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. <eos> we integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 bleu points on average over the baseline on nist chinese-english test sets.
we present a simple yet effective approach to syntactic reordering for statistical machine translation ( smt ). <eos> instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. <eos> furthermore,, we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. <eos> our experiments in english-chinese and english-japanese translations demonstrate the effectiveness of the proposed approach : we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric.
machine transliteration is an essential task for many nlp applications. <eos> however, names and loan words typically originate from various languages, obey different transliteration rules, and therefore may benefit from being modeled independently. <eos> recently, transliteration models based on bayesian learning have overcome issues with over-fitting allowing for many-to-many alignment in the training of transliteration models. <eos> we propose a novel coupled dirichlet process mixture model ( cdpmm ) that simultaneously clusters and bilingually aligns transliteration data within a single unified model. <eos> the unified model decomposes into two classes of non-parametric bayesian component models : a dirichlet process mixture model for clustering, and a set of multinomial dirichlet process models that perform bilingual alignment independently for each cluster. <eos> the experimental results show that our method considerably outperforms conventional alignment models.
the phrase-based and n-gram-based smt frameworks complement each other. <eos> while the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. <eos> some work has been done to combine insights from these two frameworks. <eos> a recent successful attempt showed the advantage of using phrasebased search on top of an n-gram-based model. <eos> we probe this question in the reverse direction by investigating whether integrating n-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. <eos> a large scale evaluation over 8 language pairs shows that performance does significantly improve.
in this paper we show how to automatically induce non-linear features for machine translation. <eos> the new features are selected to approximately maximize a bleu-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. <eos> we achieve this by applying gradient boosting machines ( friedman, 2000 ) to learn newweak learners ( features ) in the form of regression trees, using a differentiable loss function related to bleu. <eos> our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks.
an important challenge to statistical machine translation ( smt ) is the lack of parallel data for many language pairs. <eos> one common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. <eos> although pivoting is a robust technique, it introduces some low quality translations. <eos> in this paper, we present two language-independent features to improve the quality of phrase-pivot based smt. <eos> the features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. <eos> we show positive results ( 0.6 bleu points ) on persian-arabic smt as a case study.
we experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of galley et al ( 2004 ). <eos> we compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. <eos> our results demonstrate that the second approach is effective in increasing the quality of translations.
this paper presents two minimum bayes risk ( mbr ) based answer re-ranking ( mbrar ) approaches for the question answering ( qa ) task. <eos> the first approach re-ranks single qa system ? s outputs by using a traditional mbr model, by measuring correlations between answer candidates ; while the second approach reranks the combined outputs of multiple qa systems with heterogenous answer extraction components by using a mixture model-based mbr model. <eos> evaluations are performed on factoid questions selected from two different domains : jeopardy ! <eos> and web, and significant improvements are achieved on all data sets.
question answering systems have been developed for many languages, but most resources were created for english, which can be a problem when developing a system in another language such as french. <eos> in particular, for question classification, no labeled question corpus is available for french, so this paper studies the possibility to use existing english corpora and transfer a classification by translating the question and their labels. <eos> by translating the training corpus, we obtain results close to a monolingual setting.
retrieving similar questions is very important in community-based question answering ( cqa ). <eos> in this paper, we propose a unified question retrieval model based on latent semantic indexing with tensor analysis, which can capture word associations among different parts of cqa triples simultaneously. <eos> thus, our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts. <eos> the experimental result shows that our method outperforms the traditional methods.
some words are more contentful than others : for instance, make is intuitively more general than produce and fifteen is more ? precise ? <eos> than a group. <eos> in this paper, we propose to measure the ? semantic content ? <eos> of lexical items, as modelled by distributional representations. <eos> we investigate the hypothesis that semantic content can be computed using the kullbackleibler ( kl ) divergence, an informationtheoretic measure of the relative entropy of two distributions. <eos> in a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the kl divergence achieves close to 80 % precision but does not outperform a simpler ( linguistically unmotivated ) frequency measure. <eos> we suggest that this result illustrates the rather ? intensional ? <eos> aspect of distributions.
this paper aims at understanding what human think in textual entailment ( te ) recognition process and modeling their thinking process to deal with this problem. <eos> we first analyze a labeled rte-5 test set and find that the negative entailment phenomena are very effective features for te recognition. <eos> then, a method is proposed to extract this kind of phenomena from text-hypothesis pairs automatically. <eos> we evaluate the performance of using the negative entailment phenomena on both the english rte-5 dataset and chinese ntcir-9 rite dataset, and conclude the same findings.
textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. <eos> it thus can not capture the notion that the target fragment is ? almost entailed ? <eos> by the given text. <eos> the recently suggested idea of partial textual entailment may remedy this problem. <eos> we investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. <eos> indeed, our results show that these methods are useful for recognizing partial entailment. <eos> we also provide a preliminary assessment of how partial entailment may be used for recognizing ( complete ) textual entailment.
this paper introduces a supervised approach for performing sentence level dialect identification between modern standard arabic and egyptian dialectal arabic. <eos> we use token level labels to derive sentence-level features. <eos> these features are then used with other core and meta features to train a generative classifier that predicts the correct label for each sentence in the given input text. <eos> the system achieves an accuracy of 85.5 % on an arabic online-commentary dataset outperforming a previously proposed approach achieving 80.9 % and reflecting a significant gain over a majority baseline of 51.9 % and two strong baseline systems of 78.5 % and 80.4 %, respectively.
semantic parsing is a domain-dependent process by nature, as its output is defined over a set of domain symbols. <eos> motivated by the observation that interpretation can be decomposed into domain-dependent and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. <eos> our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains.
in this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. <eos> we argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. <eos> we test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature.
this paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. <eos> we applied an error detection and correction technique to the results of positive and negative documents classified by the support vector machines ( svm ). <eos> the results using reuters documents showed that the method was comparable to the current state-of-the-art biasedsvm method as the f-score obtained by our method was 0.627 and biased-svm was 0.614.
we present an automatic method for analyzing sentiment dynamics between characters in plays. <eos> this literary format ? s structured dialogue allows us to make assumptions about who is participating in a conversation. <eos> once we have an idea of who a character is speaking to, the sentiment in his or her speech can be attributed accordingly, allowing us to generate lists of a character ? s enemies and allies as well as pinpoint scenes critical to a character ? s emotional development. <eos> results of experiments on shakespeare ? s plays are presented along with discussion of how this work can be extended to unstructured texts ( i.e. <eos> novels ).
in this article, we propose a novel classifier based on quantum computation theory. <eos> different from existing methods, we consider the classification as an evolutionary process of a physical system and build the classifier by using the basic quantum mechanics equation. <eos> the performance of the experiments on two datasets indicates feasibility and potentiality of the quantum classifier.
we present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. <eos> recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. <eos> however, some methods take days or weeks to learn good embeddings, and some are notoriously difficult to train. <eos> we propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. <eos> we show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.
we introduce labr, the largest sentiment analysis dataset to-date for the arabic language. <eos> it consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. <eos> we investigate the properties of the the dataset, and present its statistics. <eos> we explore using the dataset for two tasks : sentiment polarity classification and rating classification. <eos> we provide standard splits of the dataset into training and testing, for both polarity and rating classification, in both balanced and unbalanced settings. <eos> we run baseline experiments on the dataset to establish a benchmark.
recommendation dialog systems help users navigate e-commerce listings by asking questions about users ? <eos> preferences toward relevant domain attributes. <eos> we present a framework for generating and ranking fine-grained, highly relevant questions from user-generated reviews. <eos> we demonstrate our approach on a new dataset just released by yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain.
we study subjective language in social media and create twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual twitter streams. <eos> starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. <eos> our experiments on english, spanish and russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media.
emotion classification can be generally done from both the writer ? s and reader ? s perspectives. <eos> in this study, we find that two foundational tasks in emotion classification, i.e., reader ? s emotion classification on the news and writer ? s emotion classification on the comments, are strongly related to each other in terms of coarse-grained emotion categories, i.e., negative and positive. <eos> on the basis, we propose a respective way to jointly model these two tasks. <eos> in particular, a cotraining algorithm is proposed to improve semi-supervised learning of the two tasks. <eos> experimental evaluation shows the effectiveness of our joint modeling approach. <eos> *
quotes are used in news articles as evidence of a person ? s opinion, and thus are a useful target for opinion mining. <eos> however, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. <eos> we address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. <eos> using this we construct a corpus covering 7 topics with 2,228 quotes.
bag-of-words ( bow ) is now the most popular way to model text in machine learning based sentiment classification. <eos> however, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the bow model. <eos> in this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction ( dtdp ), to address it. <eos> the basic idea of dtdp is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for ( dual ) training and ( dual ) prediction. <eos> experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification.
the task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings. <eos> in this paper, we aim to investigate a more challenging task of crosslanguage review rating prediction, which makes use of only rated reviews in a source language ( e.g. <eos> english ) to predict the rating scores of unrated reviews in a target language ( e.g. <eos> german ). <eos> we propose a new coregression algorithm to address this task by leveraging unlabeled reviews. <eos> evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results.
in this paper we present a technique to reveal definitions and hypernym relations from text. <eos> instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. <eos> the assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. <eos> afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a support vector machine classifier. <eos> the results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.
word sense disambiguation ( wsd ) is one of the toughest problems in nlp, and in wsd, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. <eos> unsupervised wsd has received widespread attention, but has performed poorly, specially on verbs. <eos> recently an unsupervised bilingual em based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora ( marathi and hindi ). <eos> but the performance of this approach is poor on verbs with accuracy level at 25-38 %. <eos> we suggest a modification to this mentioned formulation, using context and semantic relatedness of neighboring words. <eos> an improvement of 17 % 35 % in the accuracy of verb wsd is obtained compared to the existing em based approach. <eos> on a general note, the work can be looked upon as contributing to the framework of unsupervised wsd through context aware expectation maximization.
quality estimation models provide feedback on the quality of machine translated texts. <eos> they are usually trained on humanannotated datasets, which are very costly due to its task-specific nature. <eos> we investigate active learning techniques to reduce the size of these datasets and thus annotation effort. <eos> experiments on a number of datasets show that with as little as 25 % of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. <eos> in other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors.
optical character recognition ( ocr ) systems for arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize fluency. <eos> in this paper we incorporate linguistically and semantically motivated features to an existing ocr system. <eos> to do so we follow an n-best list reranking approach that exploits recent advances in learning to rank techniques. <eos> we achieve 10.1 % and 11.4 % reduction in recognition word error rate ( wer ) relative to a standard baseline system on typewritten and handwritten arabic respectively.
timeline summarization aims at generating concise summaries and giving readers a faster and better access to understand the evolution of news. <eos> it is a new challenge which combines salience ranking problem with novelty detection. <eos> previous researches in this field seldom explore the evolutionary pattern of topics such as birth, splitting, merging, developing and death. <eos> in this paper, we develop a novel model called evolutionary hierarchical dirichlet process ( ehdp ) to capture the topic evolution pattern in timeline summarization. <eos> in ehdp, time varying information is formulated as a series of hdps by considering time-dependent information. <eos> experiments on 6 different datasets which contain 3156 documents demonstrates the good performance of our system with regard to rouge scores.
we present an ilp model of concept-totext generation. <eos> unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.
the growth of the web 2.0 technologies has led to an explosion of social networking media sites. <eos> among them, twitter is the most popular service by far due to its ease for realtime sharing of information. <eos> it collects millions of tweets per day and monitors what people are talking about in the trending topics updated timely. <eos> then the question is how users can understand a topic in a short time when they are frustrated with the overwhelming and unorganized tweets. <eos> in this paper, this problem is approached by sequential summarization which aims to produce a sequential summary, i.e., a series of chronologically ordered short subsummaries that collectively provide a full story about topic development. <eos> both the number and the content of sub-summaries are automatically identified by the proposed stream-based and semantic-based approaches. <eos> these approaches are evaluated in terms of sequence coverage, sequence novelty and sequence correlation and the effectiveness of their combination is demonstrated.
in this paper, we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user. <eos> we present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents, and then selecting relevant sentences from those documents. <eos> we discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents ( surveys and tutorials ) on seven topics in natural language processing. <eos> we have manually annotated 2,625 sentences with these factoids ( around 375 sentences per topic ) to build an evaluation corpus for this task. <eos> we present evaluation results for the performance of our system using this annotated data.
stanford dependencies ( sd ) provide a functional characterization of the grammatical relations in syntactic parse-trees. <eos> the sd representation is useful for parser evaluation, for downstream applications, and, ultimately, for natural language understanding, however, the design of sd focuses on structurally-marked relations and under-represents morphosyntactic realization patterns observed in morphologically rich languages ( mrls ). <eos> we present a novel extension of sd, called unified-sd ( u-sd ), which unifies the annotation of structurally- and morphologically-marked relations via an inheritance hierarchy. <eos> we create a new resource composed of u-sdannotated constituency and dependency treebanks for the mrl modern hebrew, and present two systems that can automatically predict u-sd annotations, for gold segmented input as well as raw texts, with high baseline accuracy.
in this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. <eos> this is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. <eos> to demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. <eos> our approach achieves significant improvement on all the three pairs of data sets.
this paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. <eos> experiments show that the transformed tsinghua chinese treebank as additional training data brings significant improvement over the baseline trained on penn chinese treebank only.
in the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars ( tag ). <eos> we present a bayesian nonparametric model for estimating a probabilistic tag from a parsed corpus, along with novel block sampling methods and approximation transformations for tag that allow efficient parsing. <eos> our work shows performance improvements on the penn treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.
we show that informative lexical categories from a strongly lexicalised formalism such as combinatory categorial grammar ( ccg ) can improve dependency parsing of hindi, a free word order language. <eos> we first describe a novel way to obtain a ccg lexicon and treebank from an existing dependency treebank, using a ccg parser. <eos> we use the output of a supertagger trained on the ccgbank as a feature for a state-of-the-art hindi dependency parser ( malt ). <eos> our results show that using ccg categories improves the accuracy of malt on long distance dependencies, for which it is known to have weak rates of recovery.
higher-order dependency features are known to improve dependency parser accuracy. <eos> we investigate the incorporation of such features into a cube decoding phrase-structure parser. <eos> we find considerable gains in accuracy on the range of standard metrics. <eos> what is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests ( brown vs. wsj ). <eos> this suggests that higher-order dependency features are not simply overfitting the training material.
we present fast, accurate, direct nonprojective dependency parsers with thirdorder features. <eos> our approach uses ad3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. <eos> experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets ( english, czech, and german ).
for the cascaded task of chinese word segmentation, pos tagging and parsing, the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space. <eos> in this paper, we present a novel lattice-based framework in which a chinese sentence is first segmented into a word lattice, and then a lattice-based pos tagger and a lattice-based parser are used to process the lattice from two different viewpoints : sequential pos tagging and hierarchical tree building. <eos> a strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. <eos> experimental results on chinese treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks.
beam search incremental parsers are accurate, but not as fast as they could be. <eos> we demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in o ( n2 ), rather than linear time, because each statetransition is actually implemented as an o ( n ) operation. <eos> we present an improved implementation, based on tree structured stack ( tss ), in which a transition is performed in o ( 1 ), resulting in a real lineartime algorithm, which is verified empirically. <eos> we further improve parsing speed by sharing feature-extraction and dotproduct across beam items. <eos> practically, our methods combined offer a speedup of ? 2x over strong baselines on penn treebank sentences, and are orders of magnitude faster on much longer sentences.
supervised nlp tools and on-line services are often used on data that is very different from the manually annotated data used during development. <eos> the performance loss observed in such cross-domain applications is often attributed to covariate shifts, with out-of-vocabulary effects as an important subclass. <eos> many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features. <eos> regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts. <eos> we present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilingual cross-domain part-of-speech tagging datasets. <eos> while previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4 % error reduction.
automatically determining the temporal order of events and times in a text is difficult, though humans can readily perform this task. <eos> sometimes events and times are related through use of an explicit co-ordination which gives information about the temporal relation : expressions like ? before ? <eos> and ? as soon as ?. <eos> we investigate the ro ? le that these co-ordinating temporal signals have in determining the type of temporal relations in discourse. <eos> using machine learning, we improve upon prior approaches to the problem, achieving over 80 % accuracy at labelling the types of temporal relation between events and times that are related by temporal signals.
a new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. <eos> inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. <eos> the method is evaluated on excerpts of the fisher and ami corpora, using a crowdsourcing platform to elicit comparative relevance judgments. <eos> the results demonstrate that the method outperforms two competitive baselines.
tabular information in text documents contains a wealth of information, and so tables are a natural candidate for information extraction. <eos> there are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table. <eos> we study how natural-language tools, such as part-of-speech tagging, dependency paths, and named-entity recognition, can be used to improve the quality of relation extraction from tables. <eos> in three domains we show that ( 1 ) a model that performs joint probabilistic inference across tabular and natural language features achieves an f1 score that is twice as high as either a puretable or pure-text system, and ( 2 ) using only shallower features or non-joint inference results in lower quality.
distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. <eos> however, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. <eos> to tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. <eos> we adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. <eos> our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7 % to 61.2 % with a consistently high level of precision of around 93 % in the experiments.
appositions are adjacent nps used to add information to a discourse. <eos> we propose systems exploiting syntactic and semantic constraints to extract appositions from ontonotes. <eos> our joint log-linear model outperforms the state-of-the-art favre and hakkani-tu ? r ( 2009 ) model by ? 10 % on broadcast news, and achieves 54.3 % fscore on multiple genres.
data selection is an effective approach to domain adaptation in statistical machine translation. <eos> the idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. <eos> substantial gains have been demonstrated in previous works, which employ standard ngram language models. <eos> here, we explore the use of neural language models for data selection. <eos> we hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. <eos> in a comprehensive evaluation of 4 language pairs ( english to german, french, russian, spanish ), we found that neural language models are indeed viable tools for data selection : while the improvements are varied ( i.e. <eos> 0.1 to 1.7 gains in bleu ), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.
analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language. <eos> in this study, we revisit this learning paradigm and apply it to the transliteration task. <eos> we show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy.
we present an efficient algorithm to estimate large modified kneser-ney models including interpolation. <eos> streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of ram and variable amount of disk. <eos> using one machine with 140 gb ram for 2.8 days, we built an unpruned model on 126 billion tokens. <eos> machine translation experiments with this model show improvement of 0.8 bleu point over constrained systems for the 2013 workshop on machine translation task in three language pairs. <eos> our algorithm is also faster for small models : we estimated a model on 302 million tokens using 7.7 % of the ram and 14.0 % of the wall time taken by srilm. <eos> the code is open source as part of kenlm.
we describe a translation model adaptation approach for conversational spoken language translation ( cslt ), which encourages the use of contextually appropriate translation options from relevant training conversations. <eos> our approach employs a monolingual lda topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. <eos> a significant novelty of our adaptation technique is its incremental nature ; we continuously update the topic distribution on the evolving test conversation as new utterances become available. <eos> thus, our approach is well-suited to the causal constraint of spoken conversations. <eos> on an english-to-iraqi cslt task, the proposed approach gives significant improvements over a baseline system as measured by bleu, ter, and nist. <eos> interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.
fast alignment is essential for many natural language tasks. <eos> but in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. <eos> we describe a discriminatively trained monolingual word aligner that uses a conditional random field to globally decode the best alignment with features drawn from source and target sentences. <eos> using just part-of-speech tags and wordnet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.
we propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by esl learners. <eos> the candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. <eos> furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. <eos> experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.
measuring semantic textual similarity ( sts ) is at the cornerstone of many nlp applications. <eos> different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following : ( i ) it directly encodes input texts into relational syntactic structures ; ( ii ) relies on tree kernels to handle feature engineering automatically ; ( iii ) combines both structural and feature vector representations in a single scoring model, i.e., in support vector regression ( svr ) ; and ( iv ) delivers significant improvement over the best sts systems.
we present results from our study of which uses syntactically and semantically motivated information to group segments of sentences into unbreakable units for the purpose of typesetting those sentences in a region of a fixed width, using an otherwise standard dynamic programming line breaking algorithm, to minimize raggedness. <eos> in addition to a rule-based baseline segmenter, we use a very modest size text, manually annotated with positions of breaks, to train a maximum entropy classifier, relying on an extensive set of lexical and syntactic features, which can then predict whether or not to break after a certain word position in a sentence. <eos> we also use a simple genetic algorithm to search for a subset of the features optimizing f1, to arrive at a set of features that delivers 89.2 % precision, 90.2 % recall ( 89.7 % f1 ) on a test set, improving the rule-based baseline by about 11 points and the classifier trained on all features by about 1 point in f1.
we present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in english, danish and spanish. <eos> this article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods : majority voting with a theory-compliant backoff strategy, and mace, an unsupervised system to choose the most likely sense from all the annotations.
syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces ( baroni and lenci, 2010 ). <eos> however, they are also sparse, with resulting reliability and coverage problems. <eos> we address this problem by derivational smoothing, which uses knowledge about derivationally related words ( oldish ? <eos> old ) to improve semantic similarity estimates. <eos> we develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in german. <eos> even for models built from very large corpora, simple derivational smoothing can improve coverage considerably.
although diathesis alternations have been used as features for manual verb classification, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process. <eos> this paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. <eos> our alternation-based approach is particularly adept at leveraging information from less frequent data.
we present the first attempt to perform full framenet annotation with crowdsourcing techniques. <eos> we compare two approaches : the first one is the standard annotation methodology of lexical units and frame elements in two steps, while the second is a novel approach aimed at acquiring frames in a bottom-up fashion, starting from frame element annotation. <eos> we show that our methodology, relying on a single annotation step and on simplified role definitions, outperforms the standard one both in terms of accuracy and time.
the evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. <eos> however, there is no widely-used metric to evaluate wholesentence semantic structures. <eos> in this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. <eos> we give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.
we introduce a scheme for optimally allocating a variable number of bits per lsh hyperplane. <eos> previous approaches assign a constant number of bits per hyperplane. <eos> this neglects the fact that a subset of hyperplanes may be more informative than others. <eos> our method, dubbed variable bit quantisation ( vbq ), provides a datadriven non-uniform bit allocation across hyperplanes. <eos> despite only using a fraction of the available hyperplanes, vbq outperforms uniform quantisation by up to 168 % for retrieval across standard text and image datasets.
this paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. <eos> we focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a word sense disambiguation process that aims at improving the adequacy of context vectors. <eos> on two specialized frenchenglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.
lexical resources such as wordnet and verbnet are widely used in a multitude of nlp tasks, as are annotated corpora such as treebanks. <eos> often, the resources are used as-is, without question or examination. <eos> this practice risks missing significant performance gains and even entire techniques. <eos> this paper addresses the importance of resource quality through the lens of a challenging nlp task : detecting selectional preference violations. <eos> we present david, a simple, lexical resource-based preference violation detector. <eos> with asis lexical resources, david achieves an f1-measure of just 28.27 %. <eos> when the resource entries and parser outputs for a small sample are corrected, however, the f1-measure on that sample jumps from 40 % to 61.54 %, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. <eos> more broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements.
the use of automatic word alignment to capture sentence-level semantic relations is common to a number of cross-lingual nlp applications. <eos> despite its proved usefulness, however, word alignment information is typically considered from a quantitative point of view ( e.g. <eos> the number of alignments ), disregarding qualitative aspects ( the importance of aligned terms ). <eos> in this paper we demonstrate that integrating qualitative information can bring significant performance improvements with negligible impact on system complexity. <eos> focusing on the cross-lingual textual entailment task, we contribute with a novel method that : i ) significantly outperforms the state of the art, and ii ) is portable, with limited loss in performance, to language pairs where training data are not available.
we present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. <eos> the monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. <eos> to evaluate our method, we use the word clusters in an ner system and demonstrate a statistically significant improvement in f1 score when using bilingual word clusters instead of monolingual clusters.
we report on the first structured distributional semantic model for croatian, dm.hr. <eos> it is constructed after the model of the english distributional memory ( baroni and lenci, 2010 ), from a dependencyparsed croatian web corpus, and covers about 2m lemmas. <eos> we give details on the linguistic processing and the design principles. <eos> an evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. <eos> the resource is freely available.
the ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision. <eos> however, the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text. <eos> we address this challenge with contributions in two folds : first, we introduce the new task of image caption generalization, formulated as visually-guided sentence compression, and present an efficient algorithm based on dynamic beam search with dependency-based constraints. <eos> second, we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text. <eos> evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer.
identifying news stories that discuss the same real-world events is important for news tracking and retrieval. <eos> most existing approaches rely on the traditional vector space model. <eos> we propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. <eos> we structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. <eos> our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events.
while the resolution of term ambiguity is important for information extraction ( ie ) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. <eos> to combat this, this work looks at ambiguity detection at the term, rather than the instance, level. <eos> by making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. <eos> to address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. <eos> results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline f-measure of 0.96.
distant supervision ( ds ) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. <eos> however, the accuracy is still not satisfying. <eos> in this paper, we point out and analyze some critical factors in ds which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. <eos> we propose an approach to handle these factors. <eos> by experimenting on wikipedia articles to extract the facts in freebase ( the top 92 relations ), we show the impact of these three factors on the accuracy of ds and the remarkable improvement led by the proposed approach.
determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem. <eos> we seek to improve anand et al ? s ( 2011 ) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. <eos> experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification.
school of thought analysis is an important yet not-well-elaborated scientific knowledge discovery task. <eos> this paper makes the first attempt at this problem. <eos> we focus on one aspect of the problem : do characteristic school-of-thought words exist and whether they are characterizable ? <eos> to answer these questions, we propose a probabilistic generative school-of-thought ( sot ) model to simulate the scientific authoring process based on several assumptions. <eos> sot defines a school of thought as a distribution of topics and assumes that authors determine the school of thought for each sentence before choosing words to deliver scientific ideas. <eos> sot distinguishes between two types of school-ofthought words for either the general background of a school of thought or the original ideas each paper contributes to its school of thought. <eos> narrative and quantitative experiments show positive and promising results to the questions raised above.
in this paper, we use arabic natural language processing techniques to analyze arabic debates. <eos> the goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. <eos> the members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. <eos> we use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. <eos> we opinion predictions to represent the discussion in one of two formal representations : signed attitude network or a space of attitude vectors. <eos> we identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. <eos> we evaluate the system using a data set of labeled discussions and show that it achieves good results.
we present a system for extracting the dates of illness events ( year and month of the event occurrence ) from posting histories in the context of an online medical support community. <eos> a temporal tagger retrieves and normalizes dates mentioned informally in social media to actual month and year referents. <eos> building on this, an event date extraction system learns to integrate the likelihood of candidate dates extracted from time-rich sentences with temporal constraints extracted from eventrelated sentences. <eos> our integrated model achieves 89.7 % of the maximum performance given the performance of the temporal expression retrieval step.
in this paper, we address the problem for predicting cqa answer quality as a classification task. <eos> we propose a multimodal deep belief nets based approach that operates in two stages : first, the joint representation is learned by taking both textual and non-textual features into a deep learning network. <eos> then, the joint representation learned by the network is used as input features for a linear classifier. <eos> extensive experimental results conducted on two cqa datasets demonstrate the effectiveness of our proposed approach.
opinion mining is often regarded as a classification or segmentation task, involving the prediction of i ) subjective expressions, ii ) their target and iii ) their polarity. <eos> intuitively, these three variables are bidirectionally interdependent, but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that can not model the bidirectional interaction between these variables. <eos> towards better understanding the interaction between these variables, we propose a model that allows for analyzing the relation of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. <eos> we report results on two public datasets ( cameras and cars ), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of twitter posts.
sentiment word identification ( swi ) is a basic technique in many sentiment analysis applications. <eos> most existing researches exploit seed words, and lead to low robustness. <eos> in this paper, we propose a novel optimization-based model for swi. <eos> unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. <eos> several experiments on real datasets show that weed is effective and outperforms the state-of-the-art methods with seed words.
syntactic features are useful for many text classification tasks. <eos> among these, tree kernels ( collins and duffy, 2001 ) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use for a given task. <eos> we compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. <eos> since explicit features are easy to generate and use ( with publicly available tools ), we suggest they should always be included as baseline comparisons in tree kernel method evaluations.
computational models of infant word segmentation have not been tested on a wide range of languages. <eos> this paper applies a phonotactic segmentation model to korean. <eos> in contrast to the undersegmentation pattern previously found in english and russian, the model exhibited more oversegmentation errors and more errors overall. <eos> despite the high error rate, analysis suggested that lexical acquisition might not be problematic, provided that infants attend only to frequently segmented items.
we investigated the effect of word surprisal on the eeg signal during sentence reading. <eos> on each word of 205 experimental sentences, surprisal was estimated by three types of language model : markov models, probabilistic phrasestructure grammars, and recurrent neural networks. <eos> four event-related potential components were extracted from the eeg of 24 readers of the same sentences. <eos> surprisal estimates under each model type formed a significant predictor of the amplitude of the n400 component only, with more surprising words resulting in more negative n400s. <eos> this effect was mostly due to content words. <eos> these findings provide support for surprisal as a generally applicable measure of processing difficulty during language comprehension.
we present a system for automated phonetic clustering analysis of cognitive tests of phonemic verbal fluency, on which one must name words starting with a specific letter ( e.g., ? f ? ) <eos> for one minute. <eos> test responses are typically subjected to manual phonetic clustering analysis that is labor-intensive and subject to inter-rater variability. <eos> our system provides an automated alternative. <eos> in a pilot study, we applied this system to tests of 55 novice and experienced professional fighters ( boxers and mixed martial artists ) and found that experienced fighters produced significantly longer chains of phonetically similar words, while no differences were found in the total number of words produced. <eos> these findings are preliminary, but strongly suggest that our system can be used to detect subtle signs of brain damage due to repetitive head trauma in individuals that are otherwise unimpaired.
we have elicited human quantitative judgments of semantic relatedness for 122 pairs of nouns and compiled them into a new set of relatedness norms that we call rel-122. <eos> judgments from individual subjects in our study exhibit high average correlation to the resulting relatedness means ( r = 0.77, ? <eos> = 0.09, n = 73 ), although not as high as resnik ? s ( 1995 ) upper bound for expected average human correlation to similarity means ( r = 0.90 ). <eos> this suggests that human perceptions of relatedness are less strictly constrained than perceptions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness. <eos> we compare the results of several wordnet-based similarity and relatedness measures to our rel-122 norms and demonstrate the limitations of wordnet for discovering general indications of semantic relatedness. <eos> we also offer a critique of the field ? s reliance upon similarity norms to evaluate relatedness measures.
morphologically rich languages such as turkish may benefit from morphological analysis in natural language tasks. <eos> in this study, we examine the effects of morphological analysis on text categorization task in turkish. <eos> we use stems and word categories that are extracted with morphological analysis as main features and compare them with fixed length stemmers in a bag of words approach with several learning algorithms. <eos> we aim to show the effects of using varying degrees of morphological information.
we present a way to extract links from messages published on microblogging platforms and we classify them according to the language and possible relevance of their target in order to build a text corpus. <eos> three platforms are taken into consideration : friendfeed, identi.ca and reddit, as they account for a relative diversity of user profiles and more importantly user languages. <eos> in order to explore them, we introduce a traversal algorithm based on user pages. <eos> as we target lesser-known languages, we try to focus on non-english posts by filtering out english text. <eos> using mature open-source software from the nlp research field, a spell checker ( aspell ) and a language identification system ( langid.py ), our case study and our benchmarks give an insight into the linguistic structure of the considered services.
though there has been substantial research concerning the extraction of information from clinical notes, to date there has been less work concerning the extraction of useful infor-mation from patient-generated content. <eos> using a dataset comprised of online support group discussion content, this paper investigates two dimensions that may be important in the ex-traction of patient-generated experiences from text ; significant individuals/groups and medi-cation use. <eos> with regard to the former, the pa-per describes an approach involving the pair-ing of important figures ( e.g. <eos> family, hus-bands, doctors, etc. ) <eos> and affect, and suggests possible applications of such techniques to re-search concerning online social support, as well as integration into search interfaces for patients. <eos> additionally, the paper demonstrates the extraction of side effects and sentiment at different phases in patient medication use, e.g. <eos> adoption, current use, discontinuation and switching, and demonstrates the utility of such an application for drug safety monitoring in online discussion forums.
as one of the most challenging issues in nlp, metaphor identification and its interpretation have seen many models and methods proposed. <eos> this paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context.
in this paper we focus on practical issues of data representation for dependency parsing. <eos> we carry out an experimental comparison of ( a ) three syntactic dependency schemes ; ( b ) three data-driven dependency parsers ; and ( c ) the influence of two different approaches to lexical category disambiguation ( aka tagging ) prior to parsing. <eos> comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser.
current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times. <eos> automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models. <eos> we here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning. <eos> we create several baselines and experiment with various parameter settings for three algorithms, i.e., conditional random fields ( crf ), support vector machines ( svm ) and random forests ( rf ). <eos> also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at errors. <eos> the best performance of 79.35 % f-score is achieved by crfs when using all three feature types.
in this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification ? <eos> for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. <eos> in this case, tf.idf is often used to decide important words. <eos> on the other hand, we apply the pagerank algorithm to rank important words in each document. <eos> furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. <eos> we then classify the documents based on latent information in the documents. <eos> as a clustering method, we employ the k-means algorithm and investigate how our proposed method works for good clustering. <eos> we conduct experiments with reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.
this study addresses issues of japanese language learning concerning word combinations ( collocations ). <eos> japanese learners may be able to construct grammatically correct sentences, however, these may sound ? unnatural ?. <eos> in this work, we analyze correct word combinations using different collocation measures and word similarity methods. <eos> while other methods use well-formed text, our approach makes use of a large japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. <eos> our results show that we get better results compared to other methods that use only wellformed text.
natural language can be easily understood by everyone irrespective of their differences in age or region or qualification. <eos> the existence of a conceptual base that underlies all natural languages is an accepted claim as pointed out by schank in his conceptual dependency ( cd ) theory. <eos> inspired by the cd theory and theories in indian grammatical tradition, we propose a new set of meaning primitives in this paper. <eos> we claim that this new set of primitives captures the meaning inherent in verbs and help in forming an inter-lingual and computable ontological classification of verbs. <eos> we have identified seven primitive overlapping verb senses which substantiate our claim. <eos> the percentage of coverage of these primitives is 100 % for all verbs in sanskrit and hindi and 3750 verbs in english.
electronic health records ( ehrs ) contain important clinical information about patients. <eos> some of these data are in the form of free text and require preprocessing to be able to used in automated systems. <eos> efficient and effective use of this data could be vital to the speed and quality of health care. <eos> as a case study, we analyzed classification of ct imaging reports into binary categories. <eos> in addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. <eos> topic modeling of the corpora provides interpretable themes that exist in these reports. <eos> representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. <eos> a binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. <eos> and, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. <eos> our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.
for expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. <eos> in order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. <eos> the task of the annotator is to select the correct version among these two alternatives. <eos> to minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.
we present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. <eos> the model outperforms most systems participating in the english track of the conll ? 12 shared task.
this paper presents work in progress towards automatic recognition and classification of comparisons and similes. <eos> among possible applications, we discuss the place of this task in text simplification for readers with autism spectrum disorders ( asd ), who are known to have deficits in comprehending figurative language. <eos> we propose an approach to comparison recognition through the use of syntactic patterns. <eos> keeping in mind the requirements of autistic readers, we discuss the properties relevant for distinguishing semantic criteria like figurativeness and abstractness.
this study is devoted to the problem of question analysis for a polish question answering system. <eos> the goal of the question analysis is to determine its general structure, type of an expected answer and create a search query for finding relevant documents in a textual knowledge base. <eos> the paper contains an overview of available solutions of these problems, description of their implementation and presents an evaluation based on a set of 1137 questions from a polish quiz tv show. <eos> the results help to understand how an environment of a slavonic language affects the performance of methods created for english.
identifying complex words ( cws ) is an important, yet often overlooked, task within lexical simplification ( the process of automatically replacing cws with simpler alternatives ). <eos> if too many words are identified then substitutions may be made erroneously, leading to a loss of meaning. <eos> if too few words are identified then those which impede a user ? s understanding may be missed, resulting in a complex final text. <eos> this paper addresses the task of evaluating different methods for cw identification. <eos> a corpus of sentences with annotated cws is mined from simple wikipedia edit histories, which is then used as the basis for several experiments. <eos> firstly, the corpus design is explained and the results of the validation experiments using human judges are reported. <eos> experiments are carried out into the cw identification techniques of : simplifying everything, frequency thresholding and training a support vector machine. <eos> these are based upon previous approaches to the task and show that thresholding does not perform significantly differently to the more na ? <eos> ? ve technique of simplifying everything. <eos> the support vector machine achieves a slight increase in precision over the other two methods, but at the cost of a dramatic trade off in recall.
there are some chronic critics who always complain about the entity in social media. <eos> we are working to automatically detect these chronic critics to prevent the spread of bad rumors about the reputation of the entity. <eos> in social media, most comments are informal, and, there are sarcastic and incomplete contexts. <eos> this means that it is difficult for current nlp technology such as opinion mining to recognize the complaints. <eos> as an alternative approach for social media, we can assume that users who share the same opinions will link to each other. <eos> thus, we propose a method that combines opinion mining with graph analysis for the connections between users to identify the chronic critics. <eos> our experimental results show that the proposed method outperforms analysis based only on opinion mining techniques.
we study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting. <eos> part-of-speech tagging is a crucial preliminary process in many natural language processing applications. <eos> because many words in natural languages have more than one part-of-speech tag, resolving part-of-speech ambiguity is an important task. <eos> we claim that partof-speech ambiguity can be solved using substitute vectors. <eos> a substitute vector is constructed with possible substitutes of a target word. <eos> this study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction. <eos> experiments show that our methodology works for words with high ambiguity.
in this work we present psycholinguistically motivated computational models for the organization and processing of bangla morphologically complex words in the mental lexicon. <eos> our goal is to identify whether morphologically complex words are stored as a whole or are they organized along the morphological line. <eos> for this, we have conducted a series of psycholinguistic experiments to build up hypothesis on the possible organizational structure of the mental lexicon. <eos> next, we develop computational models based on the collected dataset. <eos> we observed that derivationally suffixed bangla words are in general decomposed during processing and compositionality between the stem and the suffix plays an important role in the decomposition process. <eos> we observed the same phenomena for bangla verb sequences where experiments showed noncompositional verb sequences are in general stored as a whole in the ml and low traces of compositional verbs are found in the mental lexicon.
machine translation ( mt ) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation. <eos> this comparison can be performed on multiple levels : lexical, syntactic or semantic. <eos> in this paper, we propose a new syntactic metric for mt evaluation based on the comparison of the dependency structures of the reference and the candidate translations. <eos> the dependency structures are obtained by means of a weighted constraints dependency grammar parser. <eos> based on experiments performed on english to german translations, we show that the new metric correlates well with human judgments at the system level.
in a multi-class document categorization using graph-based semi-supervised learning ( gbssl ), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. <eos> furthermore, it is also important to provide high-quality correct data as training data. <eos> in this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for gbssl by means of the pagerank algorithm. <eos> experimenting on reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.
we present experiments using a new unsupervised approach to automatic text simplification, which builds on sampling and ranking via a loss function informed by readability research. <eos> the main idea is that a loss function can distinguish good simplification candidates among randomly sampled sub-sentences of the input sentence. <eos> our approach is rated as equally grammatical and beginner reader appropriate as a supervised smt-based baseline system by native speakers, but our setup performs more radical changes that better resembles the variation observed in human generated simplifications.
in this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. <eos> we view language as a system and linguistic features as its components whose relationships are encoded in a directed acyclic graph ( dag ). <eos> taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. <eos> then probabilistic inference enables us to see the strength of such relationships : given the observed value of one feature ( or combination of features ), the probabilities of values of other features can be calculated. <eos> our model is not restricted to using only two values of a feature. <eos> using imputation technique and em algorithm it can handle missing values well. <eos> model averaging technique solves the problem of limited data. <eos> in addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in daum ? <eos> iii and campbell ( 2007 ).
we present a novel method of statistical morphological generation, i.e. <eos> the prediction of inflected word forms given lemma, part-of-speech and morphological features, aimed at robustness to unseen inputs. <eos> our system uses a trainable classifier to predict ? edit scripts ? <eos> that are then used to transform lemmas into inflected word forms. <eos> suffixes of lemmas are included as features to achieve robustness. <eos> we evaluate our system on 6 languages with a varying degree of morphological richness. <eos> the results show that the system is able to learn most morphological phenomena and generalize to unseen inputs, producing significantly better results than a dictionarybased baseline.
evaluation methods for distributional semantic models typically rely on behaviorally derived gold standards. <eos> these methods are difficult to deploy in languages with scarce linguistic/behavioral resources. <eos> we introduce a corpus-based measure that evaluates the stability of the lexical semantic similarity space using a pseudo-synonym same-different detection task and no external resources. <eos> we show that it enables to predict two behaviorbased measures across a range of parameters in a latent semantic analysis model.
deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. <eos> it attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. <eos> on the english-to-czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.
we present webanno, a general purpose web-based annotation tool for a wide range of linguistic annotations. <eos> webanno offers annotation project management, freely configurable tagsets and the management of users in different roles. <eos> webanno uses modern web technology for visualizing and editing annotations in a web browser. <eos> it supports arbitrarily large documents, pluggable import/export filters, the curation of annotations across various users, and an interface to farming out annotations to a crowdsourcing platform. <eos> currently webanno allows part-ofspeech, named entity, dependency parsing and co-reference chain annotations. <eos> the architecture design allows adding additional modes of visualization and editing, when new kinds of annotations are to be supported.
we implement a city-level geolocation prediction system for twitter users. <eos> the system infers a user ? s location based on both tweet text and user-declared metadata using a stacking approach. <eos> we demonstrate that the stacking method substantially outperforms benchmark methods, achieving 49 % accuracy on a benchmark dataset. <eos> we further evaluate our method on a recent crawl of twitter data to investigate the impact of temporal factors on model generalisation. <eos> our results suggest that user-declared location metadata is more sensitive to temporal change than the text of twitter messages. <eos> we also describe two ways of accessing/demoing our system.
given the increasing interest and development of computational and quantitative methods in historical linguistics, it is important that scholars have a basis for documenting, testing, evaluating, and sharing complex workflows. <eos> we present a novel open-source toolkit for quantitative tasks in historical linguistics that offers these features. <eos> this toolkit also serves as an interface between existing software packages and frequently used data formats, and it provides implementations of new and existing algorithms within a homogeneous framework. <eos> we illustrate the toolkit ? s functionality with an exemplary workflow that starts with raw language data and ends with automatically calculated phonetic alignments, cognates and borrowings. <eos> we then illustrate evaluation metrics on gold standard datasets that are provided with the toolkit.
this paper presents annomarket, an open cloud-based platform which enables researchers to deploy, share, and use language processing components and resources, following the data-as-a-service and software-as-a-service paradigms. <eos> the focus is on multilingual text analysis resources and services, based on an opensource infrastructure and compliant with relevant nlp standards. <eos> we demonstrate how the annomarket platform can be used to develop nlp applications with little or no programming, to index the results for enhanced browsing and search, and to evaluate performance. <eos> utilising annomarket is straightforward, since cloud infrastructural issues are dealt with by the platform, completely transparently to the user : load balancing, efficient data upload and storage, deployment on the virtual machines, security, and fault tolerance.
nowadays, the importance of social media is constantly growing, as people often use such platforms to share mainstream media news and comment on the events that they relate to. <eos> as such, people no loger remain mere spectators to the events that happen in the world, but become part of them, commenting on their developments and the entities involved, sharing their opinions and distributing related content. <eos> this paper describes a system that links the main events detected from clusters of newspaper articles to tweets related to them, detects complementary information sources from the links they contain and subsequently applies sentiment analysis to classify them into positive, negative and neutral. <eos> in this manner, readers can follow the main events happening in the world, both from the perspective of mainstream as well as social media and the public ? s perception on them. <eos> this system will be part of the emm media monitoring framework working live and it will be demonstrated using google earth.
we introduce dissect, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. <eos> the toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. <eos> furthermore, dissect can be useful to researchers and practitioners who need models of word meaning ( without composition ) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.
implementations of word sense disambiguation ( wsd ) algorithms tend to be tied to a particular test corpus format and sense inventory. <eos> this makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. <eos> in this paper we present dkpro wsd, a freely licensed, general-purpose framework for wsd which is both modular and extensible. <eos> dkpro wsd abstracts the wsd process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. <eos> its uima-based architecture makes it easy to add support for new resources and algorithms. <eos> related tasks such as word sense induction and entity linking are also supported.
u-compare is a uima-based workflow construction platform for building natural language processing ( nlp ) applications from heterogeneous language resources ( lrs ), without the need for programming skills. <eos> u-compare has been adopted within the context of the metanet network of excellence, and over 40 lrs that process 15 european languages have been added to the u-compare component library. <eos> in line with metanet ? s aims of increasing communication between citizens of different european countries, u-compare has been extended to facilitate the development of a wider range of applications, including both multilingual and multimodal workflows. <eos> the enhancements exploit the uima subject of analysis ( sofa ) mechanism, that allows different facets of the input data to be represented. <eos> we demonstrate how our customised extensions to u-compare allow the construction and testing of nlp applications that transform the input data in different ways, e.g., machine translation, automatic summarisation and text-to-speech.
the growing need for chinese natural language processing ( nlp ) is largely in a range of research and commercial applications. <eos> however, most of the currently chinese nlp tools or components still have a wide range of issues need to be further improved and developed. <eos> fudannlp is an open source toolkit for chinese natural language processing ( nlp ), which uses statistics-based and rule-based methods to deal with chinese nlp tasks, such as word segmentation, part-ofspeech tagging, named entity recognition, dependency parsing, time phrase recognition, anaphora resolution and so on.
we present icarus, a versatile graphical search tool to query dependency treebanks. <eos> search results can be inspected both quantitatively and qualitatively by means of frequency lists, tables, or dependency graphs. <eos> icarus also ships with plugins that enable it to interface with tool chains running either locally or remotely.
in this paper we describe a platform for embodied conversational agents with tutoring goals, which takes as input written and spoken questions and outputs answers in both forms. <eos> the platform is developed within a game environment, and currently allows speech recognition and synthesis in portuguese, english and spanish. <eos> in this paper we focus on its understanding component that supports in-domain interactions, and also small talk. <eos> most indomain interactions are answered using different similarity metrics, which compare the perceived utterances with questions/sentences in the agent ? s knowledge base ; small-talk capabilities are mainly due to aiml, a language largely used by the chatbots ? <eos> community. <eos> in this paper we also introduce edgar, the butler of monserrate, which was developed in the aforementioned platform, and that answers tourists ? <eos> questions about monserrate.
in this paper, we propose pal, a prototype chatterbot for answering non-obstructive psychological domain-specific questions. <eos> this system focuses on providing primary suggestions or helping people relieve pressure by extracting knowledge from online forums, based on which the chatterbot system is constructed. <eos> the strategies used by pal, including semantic-extension-based question matching, solution management with personal information consideration, and xml-based knowledge pattern construction, are described and discussed. <eos> we also conduct a primary test for the feasibility of our system.
this paper describes the online tool phonmatrix, which analyzes a word list with respect to the co-occurrence of sounds in a specified context within a word. <eos> the cooccurrence counts from the user-specified context are statistically analyzed according to a number of association measures that can be selected by the user. <eos> the statistical values then serve as the input for a matrix visualization where rows and columns represent the relevant sounds under investigation and the matrix cells indicate whether the respective ordered pair of sounds occurs more or less frequently than expected. <eos> the usefulness of the tool is demonstrated with three case studies that deal with vowel harmony and similar place avoidance patterns.
we describe quest, an open source framework for machine translation quality estimation. <eos> the framework allows the extraction of several quality indicators from source segments, their translations, external resources ( corpora, language models, topic models, etc. <eos> ), as well as language tools ( parsers, part-of-speech tags, etc. ). <eos> it also provides machine learning algorithms to build quality estimation models. <eos> we benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms.
the quality of automatic translation is affected by many factors. <eos> one is the divergence between the specific source and target languages. <eos> another lies in the source text itself, as some texts are more complex than others. <eos> one way to handle such texts is to modify them prior to translation. <eos> yet, an important factor that is often overlooked is the source translatability with respect to the specific translation system and the specific model that are being used. <eos> in this paper we present an interactive system where source modifications are induced by confidence estimates that are derived from the translation model in use. <eos> modifications are automatically generated and proposed for the user ? s approval. <eos> such a system can reduce postediting effort, replacing it by cost-effective pre-editing that can be done by monolinguals.
in this paper we describe travatar, a forest-to-string machine translation ( mt ) engine based on tree transducers. <eos> it provides an open-source c++ implementation for the entire forest-to-string mt pipeline, including rule extraction, tuning, decoding, and evaluation. <eos> there are a number of options for model training, and tuning includes advanced options such as hypergraph mert, and training of sparse features through online learning. <eos> the training pipeline is modeled after that of the popular moses decoder, so users familiar with moses should be able to get started quickly. <eos> we perform a validation experiment of the decoder on englishjapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. <eos> as auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. <eos> travatar is available under the lgpl at http : //phontron.com/travatar
this paper presents plis, an open source probabilistic lexical inference system which combines two functionalities : ( i ) a tool for integrating lexical inference knowledge from diverse resources, and ( ii ) a framework for scoring textual inferences based on the integrated knowledge. <eos> we provide plis with two probabilistic implementation of this framework. <eos> plis is available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications. <eos> plis is easily configurable, components can be extended or replaced with user generated ones to enable system customization and further research. <eos> plis includes an online interactive viewer, which is a powerful tool for investigating lexical inference processes.
in this paper we present a demonstration of a multilingual generalization of word-class lattices ( wcls ), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. <eos> lattices are learned from a dataset of automatically-annotated definitions from wikipedia. <eos> we release a java api for the programmatic use of multilingual wcls in three languages ( english, french and italian ), as well as a web application for definition and hypernym extraction from user-provided sentences.
developing sophisticated nlp pipelines composed of multiple processing tools and components available through different providers may pose a challenge in terms of their interoperability. <eos> the unstructured information management architecture ( uima ) is an industry standard whose aim is to ensure such interoperability by defining common data structures and interfaces. <eos> the architecture has been gaining attention from industry and academia alike, resulting in a large volume of uima-compliant processing components. <eos> in this paper, we demonstrate argo, a web-based workbench for the development and processing of nlp pipelines/workflows. <eos> the workbench is based upon uima, and thus has the potential of using many of the existing uima resources. <eos> we present features, and show examples, of facilitating the distributed development of components and the analysis of processing results. <eos> the latter includes annotation visualisers and editors, as well as serialisation to rdf format, which enables flexible querying in addition to data manipulation thanks to the semantic query language sparql. <eos> the distributed development feature allows users to seamlessly connect their tools to workflows running in argo, and thus take advantage of both the available library of components ( without the need of installing them locally ) and the analytical tools.
we present dkpro similarity, an open source framework for text similarity. <eos> our goal is to provide a comprehensive repository of text similarity measures which are implemented using standardized interfaces. <eos> dkpro similarity comprises a wide variety of measures ranging from ones based on simple n-grams and common subsequences to high-dimensional vector comparisons and structural, stylistic, and phonetic measures. <eos> in order to promote the reproducibility of experimental results and to provide reliable, permanent experimental conditions for future studies, dkpro similarity additionally comes with a set of full-featured experimental setups which can be run out-of-the-box and be used for future systems to built upon.
fluid construction grammar ( fcg ) is an open-source computational grammar formalism that is becoming increasingly popular for studying the history and evolution of language. <eos> this demonstration shows how fcg can be used to operationalise the cultural processes and cognitive mechanisms that underly language evolution and change.
recent research has shown progress in achieving high-quality, very fine-grained type classification in hierarchical taxonomies. <eos> within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. <eos> in order to achieve high-precision in type classification, current approaches are either limited to certain domains or require time consuming multistage computations. <eos> as a consequence, existing systems are incapable of performing ad-hoc type classification on arbitrary input texts. <eos> in this demo, we present a novel webbased tool that is able to perform domain independent entity type classification under real time conditions. <eos> thanks to its efficient implementation and compacted feature representation, the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations. <eos> our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. <eos> further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.
a web-scale linguistic search engine for words in context joanne boisson+, ting-hui kao*, jian-cheng wu*, tzu-his yen*, jason s. chang* +institute of information systems and applications *department of computer science national tsing hua university hsinchu, taiwan, r.o.c. <eos> 30013 { joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang } @ gmail.com abstract in this paper, we introduce a web-scale lin-guistics search engine, linggle, that retrieves lexical bundles in response to a given query. <eos> the query might contain keywords, wildcards, wild parts of speech ( pos ), synonyms, and ad-ditional regular expression ( re ) operators. <eos> in our approach, we incorporate inverted file in-dexing, pos information from bnc, and se-mantic indexing based on latent dirichlet al-location with google web 1t. <eos> the method in-volves parsing the query to transforming it in-to several keyword retrieval commands. <eos> word chunks are retrieved with counts, further filter-ing the chunks with the query as a re, and fi-nally displaying the results according to the counts, similarities, and topics. <eos> clusters of synonyms or conceptually related words are also provided. <eos> in addition, linggle provides example sentences from the new york times on demand. <eos> the current implementation of linggle is the most functionally comprehen-sive, and is in principle language and dataset independent. <eos> we plan to extend linggle to provide fast and convenient access to a wealth of linguistic information embodied in web scale datasets including google web 1t and google books ngram for many major lan-guages in the world.
pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. <eos> although such pivoted paraphrase collections have been successfully used to improve the performance of several different nlp applications, it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections. <eos> we present paraquery, a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection, analyze its utility for a particular domain, and compare it to other popular lexical similarity resources ? <eos> all within a single interface.
this paper describes a system for navigating large collections of information about cultural heritage which is applied to europeana, the european library. <eos> europeana contains over 20 million artefacts with meta-data in a wide range of european languages. <eos> the system currently provides access to europeana content with meta-data in english and spanish. <eos> the paper describes how natural language processing is used to enrich and organise this meta-data to assist navigation through europeana and shows how this information is used within the system.
the use of deep syntactic information such as typed dependencies has been shown to be very effective in information extraction. <eos> despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive nlp background. <eos> in this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating information extraction rules over dependency trees. <eos> we introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the propminer tool.
we present in this paper semilar, the semantic similarity toolkit. <eos> semilar implements a number of algorithms for assessing the semantic similarity between two texts. <eos> it is available as a java library and as a java standalone ap-plication offering gui-based access to the implemented semantic similarity methods. <eos> furthermore, it offers facilities for manual se-mantic similarity annotation by experts through its component semilat ( a semantic similarity annotation tool ).
the aim of the tag2blog system is to bring satellite tagged wild animals ? to life ? <eos> through narratives that place their movements in an ecological context. <eos> our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. <eos> we are working with one of the largest nature conservation charities in europe in this regard, focusing on a single species, the red kite. <eos> we describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.
large amount of parallel corpora is required for building statistical machine translation ( smt ) systems. <eos> we describe the transdoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. <eos> our system uses a map-reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks : ( a ) translation of constituent phrases of the sentence ; ( b ) validation of quality of the phrase translations ; and ( c ) composition of complete sentence translations from phrase translations. <eos> transdoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. <eos> we have evaluated the crowd ? s output using the meteor metric. <eos> for a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.
this work presents tsearch, a web-based application that provides mechanisms for doing complex searches over a collection of translation cases evaluated with a large set of diverse measures. <eos> tsearch uses the evaluation results obtained with the asiya toolkit for mt evaluation and it is connected to its on-line gui, which makes possible a graphical visualization and interactive access to the evaluation results. <eos> the search engine offers a flexible query language allowing to find translation examples matching a combination of numerical and structural features associated to the calculation of the quality metrics. <eos> its database design permits a fast response time for all queries supported on realistic-size test beds. <eos> in summary, tsearch, used with asiya, offers developers of mt systems and evaluation metrics a powerful tool for helping translation and error analysis.
vsem is an open library for visual semantics. <eos> starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf vsem functionalities. <eos> vsem is entirely written in matlab and its objectoriented design allows a large flexibility and reusability. <eos> the software is accompanied by a website with supporting documentation and examples.
we present an open-source framework for large-scale online structured learning. <eos> developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation ( smt ), our large-margin learner can be used with any decoder. <eos> integration with mapreduce using hadoop streaming allows efficient scaling with increasing size of training data. <eos> although designed with a focus on smt, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing.
we consider the construction of part-of-speech taggers for resource-poor languages. <eos> recently, manually constructed tag dictionaries from wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting. <eos> in this paper, we show that additional token constraints can be projected from a resourcerich source language to a resource-poor target language via word-aligned bitext. <eos> we present several models to this end ; in particular a partially observed conditional random field model, where coupled token and type constraints provide a partial signal for training. <eos> averaged across eight previously studied indo-european languages, our model achieves a 25 % relative error reduction over the prior state of the art. <eos> we further present successful results on seven additional languages from different families, empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages.
dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. <eos> for 95.899.8 % of dependency parses in various natural language treebanks, whenever an edge is crossed, the edges that cross it all have a common vertex. <eos> the optimal dependency tree that satisfies this 1-endpoint-crossing property can be found with an o ( n4 ) parsing algorithm that recursively combines forests over intervals with one exterior point. <eos> 1-endpointcrossing trees also have natural connections to linguistics and another class of graphs that has been studied in nlp.
recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. <eos> in this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. <eos> we present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. <eos> experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions.
graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. <eos> in this paper, we proposed an exact and efficient decoding algorithm based on the branch and bound ( b & b ) framework where nonlocal features are bounded by a linear combination of local features. <eos> dynamic programming is used to search the upper bound. <eos> experiments are conducted on english ptb and chinese ctb datasets. <eos> we achieved competitive unlabeled attachment score ( uas ) when no additional resources are available : 93.17 % for english and 87.25 % for chinese. <eos> parsing speed is 177 words per second for english and 97 words per second for chinese. <eos> our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models.
the context in which language is used provides a strong signal for learning to recover its meaning. <eos> in this paper, we show it can be used within a grounded ccg semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. <eos> the joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. <eos> it also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. <eos> experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60 % more instruction sets relative to the previous state of the art.
unsupervised parsing is a difficult task that infants readily perform. <eos> progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues. <eos> this paper explores the hypothesis that word duration can help with learning syntax. <eos> we describe how duration information can be incorporated into an unsupervised bayesian dependency parser whose only other source of information is the words themselves ( without punctuation or parts of speech ). <eos> our results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. <eos> these results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in nlp tasks with speech.
we introduce a novel nonparametric bayesian model for the induction of combinatory categorial grammars from pos-tagged text. <eos> it achieves state of the art performance on a number of languages, and induces linguistically plausible lexicons.
supervised learning methods and lda based topic model have been successfully applied in the field of multi-document summarization. <eos> in this paper, we propose a novel supervised approach that can incorporate rich sentence features into bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. <eos> experimental results on duc2007, tac2008 and tac2009 demonstrate the effectiveness of our approach.
we demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. <eos> profile enrichment alone yields up to 15 % improvement in the accuracy of the seed lexicon on 3way sentence-level sentiment polarity classification of essay data. <eos> using lexical expansion in addition to sentiment profiles provides a further 7 % improvement in performance. <eos> additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application ( product reviews ).
in this paper, we present the first incremental parser for tree substitution grammar ( tsg ). <eos> a tsg allows arbitrarily large syntactic fragments to be combined into complete trees ; we show how constraints ( including lexicalization ) can be imposed on the shape of the tsg fragments to enable incremental processing. <eos> we propose an efficient earley-based algorithm for incremental tsg parsing and report an f-score competitive with other incremental parsers. <eos> in addition to whole-sentence f-score, we also evaluate the partial trees that the parser constructs for sentence prefixes ; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. <eos> unlike existing parsers, our incremental tsg parser can generate partial trees that include predictions about the upcoming words in a sentence. <eos> we show that it outperforms an n-gram model in predicting more than one upcoming word.
during the course of first language acquisition, children produce linguistic forms that do not conform to adult grammar. <eos> in this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence. <eos> our corpus consists of child sentences with corrected adult forms. <eos> we bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances. <eos> our method outperforms mt and esl baselines, reducing child error by 20 %. <eos> our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition.
this paper proposes a discriminative forest reranking algorithm for dependency parsing that can be seen as a form of efficient stacked parsing. <eos> a dynamic programming shift-reduce parser produces a packed derivation forest which is then scored by a discriminative reranker, using the 1-best tree output by the shift-reduce parser as guide features in addition to third-order graph-based features. <eos> to improve efficiency and accuracy, this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arcstandard transition systems. <eos> testing on the english penn treebank data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12.
in this paper, we present dijkstra-wsa, a novel graph-based algorithm for word sense alignment. <eos> we evaluate it on four different pairs of lexical-semantic resources with different characteristics ( wordnet-omegawiki, wordnet-wiktionary, germanet-wiktionary and wordnet-wikipedia ) and show that it achieves competitive performance on 3 out of 4 datasets. <eos> dijkstra-wsa outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity. <eos> we also demonstrate that dijkstra-wsa is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall.
machine translation ( mt ) draws from several different disciplines, making it a complex subject to teach. <eos> there are excellent pedagogical texts, but problems in mt and current algorithms for solving them are best learned by doing. <eos> as a centerpiece of our mt course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key mt tasks : alignment, decoding, evaluation, and reranking. <eos> students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. <eos> a surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-ofthe-art on hard nlp problems while simultaneously learning a great deal. <eos> the problems, baseline code, and results are freely available.
we introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. <eos> distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. <eos> we follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicateargument structure. <eos> our clustering algorithm is highly scalable, allowing us to run on corpora the size of gigaword. <eos> different senses of a word are disambiguated based on their induced types. <eos> we outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the fracas suite.
this paper introduces logical semantics with perception ( lsp ), a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment. <eos> for example, given an image, lsp can map the statement ? blue mug on the table ? <eos> to the set of image segments showing blue mugs on tables. <eos> lsp learns physical representations for both categorical ( ? blue, ? <eos> ? mug ? ) <eos> and relational ( ? on ? ) <eos> language, and also learns to compose these representations to produce the referents of entire statements. <eos> we further introduce a weakly supervised training procedure that estimates lsp ? s parameters using annotated referents for entire statements, without annotated referents for individual words or the parse structure of the statement. <eos> we perform experiments on two applications : scene understanding and geographical question answering. <eos> we find that lsp outperforms existing, less expressive models that can not represent relational language. <eos> we further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort.
due to the nature of complex nlp problems, structured prediction algorithms have been important modeling tools for a wide range of tasks. <eos> while there exists evidence showing that linear structural support vector machine ( ssvm ) algorithm performs better than structured perceptron, the ssvm algorithm is still less frequently chosen in the nlp community because of its relatively slow training speed. <eos> in this paper, we propose a fast and easy-toimplement dual coordinate descent algorithm for ssvms. <eos> unlike algorithms such as perceptron and stochastic gradient descent, our method keeps track of dual variables and updates the weight vector more aggressively. <eos> as a result, this training process is as efficient as existing online learning methods, and yet derives consistently better models, as evaluated on four benchmark nlp datasets for part-ofspeech tagging, named-entity recognition and dependency parsing.
in this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing. <eos> the semantic role labeler predicts the full syntactic paths that connect predicates with their arguments. <eos> this process is framed as a linear assignment task, which allows to control some well-formedness constraints. <eos> for the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. <eos> finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. <eos> in experiments on the conll-2009 english benchmark we observe very competitive results.
this paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments. <eos> we define an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions. <eos> given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as a way to support the relation prediction. <eos> the annotated data, however, only provides labels for the relation label, and not the arguments and types. <eos> we address this by presenting two models for preposition relation labeling. <eos> our generalization of latent structure svm gives close to 90 % accuracy on relation labeling. <eos> further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we can not only improve the relation accuracy, but also significantly improve sense prediction accuracy.
in current research, most tree-based translation models are built directly from parse trees. <eos> in this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric bayesian model. <eos> in the model, we utilize synchronous tree substitution grammars ( stsg ) to capture the bilingual mapping between language pairs. <eos> to train the model efficiently, we develop a gibbs sampler with three novel gibbs operators. <eos> the sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. <eos> experimental results show that the string-totree translation system using our bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees.
this paper explores the use of adaptor grammars, a nonparametric bayesian modelling framework, for minimally supervised morphological segmentation. <eos> we compare three training methods : unsupervised training, semisupervised training, and a novel model selection method. <eos> in the model selection method, we train unsupervised adaptor grammars using an over-articulated metagrammar, then use a small labelled data set to select which potential morph boundaries identified by the metagrammar should be returned in the final output. <eos> we evaluate on five languages and show that semi-supervised training provides a boost over unsupervised training, while the model selection method yields the best average results over all languages and is competitive with state-ofthe-art semi-supervised systems. <eos> moreover, this method provides the potential to tune performance according to different evaluation metrics or downstream tasks.
head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees, under the arc-factored model. <eos> in this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. <eos> we define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage.
adjectives like good, great, and excellent are similar in meaning, but differ in intensity. <eos> intensity order information is very useful for language learners as well as in several nlp tasks, but is missing in most lexical resources ( dictionaries, wordnet, and thesauri ). <eos> in this paper, we present a primarily unsupervised approach that uses semantics from web-scale data ( e.g., phrases like good but not excellent ) to rank words by assigning them positions on a continuous scale. <eos> we rely on mixed integer linear programming to jointly determine the ranks, such that individual decisions benefit from global information. <eos> when ranking english adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics ( specifically, 70 % pairwise accuracy as compared to only 56 % by previous work ). <eos> moreover, our approach can incorporate external synonymy information ( increasing its pairwise accuracy to 78 % ) and extends easily to new languages. <eos> we also make our code and data freely available.1
dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. <eos> it has been verified to be a useful constraint for word alignment. <eos> however, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. <eos> in this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. <eos> we also propose an approximate em algorithm and a gibbs sampling algorithm to estimate model parameters in an unsupervised manner. <eos> experiments on large-scale chinese-english translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.
we present a comparative study of transition-, graph- and pcfg-based models aimed at illuminating more precisely the likely contribution of cfgs in improving chinese dependency parsing accuracy, especially by combining heterogeneous models. <eos> inspired by the impact of a constituency grammar on dependency parsing, we propose several strategies to acquire pseudo cfgs only from dependency annotations. <eos> compared to linguistic grammars learned from rich phrase-structure treebanks, well designed pseudo grammars achieve similar parsing accuracy and have equivalent contributions to parser ensemble. <eos> moreover, pseudo grammars increase the diversity of base models ; therefore, together with all other models, further improve system combination. <eos> based on automatic pos tagging, our final model achieves a uas of 87.23 %, resulting in a significant improvement of the state of the art.
grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years. <eos> in most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored. <eos> in the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent ( frank et al, 2013 ). <eos> the current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. <eos> we combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. <eos> by casting grounded language learning as a grammatical inference task, we use our parser to extend the work of johnson et al ( 2012 ), investigating the importance of discourse continuity in children ? s language acquisition and its interaction with social cues. <eos> our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators.
defining the reordering search space is a crucial issue in phrase-based smt between distant languages. <eos> in fact, the optimal tradeoff between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space. <eos> we propose a method to dynamically shape such space and, thus, capture long-range word movements without hurting translation quality nor decoding time. <eos> the space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another. <eos> the integration of this model into a phrase-based decoder improves a strong arabic-english baseline already including state-of-the-art early distortion cost ( moore and quirk, 2007 ) and hierarchical phrase orientation models ( galley and manning, 2008 ). <eos> significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline, while bleu and meteor remain stable, or even increase, at a very high distortion limit.
great writing is rare and highly admired. <eos> readers seek out articles that are beautifully written, informative and entertaining. <eos> yet information-access technologies lack capabilities for predicting article quality at this level. <eos> in this paper we present first experiments on article quality prediction in the science journalism domain. <eos> we introduce a corpus of great pieces of science journalism, along with typical articles from the genre. <eos> we implement features to capture aspects of great writing, including surprising, visual and emotional content, as well as general features related to discourse organization and sentence structure. <eos> we show that the distinction between great and typical articles can be detected fairly accurately, and that the entire spectrum of our features contribute to the distinction.
linking implicit semantic roles is a challenging problem in discourse processing. <eos> unlike prior work inspired by srl, we cast this problem as an anaphora resolution task and embed it in an entity-based coreference resolution ( cr ) architecture. <eos> our experiments clearly show that cr-oriented features yield strongest performance exceeding a strong baseline. <eos> we address the problem of data sparsity by applying heuristic labeling techniques, guided by the anaphoric nature of the phenomenon. <eos> we achieve performance beyond state-of-the art.
we present a novel adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters. <eos> a significant advantage of the new approach is that the expert rules can be easily augmented with new semantic features. <eos> we demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics. <eos> experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement, and over 5 % improvement in f1 measure for the target pronouns when evaluated on the ace 2004 newswire corpus.
this paper explores the hypothesis that semantic relatedness may be more reliably inferred by using a multilingual space, as compared to the typical monolingual representation. <eos> through evaluations using several stateof-the-art semantic relatedness systems, applied on standard datasets, we show that a multilingual approach is better suited for this task, and leads to improvements of up to 47 % with respect to the monolingual baseline.
wikipedia is a web based, freely available multilingual encyclopedia, constructed in a collaborative effort by thousands of contributors. <eos> wikipedia articles on the same topic in different languages are connected via interlingual ( or translational ) links. <eos> these links serve as an excellent resource for obtaining lexical translations, or building multilingual dictionaries and semantic networks. <eos> as these links are manually built, many links are missing or simply wrong. <eos> this paper describes a supervised learning method for generating new links and detecting existing incorrect links. <eos> since there is no dataset available to evaluate the resulting interlingual links, we create our own gold standard by sampling translational links from four language pairs using distance heuristics. <eos> we manually annotate the sampled translation links and used them to evaluate the output of our method for automatic link detection and correction.
this paper presents a novel sentence clustering scheme based on projecting sentences over term clusters. <eos> the scheme incorporates external knowledge to overcome lexical variability and small corpus size, and outperforms common sentence clustering methods on two reallife industrial datasets.
we present the results of several machine learning tasks designed to predict rhetorical relations that hold between clauses in discourse. <eos> we demonstrate that organizing rhetorical relations into different granularity categories ( based on relative degree of detail ) increases average prediction accuracy from 58 % to 70 %. <eos> accuracy further increases to 80 % with the inclusion of clause types. <eos> these results, which are competitive with existing systems, hold across several modes of written discourse and suggest that features of information structure are an important consideration in the machine learnability of discourse.
the correct choice of words has proven challenging for learners of a second language and errors of this kind form a separate category in error typology. <eos> this paper focuses on one known example of two verbs that are often confused by non-native speakers of germanic languages, to make and to do. <eos> we conduct experiments using syntactic information and immediate context for dutch and english. <eos> our results show that the methods exploiting syntactic information and distributional similarity yield the best results.
text reuse is common in many scenarios and documents are often based, at least in part, on existing documents. <eos> this paper reports an approach to detecting text reuse which identifies not only documents which have been reused verbatim but is also designed to identify cases of reuse when the original has been rewritten. <eos> the approach identifies reuse by comparing word n-grams in documents and modifies these ( by substituting words with synonyms and deleting words ) to identify when text has been altered. <eos> the approach is applied to a corpus of newspaper stories and found to outperform a previously reported method.
corpus-based thesaurus construction for morphologically rich languages ( mrl ) is a complex task, due to the morphological variability of mrl. <eos> in this paper we explore alternative term representations, complemented by clustering of morphological variants. <eos> we introduce a generic algorithmic scheme for thesaurus construction in mrl, and demonstrate the empirical benefit of our methodology for a hebrew thesaurus.
in this paper, we investigate a full-fledged supervised machine learning framework for identifying english phrasal verbs in a given context. <eos> we concentrate on those that we define as the most confusing phrasal verbs, in the sense that they are the most commonly used ones whose occurrence may correspond either to a true phrasal verb or an alignment of a simple verb with a preposition. <eos> we construct a benchmark dataset1 with 1,348 sentences from bnc, annotated via an internet crowdsourcing platform. <eos> this dataset is further split into two groups, more idiomatic group which consists of those that tend to be used as a true phrasal verb and more compositional group which tends to be used either way. <eos> we build a discriminative classifier with easily available lexical and syntactic features and test it over the datasets. <eos> the classifier overall achieves 79.4 % accuracy, 41.1 % error deduction compared to the corpus majority baseline 65 %. <eos> however, it is even more interesting to discover that the classifier learns more from the more compositional examples than those idiomatic ones.
we investigate the semantic relationship between a noun and its adjectival modifiers. <eos> we introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modifiers, and adjective-noun selectional preference. <eos> through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. <eos> we analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning.
pisa, italyalessandro.lenci @ ling.unipi.it giulia benottouniversity of pisa, dept. <eos> of linguisticsvia s. maria 36i-56126, pisa, italymezzanine.g @ gmail.comabstract in this paper we apply existing directional similarity measures to identify hypernyms with a state-of-the-art distributional semantic model. <eos> we also propose a new directional measure that achieves the best performance in hypernym identification.
we report ongoing work on the development of agents that can implicitly coordinate with their partners in referential tasks, taking as a case study colour terms. <eos> we describe algorithms for generation and resolution of colour descriptions and report results of experiments on how humans use colour terms for reference in production and comprehension.
given a set of images with related captions, our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small, as this sort of data is common in news and social media. <eos> we extend previous work in unsupervised text-only disambiguation with methods that integrate text and images. <eos> we construct a corpus by using amazon mechanical turk to caption sensetagged images gathered from imagenet. <eos> using a yarowsky-inspired algorithm, we show that gains can be made over text-only disambiguation, as well as multimodal approaches such as latent dirichlet allocation.
we present a framework, based on sejane and eger ( 2012 ), for inducing lexical semantic typologies for groups of languages. <eos> our framework rests on lexical semantic association networks derived from encoding, via bilingual corpora, each language in a common reference language, the tertium comparationis, so that distances between languages can easily be determined.
semantic role classification accuracy for most languages other than english is constrained by the small amount of annotated data. <eos> in this paper, we demonstrate how the frame-to-frame relations described in the framenet ontology can be used to improve the performance of a framenet-based semantic role classifier for swedish, a low-resource language. <eos> in order to make use of the framenet relations, we cast the semantic role classification task as a non-atomic label prediction task. <eos> the experiments show that the cross-frame generalization methods lead to a 27 % reduction in the number of errors made by the classifier. <eos> for previously unseen frames, the reduction is even more significant : 50 %.
we study the task of automatically disambiguating word combinations such as jump the gun which are ambiguous between a literal and mwe interpretation, focusing on the utility of type-level features from an mwe lexicon for the disambiguation task. <eos> to this end we combine gold-standard idiomaticity of tokens in the openmwe corpus with mwe-type-level information drawn from the recently-published jdmwe lexicon. <eos> we find that constituent modifiability in an mwe-type is more predictive of the idiomaticity of its tokens than other constituent characteristics such as semantic class or part of speech.
modeling user preferences is crucial in many real-life problems, ranging from individual and collective decision-making to strategic interactions between agents and game theory. <eos> since agents do not come with their preferences transparently given in advance, we have only two means to determine what they are if we wish to exploit them in reasoning : we can infer them from what an agent says or from his nonlinguistic actions. <eos> in this paper, we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation. <eos> to this end, we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres. <eos> this paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. <eos> our results show that preferences can be easily annotated by humans.
neurosemantics aims to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. <eos> different approaches have been used to represent individual concepts, but current state-of-the-art techniques require extensive manual intervention to scale to arbitrary words and domains. <eos> to overcome this challenge, we initiate a systematic comparison of automatically-derived corpus representations, based on various types of textual co-occurrence. <eos> we find that dependency parse-based features are the most effective, achieving accuracies similar to the leading semi-manual approaches and higher than any published for a corpus-based model. <eos> we also find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost.
this paper complements a series of works on implicative verbs such as manage to and fail to. <eos> it extends the description of simple implicative verbs to phrasal implicatives as take the time to and waste the chance to. <eos> it shows that the implicative signatures of over 300 verb-noun collocations depend both on the semantic type of the verb and the semantic type of the noun in a systematic way.
we propose an unsupervised system that learns continuous degrees of lexicality for noun-noun compounds, beating a strong baseline on several tasks. <eos> we demonstrate that the distributional representations of compounds and their parts can be used to learn a finegrained representation of semantic contribution. <eos> finally, we argue such a representation captures compositionality better than the current status-quo which treats compositionality as a binary classification problem.
over the past decade, several underspecification frameworks have been proposed that efficiently solve a big subset of scopeunderspecified semantic representations within the realm of the most popular constraint-based formalisms. <eos> however, there exists a family of coherent natural language sentences whose underspecified representation does not belong to this subset. <eos> it has remained an open question whether there exists a tractable superset of these frameworks, covering this family. <eos> in this paper, we show that the answer to this question is yes. <eos> we define a superset of the previous frameworks, which is solvable by similar algorithms with the same time and space complexity.
many types of polysemy are not word specific, but are instances of general sense alternations such as animal-food. <eos> despite their pervasiveness, regular alternations have been mostly ignored in empirical computational semantics. <eos> this paper presents ( a ) a general framework which grounds sense alternations in corpus data, generalizes them above individual words, and allows the prediction of alternations for new words ; and ( b ) a concrete unsupervised implementation of the framework, the centroid attribute model. <eos> we evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines.
we present a rule-based method to automatically create a large-coverage semantic lexicon of french adjectives by extracting paradigmatic relations from lexicographic definitions. <eos> formalized adjectival resources are, indeed, scarce for french and they mostly focus on morphological and syntactic information. <eos> our objective is, therefore, to contribute enriching the available set of resources by taking advantage of reliable lexicographic data and formalizing it with the well-established lexical functions formalism. <eos> the resulting semantic lexicon of french adjectives can be used in nlp tasks such as word sense disambiguation or machine translation. <eos> after presenting related work, we describe the extraction method and the formalization procedure of the data. <eos> our method is then quantitatively and qualitatively evaluated. <eos> we discuss the results of the evaluation and conclude on some perspectives.
this paper describes bayesian selectional preference models that incorporate knowledge from a lexical hierarchy such as wordnet. <eos> inspired by previous work on modelling with wordnet, these approaches are based either on ? cutting ? <eos> the hierarchy at an appropriate level of generalisation or on a ? walking ? <eos> model that selects a path from the root to a leaf. <eos> in an evaluation comparing against human plausibility judgements, we show that the models presented here outperform previously proposed comparable wordnet-based models, are competitive with state-of-the-art selectional preference models and are particularly wellsuited to estimating plausibility for items that were not seen in training.
we present a method for learning syntaxsemantics mappings for verbs from unannotated corpora. <eos> we learn linkings, i.e., mappings from the syntactic arguments and adjuncts of a verb to its semantic roles. <eos> by learning such linkings, we do not need to model individual semantic roles independently of one another, and we can exploit the relation between different mappings for the same verb, or between mappings for different verbs. <eos> we present an evaluation on a standard test set for semantic role labeling.
word sense disambiguation aims to label the sense of a word that best applies in a given context. <eos> graded word sense disambiguation relaxes the single label assumption, allowing for multiple sense labels with varying degrees of applicability. <eos> training multi-label classifiers for such a task requires substantial amounts of annotated data, which is currently not available. <eos> we consider an alternate method of annotating graded senses using word sense induction, which automatically learns the senses and their features from corpus properties. <eos> our work proposes three objective to evaluate performance on the graded sense annotation task, and two new methods for mapping between sense inventories using parallel graded sense annotations. <eos> we demonstrate that sense induction offers significant promise for accurate graded sense annotation.
we present an ensemble-based framework for semantic lexicon induction that incorporates three diverse approaches for semantic class identification. <eos> our architecture brings together previous bootstrapping methods for pattern-based semantic lexicon induction and contextual semantic tagging, and incorporates a novel approach for inducing semantic classes from coreference chains. <eos> the three methods are embedded in a bootstrapping architecture where they produce independent hypotheses, consensus words are added to the lexicon, and the process repeats. <eos> our results show that the ensemble outperforms individual methods in terms of both lexicon quality and instance-based semantic tagging.
we present a novel technique for jointly predicting semantic arguments for lexical predicates. <eos> the task is to find the best matching between semantic roles and sentential spans, subject to structural constraints that come from expert linguistic knowledge ( e.g., in the framenet lexicon ). <eos> we formulate this task as an integer linear program ( ilp ) ; instead of using an off-the-shelf tool to solve the ilp, we employ a dual decomposition algorithm, which we adapt for exact decoding via a branch-and-bound technique. <eos> compared to a baseline that makes local predictions, we achieve better argument identification scores and avoid all structural violations. <eos> runtime is nine times faster than a proprietary ilp solver.
discourse coherence is an important aspect of natural language that is still understudied in computational linguistics. <eos> our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicateargument structures ( pas ) in a model that exceeds the sentence level. <eos> in particular, we aim to study the case of non-realized arguments as a coherence inducing factor. <eos> this task can be broken down into two subtasks. <eos> the first aligns predicates across comparable texts, admitting partial argument structure correspondence. <eos> the resulting alignments and their contexts can then be used for developing a coherence model for argument realization. <eos> this paper introduces a large corpus of comparable monolingual texts as a prerequisite for approaching this task, including an evaluation set with manual predicate alignments. <eos> we illustrate the potential of this new resource for the empirical investigation of discourse coherence phenomena. <eos> initial experiments on the task of predicting predicate alignments across text pairs show promising results. <eos> our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks.
we investigate the effects of adding semantic annotations including word sense hypernyms to the source text for use as an extra source of information in hpsg parse ranking for the english resource grammar. <eos> the semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. <eos> we test this using two test corpora in different domains with various sources of training data. <eos> the best reduces error rate in dependency fscore by 1 % on average, while some methods produce substantial decreases in performance.
identifying textual inferences, where the meaning of one text follows from another, is a general underlying task within many natural language applications. <eos> commonly, it is approached either by generative syntactic-based methods or by ? lightweight ? <eos> heuristic lexical models. <eos> we suggest a model which is confined to simple lexical information, but is formulated as a principled generative probabilistic model. <eos> we focus our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set.
detecting emotions in microblogs and social media posts has applications for industry, health, and security. <eos> however, there exists no microblog corpus with instances labeled for emotions for developing supervised systems. <eos> in this paper, we describe how we created such a corpus from twitter posts using emotionword hashtags. <eos> we conduct experiments to show that the self-labeled hashtag annotations are consistent and match with the annotations of trained judges. <eos> we also show how the twitter emotion corpus can be used to improve emotion classification accuracy in a different domain. <eos> finally, we extract a word ? emotion association lexicon from this twitter corpus, and show that it leads to significantly better results than the manually crafted wordnet affect lexicon in an emotion classification task.1
previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity metrics over large text corpora. <eos> our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system ? s log-linear model. <eos> we compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving stateof-the-art quality.
the joint conference on lexical and computational semantics ( *sem ) each year hosts a shared task on semantic related topics. <eos> in its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. <eos> this paper presents the specifications, datasets and evaluation criteria of the task. <eos> an overview of participating systems is provided and their results are summarized.
this paper describes our participation in the closed track of the *sem 2012 shared task of finding the scope of negation. <eos> to perform the task, we propose a system that has three components : negation cue detection, scope of negation detection, and negated event detection. <eos> in the first phase, the system creates a lexicon of negation signals from the training data and uses the lexicon to identify the negation cues. <eos> then, it applies machine learning approaches to detect the scope and negated event for each negation cue identified in the first phase. <eos> using a preliminary approach, our system achieves a reasonably good accuracy in identifying the scope of negation.
this paper presents one of the two contributions from the universidad complutense de madrid to the *sem shared task 2012 on resolving the scope and focus of negation. <eos> we describe a rule-based system for detecting the presence of negations and delimitating their scope. <eos> it was initially intended for processing negation in opinionated texts, and has been adapted to fit the task requirements. <eos> it first detects negation cues using a list of explicit negation markers ( such as not or nothing ), and infers other implicit negations ( such as affixal negations, e.g, undeniable or improper ) by using semantic information from wordnet concepts and relations. <eos> it next uses the information from the syntax tree of the sentence in which the negation arises to get a first approximation to the negation scope, which is later refined using a set of post-processing rules that bound or expand such scope.
ucm-2 infers the words that are affected by negations by browsing dependency syntactic structures. <eos> it first makes use of an algorithm that detects negation cues, like no, not or nothing, and the words affected by them by traversing minipar dependency structures. <eos> second, the scope of these negation cues is computed by using a post-processing rulebased approach that takes into account the information provided by the first algorithm and simple linguistic clause boundaries. <eos> an initial version of the system was developed to handle the annotations of the bioscope corpus. <eos> for the present version, we have changed, omitted or extended the rules and the lexicon of cues ( allowing prefix and suffix negation cues, such as impossible or meaningless ), to make it suitable for the present task.
simply detecting negation cues is not sufficient to determine the semantics of negation, scope and focus must be taken into account. <eos> while scope detection has recently seen repeated attention, the linguistic notion of focus is only now being introduced into computational work. <eos> the *sem2012 shared task is pioneering this effort by introducing a suitable dataset and annotation guidelines. <eos> clac ? s negfocus system is a solid baseline approach to the task.
we use the nlp toolchain that is used to construct the groningen meaning bank to address the task of detecting negation cue and scope, as defined in the shared task ? resolving the scope and focus of negation ?. <eos> this toolchain applies the c & c tools for parsing, using the formalism of combinatory categorial grammar, and applies boxer to produce semantic representations in the form of discourse representation structures ( drss ). <eos> for negation cue detection, the drss are converted to flat, non-recursive structures, called discourse representation graphs ( drgs ). <eos> drgs simplify cue detection by means of edge labels representing relations. <eos> scope detection is done by gathering the tokens that occur within the scope of a negated drs. <eos> the result is a system that is fairly reliable for cue detection and scope detection. <eos> furthermore, it provides a fairly robust algorithm for detecting the negated event or property within the scope.
this paper describes the first of two systems submitted from the university of oslo ( uio ) to the 2012 *sem shared task on resolving negation. <eos> our submission is an adaption of the negation system of velldal et al ( 2012 ), which combines svm cue classification with svm-based ranking of syntactic constituents for scope resolution. <eos> the approach further extends our prior work in that we also identify factual negated events. <eos> while submitted for the closed track, the system was the top performer in the shared task overall.
this paper describes the second of two systems submitted from the university of oslo ( uio ) to the 2012 *sem shared task on resolving negation. <eos> the system combines svm cue classification with crf sequence labeling of events and scopes. <eos> models for scopes and events are created using lexical and syntactic features, together with a fine-grained set of labels that capture the scopal behavior of certain tokens. <eos> following labeling, negated tokens are assigned to their respective cues using simple post-processing heuristics. <eos> the system was ranked first in the open track and third in the closed track, and was one of the top performers in the scope resolution sub-task overall.
in this paper, we present a system for detecting negation in english text. <eos> we address three tasks : negation cue detection, negation scope resolution and negated event identification. <eos> we pose these tasks as sequence labeling problems. <eos> for each task, we train a conditional random field ( crf ) model on lexical, structural, and syntactic features extracted from labeled data. <eos> the models are trained and tested using the dataset distributed with the *sem shared task 2012 on resolving the scope and focus of negation. <eos> the system detects negation cues with 90.98 % f1 measure ( 94.3 % and 87.88 % recall ). <eos> it identifies negation scope with 82.70 % f1 on token-bytoken level and 64.78 % f1 on full scope level. <eos> negated events are detected with 51.10 % f1 measure.
this paper reports on a simple system for resolving the scope of negation in the closed track of the *sem 2012 shared task. <eos> cue detection is performed using regular expression rules extracted from the training data. <eos> both scope tokens and negated event tokens are resolved using a conditional random field ( crf ) sequence tagger ? <eos> namely the simpletagger library in the mallet machine learning toolkit. <eos> the full negation f1 score obtained for the task evaluation is 48.09 % ( p=74.02 %, r=35.61 % ) which ranks this system fourth among the six submitted for the closed track.
automatic detection of negation cues along with their scope and corresponding negated events is an important task that could benefit other natural language processing ( nlp ) tasks such as extraction of factual information from text, sentiment analysis, etc. <eos> this paper presents a system for this task that exploits phrasal and contextual clues apart from various token specific features. <eos> the system was developed for the participation in the task 1 ( closed track ) of the *sem 2012 shared task ( resolving the scope and focus of negation ), where it is ranked 3rd among the participating teams while attaining the highest f1 score for negation cue detection.
we describe the english lexical simplification task at semeval-2012. <eos> this is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context-aware lexical simplification approaches. <eos> the task requires that annotators and systems rank a number of alternative substitutes ? <eos> all deemed adequate ? <eos> for a target word in context, according to how ? simple ? <eos> these substitutes are. <eos> the notion of simplicity is biased towards non-native speakers of english. <eos> out of nine participating systems, the best scoring ones combine contextdependent and context-independent information, with the strongest individual contribution given by the frequency of the substitute regardless of its context.
up to now, work on semantic relations has focused on relation classification : recognizing whether a given instance ( a word pair such as virus : flu ) belongs to a specific relation class ( such as cause : effect ). <eos> however, instances of a single relation class may still have significant variability in how characteristic they are of that class. <eos> we present a new semeval task based on identifying the degree of prototypicality for instances within a given class. <eos> as a part of the task, we have assembled the first dataset of graded relational similarity ratings across 79 relation categories. <eos> three teams submitted six systems, which were evaluated using two methods.
this semeval2012 shared task is based on a recently introduced spatial annotation scheme called spatial role labeling. <eos> the spatial role labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors, landmarks and spatial indicators. <eos> in addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted. <eos> the annotated dataset contains about 1213 sentences which describe 612 images of the clef iapr tc-12 image benchmark. <eos> we have one participant system with two runs. <eos> the participant ? s runs are compared to the system in ( kordjamshidi et al, 2011c ) which is provided by task organizers.
this task focuses on evaluating word similarity computation in chinese. <eos> we follow the way of finkelstein et al ( 2002 ) to select word pairs. <eos> then we organize twenty undergraduates who are major in chinese linguistics to annotate the data. <eos> each pair is assigned a similarity score by each annotator. <eos> we rank the word pairs by the average value of similar scores among the twenty annotators. <eos> this data is used as gold standard. <eos> four systems participating in this task return their results. <eos> we evaluate their results on gold standard data in term of kendall 's tau value, and the results show three of them have a positive correlation with the rank manually created while the taus ' value is very small.
the paper presents the semeval-2012 shared task 5 : chinese semantic dependency parsing. <eos> the goal of this task is to identify the dependency structure of chinese sentences from the semantic view. <eos> we firstly introduce the motivation of providing chinese semantic dependency parsing task, and then describe the task in detail including data preparation, data format, task evaluation, and so on. <eos> over ten thousand sentences were labeled for participants to train and evaluate their systems. <eos> at last, we briefly describe the submitted systems and analyze these results.
semantic textual similarity ( sts ) measures the degree of semantic equivalence between two texts. <eos> this paper presents the results of the sts pilot task in semeval. <eos> the training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. <eos> the test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. <eos> the similarity of pairs of sentences was rated on a 0-5 scale ( low to high similarity ) by human judges using amazon mechanical turk, with high pearson correlation scores, around 90 %. <eos> 35 teams participated in the task, submitting 88 runs. <eos> the best results scored a pearson correlation > 80 %, well above a simple lexical baseline that only scored a 31 % correlation. <eos> this pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.
this paper presents the first round of the task on cross-lingual textual entailment for content synchronization, organized within semeval-2012. <eos> the task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. <eos> participants were presented with datasets for different language pairs, where multi-directional entailment relations ( ? forward ?, ? backward ?, ? bidirectional ?, ? no entailment ? ) <eos> had to be identified. <eos> we report on the training and test data used for evaluation, the process of their creation, the participating systems ( 10 teams, 92 runs ), the approaches adopted and the results achieved.
our system breaks down the problem of ranking a list of lexical substitutions according to how simple they are in a given context into a series of pairwise comparisons between candidates. <eos> for this we learn a binary classifier. <eos> as only very little training data is provided, we describe a procedure for generating artificial unlabeled data from wordnet and a corpus and approach the classification task as a semisupervised machine learning problem. <eos> we use a co-training procedure that lets each classifier increase the other classifier ? s training set with selected instances from an unlabeled data set. <eos> our features include n-gram probabilities of candidate and context in a web corpus, distributional differences of candidate in a corpus of ? easy ? <eos> sentences and a corpus of normal sentences, syntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram character language model.
in this paper we present our approach for assigning degrees of relational similarity to pairs of words in the semeval-2012 task 2. <eos> to measure relational similarity we employed lexical patterns that can match against word pairs within a large corpus of 12 million documents. <eos> patterns are weighted by obtaining statistically estimated lower bounds on their precision for extracting word pairs from a given relation. <eos> finally, word pairs are ranked based on a model predicting the probability that they belong to the relation of interest. <eos> this approach achieved the best results on the semeval 2012 task 2, obtaining a spearman correlation of 0.229 and an accuracy on reproducing human answers to maxdiff questions of 39.4 %.
we present a joint approach for recognizing spatial roles in semeval-2012 task 3. <eos> candidate spatial relations, in the form of triples, are heuristically extracted from sentences with high recall. <eos> the joint classification of spatial roles is then cast as a binary classification over the candidates. <eos> this joint approach allows for a rich feature set based on the complete relation instead of individual relation arguments. <eos> our best official submission achieves an f1measure of 0.573 on relation recognition, best in the task and outperforming the previous best result on the same data set ( 0.500 ).
this document describes three systems calculating semantic similarity between two chinese words. <eos> one is based on machine readable dictionaries and the others utilize both mrds and corpus. <eos> these systems are performed on semeval-2012 task 4 : evaluating chinese word similarity.
this paper presents our system participated on semeval-2012 task : chinese semantic dependency parsing. <eos> our system extends the second-order mst model by adding two third-order features. <eos> the two third-order features are grand-sibling and tri-sibling. <eos> in the decoding phase, we keep the k best results for each span. <eos> after using the selected third-order features, our system presently achieves las of 61.58 % ignoring punctuation tokens which is 0.15 % higher than the result of purely second-order model on the test dataset.
we present the ukp system which performed best in the semantic textual similarity ( sts ) task at semeval-2012 in two out of three metrics. <eos> it uses a simple log-linear regression model, trained on the training data, to combine multiple text similarity measures of varying complexity. <eos> these range from simple character and word n-grams and common subsequences to complex features such as explicit semantic analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources. <eos> further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. <eos> our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented.
this paper describes the two systems for determining the semantic similarity of short texts submitted to the semeval 2012 task 6. <eos> most of the research on semantic similarity of textual content focuses on large documents. <eos> however, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts. <eos> we predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. <eos> out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used ( overall pearson ? <eos> 2nd and 3rd, normalized pearson ? <eos> 1st and 3rd, weighted mean ? <eos> 2nd and 5th ).
we present an approach for the construction of text similarity functions using a parameterized resemblance coefficient in combination with a softened cardinality function called soft cardinality. <eos> our approach provides a consistent and recursive model, varying levels of granularity from sentences to characters. <eos> therefore, our model was used to compare sentences divided into words, and in turn, words divided into q-grams of characters. <eos> experimentally, we observed that a performance correlation function in a space defined by all parameters was relatively smooth and had a single maximum achievable by ? hill climbing. ? <eos> our approach used only surface text information, a stop-word remover, and a stemmer to tackle the semantic text similarity task 6 at semeval 2012. <eos> the proposed method ranked 3rd ( average ), 5th ( normalized correlation ), and 15th ( aggregated correlation ) among 89 systems submitted by 31 teams.
this paper describes the participation of uned nlp group in the semeval 2012 semantic textual similarity task. <eos> our contribution consists of an unsupervised method, heterogeneity based ranking ( hbr ), to combine similarity measures. <eos> our runs focus on combining standard similarity measures for machine translation. <eos> the pearson correlation achieved is outperformed by other systems, due to the limitation of mt evaluation measures in the context of this task. <eos> however, the combination of system outputs that participated in the campaign produces three interesting results : ( i ) combining all systems without considering any kind of human assessments achieve a similar performance than the best peers in all test corpora, ( ii ) combining the 40 less reliable peers in the evaluation campaign achieves similar results ; and ( iii ) the correlation between peers and hbr predicts, with a 0.94 correlation, the performance of measures according to human assessments.
we describe the heidelberg university system for the cross-lingual textual entailment task at semeval-2012. <eos> the system relies on features extracted with statistical machine translation methods and tools, combining monolingual and cross-lingual word alignments as well as standard textual entailment distance and bag-of-words features in a statistical learning framework. <eos> we learn separate binary classifiers for each entailment direction and combine them to obtain four entailment relations. <eos> our system yielded the best overall score for three out of four language pairs.
this paper describes a new method for crosslingual textual entailment ( clte ) detection based on machine translation ( mt ). <eos> we use sub-segment translations from different mt systems available online as a source of crosslingual knowledge. <eos> in this work we describe and evaluate different features derived from these sub-segment translations, which are used by a support vector machine classifier to detect cltes. <eos> we presented this system to the semeval 2012 task 8 obtaining an accuracy up to 59.8 % on the english ? spanish test set, the second best performing approach in the contest.
this paper describes simplex,1 a lexical simplification system that participated in the english lexical simplification shared task at semeval-2012. <eos> it operates on the basis of a linear weighted ranking function composed of context sensitive and psycholinguistic features. <eos> the system outperforms a very strong baseline, and ranked first on the shared task.
in this paper, we describe the system we submitted to the semeval-2012 lexical simplification task. <eos> our system ( mmsystem ) combines word frequency with decompositional semantics criteria based on syntactic structure in order to rank candidate substitutes of lexical forms of arbitrary syntactic complexity ( oneword, multi-word, etc. ) <eos> in descending order of ( cognitive ) simplicity. <eos> we believe that the proposed approach might help to shed light on the interplay between linguistic features and lexical complexity in general.
this paper presents the systems we developed while participating in the first task ( english lexical simplification ) of semeval 2012. <eos> our first system relies on n-grams frequencies computed from the simple english wikipedia version, ranking each substitution term by decreasing frequency of use. <eos> we experimented with several other systems, based on term frequencies, or taking into account the context in which each substitution term occurs. <eos> on the evaluation corpus, we achieved a 0.465 score with the first system.
this paper presents three systems that took part in the lexical simplification task at semeval 2012. <eos> speculating on what the concept of simplicity might mean for a word, the systems apply different approaches to rank the given candidate lists. <eos> one of the systems performs second-best ( statistically significant ) and another one performs third-best out of 9 systems and 3 baselines. <eos> notably, the thirdbest system is very close to the second-best, and at the same time much more resource-light in comparison.
this paper describes the duluth systems that participated in task 2 of semeval ? 2012. <eos> these systems were unsupervised and relied on variations of the gloss vector measure found in the freely available software package wordnet : :similarity. <eos> this method was moderately successful for the class-inclusion, similar, contrast, and non-attribute categories of semantic relations, but mimicked a random baseline for the other six categories.
we describe a system proposed for measuring the degree of relational similarity beetwen a pair of words at the task # 2 of semeval 2012. <eos> the approach presented is based on a vectorial representation using the following features : i ) the context surrounding the words with a windows size = 3, ii ) knowledge extracted from wordnet to discover several semantic relationships, such as meronymy, hyponymy, hypernymy, and part-whole between pair of words, iii ) the description of the pairs with their pos tag, morphological information ( gender, person ), and iv ) the average number of words separating the two words in text.
the goal of semantic dependency parsing is to build dependency structure and label semantic relation between a head and its modifier. <eos> to attain this goal, we concentrate on obtaining better dependency structure to predict better semantic relations, and propose a method to combine the results of three state-of-the-art dependency parsers. <eos> unfortunately, we made a mistake when we generate the final output that results in a lower score of 56.31 % in term of labeled attachment score ( las ), reported by organizers. <eos> after giving golden testing set, we fix the bug and rerun the evaluation script, this time we obtain the score of 62.8 % which is consistent with the results on developing set. <eos> we will report detailed experimental results with correct program as a comparison standard for further research.
in this paper, we introduce our work on semeval-2012 task 5 : chinese semantic dependency parsing. <eos> our system is based on mstparser and two effective methods are proposed : splitting sentence by punctuations and extracting last character of word as lemma. <eos> the experiments show that, with a combination of the two proposed methods, our system can improve las about one percent and finally get the second prize out of nine participating systems. <eos> we also try to handle the multilevel labels, but with no improvement.
this paper presents the work of the hong kong polytechnic university ( polyucomp ) team which has participated in the semantic textual similarity task of semeval-2012. <eos> the polyucomp system combines semantic vectors with skip bigrams to determine sentence similarity. <eos> the semantic vector is used to compute similarities between sentence pairs using the lexical database wordnet and the wikipedia corpus. <eos> the use of skip bigram is to introduce the order of words in measuring sentence similarity.
many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts. <eos> however, many systems that address these tasks focus on a single task and may or may not generalize well. <eos> in this work, we extend an existing machine translation metric, terp ( snover et al, 2009a ), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. <eos> these additions facilitate applications of our system, called perp, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. <eos> in the semeval 2012 semantic textual similarity task, perp performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.
in this paper, we describe the system architecture used in the semantic textual similarity ( sts ) task 6 pilot challenge. <eos> the goal of this challenge is to accurately identify five levels of semantic similarity between two sentences : equivalent, mostly equivalent, roughly equivalent, not equivalent but sharing the same topic and no equivalence. <eos> our participations were two systems. <eos> the first system ( rule-based ) combines both semantic and syntax features to arrive at the overall similarity. <eos> the proposed rules enable the system to adequately handle domain knowledge gaps that are inherent when working with knowledge resources. <eos> as such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. <eos> the second system is our baseline in which we use the cosine similarity between the words in each sentence pair.
we propose a semantic similarity learning method based on random indexing ( ri ) and ranking with boosting. <eos> unlike classical ri, we use only those context vector features that are informative for the semantics modeled. <eos> despite ignoring text preprocessing and dispensing with semantic resources, the approach was ranked as high as 22nd among 89 participants in the semeval-2012 task6 : semantic textual similarity.
chunk-based determination of semantic text similarity demetrios glinos advanced text analytics, llc orlando, florida, usa demetrios.glinos @ advancedtextanalytics.com abstract this paper describes investigations into using syntactic chunk information as the basis for determining the similarity of candidate texts at the semantic level. <eos> two approaches were con-sidered. <eos> the first was a corpus-based method that extracted lexical and semantic features from pairs of chunks from each sentence that were associated through a chunk alignment algorithm. <eos> the features were used as input to a classifier trained on the same features ex-tracted from a corpus of gold standard training data. <eos> the second approach involved breadth-first chunk association and the application of a rule-based scoring algorithm. <eos> both approaches were evaluated against the test data for the semeval 2012 semantic text similarity task. <eos> the results show that the rule-based chunk approach is superior.
this paper describes the participation of the irit team to semeval 2012 task 6 ( semantic textual similarity ). <eos> the method used consists of a n-gram based comparison method combined with a conceptual similarity measure that uses wordnet to calculate the similarity between a pair of concepts.
in this paper we present our systems for the sts task. <eos> our systems are all based on a simple process of identifying the components that correspond between two sentences. <eos> currently we use words ( that is word forms ), lemmas, distributional similar words and grammatical relations identified with a dependency parser. <eos> we submitted three systems. <eos> all systems only use open class words. <eos> our first system ( alignheuristic ) tries to obtain a mapping between every open class token using all the above sources of information. <eos> our second system ( wordsim ) uses a different algorithm and unlike alignheuristic, it does not use the dependency information. <eos> the third system ( average ) simply takes the average of the scores for each item from the other two systems to take advantage of the merits of both systems. <eos> for this reason we only provide a brief description of that. <eos> the results are promising, with pearson ? s coefficients on each individual dataset ranging from.3765 to.7761 for our relatively simple heuristics based systems that do not require training on different datasets. <eos> we provide some analysis of the results and also provide results for our data using spearman ? s, which as a nonparametric measure which we argue is better able to reflect the merits of the different systems ( average is ranked between the others ).
we estimate the semantic similarity between two sentences using regression models with features : 1 ) n-gram hit rates ( lexical matches ) between sentences, 2 ) lexical semantic similarity between non-matching words, and 3 ) sentence length. <eos> lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric. <eos> state-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on task 6 of semeval ? 12. <eos> despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the semeval test set.
this article presents the experiments carried out at jadavpur university as part of the participation in semantic textual similarity ( sts ) of task 6 @ semantic evaluation exercises ( semeval-2012 ). <eos> task-6 of semeval- 2012 focused on semantic relations of text pair. <eos> task-6 provides five different text pair files to compare different semantic relations and judge these relations through a similarity and confidence score. <eos> similarity score is one kind of multi way classification in the form of grade between 0 to 5. <eos> we have submitted one run for the sts task. <eos> our system has two basic modules - one deals with lexical relations and another deals with dependency based syntactic relations of the text pair. <eos> similarity score given to a pair is the average of the scores of the above-mentioned modules. <eos> the scores from each module are identified using rule based techniques. <eos> the pearson correlation of our system in the task is 0.3880.
this paper briefly reports our submissions to the semantic textual similarity ( sts ) task in the semeval 2012 ( task 6 ). <eos> we first use knowledge-based methods to compute word semantic similarity as well as word sense disambiguation ( wsd ). <eos> we also consider word order similarity from the structure of the sentence. <eos> finally we sum up several aspects of similarity with different coefficients and get the sentence similarity score.
the paper aims to come up with a system that examines the degree of semantic equivalence between two sentences. <eos> at the core of the paper is the attempt to grade the similarity of two sentences by finding the maximal weighted bipartite match between the tokens of the two sentences. <eos> the tokens include single words, or multiwords in case of named entitites, adjectivally and numerically modified words. <eos> two token similarity measures are used for the task - wordnet based similarity, and a statistical word similarity measure which overcomes the shortcomings of wordnet based similarity. <eos> as part of three systems created for the task, we explore a simple bag of words tokenization scheme, a more careful tokenization scheme which captures named entities, times, dates, monetary entities etc., and finally try to capture context around tokens using grammatical dependencies.
the semantic textual similarity ( sts ) shared task ( agirre et al, 2012 ) computes the degree of semantic equivalence between two sentences.1 we show that a simple unsupervised latent semantics based approach, weighted textual matrix factorization that only exploits bag-of-words features, can outperform most systems for this task. <eos> the key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to latent semantic analysis ( lsa ) and latent dirichlet allocation ( lda ). <eos> our system ranks 20 out of 89 systems according to the official evaluation metric for the task, pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.
this paper presents the unitor system that participated to the semeval 2012 task 6 : semantic textual similarity ( sts ). <eos> the task is here modeled as a support vector ( sv ) regression problem, where a similarity scoring function between text pairs is acquired from examples. <eos> the semantic relatedness between sentences is modeled in an unsupervised fashion through different similarity functions, each capturing a specific semantic aspect of the sts, e.g. <eos> syntactic vs. lexical or topical vs. paradigmatic similarity. <eos> the sv regressor effectively combines the different models, learning a scoring function that weights individual scores in a unique resulting sts. <eos> it provides a highly portable method as it does not depend on any manually built resource ( e.g. <eos> wordnet ) nor controlled, e.g. <eos> aligned, corpus.
this paper describes our system for the semeval 2012 sentence textual similarity task. <eos> the system is based on a combination of few simple vector space-based methods for word meaning similarity. <eos> evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful. <eos> the simple vector space components achieve high performance on short sentences ; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model.
we describe the systems submitted by sri international and the university of the basque country for the semantic textual similarity ( sts ) semeval-2012 task. <eos> our systems focused on using a simple set of features, featuring a mix of semantic similarity resources, lexical match heuristics, and part of speech ( pos ) information. <eos> we also incorporate precision focused scores over lexical and pos information derived from the bleu measure, and lexical and pos features computed over split-bigrams from the rouge-s measure. <eos> these were used to train support vector regressors over the pairs in the training data. <eos> from the three systems we submitted, two performed well in the overall ranking, with splitbigrams improving performance over pairs drawn from the msr research video description corpus. <eos> our third system maintained three separate regressors, each trained specifically for the sts dataset they were drawn from. <eos> it used a multinomial classifier to predict which dataset regressor would be most appropriate to score a given pair, and used it to score that pair. <eos> this system underperformed, primarily due to errors in the dataset predictor.
this paper describes the participation of fbk in the semantic textual similarity ( sts ) task organized within semeval 2012. <eos> our approach explores lexical, syntactic and semantic machine translation evaluation metrics combined with distributional and knowledgebased word similarity metrics. <eos> our best model achieves 60.77 % correlation with human judgements ( mean score ) and ranked 20 out of 88 submitted runs in the mean ranking, where the average correlation across all the sub-portions of the test set is considered.
in this paper we describe the three approaches we submitted to the semantic textual similarity task of semeval 2012. <eos> the first approach considers to calculate the semantic similarity by using the jaccard coefficient with term expansion using synonyms. <eos> the second approach uses the semantic similarity reported by mihalcea in ( mihalcea et al, 2006 ). <eos> the third approach employs random indexing and bag of concepts based on context vectors. <eos> we consider that the first and third approaches obtained a comparable performance, meanwhile the second approach got a very poor behavior. <eos> the best all result was obtained with the third approach, with a pearson correlation equal to 0.663.
this paper presents the systems that we participated with in the semantic text similarity task at semeval 2012. <eos> based on prior research in semantic similarity and relatedness, we combine various methods in a machine learning framework. <eos> the three variations submitted during the task evaluation period ranked number 5, 9 and 14 among the 89 participating systems. <eos> our evaluations show that corpus-based methods display a more robust behavior on the training data, yet combining a variety of methods allows a learning algorithm to achieve a superior decision than that achievable by any of the individual parts.
in this paper, we describe our system submitted for the semantic textual similarity ( sts ) task at semeval 2012. <eos> we implemented two approaches to calculate the degree of similarity between two sentences. <eos> first approach combines corpus-based semantic relatedness measure over the whole sentence with the knowledge-based semantic similarity scores obtained for the words falling under the same syntactic roles in both the sentences. <eos> we fed all these scores as features to machine learning models to obtain a single score giving the degree of similarity of the sentences. <eos> linear regression and bagging models were used for this purpose. <eos> we used explicit semantic analysis ( esa ) as the corpus-based semantic relatedness measure. <eos> for the knowledgebased semantic similarity between words, a modified wordnet based lin measure was used. <eos> second approach uses a bipartite based method over the wordnet based lin measure, without any modification. <eos> this paper shows a significant improvement in calculating the semantic similarity between sentences by the fusion of the knowledge-based similarity measure and the corpus-based relatedness measure against corpus based measure taken alone.
this paper describes stanford university ? s submission to semeval 2012 semantic textual similarity ( sts ) shared evaluation task. <eos> our proposed metric computes probabilistic edit distance as predictions of semantic similarity. <eos> we learn weighted edit distance in a probabilistic finite state machine ( pfsm ) model, where state transitions correspond to edit operations. <eos> while standard edit distance models can not capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pfsm model. <eos> our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features. <eos> the performance of our edit distance based models is contrasted with an adaptation of the stanford textual entailment system to the sts task. <eos> our results show that the most advanced edit distance model, ppda, outperforms our entailment system on all but one of the genres included in the sts task.
this paper describes the university of sheffield ? s submission to semeval-2012 task 6 : semantic text similarity. <eos> two approaches were developed. <eos> the first is an unsupervised technique based on the widely used vector space model and information from wordnet. <eos> the second method relies on supervised machine learning and represents each sentence as a set of n-grams. <eos> this approach also makes use of information from wordnet. <eos> results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained by the supervised approach. <eos> incorporating information from wordnet alo improves performance for both approaches.
sentences that are syntactically quite different can often have similar or same meaning. <eos> the semeval 2012 task of semantic textual similarity aims at finding the semantic similarity between two sentences. <eos> the semantic representation of universal networking language ( unl ), represents only the inherent meaning in a sentence without any syntactic details. <eos> thus, comparing the unl graphs of two sentences can give an insight into how semantically similar the two sentences are. <eos> this paper presents the unl graph matching method for the semantic textual similarity ( sts ) task.
in this paper we report the results obtained in the semantic textual similarity ( sts ) task, with a system primarily developed for textual entailment. <eos> our results are quite promising, getting a run ranked 39 in the official results with overall pearson, and ranking 29 with the mean metric.
the uow submissions to the semantic textual similarity task at semeval-2012 use a supervised machine learning algorithm along with features based on lexical, syntactic and semantic similarity metrics to predict the semantic equivalence between a pair of sentences. <eos> the lexical metrics are based on wordoverlap. <eos> a shallow syntactic metric is based on the overlap of base-phrase labels. <eos> the semantically informed metrics are based on the preservation of named entities and on the alignment of verb predicates and the overlap of argument roles using inexact matching. <eos> our submissions outperformed the official baseline, with our best system ranked above average, but the contribution of the semantic metrics was not conclusive.
we present the penn system for semeval2012 task 6, computing the degree of semantic equivalence between two sentences. <eos> we explore the contributions of different vector models for computing sentence and word similarity : collobert and weston embeddings as well as two novel approaches, namely eigenwords and selectors. <eos> these embeddings provide different measures of distributional similarity between words, and their contexts. <eos> we used regression to combine the different similarity measures, and found that each provides partially independent predictive signal above baseline models.
this paper presents a novel approach for building adaptive similarity functions based on cardinality using machine learning. <eos> unlike current approaches that build feature sets using similarity scores, we have developed these feature sets with the cardinalities of the commonalities and differences between pairs of objects being compared. <eos> this approach allows the machine-learning algorithm to obtain an asymmetric similarity function suitable for directional judgments. <eos> besides using the classic set cardinality, we used soft cardinality to allow flexibility in the comparison between words. <eos> our approach used only the information from the surface of the text, a stop-word remover and a stemmer to address the cross-lingual textual entailment task 8 at semeval 2012. <eos> we have the third best result among the 29 systems submitted by 10 teams. <eos> additionally, this paper presents better results compared with the best official score.
this article presents the experiments carried out at jadavpur university as part of the participation in cross-lingual textual entailment for content synchronization ( clte ) of task 8 @ semantic evaluation exercises ( semeval-2012 ). <eos> the work explores cross-lingual textual entailment as a relation between two texts in different languages and proposes different measures for entailment decision in a four way classification tasks ( forward, backward, bidirectional and no-entailment ). <eos> we set up different heuristics and measures for evaluating the entailment between two texts based on lexical relations. <eos> experiments have been carried out with both the text and hypothesis converted to the same language using the microsoft bing translation system. <eos> the entailment system considers named entity, noun chunks, part of speech, n-gram and some text similarity measures of the text pair to decide the entailment judgments. <eos> rules have been developed to encounter the multi way entailment issue. <eos> our system decides on the entailment judgment after comparing the entailment scores for the text pairs. <eos> four different rules have been developed for the four different classes of entailment. <eos> the best run is submitted for italian ? <eos> english language with accuracy 0.326.
this paper presents celi ? s participation in the semeval cross-lingual textual entailment for content synchronization task. <eos>
this paper overviews fbk ? s participation in the cross-lingual textual entailment for content synchronization task organized within semeval-2012. <eos> our participation is characterized by using cross-lingual matching features extracted from lexical and semantic phrase tables and dependency relations. <eos> the features are used for multi-class and binary classification using svms. <eos> using a combination of lexical, syntactic, and semantic features to create a cross-lingual textual entailment system, we report on experiments over the provided dataset. <eos> our best run achieved an accuracy of 50.4 % on the spanish-english dataset ( with the average score and the median system respectively achieving 40.7 % and 34.6 % ), demonstrating the effectiveness of a ? pure ? <eos> cross-lingual approach that avoids intermediate translations.
in this paper we present a report of the two different runs submitted to the task 8 of semeval 2012 for the evaluation of cross-lingual textual entailment in the framework of content synchronization. <eos> both approaches are based on textual similarity, and the entailment judgment ( bidirectional, forward, backward or no entailment ) is given based on a set of decision rules. <eos> the first approach uses textual similarity on the translated and original versions of the texts, whereas the second approach expands the terms by means of synonyms. <eos> the evaluation of both approaches show a similar behavior which is still close to the average and median.
there are relatively few entailment heuristics that exploit the directional nature of the entailment relation. <eos> cross-lingual text entailment ( clte ), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization ( negri et al, 2012 ). <eos> our system uses simple dictionary lookup combined with heuristic conditions to determine the possible directions of entailment between the two texts written in different languages. <eos> the key members of the conditions were derived from ( corley and mihalcea, 2005 ) formula initially for text similarity, while the entailment condition used as a starting point was that from ( tatar et al, 2009 ). <eos> we show the results obtained by our implementation of this simple and fast approach at the clte task from the semeval2012 challenge.
in this paper, we present our system description in task of cross-lingual textual entailment. <eos> the goal of this task is to detect entailment relations between two sentences written in different languages. <eos> to accomplish this goal, we first translate sentences written in foreign languages into english. <eos> then, we use edits1, an open source package, to recognize entailment relations. <eos> since edits only draws monodirectional relations while the task requires bidirectional prediction, thus we exchange the hypothesis and test to detect entailment in another direction. <eos> experimental results show that our method achieves promising results but not perfect results compared to other participants.
this paper describes our participation in the task denominated cross-lingual textual entailment ( clte ) for content synchronization. <eos> we represent an approach to clte using machine translation to tackle the problem of multilinguality. <eos> our system resides on machine learning and in the use of wordnet as semantic source knowledge. <eos> results are very promising always achieving results above mean score.
the development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. <eos> this paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simulate different aspects of predicate logic. <eos> this paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors. <eos> it provides tensor interpretations of the set of logical connectives required to model propositional calculi. <eos> it suggests a variant of these tensor calculi capable of modelling quantifiers, using few non-linear operations. <eos> it finally discusses the relation between these variants, and how this relation should constitute the subject of future work.
we combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using markov logic networks ( mlns ). <eos> we show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the mln implementation of logical connectives. <eos> we also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance.
wikipedia articles are annotated by volunteer contributors with numerous links that connect words and phrases to relevant titles. <eos> links to general senses of a word are used concurrently with links to more specific senses, without being distinguished explicitly. <eos> we present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. <eos> experimental results show that accounting for annotation ambiguity in wikipedia links leads to significant improvements in disambiguation.
in semantic textual similarity ( sts ), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar. <eos> this year we set up two tasks : ( i ) a core task ( core ), and ( ii ) a typed-similarity task ( typed ). <eos> core is similar in set up to semeval sts 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets. <eos> typed, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description. <eos> several types of similarity have been defined, including similar author, similar time period or similar location. <eos> the annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62 % to 87 %. <eos> the core task attracted 34 participants with 89 runs, and the typed task attracted 6 teams with 14 runs.
we describe three semantic text similarity systems developed for the *sem 2013 sts shared task and the results of the corresponding three runs. <eos> all of them shared a word similarity feature that combined lsa word similarity and wordnet knowledge. <eos> the first, which achieved the best mean score of the 89 submitted runs, used a simple term alignment algorithm augmented with penalty terms. <eos> the other two runs, ranked second and fourth, used support vector regression models to combine larger sets of features.
this paper describes the participation of ikernels system in the semantic textual similarity ( sts ) shared task at *sem 2013. <eos> different from the majority of approaches, where a large number of pairwise similarity features are used to learn a regression model, our model directly encodes the input texts into syntactic/semantic structures. <eos> our systems rely on tree kernels to automatically extract a rich set of syntactic patterns to learn a similarity score correlated with human judgements. <eos> we experiment with different structural representations derived from constituency and dependency trees. <eos> while showing large improvements over the top results from the previous year task ( sts-2012 ), our best system ranks 21st out of total 88 participated in the sts2013 task. <eos> nevertheless, a slight refinement to our model makes it rank 4th.
this paper presents the unitor system that participated in the *sem 2013 shared task on semantic textual similarity ( sts ). <eos> the task is modeled as a support vector ( sv ) regression problem, where a similarity scoring function between text pairs is acquired from examples. <eos> the proposed approach has been implemented in a system that aims at providing high applicability and robustness, in order to reduce the risk of over-fitting over a specific datasets. <eos> moreover, the approach does not require any manually coded resource ( e.g. <eos> wordnet ), but mainly exploits distributional analysis of unlabeled corpora. <eos> a good level of accuracy is achieved over the shared task : in the typed sts task the proposed system ranks in 1st and 2nd position.
the paper outlines the work carried out at ntnu as part of the *sem ? 13 shared task on semantic textual similarity, using an approach which combines shallow textual, distributional and knowledge-based features by a support vector regression model. <eos> feature sets include ( 1 ) aggregated similarity based on named entity recognition with wordnet and levenshtein distance through the calculation of maximum weighted bipartite graphs ; ( 2 ) higher order word co-occurrence similarity using a novel method called ? multisense random indexing ? <eos> ; ( 3 ) deeper semantic relations based on the relex semantic dependency relationship extraction system ; ( 4 ) graph edit-distance on dependency trees ; ( 5 ) reused features of the takelab and dkpro systems from the sts ? 12 shared task. <eos> the ntnu systems obtained 9th place overall ( 5th best team ) and 1st place on the smt data set.
this paper describes our system submitted to *sem 2013 semantic textual similarity ( sts ) core task which aims to measure semantic similarity of two given text snippets. <eos> in this shared task, we propose an interpolation sts model named model_lim integrating framenet parsing information, which has a good performance with low time complexity compared with former submissions.
we describe a number of techniques for automatically deriving lists of common and proper nouns, and show that the distinction between the two can be made automatically using a vector space model learning algorithm. <eos> we present a direct evaluation on the british national corpus, and application based evaluations on twitter messages and on automatic speech recognition ( where the system could be employed to restore case ).
this paper describes methods that were submitted as part of the *sem shared task on semantic textual similarity. <eos> multiple kernels provide different views of syntactic structure, from both tree and dependency parses. <eos> the kernels are then combined with simple lexical features using gaussian process regression, which is trained on different subsets of training data for each run. <eos> we found that the simplest combination has the highest consistency across the different data sets, while introduction of more training data and models requires training and test data with matching qualities.
the semantic textual similarity ( sts ) task aims to exam the degree of semantic equivalence between sentences ( agirre et al., 2012 ). <eos> this paper presents the work of the hong kong polytechnic university ( polyucomp ) team which has participated in the sts core and typed tasks of semeval2013. <eos> for the sts core task, the polyucomp system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses they shared. <eos> for the sts typed task, the string kernel ( lodhi et al, 2002 ) is used to compute similarity between two entities to avoid string variations in entities.
this paper describes a system for automatically measuring the semantic similarity between two texts, which was the aim of the 2013 semantic textual similarity ( sts ) task ( agirre et al, 2013 ). <eos> for the 2012 sts task, heilman and madnani ( 2012 ) submitted the perp system, which performed competitively in relation to other submissions. <eos> however, approaches including word and n-gram features also performed well ( ba ? r et al, 2012 ; s ? aric ? <eos> et al, 2012 ), and the 2013 sts task focused more on predicting similarity for text pairs from new domains. <eos> therefore, for the three variations of our system that we were allowed to submit, we used stacking ( wolpert, 1992 ) to combine perp with word and ngram features and applied the domain adaptation approach outlined by daume iii ( 2007 ) to facilitate generalization to new domains. <eos> our submissions performed well at most subtasks, particularly at measuring the similarity of news headlines, where one of our submissions ranked 2nd among 89 from 34 teams, but there is still room for improvement.
this paper describes our submission for the *sem shared task of semantic textual similarity. <eos> we estimate the semantic similarity between two sentences using regression models with features : 1 ) n-gram hit rates ( lexical matches ) between sentences, 2 ) lexical semantic similarity between non-matching words, 3 ) string similarity metrics, 4 ) affective content similarity and 5 ) sentence length. <eos> domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.
this paper describes the specifications and results of umcc_dlsi system, which participated in the semantic textual similarity task ( sts ) of semeval-2013. <eos> our supervised system uses different types of lexical and semantic features to train a bagging classifier used to decide the correct option. <eos> related to the different features we can highlight the resource isr-wn used to extract semantic relations among words and the use of different algorithms to establish semantic and lexical similarities. <eos> in order to establish which features are the most appropriate to improve sts results we participated with three runs using different set of features. <eos> our best run reached the position 44 in the official ranking, obtaining a general correlation coefficient of 0.61.
this paper deals with knowledge-based text processing which aims at an intuitive notion of textual similarity. <eos> entities and relations relevant for a particular domain are identified and disambiguated by means of semi-supervised machine learning techniques and resulting annotations are applied for computing typedsimilarity of individual texts. <eos> the work described in this paper particularly shows effects of the mentioned processes in the context of the *sem 2013 pilot task on typed-similarity, a part of the semantic textual similarity shared task. <eos> the goal is to evaluate the degree of semantic similarity between semi-structured records. <eos> as the evaluation dataset has been taken from europeana ? <eos> a collection of records on european cultural heritage objects ? <eos> we focus on computing a semantic distance on field author which has the highest potential to benefit from the domain knowledge. <eos> specific features that are employed in our system but-typed are briefly introduced together with a discussion on their efficient acquisition. <eos> support vector regression is then used to combine the features and to provide a final similarity score. <eos> the system ranked third on the attribute author among 15 submitted runs in the typed-similarity task.
this paper reports our submissions to the semantic textual similarity ( sts ) task in ? sem shared task 2013. <eos> we submitted three support vector regression ( svr ) systems in core task, using 6 types of similarity measures, i.e., string similarity, number similarity, knowledge-based similarity, corpus-based similarity, syntactic dependency similarity and machine translation similarity. <eos> our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs. <eos> we also submitted two systems in typed task using string based measure and named entity based measure. <eos> our best system ranks 5 out of 15 runs.
we approach the typed-similarity task using a range of heuristics that rely on information from the appropriate metadata fields for each type of similarity. <eos> in addition we train a linear regressor for each type of similarity. <eos> the results indicate that the linear regression is key for good performance. <eos> our best system was ranked third in the task.
in this paper we describe knce2013-core, a system to compute the semantic similarity of two short text snippets. <eos> the system computes a number of features which are gathered from different knowledge bases, namely wordnet, wikipedia and wiktionary. <eos> the similarity scores derived from these features are then fed into several multilayer perceptron neuronal networks. <eos> depending on the size of the text snippets different parameters for the neural networks are used. <eos> the final output of the neural networks is compared to human judged data. <eos> in the evaluation our system performed sufficiently well for text snippets of equal length, but the performance dropped considerably once the pairs of text snippets differ in size.
in this paper we discuss our participation to the 2013 semeval semantic textual similarity task. <eos> our core features include ( i ) a set of metrics borrowed from automatic machine translation, originally intended to evaluate automatic against reference translations and ( ii ) an instance of explicit semantic analysis, built upon opening paragraphs of wikipedia 2010 articles. <eos> our similarity estimator relies on a support vector regressor with rbf kernel. <eos> our best approach required 13 machine translation metrics + explicit semantic analysis and ranked 65 in the competition. <eos> our postcompetition analysis shows that the features have a good expression level, but overfitting and ? mainly ? <eos> normalization issues caused our correlation values to decrease.
the semantic textual similarity ( sts ) task examines semantic similarity at a sentencelevel. <eos> we explored three representations of semantics ( implicit or explicit ) : named entities, semantic vectors, and structured vectorial semantics. <eos> from a dkpro baseline, we also performed feature selection and used sourcespecific linear regression models to combine our features. <eos> our systems placed 5th, 6th, and 8th among 90 submitted systems.
in this year ? s semantic textual similarity evaluation, we explore the contribution of models that provide soft similarity scores across spans of multiple words, over the previous year ? s system. <eos> to this end, we explored the use of neural probabilistic language models and a tf-idf weighted variant of explicit semantic analysis. <eos> the neural language model systems used vector representations of individual words, where these vectors were derived by training them against the context of words encountered, and thus reflect the distributional characteristics of their usage. <eos> to generate a similarity score between spans, we experimented with using tiled vectors and restricted boltzmann machines to identify similar encodings. <eos> we find that these soft similarity methods generally outperformed our previous year ? s systems, albeit they did not perform as well in the overall rankings. <eos> a simple analysis of the soft similarity resources over two word phrases is provided, and future areas of improvement are described.
this paper describes the system used by the lipn team in the semantic textual similarity task at *sem 2013. <eos> it uses a support vector regression model, combining different text similarity measures that constitute the features. <eos> these measures include simple distances like levenshtein edit distance, cosine, named entities overlap and more complex distances like explicit semantic analysis, wordnet-based similarity, ir-based similarity, and a similarity measure based on syntactic dependencies.
this paper describes the uniba participation in the semantic textual similarity ( sts ) core task 2013. <eos> we exploited three different systems for computing the similarity between two texts. <eos> a system is used as baseline, which represents the best model emerged from our previous participation in sts 2012. <eos> such system is based on a distributional model of semantics capable of taking into account also syntactic structures that glue words together. <eos> in addition, we investigated the use of two different learning strategies exploiting both syntactic and semantic features. <eos> the former uses ensemble learning in order to combine the best machine learning techniques trained on 2012 training and test sets. <eos> the latter tries to overcome the limit of working with different datasets with varying characteristics by selecting only the more suitable dataset for the training purpose.
we present a system submitted in the semantic textual similarity ( sts ) task at the second joint conference on lexical and computational semantics ( *sem 2013 ). <eos> given two short text fragments, the goal of the system is to determine their semantic similarity. <eos> our system makes use of three different measures of text similarity : word n-gram overlap, character n-gram overlap and semantic overlap. <eos> using these measures as features, it trains a support vector regression model on semeval sts 2012 data. <eos> this model is then applied on the sts 2013 data to compute textual similarities. <eos> two different selections of training data result in very different performance levels : while a correlation of 0.4135 with gold standards was observed in the official evaluation ( ranked 63rd among all systems ) for one selection, the other resulted in a correlation of 0.5352 ( that would rank 21st ).
this paper describes our system entered for the *sem 2013 shared task on semantic textual similarity ( sts ). <eos> we focus on the core task of predicting the semantic textual similarity of sentence pairs. <eos> the current system utilizes machine learning techniques trained on semantic similarity ratings from the *sem 2012 shared task ; it achieved rank 20 out of 90 submissions from 35 different teams. <eos> given the simple nature of our approach, which uses only wordnet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications.
we present in this paper the systems we participated with in the semantic textual similarity task at sem 2013. <eos> the semantic textual similarity core task ( sts ) computes the degree of semantic equivalence between two sentences where the participant systems will be compared to the manual scores, which range from 5 ( semantic equivalence ) to 0 ( no relation ). <eos> we combined multiple text similarity measures of varying complexity. <eos> the experiments illustrate the different effect of four feature types including direct lexical matching, idf-weighted lexical matching, modified bleu n-gram matching and named entities matching. <eos> our team submitted three runs during the task evaluation period and they ranked number 11, 15 and 19 among the 90 participating systems according to the official mean pearson correlation metric for the task. <eos> we also report an unofficial run with mean pearson correlation of 0.59221 on sts2013 test dataset, ranking as the 3rd best system among the 90 participating systems.
soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity ( sts ), obtaining 3rd place in semeval-2012. <eos> at *sem-2013 shared task, beside the plain textoverlapping approach, we tested within soft cardinality two distributional word-similarity functions derived from the ukwack corpus. <eos> unfortunately, we combined these measures with other features using regression, obtaining positions 18th, 22nd and 23rd among the 90 participants systems in the official ranking. <eos> already after the release of the gold standard annotations of the test data, we observed that using only the similarity measures without combining them with other features would have obtained positions 6th, 7th and 8th ; moreover, an arithmetic average of these similarity measures would have been 4th ( mean=0.5747 ). <eos> this paper describes both the 3 systems as they were submitted and the similarity measures that would obtained those better results.
clac-core, an exhaustive feature combination system ranked 4th among 34 teams in the semantic textual similarity shared task sts 2013. <eos> using a core set of 11 lexical features of the most basic kind, it uses a support vector regressor which uses a combination of these lexical features to train a model for predicting similarity between sentences in a two phase method, which in turn uses all combinations of the features in the feature space and trains separate models based on each combination. <eos> then it creates a meta-feature space and trains a final model based on that. <eos> this two step process improves the results achieved by singlelayer standard learning methodology over the same simple features. <eos> we analyze the correlation of feature combinations with the data sets over which they are effective.
in this paper we present our systems for calculating the degree of semantic similarity between two texts that we submitted to the semantic textual similarity task at semeval2013. <eos> our systems predict similarity using a regression over features based on the following sources of information : string similarity, topic distributions of the texts based on latent dirichlet alocation, and similarity between the documents returned by an information retrieval engine when the target texts are used as queries. <eos> we also explore methods for integrating predictions using different training datasets and feature sets. <eos> our best system was ranked 17th out of 89 participating systems. <eos> in our post-task analysis, we identify simple changes to our system that further improve our results.
this paper describes the system that was submitted in the *sem 2013 semantic textual similarity shared task. <eos> the task aims to find the similarity score between a pair of sentences. <eos> we describe a universal networking language ( unl ) based semantic extraction system for measuring the semantic similarity. <eos> our approach combines syntactic and word level similarity measures along with the unl based semantic similarity measures for finding similarity scores between sentences.
this article provides a detailed overview of the cpn text-to-text similarity system that we participated with in the semantic textual similarity task evaluations hosted at *sem 2013. <eos> in addition to more traditional components, such as knowledge-based and corpus-based metrics leveraged in a machine learning framework, we also use opinion analysis features to achieve a stronger semantic representation of textual units. <eos> while the evaluation datasets are not designed to test the similarity of opinions, as a component of textual similarity, nonetheless, our system variations ranked number 38, 39 and 45 among the 88 participating systems.
this paper presents three methods to evaluate the semantic textual similarity ( sts ). <eos> the first two methods do not require labeled training data ; instead, they automatically extract semantic knowledge in the form of word associations from a given reference corpus. <eos> two kinds of word associations are considered : cooccurrence statistics and the similarity of word contexts. <eos> the third method was done in collaboration with groups from the universities of paris 13, matanzas and alicante. <eos> it uses several word similarity measures as features in order to construct an accurate prediction model for the sts.
we created a dataset of syntactic-ngrams ( counted dependency-tree fragments ) based on a corpus of 3.5 million english books. <eos> the dataset includes over 10 billion distinct items covering a wide range of syntactic configurations. <eos> it also includes temporal information, facilitating new kinds of research into lexical semantics over time. <eos> this paper describes the dataset, the syntactic representation, and the kinds of information provided.
we propose an unsupervised method for automatically calculating word usage similarity in social media data based on topic modelling, which we contrast with a baseline distributional method and weighted textual matrix factorization. <eos> we evaluate these methods against a novel dataset made up of human ratings over 550 twitter message pairs annotated for usage similarity for a set of 10 nouns. <eos> the results show that our topic modelling approach outperforms the other two methods.
this paper explores two hypotheses regarding vector space models that predict the compositionality of german noun-noun compounds : ( 1 ) against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. <eos> our overall best result is state-of-the-art, reaching spearman ? s ? <eos> = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. <eos> ( 2 ) while there are no significant differences in predicting compound ? modifier vs. compound ? head ratings on compositionality, we show that the modifier ( rather than the head ) properties predominantly influence the degree of compositionality of the compound.
automatic metaphor identification and interpretation in text have been traditionally considered as two separate tasks in natural language processing ( nlp ) and addressed individually within computational frameworks. <eos> however, cognitive evidence suggests that humans are likely to perform these two tasks simultaneously, as part of a holistic metaphor comprehension process. <eos> we present a novel method that performs metaphor identification through its interpretation, being the first one in nlp to combine the two tasks in one step. <eos> it outperforms the previous approaches to metaphor identification both in terms of accuracy and coverage, as well as providing an interpretation for each identified expression.
short answer questions for reading comprehension are a common task in foreign language learning. <eos> automatic short answer scoring is the task of automatically assessing the semantic content of a student ? s answer, marking it e.g. <eos> as correct or incorrect. <eos> while previous approaches mainly focused on comparing a learner answer to some reference answer provided by the teacher, we explore the use of the underlying reading texts as additional evidence for the classification. <eos> first, we conduct a corpus study targeting the links between sentences in reading texts for learners of german and answers to reading comprehension questions based on those texts. <eos> second, we use the reading text directly for classification, considering three different models : an answer-based classifier extended with textual features, a simple text-based classifier, and a model that combines the two according to confidence of the text-based classification. <eos> the most promising approach is the first one, results for which show that textual features improve classification accuracy. <eos> while the other two models do not improve classification accuracy, they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors.
social scientists are increasingly using the vast amount of text available on social media to measure variation in happiness and other psychological states. <eos> such studies count words deemed to be indicators of happiness and track how the word frequencies change across locations or time. <eos> this word count approach is simple and scalable, yet often picks up false signals, as words can appear in different contexts and take on different meanings. <eos> we characterize the types of errors that occur using the word count approach, and find lexical ambiguity to be the most prevalent. <eos> we then show that one can reduce error with a simple refinement to such lexica by automatically eliminating highly ambiguous words. <eos> the resulting refined lexica improve precision as measured by human judgments of word occurrences in facebook posts.
implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing. <eos> one reason for this lies in the scarce amount of annotated data sets available. <eos> we argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and to enable more in-depth studies of the phenomenon itself. <eos> in this paper, we present a range of studies that empirically validate this claim. <eos> our contributions are threefold : we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts ; we show how the induced data can be used as training data for improving existing argument linking models ; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references.
we present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling ( srl ) models. <eos> the setting is similar to co-training, except for the intermediate model required to convert the srl structure between the two annotation schemes used for different languages. <eos> our approach can facilitate the construction of srl models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it. <eos> we evaluate the model on four language pairs, english vs german, spanish, czech and chinese. <eos> consistent improvements are observed over the self-training baseline.
existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding databases. <eos> this paper introduces freeparser, a system that trains on one domain and one set of predicate and constant symbols, and then can parse sentences for any new domain, including sentences that refer to symbols never seen during training. <eos> freeparser uses a domain-independent architecture to automatically identify sentences relevant to each new database symbol, which it uses to supplement its manually-annotated training data from the training domain. <eos> in cross-domain experiments involving 23 domains, freeparser can parse sentences for which it has seen comparable unannotated sentences with an f1 of 0.71.
within the semeval-2013 evaluation exercise, the tempeval-3 shared task aims to advance research on temporal information processing. <eos> it follows on from tempeval-1 and -2, with : a three-part structure covering temporal expression, event, and temporal relation extraction ; a larger dataset ; and new single measures to rank systems ? <eos> in each task and in general. <eos> in this paper, we describe the participants ? <eos> approaches, results, and the observations from the results, which may guide future research in this area.
the cleartk-timeml submission to tempeval 2013 competed in all english tasks : identifying events, identifying times, and identifying temporal relations. <eos> the system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types. <eos> cleartktimeml ranked 1st for temporal relation f1, time extent strict f1 and event tense accuracy.
in this paper, we describe our participation in the tempeval-3 challenge. <eos> with our multilingual temporal tagger heideltime, we addressed task a, the extraction and normalization of temporal expressions for english and spanish. <eos> exploiting heideltime ? s strict separation between source code and languagedependent parts, we tuned heideltime ? s existing english resources and developed new spanish resources. <eos> for both languages, we achieved the best results among all participants for task a, the combination of extraction and normalization. <eos> both the improved english and the new spanish resources are publicly available with heideltime.
in this paper we present the results of experiments comparing ( a ) rich syntactic and semantic feature sets and ( b ) big context windows, for the tempeval time expression and event segmentation and classification tasks. <eos> we show that it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets.
this paper presents the second round of the task on cross-lingual textual entailment for content synchronization, organized within semeval-2013. <eos> the task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. <eos> participants were presented with datasets for different language pairs, where multi-directional entailment relations ( ? forward ?, ? backward ?, ? bidirectional ?, ? no entailment ? ) <eos> had to be identified. <eos> we report on the training and test data used for evaluation, the process of their creation, the participating systems ( six teams, 61 runs ), the approaches adopted and the results achieved.
in this paper we describe our system submitted for evaluation in the clte-semeval-2013 task, which achieved the best results in two of the four data sets, and finished third in average. <eos> this system consists of a svm classifier with features extracted from texts ( and their translations smt ) based on a cardinality function. <eos> such function was the soft cardinality. <eos> furthermore, this system was simplified by providing a single model for the 4 pairs of languages obtaining better ( unofficial ) results than separate models for each language pair. <eos> we also evaluated the use of additional circular-pivoting translations achieving results 6.14 % above the best official results.
this paper describes the semeval-2013 task 5 : ? evaluating phrasal semantics ?. <eos> its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length. <eos> the second one addresses deciding the compositionality of phrases in a given context. <eos> the paper discusses the importance and background of these subtasks and their structure. <eos> in succession, it introduces the systems that participated and discusses evaluation results.
this paper describes the approach of the hochschule hannover to the semeval 2013 task evaluating phrasal semantics. <eos> in order to compare a single word with a two word phrase we compute various distributional similarities, among which a new similarity measure, based on jensen-shannon divergence with a correction for frequency effects. <eos> the classification is done by a support vector machine that uses all similarities as features. <eos> the approach turned out to be the most successful one in the task.
this paper describes a temporal expression identification and normalization system, mantime, developed for the tempeval-3 challenge. <eos> the identification phase combines the use of conditional random fields along with a post-processing identification pipeline, whereas the normalization phase is carried out using norma, an open-source rule-based temporal normalizer. <eos> we investigate the performance variation with respect to different feature types. <eos> specifically, we show that the use of wordnet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. <eos> on the test data, the best run achieved 0.95 ( p ), 0.85 ( r ) and 0.90 ( f1 ) in the identification phase. <eos> normalization accuracies are 0.84 ( type attribute ) and 0.77 ( value attribute ). <eos> surprisingly, the use of the silver data ( alone or in addition to the gold annotated ones ) does not improve the performance.
we describe fss-timex, a module for the recognition and normalization of temporal expressions we submitted to task a and b of the tempeval-3 challenge. <eos> fss-timex was developed as part of a multilingual event extraction system, nexus, which runs on top of the emm news processing engine. <eos> it consists of finite-state rule cascades, using minimalistic text processing stages and simple heuristics to model the relations between events and temporal expressions. <eos> although fss-timex is already deployed within an ie application in the medical domain, we found it useful to customize its output to the timeml standard in order to have an independent performance measure and guide further developments.
in this paper, we present the jucse system, designed for the tempeval-3 shared task. <eos> the system extracts events and temporal information from natural text in english. <eos> we have participated in all the tasks of tempeval-3, namely task a, task b & task c. we have primarily utilized the conditional random field ( crf ) based machine learning technique, for all the above tasks. <eos> our system seems to perform quite competitively in task a and task b. <eos> in task c, the system ? s performance is comparatively modest at the initial stages of system development. <eos> we have incorporated various features based on different lexical, syntactic and semantic information, using stanford corenlp and wordnet based tools.
this paper describes a complete event/time ordering system that annotates raw text with events, times, and the ordering relations between them at the semeval-2013 task 1. <eos> task 1 is a unique challenge because it starts from raw text, rather than pre-annotated text with known events and times. <eos> a working system first identifies events and times, then identifies which events and times should be ordered, and finally labels the ordering relation between them. <eos> we present a split classifier approach that breaks the ordering tasks into smaller decision points. <eos> experiments show that more specialized classifiers perform better than few joint classifiers. <eos> the navytime system ranked second both overall and in most subtasks like event extraction and relation labeling.
we analyze the performance of sutime, a temporal tagger for recognizing and normalizing temporal expressions, on tempeval-3 task a for english. <eos> sutime is available as part of the stanford corenlp pipeline and can be used to annotate documents with temporal information. <eos> testing on the tempeval-3 evaluation corpus showed that this system is competitive with state-of-the-art techniques.
this paper describes a system for temporal processing of text, which participated in the temporal evaluations 2013 campaign. <eos> the system employs a number of machine learning classifiers to perform the core tasks of : identification of time expressions and events, recognition of their attributes, and estimation of temporal links between recognized events and times. <eos> the central feature of the proposed system is temporal parsing ? <eos> an approach which identifies temporal relation arguments ( eventevent and event-timex pairs ) and the semantic label of the relation as a single decision.
in this paper, we present a system, uttime, which we submitted to tempeval-3 for task c : annotating temporal relations. <eos> the system uses logistic regression classifiers and exploits features extracted from a deep syntactic parser, including paths between event words in phrase structure trees and their path lengths, and paths between event words in predicateargument structures and their subgraphs. <eos> uttime achieved an f1 score of 34.9 based on the graphed-based evaluation for task c ( ranked 2nd ) and 56.45 for task c-relationonly ( ranked 1st ) in the tempeval-3 evaluation.
this paper describes the specifications and results of umcc_dlsi- ( eps ) system, which participated in the first evaluating phrasal semantics of semeval-2013. <eos> our supervised system uses different kinds of semantic features to train a bagging classifier used to select the correct similarity option. <eos> related to the different features we can highlight the resource wordnet used to extract semantic relations among words and the use of different algorithms to establish semantic similarities. <eos> our system obtains promising results with a precision value around 78 % for the english corpus and 71.84 % for the italian corpus.
in this paper we present our system for the semeval 2013 task 5a on semantic similarity of words and compositional phrases. <eos> our system uses a dependency-based vector space model, in combination with a technique called latent vector weighting. <eos> the system computes the similarity between a particular noun instance and the head noun of a particular noun phrase, which was weighted according to the semantics of the modifier. <eos> the system is entirely unsupervised ; one single parameter, the similarity threshold, was tuned using the training data.
this paper describes the iirg 1 system entered in semeval-2013, the 7th international workshop on semantic evaluation. <eos> we participated in task 5 evaluating phrasal semantics. <eos> we have adopted a token-based approach to solve this task using 1 ) na ? <eos> ? ve bayes methods and 2 ) word overlap methods, both of which rely on the extraction of syntactic features. <eos> we found that the word overlap method significantly out-performs the na ? <eos> ? ve bayes methods, achieving our highest overall score with an accuracy of approximately 78 %.
the measurement of phrasal semantic relatedness is an important metric for many natural language processing applications. <eos> in this paper, we present three approaches for measuring phrasal semantics, one based on a semantic network model, another on a distributional similarity model, and a hybrid between the two. <eos> our hybrid approach achieved an fmeasure of 77.4 % on the task of evaluating the semantic similarity of words and compositional phrases.
in this paper we describe the system used to participate in the sub task 5b in the phrasal semantics challenge ( task 5 ) in semeval 2013. <eos> this sub task consists in discriminating literal and figurative usage of phrases with compositional and non-compositional meanings in context. <eos> the proposed approach is based on part-of-speech tags, stylistic features and distributional statistics gathered from the same development-training-test text collection. <eos> the system obtained a relative improvement in accuracy against the most-frequentclass baseline of 49.8 % in the ? unseen contexts ? <eos> ( lexsample ) setting and 8.5 % in ? unseen phrases ? <eos> ( allwords ).
this paper presents our approach used for cross-lingual textual entailment task ( task 8 ) organized within semeval 2013. <eos> crosslingual textual entailment ( clte ) tries to detect the entailment relationship between two text fragments in different languages. <eos> we solved this problem in three steps. <eos> firstly, we use a off-the-shelf machine translation ( mt ) tool to convert the two input texts into the same language. <eos> then after performing a text preprocessing, we extract multiple feature types with respect to surface text and grammar. <eos> we also propose novel feature types regarding to sentence difference and semantic similarity based on our observations in the preliminary experiments. <eos> finally, we adopt a multiclass svm algorithm for classification. <eos> the results on the cross-lingual data collections provided by semeval 2013 show that ( 1 ) we can build portable and effective systems across languages using mt and multiple effective features ; ( 2 ) our systems achieve the best results among the participants on two test datasets, i.e., fra-eng and deu-eng.
this paper describes the evaluation of different kinds of textual features for the crosslingual textual entailment task of semeval 2013. <eos> we have counted the number of n grams for three types of textual entities ( character, word and pos tags ) that exist in the pair of sentences from which we are interested in determining the judgment of textual entailment. <eos> difference, intersection and distance ( euclidian, manhattan and jaccard ) of n -grams were considered for constructing a feature vector which is further introduced in a support vector machine classifier which allows to construct a classification model. <eos> five different runs were submitted, one of them considering voting system of the previous four approaches. <eos> the results obtained show a performance below the median of six teams that have participated in the competition.
we present a supervised learning approach to cross-lingual textual entailment that explores statistical word alignment models to predict entailment relations between sentences written in different languages. <eos> our approach is language independent, and was used to participate in the clte task ( task # 8 ) organized within semeval 2013 ( negri et al, 2013 ). <eos> the four runs submitted, one for each language combination covered by the test data ( i.e. <eos> spanish/english, german/english, french/english and italian/english ), achieved encouraging results. <eos> in terms of accuracy, performance ranges from 38.8 % ( for german/english ) to 43.2 % ( for italian/english ). <eos> on the italian/english and spanish/english test sets our systems ranked second among five participants, close to the top results ( respectively 43.4 % and 45.4 % ).
this paper describes the university of melbourne nlp group submission to the crosslingual textual entailment shared task, our first tentative attempt at the task. <eos> the approach involves using parallel corpora and automatic word alignment to align text fragment pairs, and statistics based on unaligned words as features to classify items as forward and backward before a compositional combination into the final four classes, as well as experiments with additional string similarity features.
in this paper, we describe semeval-2013 task 4 : the definition, the data, the evaluation and the results. <eos> the task is to capture some of the meaning of english noun compounds via paraphrasing. <eos> given a two-word noun compound, the participating system is asked to produce an explicitly ranked list of its free-form paraphrases. <eos> the list is automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators, recruited and managed through amazon ? s mechanical turk. <eos> the comparison of raw paraphrases is sensitive to syntactic and morphological variation. <eos> the ? gold ? <eos> ranking is based on the relative popularity of paraphrases among annotators. <eos> to make the ranking more reliable, highly similar paraphrases are grouped, so as to downplay superficial differences in syntax and morphology. <eos> three systems participated in the task. <eos> they all beat a simple baseline on one of the two evaluation measures, but not on both measures. <eos> this shows that the task is difficult.
this paper describes the system submitted by the melodi team for the semeval-2013 task 4 : free paraphrases of noun compounds ( hendrickx et al, 2013 ). <eos> our approach combines the strength of an unsupervised distributional word space model with a supervised maximum-entropy classification model ; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classifier to induce a number of appropriate paraphrases.
this paper presents an approach for generating free paraphrases of compounds ( task 4 at semeval 2013 ) by decomposing the training data into a collection of templates and fillers and recombining/scoring these based on a generative language model and discriminative maxent reranking. <eos> the system described in this paper achieved the highest score ( with a very small margin ) in the ( default ) isomorphic setting of the scorer, for which it was optimized, at a disadvantage to the non-isomorphic score.
this paper presents a system for automatically generating a set of plausible paraphrases for a given noun compound and rank them in decreasing order of their usage represented by the confidence value provided by the human annotators. <eos> our system implements a corpusdriven probabilistic co-occurrence based model for predicting the paraphrases, that uses a seed list of paraphrases extracted from corpus to predict other paraphrases based on their co-occurrences. <eos> the corpus study reveals that the prepositional paraphrases for the noun compounds are quite frequent and well covered but the verb paraphrases, on the other hand, are scarce, revealing the unsuitability of the model for standalone corpus-driven approach. <eos> therefore, to predict other paraphrases, we adopt a two-fold approach : ( i ) prediction based on verb-verb cooccurrences, in case the seed paraphrases are greater than threshold ; and ( ii ) prediction based on semantic relation of nc, otherwise. <eos> the system achieves a comparabale score of 0.23 for the isomorphic system while maintaining a score of 0.26 for the non-isomorphic system.
the goal of the cross-lingual word sense disambiguation task is to evaluate the viability of multilingual wsd on a benchmark lexical sample data set. <eos> the traditional wsd task is transformed into a multilingual wsd task, where participants are asked to provide contextually correct translations of english ambiguous nouns into five target languages, viz. <eos> french, italian, english, german and dutch. <eos> we report results for the 12 official submissions from 5 different research teams, as well as for the parasense system that was developed by the task organizers.
this paper describes the xling system participation in semeval-2013 crosslingual word sense disambiguation task. <eos> the xling system introduces a novel approach to skip the sense disambiguation step by matching query sentences to sentences in a parallel corpus using topic models ; it returns the word alignments as the translation for the target polysemous words. <eos> although, the topic-model base matching underperformed, the matching approach showed potential in the simple cosine-based surface similarity matching.
we present our entries for the semeval2013 cross-language word-sense disambiguation task ( lefever and hoste, 2013 ). <eos> we submitted three systems based on classifiers trained on local context features, with some elaborations. <eos> our three systems, in increasing order of complexity, were : maximum entropy classifiers trained to predict the desired targetlanguage phrase using only monolingual features ( we called this system l1 ) ; similar classifiers, but with the desired target-language phrase for the other four languages as features ( l2 ) ; and lastly, networks of five classifiers, over which we do loopy belief propagation to solve the classification tasks jointly ( mrf ).
we describe the limsi system for the semeval-2013 cross-lingual word sense disambiguation ( clwsd ) task. <eos> word senses are represented by means of translation clusters in different languages built by a cross-lingual word sense induction ( wsi ) method. <eos> our clwsd classifier exploits the wsi output for selecting appropriate translations for target words in context. <eos> we present the design of the system and the obtained results.
we present our system wsd2 which participated in the cross-lingual word-sense disambiguation task for semeval 2013 ( lefever and hoste, 2013 ). <eos> the system closely resembles our winning system for the same task in semeval 2010. <eos> it is based on k-nearest neighbour classifiers which map words with local and global context features onto their translation, i.e. <eos> their cross-lingual sense. <eos> the system participated in the task for all five languages and obtained winning scores for four of them when asked to predict the best translation ( s ). <eos> we tested various configurations of our system, focusing on various levels of hyperparameter optimisation and feature selection. <eos> our final results indicate that hyperparameter optimisation did not lead to the best results, indicating overfitting by our optimisation method in this aspect. <eos> feature selection does have a modest positive impact.
this paper describes the nrc submission to the spanish cross-lingual word sense disambiguation task at semeval-2013. <eos> since this word sense disambiguation task uses spanish translations of english words as gold annotation, it can be cast as a machine translation problem. <eos> we therefore submitted the output of a standard phrase-based system as a baseline, and investigated ways to improve its sense disambiguation performance. <eos> using only local context information and no linguistic analysis beyond lemmatization, our machine translation system surprisingly yields top precision score based on the best predictions. <eos> however, its top 5 predictions are weaker than those from other systems.
in this paper we describe our semeval-2013 task on word sense induction and disambiguation within an end-user application, namely web search result clustering and diversification. <eos> given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query. <eos> the task enables the end-to-end evaluation and comparison of systems.
the duluth systems that participated in task 11 of semeval ? 2013 carried out word sense induction ( wsi ) in order to cluster web search results. <eos> they relied on an approach that represented web snippets using second ? order co ? <eos> occurrences. <eos> these systems were all implemented using senseclusters, a freely available open source software package.
the aim of this paper is to perform word sense induction ( wsi ) ; which clusters web search results and produces a diversified list of search results. <eos> it describes the wsi system developed for task 11 of semeval - 2013. <eos> this paper implements the idea of monotone submodular function optimization using greedy algorithm.
in this paper, we describe the ukp lab system participating in the semeval-2013 task ? word sense induction and disambiguation within an end-user application ?. <eos> our approach uses preprocessing, co-occurrence extraction, graph clustering, and a state-of-theart word sense disambiguation system. <eos> we developed a configurable pipeline which can be used to integrate and evaluate other components for the various steps of the complex task.
this paper describes our system for task 11 of semeval-2013. <eos> in the task, participants are provided with a set of ambiguous search queries and the snippets returned by a search engine, and are asked to associate senses with the snippets. <eos> the snippets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters. <eos> our system adopts a preexisting word sense induction ( wsi ) methodology based on hierarchical dirichlet process ( hdp ), a non-parametric topic model. <eos> our system is trained over extracts from the full text of english wikipedia, and is shown to perform well in the shared task.
this paper presents the semeval-2013 task on multilingual word sense disambiguation. <eos> we describe our experience in producing a multilingual sense-annotated corpus for the task. <eos> the corpus is tagged with babelnet 1.1.1, a freely-available multilingual encyclopedic dictionary and, as a byproduct, wordnet 3.0 and the wikipedia sense inventory. <eos> we present and analyze the results of participating systems, and discuss future directions.
this article presents the getalp system for the participation to semeval-2013 task 12, based on an adaptation of the lesk measure propagated through an ant colony algorithm, that yielded good results on the corpus of semeval 2007 task 7 ( wordnet 2.1 ) as well as the trial data for task 12 semeval 2013 ( babelnet 1.0 ). <eos> we approach the parameter estimation to our algorithm from two perspectives : edogenous estimation where we maximised the sum the local lesk scores ; exogenous estimation where we maximised the f1 score on trial data. <eos> we proposed three runs of out system, exogenous estimation with babelnet 1.1.1 synset id annotations, endogenous estimation with babelnet 1.1.1 synset id annotations and endogenous estimation with wordnet 3.1 sense keys. <eos> a bug in our implementation led to incorrect results and here, we present an amended version thereof. <eos> our system arrived third on this task and a more fine grained analysis of our results reveals that the algorithms performs best on general domain texts with as little named entities as possible. <eos> the presence of many named entities leads the performance of the system to plummet greatly.
this work introduces a new unsupervised approach to multilingual word sense disambiguation. <eos> its main purpose is to automatically choose the intended sense ( meaning ) of a word in a particular context for different languages. <eos> it does so by selecting the correct babel synset for the word and the various wiki page titles that mention the word. <eos> babelnet contains all the output information that our system needs, in its babel synset. <eos> through babel synset, we find all the possible synsets for the word in wordnet. <eos> using these synsets, we apply the disambiguation method ppr+freq to find what we need. <eos> to facilitate the work with wordnet, we use the isr-wn which offers the integration of different resources to wordnet. <eos> our system, recognized as the best in the competition, obtains results around 69 % of recall.
we introduce peripheral diversity ( pd ) as a knowledge-based approach to achieve multilingual word sense disambiguation ( wsd ). <eos> pd exploits the frequency and diverse use of word senses in semantic subgraphs derived from larger sense inventories such as babelnet, wikipedia, and wordnet in order to achieve wsd. <eos> pd ? s f -measure scores for semeval 2013 task 12 outperform the most frequent sense ( mfs ) baseline for two of the five languages : english, french, german, italian, and spanish. <eos> despite pd remaining under-developed and under-explored, it demonstrates that it is robust, competitive, and encourages development.
many nlp applications require information about locations of objects referenced in text, or relations between them in space. <eos> for example, the phrase a book on the desk contains information about the location of the object book, as trajector, with respect to another object desk, as landmark. <eos> spatial role labeling ( sprl ) is an evaluation task in the information extraction domain which sets a goal to automatically process text and identify objects of spatial scenes and relations between them. <eos> this paper describes the task in semantic evaluations 2013, annotation schema, corpora, participants, methods and results obtained by the participants.
we present the results of the joint student response analysis and 8th recognizing textual entailment challenge, aiming to bring together researchers in educational nlp technology and textual entailment. <eos> the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. <eos> thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way rte-style tasks on educational data. <eos> in addition, a partial entailment task was piloted. <eos> we present and compare results from 9 participating teams, and discuss future directions.
automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data ( i.e., human-scored responses ) may or may not be available due to the variety of possible questions and topics. <eos> as such, it seems desirable to integrate various approaches, making use of model answers from experts ( e.g., to give higher scores to responses that are similar ), prescored student responses ( e.g., to learn direct associations between particular phrases and scores ), etc. <eos> here, we describe a system that uses stacking ( wolpert, 1992 ) and domain adaptation ( daume iii, 2007 ) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures ( heilman and madnani, 2012 ). <eos> we report encouraging results from the joint student response analysis and 8th recognizing textual entailment challenge.
in this paper we describe our system used to participate in the student-response-analysis task-7 at semeval 2013. <eos> this system is based on text overlap through the soft cardinality and a new mechanism for weight propagation. <eos> although there are several official performance measures, taking into account the overall accuracy throughout the two availabe data sets ( beetle and scientsbank ), our system ranked first in the 2 way classification task and second in the others. <eos> furthermore, our system performs particularly well with ? unseendomains ? <eos> instances, which was the more challenging test set. <eos> this paper also describes another system that integrates this method with the lexical-overlap baseline provided by the task organizers obtaining better results than the best official results. <eos> we concluded that the soft cardinality method is a very competitive baseline for the automatic evaluation of student responses.
our system combines text similarity measures with a textual entailment system. <eos> in the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. <eos> we also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline.
most work on word sense disambiguation has assumed that word usages are best labeled with a single sense. <eos> however, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage. <eos> we present a new semeval task for evaluating word sense induction and disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability. <eos> four teams submitted nine systems, which were evaluated in two settings.
word sense induction aims to discover different senses of a word from a corpus by using unsupervised learning approaches. <eos> once a sense inventory is obtained for an ambiguous word, word sense discrimination approaches choose the best-fitting single sense for a given context from the induced sense inventory. <eos> however, there may not be a clear distinction between one sense and another, although for a context, more than one induced sense can be suitable. <eos> graded word sense method allows for labeling a word in more than one sense. <eos> in contrast to the most common approach which is to apply clustering or graph partitioning on a representation of first or second order co-occurrences of a word, we propose a system that creates a substitute vector for each target word from the most likely substitutes suggested by a statistical language model. <eos> word samples are then taken according to probabilities of these substitutes and the results of the co-occurrence model are clustered. <eos> this approach outperforms the other systems on graded word sense induction task in semeval-2013.
this paper describes our system for shared task 13 ? word sense induction for graded and non-graded senses ? <eos> of semeval-2013. <eos> the task is on word sense induction ( wsi ), and builds on earlier semeval wsi tasks in exploring the possibility of multiple senses being compatible to varying degrees with a single contextual instance : participants are asked to grade senses rather than selecting a single sense like most word sense disambiguation ( wsd ) settings. <eos> the evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance. <eos> we adopt a previously-proposed wsi methodology for the task, which is based on a hierarchical dirichlet process ( hdp ), a nonparametric topic model. <eos> our system requires no parameter tuning, uses the english ukwac as an external resource, and achieves encouraging results over the shared task.
in recent years, sentiment analysis in social media has attracted a lot of research interest and has been used for a number of applications. <eos> unfortunately, research has been hindered by the lack of suitable datasets, complicating the comparison between approaches. <eos> to address this issue, we have proposed semeval-2013 task 2 : sentiment analysis in twitter, which included two subtasks : a, an expression-level subtask, and b, a messagelevel subtask. <eos> we used crowdsourcing on amazon mechanical turk to label a large twitter training dataset alng with additional test sets of twitter and sms messages for both subtasks. <eos> all datasets used in the evaluation are released to the research community. <eos> the task attracted significant interest and a total of 149 submissions from 44 teams. <eos> the bestperforming team achieved an f1 of 88.9 % and 69 % for subtasks a and b, respectively.
in this paper, we describe how we created two state-of-the-art svm classifiers, one to detect the sentiment of messages such as tweets and sms ( message-level task ) and one to detect the sentiment of a term within a message ( term-level task ). <eos> among submissions from 44 teams in a competition, our submissions stood first in both tasks on tweets, obtaining an f-score of 69.02 in the message-level task and 88.93 in the term-level task. <eos> we implemented a variety of surface-form, semantic, and sentiment features. <eos> we also generated two large word ? sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons. <eos> in the message-level task, the lexicon-based features provided a gain of 5 f-score points over all others. <eos> both of our systems can be replicated using freely available resources.1
this paper describes the details of our system submitted to the semeval-2013 shared task on sentiment analysis in twitter. <eos> our approach to predicting the sentiment of tweets and sms is based on supervised machine learning techniques and task-specific feature engineering. <eos> we used a linear classifier trained by stochastic gradient descent with hinge loss and elastic net regularization to make our predictions, which were ranked first or second in three of the four experimental conditions of the shared task. <eos> furthermore, our system makes use of social media specific text preprocessing and linguistically motivated features, such as word stems, word clusters and negation handling.
this paper describes the systems submitted by avaya labs ( avaya ) to semeval-2013 task 2 - sentiment analysis in twitter. <eos> for the constrained conditions of both the message polarity classification and contextual polarity disambiguation subtasks, our approach centers on training high-dimensional, linear classifiers with a combination of lexical and syntactic features. <eos> the constrained message polarity model is then used to tag nearly half a million unlabeled tweets. <eos> these automatically labeled data are used for two purposes : 1 ) to discover prior polarities of words and 2 ) to provide additional training examples for self-training. <eos> our systems performed competitively, placing in the top five for all subtasks and data conditions. <eos> more importantly, these results show that expanding the polarity lexicon and augmenting the training data with unlabeled tweets can yield improvements in precision and recall in classifying the polarity of non-neutral messages and contexts.
the ddiextraction 2013 task concerns the recognition of drugs and extraction of drugdrug interactions that appear in biomedical literature. <eos> we propose two subtasks for the ddiextraction 2013 shared task challenge : 1 ) the recognition and classification of drug names and 2 ) the extraction and classification of their interactions. <eos> both subtasks have been very successful in participation and results. <eos> there were 14 teams who submitted a total of 38 runs. <eos> the best result reported for the first subtask was f1 of 71.5 % and 65.1 % for the second one.
this paper presents the multi-phase relation extraction ( re ) approach which was used for the ddi extraction task of semeval 2013. <eos> as a preliminary step, the proposed approach indirectly ( and automatically ) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data. <eos> then, a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not filtered out by the previous step. <eos> the official results of the task show that our approach yields an f-score of 0.80 for ddi detection and an f-score of 0.65 for ddi detection and classification. <eos> our system obtained significantly higher results than all the other participating teams in this shared task and has been ranked 1st.
named entity recognition ( ner ) systems are often based on machine learning techniques to reduce the labor-intensive development of hand-crafted extraction rules and domain-dependent dictionaries. <eos> nevertheless, time-consuming feature engineering is often needed to achieve state-of-the-art performance. <eos> in this study, we investigate the impact of such domain-specific features on the performance of recognizing and classifying mentions of pharmacological substances. <eos> we compare the performance of a system based on general features, which have been successfully applied to a wide range of ner tasks, with a system that additionally uses features generated from the output of an existing chemical ner tool and a collection of domain-specific resources. <eos> we demonstrate that acceptable results can be achieved with the former system. <eos> still, our experiments show that using domain-specific features outperforms this general approach. <eos> our system ranked first in the semeval-2013 task 9.1 : recognition and classification of pharmacological substances.
in this paper, we describe our system that participated in semeval-2013, task 2.b ( sentiment analysis in twitter ). <eos> our approach consists of adapting naive bayes probabilities in order to take into account prior knowledge ( represented in the form of a sentiment lexicon ). <eos> we propose two different methods to efficiently incorporate prior knowledge. <eos> we show that our approach outperforms the classical naive bayes method and shows competitive results with svm while having less computational complexity.
in this paper, the unitor system participating in the semeval-2013 sentiment analysis in twitter task is presented. <eos> the polarity detection of a tweet is modeled as a classification task, tackled through a multiple kernel approach. <eos> it allows to combine the contribution of complex kernel functions, such as the latent semantic kernel and smoothed partial tree kernel, to implicitly integrate syntactic and lexical information of annotated examples. <eos> in the challenge, unitor system achieves good results, even considering that no manual feature engineering is performed and no manually coded resources are employed. <eos> these kernels in-fact embed distributional models of lexical semantics to determine expressive generalization of tweets.
this paper presents our system, tjp, which participated in semeval 2013 task 2 part a : contextual polarity disambiguation. <eos> the goal of this task is to predict whether marked contexts are positive, neutral or negative. <eos> however, only the scores of positive and negative class will be used to calculate the evaluation result using f-score. <eos> we chose to work as ? constrained ?, which used only the provided training and development data without additional sentiment annotated resources. <eos> our approach considered unigram, bigram and trigram using na ? ve bayes training model with the objective of establishing a simpleapproach baseline. <eos> our system achieved fscore 81.23 % and f-score 78.16 % in the results for sms messages and tweets respectively.
we present two systems developed at the university of ottawa for the semeval 2013 task 2. <eos> the first system ( for task a ) classifies the polarity / sentiment orientation of one target word in a twitter message. <eos> the second system ( for task b ) classifies the polarity of whole twitter messages. <eos> our two systems are very simple, based on supervised classifiers with bag-ofwords feature representation, enriched with information from several sources. <eos> we present a few additional results, besides results of the submitted runs.
this paper describes our system for participating semeval2013 task2-b ( kozareva et al, 2013 ) : sentiment analysis in twitter. <eos> given a message, our system classifies whether the message is positive, negative or neutral sentiment. <eos> it uses a co-occurrence rate model. <eos> the training data are constrained to the data provided by the task organizers ( no other tweet data are used ). <eos> we consider 9 types of features and use a subset of them in our submitted system. <eos> to see the contribution of each type of features, we do experimental study on features by leaving one type of features out each time. <eos> results suggest that unigrams are the most important features, bigrams and pos tags seem not helpful, and stopwords should be retained to achieve the best results. <eos> the overall results of our system are promising regarding the constrained features and data we use.
this paper describes a dual-classifier approach to contextual sentiment analysis at the semeval-2013 task 2. <eos> contextual analysis of polarity focuses on a word or phrase, rather than the broader task of identifying the sentiment of an entire text. <eos> the task 2 definition includes target word spans that range in size from a single word to entire sentences. <eos> however, the context of a single word is dependent on the word ? s surrounding syntax, while a phrase contains most of the polarity within itself. <eos> we thus describe separate treatment with two independent classifiers, outperforming the accuracy of a single classifier. <eos> our system ranked 6th out of 19 teams on sms message classification, and 8th of 23 on twitter data. <eos> we also show a surprising result that a very small amount of word context is needed for high-performance polarity extraction.
this paper describes our approach to the semeval-2013 task on ? sentiment analysis in twitter ?. <eos> we use simple bag-of-words models, a freely available sentiment dictionary automatically extended with distributionally similar terms, as well as lists of emoticons and internet slang abbreviations in conjunction with fast and robust machine learning algorithms. <eos> the resulting system is resource-lean, making it relatively independent of a specific language. <eos> despite its simplicity, the system achieves competitive accuracies of 0.70 ? 0.72 in detecting the sentiment of text messages. <eos> we also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message.
this paper describes the participation of the sinai research group in the 2013 edition of the international workshop semeval. <eos> the sinai research group has submitted two systems, which cover the two main approaches in the field of sentiment analysis : supervised and unsupervised.
this paper briefly reports our submissions to the two subtasks of semantic analysis in twitter task in semeval 2013 ( task 2 ), i.e., the contextual polarity disambiguation task ( an expression-level task ) and the message polarity classification task ( a message-level task ). <eos> we extract features from surface information of tweets, i.e., content features, microblogging features, emoticons, punctuation and sentiment lexicon, and adopt svm to build classifier. <eos> for subtask a, our system on twitter data ranks 2 on unconstrained rank and on sms data ranks 1 on unconstrained rank.
this paper presents the contribution of our team at task 2 of semeval 2013 : sentiment analysis in twitter. <eos> we submitted a constrained run for each of the two subtasks. <eos> in the contextual polarity disambiguation subtask, we use a sentiment lexicon approach combined with polarity shift detection and tree kernel based classifiers. <eos> in the message polarity classification subtask, we focus on the influence of domain information on sentiment classification.
this paper is an overview of the swatcs system submitted to semeval-2013 task 2a : contextual polarity disambiguation. <eos> the sentiment of individual phrases within a tweet are labeled using a combination of classifiers trained on a range of lexical features. <eos> the classifiers are combined by estimating the accuracy of the classifiers on each tweet. <eos> performance is measured when using only the provided training data, and separately when including external data.
the paper describes experiments using grid searches over various combinations of machine learning algorithms, features and preprocessing strategies in order to produce the optimal systems for sentiment classification of microblog messages. <eos> the approach is fairly domain independent, as demonstrated by the systems achieving quite competitive results when applied to short text message data, i.e., input they were not originally trained on.
this paper describes our submission for semeval2013 task 2 : sentiment analysis in twitter. <eos> for the limited data condition we use a lexicon-based model. <eos> the model uses an affective lexicon automatically generated from a very large corpus of raw web data. <eos> statistics are calculated over the word and bigram affective ratings and used as features of a naive bayes tree model. <eos> for the unconstrained data scenario we combine the lexicon-based model with a classifier built on maximum entropy language models and trained on a large external dataset. <eos> the two models are fused at the posterior level to produce a final output. <eos> the approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features.
in this paper, we describe the development and performance of the supervised system umcc_dlsi- ( sa ). <eos> this system uses corpora where phrases are annotated as positive, negative, objective, and neutral, to achieve new sentiment resources involving word dictionaries with their associated polarity. <eos> as a result, new sentiment inventories are obtained and applied in conjunction with detected informal patterns, to tackle the challenges posted in task 2b of the semeval2013 competition. <eos> assessing the effectiveness of our application in sentiment classification, we obtained a 69 % f-measure for neutral and an average of 43 % f-measure for positive and negative using tweets and sms messages.
this paper describes university of leipzig ? s approach to semeval-2013 task 2b on sentiment analysis in twitter : message polarity classification. <eos> our system is designed to function as a baseline, to see what we can accomplish with well-understood and purely data-driven lexical features, simple generalizations as well as standard machine learning techniques : we use one-against-one support vector machines with asymmetric cost factors and linear ? kernels ? <eos> as classifiers, word uni- and bigrams as features and additionally model negation of word uni- and bigrams in word n-gram feature space. <eos> we consider generalizations of urls, user names, hash tags, repeated characters and expressions of laughter. <eos> our method ranks 23 out of all 48 participating systems, achieving an averaged ( positive, negative ) f-score of 0.5456 and an averaged ( positive, negative, neutral ) f-score of 0.595, which is above median and average.
sentiment analysis in twitter has become an important task due to the huge user-generated content published over such media. <eos> such analysis could be useful for many domains such as marketing, finance, politics, and social. <eos> we propose to use many features in order to improve a trained classifier of twitter messages ; these features extend the feature vector of uni-gram model by the concepts extracted from dbpedia, the verb groups and the similar adjectives extracted from wordnet, the sentifeatures extracted using sentiwordnet and some useful domain specific features. <eos> we also built a dictionary for emotion icons, abbreviation and slang words in tweets which is useful before extending the tweets with different features. <eos> adding these features has improved the f-measure accuracy 2 % with svm and 4 % with naivebayes.
the fast development of social media made it possible for people to no loger remain mere spectators to the events that happen in the world, but become part of them, commenting on their developments and the entities involved, sharing their opinions and distributing related content. <eos> this phenomenon is of high importance to news monitoring systems, whose aim is to obtain an informative snapshot of media events and related comments. <eos> this paper presents the strategies employed in the optwima participation to semeval 2013 task 2-sentiment analysis in twitter. <eos> the main goal was to evaluate the best settings for a sentiment analysis component to be added to the online news monitoring system. <eos> we describe the approaches used in the competition and the additional experiments performed combining different datasets for training, using or not slang replacement and generalizing sentiment-bearing terms by replacing them with unique labels. <eos> the results regarding tweet classification are promising and show that sentiment generalization can be an effective approach for tweets and that sms language is difficult to tackle, even when specific normalization resources are employed.
this paper presents the tweetsted system implemented for the semeval 2013 task on sentiment analysis in twitter. <eos> in particular, we participated in task b on message polarity classification in the constrained setting. <eos> the approach is based on the exploitation of various resources such as sentiwordnet and liwc. <eos> official results show that our approach yields a f-score of 0.5976 for twitter messages ( 11th out of 35 ) and a f-score of 0.5487 for sms messages ( 8th out of 28 participants ).
sentiment analysis refers to automatically extracting the sentiment present in a given natural language text. <eos> we present our participation to the semeval2013 competition, in the sentiment analysis of twitter and sms messages. <eos> our approach for this task is the combination of two sentiment analysis subsystems which are combined together to build the final system. <eos> both subsystems use supervised learning using features based on various polarity lexicons.
we present a supervised sentiment detection system that classifies the polarity of subjective phrases as positive, negative, or neutral. <eos> it is tailored towards online genres, specifically twitter, through the inclusion of dictionaries developed to capture vocabulary used in online conversations ( e.g., slang and emoticons ) as well as stylistic features common to social media. <eos> we show how to incorporate these new features within a state of the art system and evaluate it on subtask a in semeval-2013 task 2 : sentiment analysis in twitter.
this paper describes the system implemented by fundacio ? <eos> barcelona media ( fbm ) for classifying the polarity of opinion expressions in tweets and smss, and which is supported by a uima pipeline for rich linguistic and sentiment annotations. <eos> fbm participated in the semeval 2013 task 2 on polarity classification. <eos> it ranked 5th in task a ( constrained track ) using an ensemble system combining ml algorithms with dictionary-based heuristics, and 7th ( task b, constrained ) using an svm classifier with features derived from the linguistic annotations and some heuristics.
we evaluate a naive machine learning approach to sentiment classification focused on twitter in the context of the sentiment analysis task of semeval-2013. <eos> we employ a classifier based on the random forests algorithm to determine whether a tweet expresses overall positive, negative or neutral sentiment. <eos> the classifier was trained only with the provided dataset and uses as main features word vectors and lexicon word counts. <eos> our average f-score for all three classes on the twitter evaluation dataset was 51.55 %. <eos> the average f-score of both positive and negative classes was 45.01 %. <eos> for the optional sms evaluation dataset our overall average f-score was 58.82 %. <eos> the average between positive and negative fscores was 50.11 %.
this paper describes the specifications and results of ssa-uo, unsupervised system, presented in semeval 2013 for sentiment analysis in twitter ( task 2 ) ( wilson et al, 2013 ). <eos> the proposal system includes three phases : data preprocessing, contextual word polarity detection and message classification. <eos> the preprocessing phase comprises treatment of emoticon, slang terms, lemmatization and pos-tagging. <eos> word polarity detection is carried out taking into account the sentiment associated with the context in which it appears. <eos> for this, we use a new contextual sentiment classification method based on coarse-grained word sense disambiguation, using wordnet ( miller, 1995 ) and a coarse-grained sense inventory ( sentiment inventory ) built up from sentiwordnet ( baccianella et al, 2010 ). <eos> finally, the overall sentiment is determined using a rule-based classifier. <eos> as it may be observed, the results obtained for twitter and sms sentiment classification are good considering that our proposal is unsupervised.
this article describes a sentiment analysis ( sa ) system named senti.ue-en, built for participation in semeval-2013 task 2, a twitter sa challenge. <eos> in both challenge subtasks we used the same supervised machine learning approach, including two classifiers in pipeline, with 22 semantic oriented features, such as polarized term presence and index, and negation presence. <eos> our system achieved a better score on task a ( 0.7413 ) than in the task b ( 0.4785 ). <eos> in the first subtask, there is a better result for sms than the obtained for the more trained type of data, the tweets.
for semeval-2013 task 2, a and b ( sentiment analysis in twitter ), we use a rulebased pattern matching system that is based on an existing ? domain independent ? <eos> sentiment taxonomy for english, essentially a highly phrasal sentiment lexicon. <eos> we have made some modifications to our set of rules, based on what we found in the annotated training data that was made available for the task. <eos> the resulting system scores competitively, especially on task b.
this paper briefly reports our system for the semeval-2013 task 2 : sentiment analysis in twitter. <eos> we first used an svm classifier with a wide range of features, including bag of word features ( unigram, bigram ), pos features, stylistic features, readability scores and other statistics of the tweet being analyzed, domain names, abbreviations, emoticons in the twitter text. <eos> then we investigated the effectiveness of these features. <eos> we also used character n-gram language models to address the problem of high lexical variation in twitter text and combined the two approaches to obtain the final results. <eos> our system is robust and achieves good performance on the twitter test data as well as the sms test data.
in this paper, we describe our system for the semeval-2013 task 2, sentiment analysis in twitter. <eos> we formed features that take into account the context of the expression and take a supervised approach towards subjectivity and polarity classification. <eos> experiments were performed on the features to find out whether they were more suited for subjectivity or polarity classification. <eos> we tested our model for sentiment polarity classification on twitter as well as sms chat expressions, analyzed their f-measure scores and drew some interesting conclusions from them.
this paper describes an expression-level sentiment detection system that participated in the subtask a of semeval-2013 task 2 : sentiment analysis in twitter. <eos> our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive, negative or neutral. <eos> the proposed approach helps to understand the relevant features that contribute most in this classification task.
in this paper, we describe our system submitted for the sentiment analysis task at semeval 2013 ( task 2 ). <eos> we implemented a combination of explicit semantic analysis ( esa ) with naive bayes classifier. <eos> esa represents text as a high dimensional vector of explicitly defined topics, following the distributional semantic model. <eos> this approach is novel in the sense that esa has not been used for sentiment analysis in the literature, to the best of our knowledge.
using labeled twitter training data from semeval-2013, we train both a subjectivity classifier and a polarity classifier separately, and then combine the two into a single hierarchical classifier. <eos> using additional unlabeled data that is believed to contain sentiment, we allow the polarity classifier to continue learning using self-training. <eos> the resulting system is capable of classifying a document as neutral, positive, or negative with an overall accuracy of 61.2 % using our hierarchical naive bayes classifier.1
this paper describes the system developed by the serendio team for the semeval-2013 task 2 competition ( task a ). <eos> we use a lexicon based approach for discovering sentiments. <eos> our lexicon is built from the serendio taxonomy. <eos> the serendio taxonomy consists of positive, negative, negation, stop words and phrases. <eos> a typical tweet contains word variations, emoticons, hashtags etc. <eos> we use preprocessing steps such as stemming, emoticon detection and normalization, exaggerated word shortening and hashtag detection. <eos> after the preprocessing, the lexicon-based system classifies the tweets as positive or negative based on the contextual sentiment orientation of the words. <eos> our system yields an f-score of 0.8004 on the test dataset.
in this paper we introduce our contribution to the semeval-2013 task 2 on ? sentiment analysis in twitter ?. <eos> we participated in ? task b ?, where the objective was to build models which classify tweets into three classes ( positive, negative or neutral ) by their contents. <eos> to solve this problem we basically followed the supervised learning approach and proposed several domain ( i.e. <eos> microblog ) specific improvements including text preprocessing and feature engineering. <eos> beyond the supervised setting we also introduce some early results employing a huge, automatically annotated tweet dataset.
the widespread use of twitter makes it very interesting to determine the opinions and the sentiments expressed by its users. <eos> the shortness of the length and the highly informal nature of tweets render it very difficult to automatically detect such information. <eos> this paper reports the results to a challenge, set forth by semeval-2013 task 2, to determine the positive, neutral, or negative sentiments of tweets. <eos> two systems are explained : system a for determining the sentiment of a phrase within a tweet and system b for determining the sentiment of a tweet. <eos> both approaches rely on rich feature sets, which are explained in detail.
this paper describes the systems with which we participated in the task sentiment analysis in twitter of semeval 2013 and specifically the message polarity classification. <eos> we used a 2-stage pipeline approach employing a linear svm classifier at each stage and several features including bow features, pos based features and lexicon based features. <eos> we have also experimented with naive bayes classifiers trained with bow features.
this paper describes the nilc usp system that participated in semeval-2013 task 2 : sentiment analysis in twitter. <eos> our system adopts a hybrid classification process that uses three classification approaches : rulebased, lexicon-based and machine learning approaches. <eos> we suggest a pipeline architecture that extracts the best characteristics from each classifier. <eos> our system achieved an fscore of 56.31 % in the twitter message-level subtask.
in this paper the unitor-hmm-tk system participating in the spatial role labeling task at semeval 2013 is presented. <eos> the spatial roles classification is addressed as a sequence-based word classification problem : the svmhmm learning algorithm is applied, based on a simple feature modeling and a robust lexical generalization achieved through a distributional model of lexical semantics. <eos> in the identification of spatial relations, roles are combined to generate candidate relations, later verified by a svm classifier. <eos> the smoothed partial tree kernel is applied, i.e. <eos> a convolution kernel that enhances both syntactic and lexical properties of the examples, avoiding the need of a manual feature engineering phase. <eos> finally, results on three of the five tasks of the challenge are reported.
we present a 5-way supervised system based on syntactic-semantic similarity features. <eos> the model deploys : text overlap measures, wordnet-based lexical similarities, graphbased similarities, corpus-based similarities, syntactic structure overlap and predicateargument overlap measures. <eos> these measures are applied to question, reference answer and student answer triplets. <eos> we take into account the negation in the syntactic and predicateargument overlap measures. <eos> our system uses the domain-specific data as one dataset to build a robust system. <eos> the results show that our system is above the median and mean on all the evaluation scenarios of the semeval2013 task # 7.
this paper presents celi ? s participation in the semeval the joint student response analysis and 8th recognizing textual entailment challenge ( task7 ) and cross-lingual textual entailment for content synchronization task ( task 8 ). <eos>
in this paper, we describe a method for assessing student answers, modeled as a paraphrase identification problem, based on substitution by basic english variants. <eos> basic english paraphrases are acquired from the simple english wiktionary. <eos> substitutions are applied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary. <eos> the evaluation of our approach on the semeval 2013 joint student response analysis and 8th recognizing textual entailment challenge data shows promising results, and this work is a first step toward an opendomain system able to exhibit deep text understanding capabilities.
assessing student understanding by evaluating their free text answers to posed questions is a very important task. <eos> however, manually, it is time-consuming and computationally, it is difficult. <eos> this paper details our shallow nlp approach to computationally assessing student free text answers when a reference answer is provided. <eos> for four out of the five test sets, our system achieved an overall accuracy above the median and mean.
this paper describes the comet system, our contribution to the semeval 2013 task 7 challenge, focusing on the task of automatically assessing student answers to factual questions. <eos> comet is based on a meta-classifier that uses the outputs of the sub-systems we developed : comic, cosec, and three shallower bag approaches. <eos> we sketch the functionality of all sub-systems and evaluate their performance against the official test set of the challenge. <eos> comet obtained the best result ( 73.1 % accuracy ) for the 3-way unseen answers in beetle among all challenge participants. <eos> we also discuss possible improvements and directions for future research.
the domain of ddi identification is constantly showing a rise of interest from scientific community since it represents a decrease of time and healthcare cost. <eos> in this paper we purpose a new approach based on shallow linguistic kernel methods to identify ddis in biomedical manuscripts. <eos> the approach outlines a first step in the usage of semantic information for ddi identification. <eos> the system obtained an f1 measure of 0.534.
drug name entity recognition focuses on identifying concepts appearing in the text that correspond to a chemical substance used in pharmacology for treatment, cure, prevention or diagnosis of diseases. <eos> this paper describes a system based on ontologies for identifying the chemical substances in biomedical text. <eos> the system achieves an f-1 measure of 0.529 in the task.
this work describes the participation of the wbi-ddi team on the semeval 2013 ? <eos> task 9.2 ddi extraction challenge. <eos> the task consisted of extracting interactions between pairs of drugs from two collections of documents ( drugbank and medline ) and their classification into four subtypes : advise, effect, mechanism, and int. <eos> we developed a two-step approach in which pairs are initially extracted using ensembles of up to five different classifiers and then relabeled to one of the four categories. <eos> our approach achieved the second rank in the ddi competition. <eos> for interaction detection we achieved f1 measures ranging from 73 % to almost 76 % depending on the run. <eos> these results are on par or even higher than the performance estimation on the training dataset. <eos> when considering the four interaction subtypes we achieved an f1 measure of 60.9 %.
a drug-drug interaction ( ddi ) occurs when one drug affects the level or activity of another drug. <eos> semeval 2013 ddi extraction challenge is going to be held with the aim of identifying the state of the art relation extraction algorithms. <eos> in this paper we firstly review some of the existing approaches in relation extraction generally and biomedical relations especially. <eos> and secondly we will explain our svm based approaches that use lexical, morphosyntactic and parse tree features. <eos> our combination of sequence and tree kernels have shown promising performance with a best result of 0.54 f1 macroaverage on the test dataset.
the ddiextraction 2013 task in the semeval conference concerns the detection of drug names and statements of drug-drug interactions ( ddi ) from text. <eos> extraction of ddis is important for providing up-to-date knowledge on adverse interactions between coadministered drugs. <eos> we apply the machine learning based turku event extraction system to both tasks. <eos> we evaluate three feature sets, syntactic features derived from deep parsing, enhanced optionally with features derived from drugbank or from both drugbank and metamap. <eos> tees achieves f-scores of 60 % for the drug name recognition task and 59 % for the ddi extraction task.
for participating in the semeval 2013 challenge of recognition and classification of drug names, we adapted our chemical entity recognition approach consisting in conditional random fields for recognizing chemical terms and lexical similarity for entity resolution to the chebi ontology. <eos> we obtained promising results, with a best f-measure of 0.81 for the partial matching task when using post-processing. <eos> using only conditional random fields the results are slightly lower, achieving still a good result in terms of fmeasure. <eos> using the chebi ontology allowed a significant improvement in precision ( best precision of 0.93 in partial matching task ), which indicates that taking advantage of an ontology can be extremely useful for enhancing chemical entity recognition.
s : classifying drug-drug interactions with two-stage svm and post-processing majid rastegar-mojarad richard d. boyce rashmi prasad university of wisconsin-milwaukee university of pittsburgh university of wisconsin-milwaukee milwaukee, wi, usa pittsburgh, pa, usa milwaukee, wi, usa rastega3 @ uwm.edu rdb20 @ pitt.edu prasadr @ uwm.edu abstract we describe our system for the ddiextraction-2013 shared task of classifying drug-drug interactions ( ddis ) given labeled drug mentions. <eos> the challenge called for a five-way classification of all drug pairs in each sentence : a drug pair is either non-interacting, or interacting as one of four types. <eos> our approach begins with the use of a two-stage weighted svm classifier to handle the highly unbalanced class distribution : the first stage for a binary classification of drug pairs as interacting or non-interacting, and the second stage for further classification of interacting pairs from the first stage into one of the four interacting types. <eos> our svm features exploit stemmed words, lemmas, bigrams, part of speech tags, verb lists, and similarity measures, among others. <eos> for each stage, we also developed a set of post-processing rules based on observations in the training data. <eos> our best system achieved 0.472 f-measure.
automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. <eos> the ddiextraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. <eos> we present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. <eos> resampling, balancing and ensemble learning experiments are performed to infer the best configuration. <eos> for general drugdrug relation extraction, the system achieves 70.4 % in f1 score.
in this paper, we present our approach to semeval-2013 task 9.2. <eos> it is a feature rich classification using libsvm for drug-drug interactions detection in the biomedical domain. <eos> the features are extracted considering morphosyntactic, lexical and semantic concepts. <eos> tools like opendmap and tees are used to extract semantic concepts from the corpus. <eos> the best f-score that we got for drugdrug interaction ( ddi ) detection is 50 % and 61 % and the best f-score for ddi detection and classification is 34 % and 48 % for test and development data respectively. <eos> keywords : text mining, event extraction, machine learning, feature extraction.
this paper presents uos, a graph-based word sense induction system which attempts to find all applicable senses of a target word given its context, grading each sense according to its suitability to the context. <eos> senses of a target word are induced through use of a non-parameterised, linear-time clustering algorithm that returns maximal quasi-strongly connected components of a target word graph in which vertex pairs are assigned to the same cluster if either vertex has the highest edge weight to the other. <eos> uos participated in semeval-2013 task 13 : word sense induction for graded and non-graded senses. <eos> two system were submitted ; both systems returned results comparable with those of the best performing systems.
this paper describes the experimental combination of traditional natural language processing ( nlp ) technology with the semantic web building stack in order to extend the expert knowledge required for a machine translation ( mt ) task. <eos> therefore, we first give a short introduction in the state of the art of mt and the semantic web and discuss the problem of disambiguation being one of the common challenges in mt which can only be solved using world knowledge during the disambiguation process. <eos> in the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem. <eos> we conclude with a critical view on the developed approach.
in this paper we present and evaluate three approaches to measure comparability of documents in non-parallel corpora. <eos> we develop a task-oriented definition of comparability, based on the performance of automatic extraction of translation equivalents from the documents aligned by the proposed metrics, which formalises intuitive definitions of comparability for machine translation research. <eos> we demonstrate application of our metrics for the task of automatic extraction of parallel and semiparallel translation equivalents and discuss how these resources can be used in the frameworks of statistical and rule-based machine translation.
in this paper we present an smt-based approach to question answering ( qa ). <eos> qa is the task of extracting exact answers in response to natural language questions. <eos> in our approach, the answer is a translation of the question obtained with an smt system. <eos> we use the n-best translations of a given question to find similar sentences in the document collection that contain the real answer. <eos> although it is not the first time that smt inspires a qa system, it is the first approach that uses a full machine translation system for generating answers. <eos> our approach is validated with the datasets of the trec qa evaluation.
in this paper we evaluate the possibility of improving the performance of a statistical machine translation system by relaxing the complexity of the translation task by removing the most frequent and predictable terms from the target language vocabulary. <eos> afterwards, the removed terms are inserted back in the relaxed output by using an n-gram based word predictor. <eos> empirically, we have found that when these words are omitted from the text, the perplexity of the text decreases, which may imply the reduction of confusion in the text. <eos> we conducted some machine translation experiments to see if this perplexity reduction produced a better translation output. <eos> while the word prediction results exhibits 77 % accuracy in predicting 40 % of the most frequent words in the text, the perplexity reduction did not help to produce better translations.
as video contents continue to expand, it is increasingly important to properly annotate videos for effective search, mining and retrieval purposes. <eos> while the idea of annotating images with keywords is relatively well explored, work is still needed for annotating videos with natural language to improve the quality of video search. <eos> the focus of this work is to present a video dataset with natural language descriptions which is a step ahead of keywords based tagging. <eos> we describe our initial experiences with a corpus consisting of descriptions for video segments crafted from trec video data. <eos> analysis of the descriptions created by 13 annotators presents insights into humans ? <eos> interests and thoughts on videos. <eos> such resource can also be used to evaluate automatic natural language generation systems for video.
in this paper we present a hybrid statistical machine translation ( smt ) -example-based mt ( ebmt ) system that shows significant improvement over both smt and ebmt baseline systems. <eos> first we present a runtime ebmt system using a subsentential translation memory ( tm ). <eos> the ebmt system is further combined with an smt system for effective hybridization of the pair of systems. <eos> the hybrid system shows significant improvement in translation quality ( 0.82 and 2.75 absolute bleu points ) for two different language pairs ( english ? turkish ( en ? tr ) and english ? <eos> french ( en ? fr ) ) over the baseline smt system. <eos> however, the ebmt approach suffers from significant time complexity issues for a runtime approach. <eos> we explore two methods to make the system scalable at runtime. <eos> first, we use an heuristic-based approach. <eos> secondly, we use an ir-based indexing technique to speed up the time-consuming matching procedure of the ebmt system. <eos> the index-based matching procedure substantially improves run-time speed without affecting translation quality.
vertan university of hamburg research group ? computerphilology ? <eos> von-mell park 6, 20146 hamburg, germany cristina.vertan @ uni-hamburg.de ? <eos> abstract in this paper we present two approaches for integrating translation into cross-lingual search engines : the first approach relies on term translation via a language ontology, the other one is based on machine translation of specific information.
the main purpose of the project atlas ( applied technology for language-aided cms ) is to facilitate multilingual web content development and management. <eos> its main innovation is the integration of language technologies wi th in a web content management sys tem. <eos> the language processing framework, integrated with web content management, provides automatic annotation of important words, phrases and named entities, suggestions for categorisation o f documen t s, au toma t i c summary generation, and machine translation of summaries of documents. <eos> a machine translation approach, as well as methods for obtaining and constructing training data for machine translation are under development.
i present an automatic post-editing approach that combines translation systems which produce syntactic trees as output. <eos> the nodes in the generation tree and targetside scfg tree are aligned and form the basis for computing structural similarity. <eos> structural similarity computation aligns subtrees and based on this alignment, subtrees are substituted to create more accurate translations. <eos> two different techniques have been implemented to compute structural similarity : leaves and tree-edit distance. <eos> i report on the translation quality of a machine translation ( mt ) system where both techniques are implemented. <eos> the approach shows significant improvement over the baseline for mt systems with limited training data and structural improvement for mt systems trained on europarl.
we report on a series of experiments aimed at improving the machine translation of ambig-uous lexical items by using wordnet-based unsupervised word sense disambiguation ( wsd ) and comparing its results to three mt systems. <eos> our experiments are performed for the english-slovene language pair using ukb, a freely available graph-based word sense disambiguation system. <eos> results are evaluated in three ways : a manual evaluation of wsd performance from mt perspective, an analysis of agreement between the wsd-proposed equivalent and those suggested by the three systems, and finally by computing bleu, nist and meteor scores for all translation versions. <eos> our results show that wsd performs with a mt-relevant precision of 71 % and that 21 % of sense-related mt er-rors could be prevented by using unsuper-vised wsd.
the processing of parallel corpus plays very crucial role for improving the overall performance in phrase based statistical machine translation systems ( pbsmt ). <eos> in this paper the automatic alignments of different kind of chunks have been studied that boosts up the word alignment as well as the machine translation quality. <eos> single-tokenization of noun-noun mwes, phrasal preposition ( source side only ) and reduplicated phrases ( target side only ) and the alignment of named entities and complex predicates provide the best smt model for bootstrapping. <eos> automatic bootstrapping on the alignment of various chunks makes significant gains over the previous best english-bengali pb-smt system. <eos> the source chunks are translated into the target language using the pb-smt system and the translated chunks are compared with the original target chunk. <eos> the aligned chunks increase the size of the parallel corpus. <eos> the processes are run in a bootstrapping manner until all the source chunks have been aligned with the target chunks or no new chunk alignment is identified by the bootstrapping process. <eos> the proposed system achieves significant improvements ( 2.25 bleu over the best system and 8.63 bleu points absolute over the baseline system, 98.74 % relative improvement over the baseline system ) on an english- bengali translation task.
we describe a substitution-based, hybrid machine translation ( mt ) system that has been extended with a machine learning component controlling its phrase selection. <eos> our approach is based on a rule-based mt ( rbmt ) system which creates template translations. <eos> based on the generation parse tree of the rbmt system and standard word alignment computation, we identify potential ? translation snippets ? <eos> from one or more translation engines which could be substituted into our translation templates. <eos> the substitution process is controlled by a binary classifier trained on feature vectors from the different mt engines. <eos> using a set of manually annotated training data, we are able to observe improvements in terms of bleu scores over a baseline version of the hybrid system.
in this paper, we present our linguisticallyaugmented statistical machine translation model from bulgarian to english, which combines a statistical machine translation ( smt ) system ( as backbone ) with deep linguistic features ( as factors ). <eos> the motivation is to take advantages of the robustness of the smt system and the linguistic knowledge of morphological analysis and the hand-crafted grammar through system combination approach. <eos> the preliminary evaluation has shown very promising results in terms of bleu scores ( 38.85 ) and the manual analysis also confirms the high quality of the translation the system delivers.
this article shows how the automatic disambiguation of discourse connectives can improve statistical machine translation ( smt ) from english to french. <eos> connectives are firstly disambiguated in terms of the discourse relation they signal between segments. <eos> several classifiers trained using syntactic and semantic features reach stateof-the-art performance, with f1 scores of 0.6 to 0.8 over thirteen ambiguous english connectives. <eos> labeled connectives are then used into smt systems either by modifying their phrase table, or by training them on labeled corpora. <eos> the best modified smt systems improve the translation of connectives without degrading bleu scores. <eos> a threshold-based smt system using only high-confidence labels improves bleu scores by 0.2 ? 0.4 points.
we present a quantitative investigation of the cross-linguistic usage of some ( relatively ) newly minted derivational morphemes. <eos> in particular, we examine the lexical semantic content expressed by three suffixes originating in english : -gate, -geddon and -athon. <eos> using data from newspapers, we look at the distribution and lexical semantic usage of these morphemes not only within english, but across several languages and also across time, with a time-depth of 20 years. <eos> the occurrence of these suffixes in available corpora are comparatively rare, however, by investigating huge amounts of data, we are able to arrive at interesting insights into the distribution, meaning and spread of the suffixes. <eos> processing and understanding the huge amounts of data is accomplished via visualization methods that allow the presentation of an overall distributional picture, with further details and different types of perspectives available on demand.
in statistical nlp, semantic vector spaces ( svs ) are the standard technique for the automatic modeling of lexical semantics. <eos> however, it is largely unclear how these black-box techniques exactly capture word meaning. <eos> to explore the way an svs structures the individual occurrences of words, we use a non-parametric mds solution of a token-by-token similarity matrix. <eos> the mds solution is visualized in an interactive plot with the google chart tools. <eos> as a case study, we look at the occurrences of 476 dutch nouns grouped in 214 synsets.
each expanding and developing system requires some feedback to evaluate the normal trends of the system and also the unsystematic steps. <eos> in this paper two lexicalsemantic databases ? <eos> princeton wordnet ( prwn ) and estonian wordnet ( estwn ) - are being examined from the visualization point of view. <eos> the visualization method is described and the aim is to find and to point to possible problems of synsets and their semantic relations.
this paper presents a novel way of visualising relationships between languages. <eos> the key feature of the visualisation is that it brings geographic, phylogenetic, and linguistic data together into a single image, allowing a new visual perspective on linguistic typology. <eos> the data presented here is extracted from the world atlas of language structures ( wals ) ( dryer and haspelmath, 2011 ). <eos> after pruning due to low coverage of wals, we filter the typological data by geographical proximity in order to ascertain areal typological effects. <eos> the data are displayed in heat maps which reflect the strength of similarity between languages for different linguistic features. <eos> finally, the heat maps are annotated for language family membership. <eos> the images so produced allow a multi-faceted perspective on the data which we hope will facilitate the interpretation of results and perhaps illuminate new areas of research in linguistic typology.
we demonstrate how data-driven approaches to learner corpora can support second language acquisition research when integrated with visualisation tools. <eos> we present a visual user interface supporting the investigation of a set of linguistic features discriminating between pass and fail ? english as a second or other language ? <eos> exam scripts. <eos> the system displays directed graphs to model interactions between features and supports exploratory search over a set of learner scripts. <eos> we illustrate how the interface can support the investigation of the co-occurrence of many individual features, and discuss how such investigations can shed light on understanding the linguistic abilities that characterise different levels of attainment and, more generally, developmental aspects of learner grammars.
the present paper describes procedures to visualise diachronic language changes in academic discourse to support analysis. <eos> these changes are reflected in the distribution of different lexico-grammatical features according to register. <eos> findings about register differences are relevant for both linguistic applications ( e.g., discourse analysis and translation studies ) and nlp tasks ( notably automatic text classification ).
words are important both in historical linguistics and natural language processing. <eos> they are not indivisible abstract atoms ; much can be gained by considering smaller units such as morphemes, phonemes, syllables, and letters. <eos> in this presentation, i attempt to sketch the similarity patterns among a number of diverse research projects in which i participated.
in this paper, we propose a novel approach to compare languages on the basis of parallel texts. <eos> instead of using word lists or abstract grammatical characteristics to infer ( phylogenetic ) relationships, we use multilingual alignments of words in sentences to establish measures of language similarity. <eos> to this end, we introduce a new method to quickly infer a multilingual alignment of words, using the co-occurrence of words in a massively parallel text ( mpt ) to simultaneously align a large number of languages. <eos> the idea is that a simultaneous multilingual alignment yields a more adequate clustering of words across different languages than the successive analysis of bilingual alignments. <eos> since the method is computationally demanding for a larger number of languages, we reformulate the problem using sparse matrix calculations. <eos> the usefulness of the approach is tested on an mpt that has been extracted from pamphlets of the jehova ? s witnesses. <eos> our preliminary experiments show that this approach can supplement both the historical and the typological comparison of languages.
this paper proposes a simple metric of dialect distance, based on the ratio between identical word pairs and cognate word pairs occurring in two texts. <eos> different variations of this metric are tested on a corpus containing comparable texts from different swiss german dialects and evaluated on the basis of spatial autocorrelation measures. <eos> the visualization of the results as cluster dendrograms shows that closely related dialects are reliably clustered together, while multidimensional scaling produces graphs that show high agreement with the geographic localization of the original texts.
a shibboleth is a pronunciation, or, more generally, a variant of speech that betrays where a speaker is from ( judges 12:6 ). <eos> we propose a generalization of the well-known precision and recall scores to deal with the case of detecting distinctive, characteristic variants when the analysis is based on numerical difference scores. <eos> we also compare our proposal to fisher ? s linear discriminant, and we demonstrate its effectiveness on dutch and german dialect data. <eos> it is a general method that can be applied both in synchronic and diachronic linguistics that involve automatic classification of linguistic entities.
the paper reports several studies about quantifying language similarity via phonetic alignment of core vocabulary items ( taken from wichman ? s automated similarity judgement program data base ). <eos> it turns out that weighted alignment according to the needleman-wunsch algorithm yields best results. <eos> for visualization and data exploration purposes, we used an implementation of the fruchterman-reingold algorithm, a version of force directed graph layout. <eos> this software projects large amounts of data points to a two- or three-dimensional structure in such a way that groups of mutually similar items form spatial clusters. <eos> the exploratory studies conducted along these ways lead to suggestive results that provide evidence for historical relationships beyond the traditionally recognized language families.
this paper presents a novel method for aligning etymological data, which models context-sensitive rules governing sound change, and utilizes phonetic features of the sounds. <eos> the goal is, for a given corpus of cognate sets, to find the best alignment at the sound level. <eos> we introduce an imputation procedure to compare the goodness of the resulting models, as well as the goodness of the data sets. <eos> we present evaluations to demonstrate that the new model yields improvements in performance, compared to previously reported models.
in this paper, a new method for automatic cognate detection in multilingual wordlists will be presented. <eos> the main idea behind the method is to combine different approaches to sequence comparison in historical linguistics and evolutionary biology into a new framework which closely models the most important aspects of the comparative method. <eos> the method is implemented as a python program and provides a convenient tool which is publicly available, easily applicable, and open for further testing and improvement. <eos> testing the method on a large gold standard of ipaencoded wordlists showed that its results are highly consistent and outperform previous methods.
keystroke-logging tools are widely used in writing process research. <eos> these applications are designed to capture each character and mouse movement as isolated events as an indicator of cognitive processes. <eos> the current research project explores the possibilities of aggregating the logged process data from the letter level ( keystroke ) to the word level by merging them with existing lexica and using nlp tools. <eos> linking writing process data to lexica and using nlp tools enables researchers to analyze the data on a higher, more complex level. <eos> in this project the output data of inputlog are segmented on the sentence level and then tokenized. <eos> however, by definition writing process data do not always represent clean and grammatical text. <eos> coping with this problem was one of the main challenges in the current project. <eos> therefore, a parser has been developed that extracts three types of data from the s-notation : word-level revisions, deleted fragments, and the final writing product. <eos> the within-word typing errors are identified and excluded from further analyses. <eos> at this stage the inputlog process data are enriched with the following linguistic information : part-of-speech tags, lemmas, chunks, syllable boundaries and word frequencies.
this paper reports on the development of methods for the automated detection of violations of style guidelines for legislative texts, and their implementation in a prototypical tool. <eos> to this aim, the approach of error modelling employed in automated style checkers for technical writing is enhanced to meet the requirements of legislative editing. <eos> the paper identifies and discusses the two main sets of challenges that have to be tackled in this process : ( i ) the provision of domain-specific nlp methods for legislative drafts, and ( ii ) the concretisation of guidelines for legislative drafting so that they can be assessed by machine. <eos> the project focuses on german-language legislative drafting in switzerland.
this essay provides a summary of research related to my reviewers, a web-based application that can be used for teaching and assessment purposes. <eos> the essay concludes with speculation about ongoing development efforts, including a social helpfulness algorithm, a badging system, and natural language processing ( nlp ) features.
in this research we explore the possibility of using a large n-gram corpus ( google books ) to derive lexical transition probabilities from the frequency of word n-grams and then use them to check and suggest corrections in a target text without the need for grammar rules. <eos> we conduct several experiments in spanish, although our conclusions also reach other languages since the procedure is corpus-driven. <eos> the paper reports on experiments involving different types of grammar errors, which are conducted to test different grammar-checking procedures, namely, spotting possible errors, deciding between different lexical possibilities and filling-in the blanks in a text.
this paper focuses on computer writing tools used during the production of documents in a professional setting. <eos> computer writing tools include language technologies, for example electronic dictionaries and text correction software, as well as information and communication technologies, for example collaborative platforms and search engines. <eos> as we will see, professional writing has become an entirely computerised activity. <eos> first, we report on a focus group with professional writers, during which they discussed their experience using computer tools to write documents. <eos> we will describe their practices, point out the most important problems they encounter, and analyse their needs. <eos> second, we describe linguistech, a reference web site for language professionals ( translators, writers, language instructors, etc. ) <eos> that was launched in canada in september, 2011. <eos> we comment on a preliminary evaluation that we conducted to determine if this new platform meets professional writers ? <eos> needs.
the present paper addresses the question of the nature of deception language. <eos> specifically, the main aim of this piece of research is the exploration of deceit in spanish written communication. <eos> we have designed an automatic classifier based on support vector machines ( svm ) for the identification of deception in an ad hoc opinion corpus. <eos> in order to test the effectiveness of the liwc2001 categories in spanish, we have drawn a comparison with a bag-of-words ( bow ) model. <eos> the results indicate that the classification of the texts is more successful by means of our initial set of variables than with the latter system. <eos> these findings are potentially applicable to areas such as forensic linguistics and opinion mining, where extensive research on languages other than english is needed.
in this study, we explore several popular techniques for obtaining corpora for deception research. <eos> through a survey of traditional as well as non-gold standard creation approaches, we identify advantages and limitations of these techniques for webbased deception detection and offer crowdsourcing as a novel avenue toward achieving a gold standard corpus. <eos> through an indepth case study of online hotel reviews, we demonstrate the implementation of this crowdsourcing technique and illustrate its applicability to a broad array of online reviews.
research in high stakes deception has been held back by the sparsity of ground truth verification for data collected from real world sources. <eos> we describe a set of guidelines for acquiring and developing corpora that will enable researchers to build and test models of deceptive narrative while avoiding the problem of sanctioned lying that is typically required in a controlled experiment. <eos> our proposals are drawn from our experience in obtaining data from court cases and other testimony, and uncovering the background information that enabled us to annotate claims made in the narratives as true or false.
recent studies on deceptive language suggest that machine learning algorithms can be employed with good results for classification of texts as truthful or untruthful. <eos> however, the models presented so far do not attempt to take advantage of the differences between subjects. <eos> in this paper, models have been trained in order to classify statements issued in court as false or not-false, not only taking into consideration the whole corpus, but also by identifying more homogenous subsets of producers of deceptive language. <eos> the results suggest that the models are effective in recognizing false statements, and their performance can be improved if subsets of homogeneous data are provided.
contextual differences present significant challenges when developing computational methods for detecting deception. <eos> we conducted a field experiment with border guards from the european union in order to demonstrate that deception detection can be done robustly using context specific computational models. <eos> in the study, some of the participants were given a ? fraudulent ? <eos> document with incorrect data and asked to pass through a checkpoint. <eos> an automated system used an embodied conversational agent ( eca ) to conduct interviews. <eos> based on the participants ? <eos> vocalic and ocular behavior our specific model classified 100 % of the imposters while limiting false positive errors. <eos> the overall accuracy was 94.47 %.
a person ? s expressive behavior is different in situations where he or she is alone, or where an additional person is present. <eos> this study looks at the extent to which such physical co-presence effects have an impact on a child ? s ability to deceive. <eos> using an experimental digitized puppet show, truthful and deceptive utterances were elicited from children who were interacting with two story characters. <eos> the children were sitting alone, or as a couple together with another child. <eos> a first perception study in which minimal pairs of truthful and deceptive utterances were shown ( vision-only ) to adult observers revealed that the correct detection of deceptive utterances is dependent on whether the stimuli were produced by a child alone or together with another child ( both being visible ). <eos> a second perception study presented participants with videos from children of the couples condition that were edited so that only one child was visible. <eos> the study revealed that the deceptive utterances could more often be detected correctly in the more talkative children than in the more passive ones.
we applied hierarchical clustering using rank distance, previously used in computational stylometry, on literary texts written by mateiu caragiale and a number of different authors who attempted to impersonate caragiale after his death, or simply to mimic his style. <eos> their pastiches were consistently clustered opposite to the original work, thereby confirming the performance of the method and proposing an extension of the method from simple authorship attribution to the more complicated problem of pastiche detection. <eos> the novelty of our work is the use of frequency rankings of stopwords as features, showing that this idea yields good results for pastiche detection.
the ability to detect deceptive statements in predatory communications can help in the identification of sexual predators, a type of deception that is recently attracting the attention of the research community. <eos> due to the intention of a pedophile of hiding his/her true identity ( name, age, gender and location ) its detection is a challenge. <eos> according to previous research, fixated discourse is one of the main characteristics inherent to the language of online sexual predation. <eos> in this paper we approach this problem by computing sexrelated lexical chains spanning over the conversation. <eos> our study shows a considerable variation in the length of sex-related lexical chains according to the nature of the corpus, which supports our belief that this could be a valuable feature in an automated pedophile detection system.
whistleblowers and activists need the ability to communicate without disclosing their identity, as of course do kidnappers and terrorists. <eos> recent advances in the technology of stylometry ( the study of authorial style ) or ? authorship attribution ? <eos> have made it possible to identify the author with high reliability in a non-confrontational setting. <eos> in a confrontational setting, where the author is deliberately masking their identity ( i.e. <eos> attempting to deceive ), the results are much less promising. <eos> in this paper, we show that although the specific author may not be identifiable, the intent to deceive and to hide his identity can be. <eos> we show this by a reanalysis of the brennan and greenstadt ( 2009 ) deception corpus and discuss some of the implications of this surprising finding.
numerous sentiment analysis applications make usage of a sentiment lexicon. <eos> in this paper we present experiments on hybrid sentiment lexicon acquisition. <eos> the approach is corpus-based and thus suitable for languages lacking general dictionarybased resources. <eos> the approach is a hybrid two-step process that combines semisupervised graph-based algorithms and supervised models. <eos> we evaluate the performance on three tasks that capture different aspects of a sentiment lexicon : polarity ranking task, polarity regression task, and sentiment classification task. <eos> extensive evaluation shows that the results are comparable to those of a well-known sentiment lexicon sentiwordnet on the polarity ranking task. <eos> on the sentiment classification task, the results are also comparable to sentiwordnet when restricted to monosentimous ( all senses carry the same sentiment ) words. <eos> this is satisfactory, given the absence of explicit semantic relations between words in the corpus.
this paper describes several novel hybrid semantic similarity measures. <eos> we study various combinations of 16 baseline measures based on wordnet, web as a corpus, corpora, dictionaries, and encyclopedia. <eos> the hybrid measures rely on 8 combination methods and 3 measure selection techniques and are evaluated on ( a ) the task of predicting semantic similarity scores and ( b ) the task of predicting semantic relation between two terms. <eos> our results show that hybrid measures outperform single measures by a wide margin, achieving a correlation up to 0.890 and map ( 20 ) up to 0.995.
dependency parsing has made many advancements in recent years, in particular for english. <eos> there are a few dependency parsers that achieve comparable accuracy scores with each other but with very different types of errors. <eos> this paper examines creating a new dependency structure through ensemble learning using a hybrid of the outputs of various parsers. <eos> we combine all tree outputs into a weighted edge graph, using 4 weighting mechanisms. <eos> the weighted edge graph is the input into our ensemble system and is a hybrid of very different parsing techniques ( constituent parsers, transitionbased dependency parsers, and a graphbased parser ). <eos> from this graph we take a maximum spanning tree. <eos> we examine the new dependency structure in terms of accuracy and errors on individual part-of-speech values. <eos> the results indicate that using a greater number of more varied parsers will improve accuracy results. <eos> the combined ensemble system, using 5 parsers based on 3 different parsing techniques, achieves an accuracy score of 92.58 %, beating all single parsers on the wall street journal section 23 test set. <eos> additionally, the ensemble system reduces the average relative error on selected pos tags by 9.82 %.
this contribution addresses generation of natural language descriptions for human actions, behaviour and their relations with other objects observed in video streams. <eos> the work starts with implementation of conventional image processing techniques to extract high level features from video. <eos> these features are converted into natural language descriptions using context free grammar. <eos> although feature extraction processes are erroneous at various levels, we explore approaches to putting them together to produce a coherent description. <eos> evaluation is made by calculating rouge scores between human annotated and machine generated descriptions. <eos> further we introduce a task based evaluation by human subjects which provides qualitative evaluation of generated descriptions.
ocr ( optical character recognition ) scanners do not always produce 100 % accuracy in recognizing text documents, leading to spelling errors that make the texts hard to process further. <eos> this paper presents an investigation for the task of spell checking for ocr-scanned text documents. <eos> first, we conduct a detailed analysis on characteristics of spelling errors given by an ocr scanner. <eos> then, we propose a fully automatic approach combining both error detection and correction phases within a unique scheme. <eos> the scheme is designed in an unsupervised & data-driven manner, suitable for resource-poor languages. <eos> based on the evaluation on real dataset in vietnamese language, our approach gives an acceptable performance ( detection accuracy 86 %, correction accuracy 71 % ). <eos> in addition, we also give a result analysis to show how accurate our approach can achieve.
this paper contrasts the content and form of objective versus subjective texts. <eos> a collection of on-line newspaper news items serve as objective texts, while parliamentary speeches ( debates ) and blog posts form the basis of our subjective texts, all in portuguese. <eos> the aim is to provide general linguistic patterns as used in objective written media and subjective speeches and blog posts, to help construct domainindependent templates for information extraction and opinion mining. <eos> our hybrid approach combines statistical data along with linguistic knowledge to filter out irrelevant patterns. <eos> as resources for subjective classification are still limited for portuguese, we use a parallel corpus and tools developed for english to build our subjective spoken corpus, through annotations produced for english projected onto a parallel corpus in portuguese. <eos> a measure for the saliency of n-grams is used to extract relevant linguistic patterns deemed ? objective ? <eos> and ? subjective ?. <eos> perhaps unsurprisingly, our contrastive approach shows that, in portuguese at least, subjective texts are characterized by markers such as descriptive, reactive and opinionated terms, while objective texts are characterized mainly by the absence of subjective markers.
we present a joint system for named entity recognition ( ner ) and entity linking ( el ), allowing for named entities mentions extracted from textual data to be matched to uniquely identifiable entities. <eos> our approach relies on combined ner modules which transfer the disambiguation step to the el component, where referential knowledge about entities can be used to select a correct entity reading. <eos> hybridation is a main feature of our system, as we have performed experiments combining two types of ner, based respectively on symbolic and statistical techniques. <eos> furthermore, the statistical el module relies on entity knowledge acquired over a large news corpus using a simple rule-base disambiguation tool. <eos> an implementation of our system is described, along with experiments and evaluation results on french news wires. <eos> linking accuracy reaches up to 87 %, and the ner fscore up to 83 %.
within information extraction tasks, named entity recognition has received much attention over latest decades. <eos> from symbolic / knowledge-based to data-driven / machine-learning systems, many approaches have been experimented. <eos> our work may be viewed as an attempt to bridge the gap from the data-driven perspective back to the knowledge-based one. <eos> we use a knowledge-based system, based on manually implemented transducers, that reaches satisfactory performances. <eos> it has the undisputable advantage of being modular. <eos> however, such a hand-crafted system requires substantial efforts to cope with dedicated tasks. <eos> in this context, we implemented a pattern extractor that extracts symbolic knowledge, using hierarchical sequential pattern mining over annotated corpora. <eos> to assess the accuracy of mined patterns, we designed a module that recognizes named entities in texts by determining their most probable boundaries. <eos> instead of considering named entity recognition as a labeling task, it relies on complex context-aware features provided by lower-level systems and considers the tagging task as a markovian process. <eos> using thos systems, coupling knowledge-based system with extracted patterns is straightforward and leads to a competitive hybrid ne-tagger. <eos> we report experiments using this system and compare it to other hybridization strategies along with a baseline crf model.
when digitizing a print bilingual dictionary, whether via optical character recognition or manual entry, it is inevitable that errors are introduced into the electronic version that is created. <eos> we investigate automating the process of detecting errors in an xml representation of a digitized print dictionary using a hybrid approach that combines rulebased, feature-based, and language modelbased methods. <eos> we investigate combining methods and show that using random forests is a promising approach. <eos> we find that in isolation, unsupervised methods rival the performance of supervised methods. <eos> random forests typically require training data so we investigate how we can apply random forests to combine individual base methods that are themselves unsupervised without requiring large amounts of training data. <eos> experiments reveal empirically that a relatively small amount of data is sufficient and can potentially be further reduced through specific selection criteria.
question answering systems answer correctly to different questions because they are based on different strategies. <eos> in order to increase the number of questions which can be answered by a single process, we propose solutions to combine two question answering systems, qaval and ritel. <eos> qaval proceeds by selecting short passages, annotates them by question terms, and then extracts from them answers which are ordered by a machine learning validation process. <eos> ritel develops a multi-level analysis of questions and documents. <eos> answers are extracted and ordered according to two strategies : by exploiting the redundancy of candidates and a bayesian model. <eos> in order to merge the system results, we developed different methods either by merging passages before answer ordering, or by merging end-results. <eos> the fusion of endresults is realized by voting, merging, and by a machine learning process on answer characteristics, which lead to an improvement of the best system results of 19 %.
many tasks in natural language processing require that sentences be classified from a set of discrete interpretations. <eos> in these cases, there appear to be great benefits in using hybrid systems which apply multiple analyses to the test cases. <eos> in this paper, we examine a general principle for building hybrid systems, based on combining the results of several, high precision heuristics. <eos> by generalising the results of systems for sentiment analysis and ambiguity recognition, we argue that if correctly combined, multiple techniques classify better than single techniques. <eos> more importantly, the combined techniques can be used in tasks where no single classification is appropriate.
prepositions are hard to translate, because their meaning is often vague, and the choice of the correct preposition is often arbitrary. <eos> at the same time, making the correct choice is often critical to the coherence of the output text. <eos> in the context of statistical machine translation, this difficulty is enhanced due to the possible long distance between the preposition and the head it modifies, as opposed to the local nature of standard language models. <eos> in this work we use monolingual language resources to determine the set of prepositions that are most likely to occur with each verb. <eos> we use this information in a transfer-based arabic-to-hebrew statistical machine translation system. <eos> we show that incorporating linguistic knowledge on the distribution of prepositions significantly improves the translation quality.
summarization, like other natural language processing tasks, is tackled with a range of different techniques - particularly machine learning approaches, where human intuition goes into attribute selection and the choice and tuning of the learning algorithm. <eos> such techniques tend to apply differently in different contexts, so in this paper we describe a hybrid approach in which a number of different summarization techniques are combined in a rule-based system using manual knowledge acquisition, where human intuition, supported by data, specifies not only attributes and algorithms, but the contexts where these are best used. <eos> we apply this approach to automatic summarization of legal case reports. <eos> we show how a preliminary knowledge base, composed of only 23 rules, already outperforms competitive baselines.
unsupervised part-of-speech ( pos ) tagging has recently been shown to greatly benefit from bayesian approaches where hmm parameters are integrated out, leading to significant increases in tagging accuracy. <eos> these improvements in unsupervised methods are important especially in specialized social media domains such as twitter where little training data is available. <eos> here, we take the bayesian approach one step further by integrating semantic information from an lda-like topic model with an hmm. <eos> specifically, we present part-of-speech lda ( poslda ), a syntactically and semantically consistent generative probabilistic model. <eos> this model discovers pos specific topics from an unlabelled corpus. <eos> we show that this model consistently achieves improvements in unsupervised pos tagging and language modeling over the bayesian hmm approach with varying amounts of side information in the noisy and esoteric domain of twitter.
in this paper, we address the issue of how different personalities interact in twitter. <eos> in particular we study users ? <eos> interactions using one trait of the standard model known as the ? big five ? <eos> : emotional stability. <eos> we collected a corpus of about 200000 twitter posts and we annotated it with an unsupervised personality recognition system. <eos> this system exploits linguistic features, such as punctuation and emoticons, and statistical features, such as followers count and retweeted posts. <eos> we tested the system on a dataset annotated with personality models produced from human judgements. <eos> network analysis shows that neurotic users post more than secure ones and have the tendency to build longer chains of interacting users. <eos> secure users instead have more mutual connections and simpler networks.
classifying blog posts by topics is useful for applications such as search and marketing. <eos> however, topic classification is time consuming and error prone, especially in an open domain such as the blogosphere. <eos> the state-of-the-art relies on supervised methods, requiring considerable training effort, that use the whole corpus vocabulary as features, demanding considerable memory to process. <eos> we show an effective alternative whereby distant supervision is used to obtain training data : we use wikipedia articles labelled with freebase domains. <eos> we address the memory requirements by using only named entities as features. <eos> we test our classifier on a sample of blog posts, and report up to 0.69 accuracy for multi-class labelling and 0.9 for binary classification.
originating from a multidisciplinary research project that gathers, around the semantic web standards and principles, social networking and natural language processing along with some bioinformatics notions, this paper sheds the light on some of the most critical aspects of the correspondingly adopted framework and realtime knowledge architecture and modeling platform. <eos> it recognizes the considerable profits of an appropriate fusion between the aforementioned disciplines, especially via the proper exploitation of owl 2 ( web ontology language ) features and novelties, typically owl 2 language profiles. <eos> accordingly, it proposes a distinctive workflow with well-defined strategies for an ontology-aware user and nlp-assisted flexible and multidimensional approach for the management of the abundantly available social data. <eos> application scenarios related to awareness and orientation recommender systems based on biomedical domain ontologies for childhood obesity prevention and surveillance are explored as typical proof of concept application areas.
text mining of massive social media postings presents interesting challenges for nlp applications due to sparse interpretation contexts, grammatical and orthographical variability as well as its very fragmentary nature. <eos> no single methodological approach can be expected to work across such diverse typologies as twitter micro-blogging, customer reviews, carefully edited blogs, etc. <eos> in this paper we present a modular and scalable framework to social media opinion mining that combines stochastic and symbolic techniques to structure a semantic space to exploit and interpret efficiently. <eos> we describe the use of this framework for the discovery and clustering of opinion targets and topics in user-generated comments for the telecom and automotive domains.
to what extend can one use twitter in opinion polls for political elections ? <eos> merely counting twitter messages mentioning political party names is no guarantee for obtaining good election predictions. <eos> by improving the quality of the document collection and by performing sentiment analysis, predictions based on entity counts in tweets can be considerably improved, and become nearly as good as traditionally obtained opinion polls.
in this paper, we propose the use of finegrained information such as opinions and suggestions extracted from users ? <eos> reviews about products, in order to improve a recommendation system. <eos> while typical recommender systems compare a user profile with some reference characteristics to rate unseen items, they rarely make use of the content of reviews users have done on a given product. <eos> in this paper, we show how we applied an opinion extraction system to extract opinions but also suggestions from the content of the reviews, use the results to compare other products with the reviewed one, and eventually recommend a better product to the user.
unsupervised dependency parsing is one of the most challenging tasks in natural languages processing. <eos> the task involves finding the best possible dependency trees from raw sentences without getting any aid from annotated data. <eos> in this paper, we illustrate that by applying a supervised incremental parsing model to unsupervised parsing ; parsing with a linear time complexity will be faster than the other methods. <eos> with only 15 training iterations with linear time complexity, we gain results comparable to those of other state of the art methods. <eos> by employing two simple universal linguistic rules inspired from the classical dependency grammar, we improve the results in some languages and get the state of the art results. <eos> we also test our model on a part of the ongoing persian dependency treebank. <eos> this work is the first work done on the persian language.
building shallow semantic representations from text corpora is the first step to perform more complex tasks such as text entailment, enrichment of knowledge bases, or question answering. <eos> open information extraction ( oie ) is a recent unsupervised strategy to extract billions of basic assertions from massive corpora, which can be considered as being a shallow semantic representation of those corpora. <eos> in this paper, we propose a new multilingual oie system based on robust and fast rule-based dependency parsing. <eos> it permits to extract more precise assertions ( verb-based triples ) from text than state of the art oie systems, keeping a crucial property of those systems : scaling to web-size document collections.
topic models ( tm ) such as latent dirichlet allocation ( lda ) are increasingly used in natural language processing applications. <eos> at this, the model parameters and the influence of randomized sampling and inference are rarely examined ? <eos> usually, the recommendations from the original papers are adopted. <eos> in this paper, we examine the parameter space of lda topic models with respect to the application of text segmentation ( ts ), specifically targeting error rates and their variance across different runs. <eos> we find that the recommended settings result in error rates far from optimal for our application. <eos> we show substantial variance in the results for different runs of model estimation and inference, and give recommendations for increasing the robustness and stability of topic models. <eos> running the inference step several times and selecting the last topic id assigned per token, shows considerable improvements. <eos> similar improvements are achieved with the mode method : we store all assigned topic ids during each inference iteration step and select the most frequent topic id assigned to each word. <eos> these recommendations do not only apply to ts, but are generic enough to transfer to other applications.
clustered word classes have been used in connection with statistical machine translation, for instance for improving word alignments. <eos> in this work we investigate if clustered word classes can be used in a preordering strategy, where the source language is reordered prior to training and translation. <eos> part-of-speech tagging has previously been successfully used for learning reordering rules that can be applied before training and translation. <eos> we show that we can use word clusters for learning rules, and significantly improve on a baseline with only slightly worse performance than for standard pos-tags on an english ? german translation task. <eos> we also show the usefulness of the approach for the less-resourced language haitian creole, for translation into english, where the suggested approach is significantly better than the baseline.
relation extraction is frequently and successfully addressed by machine learning methods. <eos> the downside of this approach is the need for annotated training data, typically generated in tedious manual, cost intensive work. <eos> distantly supervised approaches make use of weakly annotated data, like automatically annotated corpora. <eos> recent work in the biomedical domain has applied distant supervision for proteinprotein interaction ( ppi ) with reasonable results making use of the intact database. <eos> such data is typically noisy and heuristics to filter the data are commonly applied. <eos> we propose a constraint to increase the quality of data used for training based on the assumption that no self-interaction of realworld objects are described in sentences. <eos> in addition, we make use of the university of kansas proteomics service ( kups ) database. <eos> these two steps show an increase of 7 percentage points ( pp ) for the ppi corpus aimed. <eos> we demonstrate the broad applicability of our approach by using the same workflow for the analysis of drug-drug interactions, utilizing relationships available from the drug database drugbank. <eos> we achieve 37.31 % in f1 measure without manually annotated training data on an independent test set.
we introduce conflict-driven co-clustering, a novel algorithm for data co-clustering, and apply it to the problem of inducing parts-ofspeech in a corpus of child-directed spoken english. <eos> co-clustering is preferable to unidimensional clustering as it takes into account both item and context ambiguity. <eos> we show that the categorization performance of the algorithm is comparable with the coclustering algorithm of leibbrandt and powers ( 2008 ), but out-performs that algorithm in robustly pruning less-useful clusters and merging them into categories strongly corresponding to the three main open classes of english.
dependency parsing domain adaptation involves adapting a dependency parser, trained on an annotated corpus from a given domain ( e.g., newspaper articles ), to work on a different target domain ( e.g., legal documents ), given only an unannotated corpus from the target domain. <eos> we present a shift/reduce dependency parser that can handle unlabeled sentences in its training set using a transductive svm as its action selection classifier. <eos> we illustrate the the experiments we performed with this parser on a domain adaptation task for the italian language.
an open question in [ fu ? lo ? p, maletti, vogler : weighted extended tree transducers. <eos> fundamenta informaticae 111 ( 2 ), 2011 ] asks whether weighted linear extended tree transducers preserve recognizability in countably complete commutative semirings. <eos> in this contribution, the question is answered positively, which is achieved with a construction that utilizes inside weights. <eos> due to the completeness of the semiring, the inside weights always exist, but the construction is only effective if they can be effectively determined. <eos> it is demonstrated how to achieve this in a number of important cases.
it has remained an open question whether the twins property for weighted tree automata is decidable. <eos> this property is crucial for determinizing such an automaton, and it has been argued that determinization improves the output of parsers and translation systems. <eos> we show that the twins property for weighted tree automata over extremal semifields is decidable.
in this paper we present the tree to tree transduction language, ttt. <eos> we motivate the overall ? template-to-template ? <eos> approach to the design of the language, and outline its constructs, also providing some examples. <eos> we then show that ttt allows transparent formalization of rules for parse tree refinement and correction, logical form refinement and predicate disambiguation, inference, and verbalization of logical forms.
the simultaneously phonological and syntactic grammar of second position clitics is an instance of the broader problem of applying constraints across multiple levels of linguistic analysis. <eos> syntax frameworks extended with simple tree transductions can make efficient use of these necessary additional forms of structure. <eos> an analysis of sahidic coptic second position clitics in a context-free grammar extended by a monadic second-order transduction exemplifies this approach.
languages evolve, undergoing repeated small changes, some with permanent effect and some not. <eos> changes affecting a language may be independent or contactinduced. <eos> independent changes arise internally or, if externally, from non-linguistic causes. <eos> en masse, such changes cause isolated languages to drift apart in lexical form and grammatical structure. <eos> contactinduced changes can happen when languages share speakers, or when their speakers are in contact. <eos> frequently, languages in contact are related, having a common ancestor from which they still retain visible structure. <eos> this relatedness makes it difficult to distinguish contact-induced change from inherited similarities. <eos> in this paper, we present a simulation of contact-induced change. <eos> we show that it is possible to distinguish contact-induced change from independent change given ( a ) enough data, and ( b ) that the contactinduced change is strong enough. <eos> for a particular model, we determine how much data is enough to distinguish these two cases at p < 0.05.
in this demonstration we present our web services to perform bayesian learning for classification tasks. <eos>
english phonotactic learning is modeled by means of the phacts algorithm, a topological neuronal receptive field implementing a phonotactic activation function aimed at capturing both local ( i.e., phonemic ) and global ( i.e., word-level ) similarities among strings. <eos> limits and merits of the model are presented.
in this paper we present a profile of verb usage across ages in child-produced sentences in english and portuguese. <eos> we examine in particular lexical and syntactic characteristics of verbs and find common trends in these languages as children ? s ages increase, such as the prominence of general and polysemic verbs, as well as divergences such as the proportion of subject dropping. <eos> we also find a correlation between the age of acquisition and the number of complements of a verb for english.
much has been discussed about the challenges posed by multiword expressions ( mwes ) given their idiosyncratic, flexible and heterogeneous nature. <eos> nonetheless, children successfully learn to use them and eventually acquire a number of multiword expressions comparable to that of simplex words. <eos> in this paper we report a wide-coverage investigation of a particular type of mwe : verb-particle constructions ( vpcs ) in english and their usage in child-produced and child-directed sentences. <eos> given their potentially higher complexity in relation to simplex verbs, we examine whether they appear less prominently in child-produced than in childdirected speech, and whether the vpcs that children produce are more conservative than adults, displaying proportionally reduced lexical repertoire of vpcs or of verbs in these combinations. <eos> the results obtained indicate that regardless of any additional complexity vpcs feature widely in children data following closely adult usage. <eos> studies like these can inform the development of computational models for language acquisition.
this paper presents brazilian portuguese phoneme patterns of distribution, according to an automatic grammar rulesbased grapheme to phoneme converter. <eos> the software nhenh ? m ( vasil ? vski, 2008 ) was used for treating data : written texts which were decoded into phonologic symbols, forming a corpus, and subjected to a statistical analysis. <eos> results support the high level of predictability of brazilian portuguese phonemes distribution, the consonantvowel syllabic pattern as the most common, as well as the stress pattern distribution 'cv.cv #. <eos> the efficiency of a phoneme-grapheme converter based entirely on rules is also proven. <eos> these results are displayed and discussed, as well as some aspects of nhe-nh ? m building.
this paper describes a web-based editor called cobalt ( corpus-based lexicon tool ), developed to construct corpusbased computational lexica and to correct word-level annotations and transcription errors in corpora. <eos> the paper describes the tool as well as our experience in using it to annotate a reference corpus and compile a large lexicon of historical slovene. <eos> the annotations used in our project are modern-day word form equivalent, lemma, part-of-speech tag and optional gloss. <eos> the cobalt interface is word form oriented and compact. <eos> it enables wildcard word searching and sorting according to several criteria, which makes the editing process flexible and efficient. <eos> the tool accepts preannotated corpora in tei p5 format and is able to export the corpus and lexicon in tei p5 as well. <eos> the tool is implemented using the lamp architecture and is freely available for research purposes.
we present a new transcription mode for the annotation tool elan. <eos> this mode is designed to speed up the process of creating transcriptions of primary linguistic data ( video and/or audio recordings of linguistic behaviour ). <eos> we survey the basic transcription workflow of some commonly used tools ( transcriber, blitzscribe, and elan ) and describe how the new transcription interface improves on these existing implementations. <eos> we describe the design of the transcription interface and explore some further possibilities for improvement in the areas of segmentation and computational enrichment of annotations.
we present work on a verse-composition assistant for composing, checking correctness of, and singing traditional basque bertsoak ? impromptu verses on particular themes. <eos> a performing bertsolari ? a verse singer in the basque country ? must adhere to strict rules that dictate the format and content of the verses sung. <eos> to help the aspiring bertsolari, we provide a tool that includes a web interface that is able to analyze, correct, provide suggestions and synonyms, and tentatively also sing ( using text-to-speech synthesis ) verses composed by the user.
today museums and other cultural heritage institutions are increasingly storing object descriptions using semantic web domain ontologies. <eos> to make this content accessible in a multilingual world, it will need to be conveyed in many languages, a language generation task which is domain specific and language dependent. <eos> this paper describes how semantic and syntactic information such as that provided in a framenet can contribute to solving this task. <eos> it is argued that the kind of information offered by such lexical resources enhances the output quality of a multilingual language generation application, in particular when generating domain specific content.
we describe ongoing work aiming at deriving a multilingual controlled vocabulary ( german, french, italian ) from the combined subject indices from 22 volumes of a large-scale critical edition of historical documents. <eos> the controlled vocabulary is intended to support editors in assigning descriptors to new documents and to support users in retrieving documents of interest regardless of the spelling or language variety used in the documents.
we present on-going work on the automated ontology-based detection and recognition of characters in folktales, restricting ourselves for the time being to the analysis of referential nominal phrases occurring in such texts. <eos> focus of the presently reported work was to investigate the interaction between an ontology and linguistic analysis of indefinite and indefinite nominal phrase for both the incremental annotation of characters in folktales text, including some inference based co-reference resolution, and the incremental population of the ontology. <eos> this in depth study was done at this early stage using only a very small textual base, but the demonstrated feasibility and the promising results of our smallscale experiment are encouraging us to deploy the strategy on a larger text base, covering more linguistic phenomena in a multilingual fashion.
the volumes of digitized literary collections in various languages increase at a rapid pace, which results also in a growing demand for computational support to analyze such linguistic data. <eos> this paper combines robust text analysis with advanced visual analytics and brings a new set of tools to literature analysis. <eos> visual analytics techniques can offer new and unexpected insights and knowledge to the literary scholar. <eos> we analyzed a small subset of a large literary collection, the swedish literature bank, by focusing on the extraction of persons ? <eos> names, their gender and their normalized, linked form, including mentions of theistic beings ( e.g., gods ? <eos> names and mythological figures ), and examined their appearance over the course of the novel. <eos> a case study based on 13 novels, from the aforementioned collection, shows a number of interesting applications of visual analytics methods to literature problems, where named entities can play a prominent role, demonstrating the advantage of visual literature analysis. <eos> our work is inspired by the notion of distant reading or macroanalysis for the analyses of large literature collections.
this paper illustrates the use of distributional techniques, as investigated in computational semantics, for supplying data from large-scale corpora to areas of the humanities which focus on the analysis of concepts. <eos> we suggest that the distributional notion of ? characteristic context ? <eos> can be seen as evidence for some representative tendencies of general discourse. <eos> we present a case study where distributional data is used by philosophers working in the areas of gender studies and intersectionality as confirmation of certain trends described in previous work. <eos> further, we highlight that different models of phrasal distributions can be compared to support the claim of intersectionality theory that ? there is more to a phrase than the intersection of its parts ?.
even though nlp tools are widely used for contemporary text today, there is a lack of tools that can handle historical documents. <eos> such tools could greatly facilitate the work of researchers dealing with large volumes of historical texts. <eos> in this paper we propose a method for extracting verbs and their complements from historical swedish text, using nlp tools and dictionaries developed for contemporary swedish and a set of normalisation rules that are applied before tagging and parsing the text. <eos> when evaluated on a sample of texts from the period 1550 ? <eos> 1880, this method identifies verbs with an f-score of 77.2 % and finds a partially or completely correct set of complements for 55.6 % of the verbs. <eos> although these results are in general lower than for contemporary swedish, they are strong enough to make the approach useful for information extraction in historical research. <eos> moreover, the exact match rate for complete verb constructions is in fact higher for historical texts than for contemporary texts ( 38.7 % vs. 30.8 % ).
we introduce a corpus of classical chinese poems that has been word segmented and tagged with parts-ofspeech ( pos ). <eos> due to the ill-defined concept of a ? word ? <eos> in chinese, previous chinese corpora suffer from a lack of standardization in word segmentation, resulting in inconsistencies in pos tags, therefore hindering interoperability among corpora. <eos> we address this problem with nested pos tags, which accommodates different theories of wordhood and facilitates research objectives requiring annotations of the ? word ? <eos> at different levels of granularity.
a significant amount of information about cultural heritage artefacts is now available in digital format and has been made available in digital libraries. <eos> being able to identify items that are similar would be useful for search and navigation through these data sets. <eos> information about items in these repositories is often multimodal, such as pictures of the artefact and an accompanying textual description. <eos> this paper explores the use of information from these various media for computing similarity between cultural heritage artefacts. <eos> results show that combining information from images and text produces better estimates of similarity than when only a single medium is considered.
over the past years large digital cultural heritage collections have become increasingly available. <eos> while these provide adequate search functionality for the expert user, this may not offer the best support for non-expert or novice users. <eos> in this paper we propose a novel mechanism for introducing new users to the items in a collection by allowing them to browse wikipedia articles, which are augmented with items from the cultural heritage collection. <eos> using europeana as a case-study we demonstrate the effectiveness of our approach for encouraging users to spend longer exploring items in europeana compared with the existing search provision.
large numbers of cultural heritage items are now archived digitally along with accompanying metadata and are available to anyone with internet access. <eos> this information could be enriched by adding links to resources that provide background information about the items. <eos> techniques have been developed for automatically adding links to wikipedia to text but the methods are general and not designed for use with cultural heritage data. <eos> this paper explores a range of methods for adapting a system for adding links to wikipedia to cultural heritage items. <eos> the approaches make use of the structure of wikipedia, including the category hierarchy. <eos> it is found that an approach that makes use of wikipedia ? s link structure can be used to improve the quality of the wikipedia links that are added.
document layout analysis is an important task needed for handwritten text recognition among other applications. <eos> text layout commonly found in handwritten legacy documents is in the form of one or more paragraphs composed of parallel text lines. <eos> an approach for handwritten text line detection is presented which uses machinelearning techniques and methods widely used in natural language processing. <eos> it is shown that text line detection can be accurately solved using a formal methodology, as opposed to most of the proposed heuristic approaches found in the literature. <eos> experimental results show the impact of using increasingly constrained ? vertical layout language models ? <eos> in text line detection accuracy.
one important subtask of referring expression generation ( reg ) algorithms is to select the attributes in a definite description for a given object. <eos> in this paper, we study how much training data is required for algorithms to do this properly. <eos> we compare two reg algorithms in terms of their performance : the classic incremental algorithm and the more recent graph algorithm. <eos> both rely on a notion of preferred attributes that can be learned from human descriptions. <eos> in our experiments, preferences are learned from training sets that vary in size, in two domains and languages. <eos> the results show that depending on the algorithm and the complexity of the domain, training on a handful of descriptions can already lead to a performance that is not significantly different from training on a much larger data set.
commonly, the result of referring expression generation algorithms is a single noun phrase. <eos> in interactive settings with a shared workspace, however, human dialog partners often split referring expressions into installments that adapt to changes in the context and to actions of their partners. <eos> we present a corpus of human ? human interactions in the give-2 setting in which instructions are spoken. <eos> a first study of object descriptions in this corpus shows that references in installments are quite common in this scenario and suggests that contextual factors partly determine their use. <eos> we discuss what new challenges this creates for nlg systems.
we describe preliminary work on generating contextualized text for nature conservation volunteers. <eos> this natural language generation ( nlg ) differs from other ways of describing spatio-temporal data, in that it deals with abstractions on data across large geographical spaces ( total projected area 20,600 km2 ), as well as temporal trends across longer time frames ( ranging from one week up to a year ). <eos> we identify challenges at all stages of the classical nlg pipeline.
until recently, deep stochastic surface realization has been hindered by the lack of semantically annotated corpora. <eos> this is about to change. <eos> such corpora are increasingly available, e.g., in the context of conll shared tasks. <eos> however, recent experiments with conll 2009 corpora show that these popular resources, which serve well for other applications, may not do so for generation. <eos> the attempts to adapt them for generation resulted so far in a better performance of the realizers, but not yet in a genuinely semantic generationoriented annotation schema. <eos> our goal is to initiate a debate on how a generation suitable annotation schema should be defined. <eos> we define some general principles of a semantic generation-oriented annotation and propose an annotation schema that is based on these principles. <eos> experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them.
while in computer science, grammar engineering has led to the development of various tools for checking grammar coherence, completion, under- and over-generation, in natural langage processing, most approaches developed to improve a grammar have focused on detecting under-generation and to a much lesser extent, over-generation. <eos> we argue that generation can be exploited to address other issues that are relevant to grammar engineering such as in particular, detecting grammar incompleteness, identifying sources of overgeneration and analysing the linguistic coverage of the grammar. <eos> we present an algorithm that implements these functionalities and we report on experiments using this algorithm to analyse a feature-based lexicalised tree adjoining grammar consisting of roughly 1500 elementary trees.
variation in language style can lead to different perceptions of the interaction, and different behaviour outcomes. <eos> using the crag 2 language generation system we examine how accurately judges can perceive character personality from short, automatically generated dialogues, and how alignment ( similarity between speakers ) alters judge perceptions of the characters ? <eos> relationship. <eos> whilst personality perception of our dialogues is consistent with perceptions of human behaviour, we find that the introduction of alignment leads to negative perceptions of the dialogues and the interlocutors ? <eos> relationship. <eos> a follow up evaluation study of the perceptions of different forms of alignment in the dialogues reveals that while similarity at polarity, topic and construction levels is viewed positively, similarity at the word level is regarded negatively. <eos> we discuss our findings in relation to the literature and in the context of dialogue systems.
recent studies have shown that incremental systems are perceived as more reactive, natural, and easier to use than non-incremental systems. <eos> however, previous work on incremental nlg has not employed recent advances in statistical optimisation using machine learning. <eos> this paper combines the two approaches, showing how the update, revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a markov decision process. <eos> we design a model of incremental nlg that generates output based on micro-turn interpretations of the user ? s utterances and is able to optimise its decisions using statistical machine learning. <eos> we present a proof-of-concept study in the domain of information presentation ( ip ), where a learning agent faces the trade-off of whether to present information as soon as it is available ( for high reactiveness ) or else to wait until input asr hypotheses are more reliable. <eos> results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence.
linguist ? s assistant ( la ) is a large scale semantic analyzer and multi-lingual natural language generator designed and developed entirely from a linguist ? s perspective. <eos> the system incorporates extensive typological, semantic, syntactic, and discourse research into its semantic representational system and its transfer and synthesizing grammars. <eos> la has been tested with english, korean, kewa ( papua new guinea ), jula ( cote d ? ivoure ), and north tanna ( vanuatu ), and proof-of-concept lexicons and grammars have been developed for spanish, urdu, tagalog, chinantec ( mexico ), and angas ( nigeria ). <eos> this paper will summarize the major components of the nlg system, and then present the results of experiments that were performed to determine the quality of the generated texts. <eos> the experiments indicate that when experienced mothertongue translators use the drafts generated by la, their productivity is typically quadrupled without any loss of quality.
despite their flat, semantics-free structure, ontology identifiers are often given names or labels corresponding to natural language words or phrases which are very dense with information as to their intended referents. <eos> we argue that by taking advantage of this information density, nlg systems applied to ontologies can guide the choice and construction of sentences to express useful ontological information, solely through the verbalisations of identifier names, and that by doing so, they can replace the extremely fussy and repetitive texts produced by ontology verbalisers with shorter and simpler texts which are clearer and easier for human readers to understand. <eos> we specify which axioms in an ontology are ? defining axioms ? <eos> for linguistically-complex identifiers and analyse a large corpus of owl ontologies to identify common patterns among all defining axioms. <eos> by generating texts from ontologies, and selectively including or omitting these defining axioms, we show by surveys that human readers are typically capable of inferring information implicitly encoded in identifier phrases, and that texts which do not make such ? obvious ? <eos> information explicit are preferred by readers and yet communicate the same information as the longer texts in which such information is spelled out explicitly.
during the last decade, there has been a shift from developing natural language generation systems to developing generic systems that are capable of producing natural language descriptions directly from web ontologies. <eos> to make these descriptions coherent and accessible in different languages, a methodology is needed for identifying the general principles that would determine the distribution of referential forms. <eos> previous work has proved through crosslinguistic investigations that strategies for building coreference are language dependent. <eos> however, to our knowledge, there is no language generation methodology that makes a distinction between languages about the generation of referential chains. <eos> to determine the principles governing referential chains, we gathered data from three languages : english, swedish and hebrew, and studied how coreference is expressed in a discourse. <eos> as a result of the study, a set of language specific coreference strategies were identified. <eos> using these strategies, an ontology-based multilingual grammar for generating written natural language descriptions about paintings was implemented in the grammatical framework. <eos> a preliminary evaluation of our method shows languagedependent coreference strategies lead to better generation results. <eos> createdby ( guernica, pablopicasso ) currentlocation ( guernica, museoreinasof ? a ) hascolor ( guernica, white ) hascolor ( guernica, gray ) hascolor ( guernica, black ) guernica is created by pablo picasso. <eos> guernica has as current location the museo reina sof ? a. <eos> guernica has as color white, gray and black. <eos> figure 1 : a natural language description generated from a set of ontology statements.
human-written, good quality extractive summaries pay great attention to the text intermixing the extracts. <eos> in this work, we focused on the lexical choice for verbs introducing quoted text. <eos> we analyzed 4000+ high quality summaries for a high traffic mailing list and manually assembled 39 quotation-introducing verb classes that cover the majority of the verb occurrences. <eos> a significant amount of the data is covered by on-going work on e-mail ? speech acts. ? <eos> however, we found that one third of the ? tail ? <eos> is composed by ? risky ? <eos> verbs that most likely will be beyond the state of the art for longer time. <eos> we used this fact to highlight the trade-offs of risk taking in nlg, where interesting prose might come at the cost of unsettling some of the readers.
we present an approach for generation of morphologically rich languages using statistical machine translation. <eos> given a sequence of lemmas and any subset of morphological features, we produce the inflected word forms. <eos> testing on arabic, a morphologically rich language, our models can reach 92.1 % accuracy starting only with lemmas, and 98.9 % accuracy if all the gold features are provided.
while some recent work in tutorial dialogue has touched upon tutor reformulations of student contributions, there has not yet been an attempt to characterize the intentions of reformulations in this educational context nor an attempt to determine which types of reformulation actually contribute to student learning. <eos> in this paper we take an initial look at tutor reformulations of student contributions in naturalistic tutorial dialogue in order to characterize the range of pedagogical intentions that may be associated with these reformulations. <eos> we further outline our plans for implementing reformulation in our tutorial dialogue system, rimac, which engages high school physics students in post problem solving reflective discussions. <eos> by implementing reformulations in a tutorial dialogue system we can begin to test their impact on student learning in a more controlled way in addition to testing whether our approximation of reformulation is adequate.
nlg developers must work closely with domain experts in order to build good nlg systems, but relatively little has been published about this process. <eos> in this paper, we describe how nlg developers worked with clinicians ( nurses ) to improve an nlg system which generates information for parents of babies in a neonatal intensive care unit, using a structured revision process. <eos> we believe that such a process can significantly enhance the quality of many nlg systems, in medicine and elsewhere.
this paper concerns the architecture of a generator for italian sign language. <eos> in particular we describe a microplanner based on an expert-system and a combinatory categorial grammar used in realization.
a useful enhancement of an nlg system for verbalising ontologies would be a module capable of explaining undesired entailments of the axioms encoded by the developer. <eos> this task raises interesting issues of content planning. <eos> one approach, useful as a baseline, is simply to list the subset of axioms relevant to inferring the entailment ; however, in many cases it will still not be obvious, even to owl experts, why the entailment follows. <eos> we suggest an approach in which further statements are added in order to construct a proof tree, with every step based on a relatively simple deduction rule of known difficulty ; we also describe an empirical study through which the difficulty of these simple deduction patterns has been measured.
question answering is an age old ai challenge. <eos> how we approach this challenge is determined by decisions regarding the linguistic and domain knowledge our system will need, the technical and business acumen of our users, the interface used to input questions, and the form in which we should present answers to a user ? s questions. <eos> our approach to question answering involves the interactive construction of natural language queries. <eos> we describe and evaluate a question answering system that provides a point-and-click, webbased interface in conjunction with a semantic grammar to support user-controlled natural language question generation. <eos> a preliminary evaluation is performed using a selection of 12 questions based on the adventure works sample database.
this paper proposes the use of nlg to enhance public engagement during the course of species reintroductions. <eos> we examine whether ecological insights can be effectively communicated through blogs about satellite-tagged individuals, and whether such blogs can help create a positive perception of the species in readers ? <eos> minds, a requirement for successful reintroduction. <eos> we then discuss the implications for nlg systems that generate blogs from satellite-tag data.
g natural language summaries for multimedia duo ding, florian metze, shourabh rawat, peter f. schulam, susanne burger school of computer science, carnegie mellon university pittsburgh, pa, usa 15213 { dding, fmetze, srawat, pschulam, sburger } @ cs.cmu.edu abstract in this paper we introduce an automatic sys-tem that generates textual summaries of inter-net-style video clips by first identifying suitable high-level descriptive features that have been detected in the video ( e.g. <eos> visual concepts, recognized speech, actions, objects, persons, etc. ). <eos> then a natural language genera-tor is constructed using simplenlg to com-pile the high-level features into a textual form. <eos> the generated summary contains information from both visual and acoustic sources, intend-ing to give a general review and summary of the video. <eos> to reduce the complexity of the task, we restrict ourselves to work with videos that show a limited number of ? events ?. <eos> in this demo paper, we describe the design of the system and present example outputs generated by the video summarization system.
we demonstrate a novel, robust vision-tolanguage generation system called midge. <eos> midge is a prototype system that connects computer vision to syntactic structures with semantic constraints, allowing for the automatic generation of detailed image descriptions. <eos> we explain how to connect vision detections to trees in penn treebank syntax, which provides the scaffolding necessary to further refine data-driven statistical generation approaches for a variety of end goals.
the surface realisation shared task was first run in 2011. <eos> two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs. <eos> however, the input representations had several shortcomings which we have been aiming to address in the time since. <eos> this paper reports on our work to date on improving the input representations and on our plans for the next edition of the sr task. <eos> we also briefly summarise other related developments in nlg shared tasks and outline how the different ideas may be usefully brought together in the future.
we describe a new shared task on syntactic paraphrase ranking that is intended to run in conjunction with the main surface realization shared task. <eos> taking advantage of the human judgments collected to evaluate the surface realizations produced by competing systems, the task is to automatically rank these realizations ? viewed as syntactic paraphrases ? in a way that agrees with the human judgments as often as possible. <eos> the task is designed to appeal to developers of surface realization systems as well as machine translation evaluation metrics : for surface realization systems, the task sidesteps the thorny issue of converting inputs to a common representation ; for mt evaluation metrics, the task provides a challenging framework for advancing automatic evaluation, as many of the paraphrases are expected to be of high quality, differing only in subtle syntactic choices.
conversations in poster sessions in academic events, referred to as poster conversations, pose interesting and challenging topics on multi-modal analysis of multi-party dialogue. <eos> this article gives an overview of our project on multi-modal sensing, analysis and ? understanding ? <eos> of poster conversations. <eos> we focus on the audience ? s feedback behaviors such as non-lexical backchannels ( reactive tokens ) and noddings as well as joint eye-gaze events by the presenter and the audience. <eos> we investigate whether we can predict when and who will ask what kind of questions, and also interest level of the audience. <eos> based on these analyses, we design a smart posterboard which can sense human behaviors and annotate interactions and interest level during poster sessions.
we present and evaluate two state-of-the art dialogue systems developed to support dialog with french speaking virtual characters in the context of a serious game : one hybrid statistical/symbolic and one purely statistical. <eos> we conducted a quantitative evaluation where we compare the accuracy of the interpreter and of the dialog manager used by each system ; a user based evaluation based on 22 subjects using both the statistical and the hybrid system ; and a corpus based evaluation where we examine such criteria as dialog coherence, dialog success, interpretation and generation errors in the corpus of human-system interactions collected during the user-based evaluation. <eos> we show that although the statistical approach is slightly more robust, the hybrid strategy seems to be better at guiding the player through the game.
one challenge of implementing spoken dialogue systems for long-term interaction is how to adapt the dialogue as user and system become more familiar. <eos> we believe this challenge includes evoking and signaling aspects of long-term relationships such as rapport. <eos> for tutoring systems, this may additionally require knowing how relationships are signaled among non-adult users. <eos> we therefore investigate conversational strategies used by teenagers in peer tutoring dialogues, and how these strategies function differently among friends or strangers. <eos> in particular, we use annotated and automatically extracted linguistic devices to predict impoliteness and positivity in the next turn. <eos> to take into account the sparse nature of these features in real data we use models including lasso, ridge estimator, and elastic net. <eos> we evaluate the predictive power of our models under various settings, and compare our sparse models with standard non-sparse solutions. <eos> our experiments demonstrate that our models are more accurate than non-sparse models quantitatively, and that teens use unexpected kinds of language to do relationship work such as signaling rapport, but friends and strangers, tutors and tutees, carry out this work in quite different ways from one another.
the ability to monitor the communicative success of its utterances and, if necessary, provide feedback and repair is useful for a dialog system. <eos> we show that in situated communication, eyetracking can be used to reliably and efficiently monitor the hearer ? s reference resolution process. <eos> an interactive system that draws on hearer gaze to provide positive or negative feedback after referring to objects outperforms baseline systems on metrics of referential success and user confusion.
we present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify ? summaryworthy ? <eos> words. <eos> concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decisionmaking process. <eos> moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. <eos> finally, context information is also investigated to add additional relevant information to the summary.
this paper proposes an unsupervised approach to user simulation in order to automatically furnish updates and assessments of a deployed spoken dialog system. <eos> the proposed method adopts a dynamic bayesian network to infer the unobservable true user action from which the parameters of other components are naturally derived. <eos> to verify the quality of the simulation, the proposed method was applied to the let ? s go domain ( raux et al, 2005 ) and a set of measures was used to analyze the simulated data at several levels. <eos> the results showed a very close correspondence between the real and simulated data, implying that it is possible to create a realistic user simulator that does not necessitate human intervention.
conversational practices do not occur at a single unit of analysis. <eos> to understand the interplay between social positioning, information sharing, and rhetorical strategy in language, various granularities are necessary. <eos> in this work we present a machine learning model for multi-party chat which predicts conversation structure across differing units of analysis. <eos> first, we mark sentence-level behavior using an information sharing annotation scheme. <eos> by taking advantage of integer linear programming and a sociolinguistic framework, we enforce structural relationships between sentence-level annotations and sequences of interaction. <eos> then, we show that clustering these sequences can effectively disentangle the threads of conversation. <eos> this model is highly accurate, performing near human accuracy, and performs analysis on-line, opening the door to real-time analysis of the discourse of conversation.
we herein propose a method for the rapid development of a spoken dialogue system based on collaboratively constructed semantic resources and compare the proposed method with a conventional method that is based on a relational database. <eos> previous development frameworks of spoken dialogue systems, which presuppose a relational database management system as a background application, require complex data definition, such as making entries in a task-dependent language dictionary, templates of semantic frames, and conversion rules from user utterances to the query language of the database. <eos> we demonstrate that a semantic web oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations in the rapid development process of spoken dialogue systems.
in recent years statistical dialogue systems have gained significant attention due to their potential to be more robust to speech recognition errors. <eos> however, these systems must also be robust to changes in user behaviour caused by cognitive loading. <eos> in this paper, a statistical dialogue system providing restaurant information is evaluated in a set-up where the subjects used a driving simulator whilst talking to the system. <eos> the influences of cognitive loading were investigated and some clear differences in behaviour were discovered. <eos> in particular, it was found that users chose to respond to different system questions and use different speaking styles, which indicate the need for an incremental dialogue approach.
recent work on consultations between outpatients with schizophrenia and psychiatrists has shown that adherence to treatment can be predicted by patterns of repair ? <eos> specifically, the pro-activity of the patient in checking their understanding, i.e. <eos> patient clarification. <eos> using machine learning techniques, we investigate whether this tendency can be predicted from high-level dialogue features, such as backchannels, overlap and each participant ? s proportion of talk. <eos> the results indicate that these features are not predictive of a patient ? s adherence to treatment or satisfaction with the communication, although they do have some association with symptoms. <eos> however, all these can be predicted if we allow features at the word level. <eos> these preliminary experiments indicate that patient adherence is predictable from dialogue transcripts, but further work is necessary to develop a meaningful, general and reliable feature set.
we use reinforcement learning ( rl ) to learn question-answering dialogue policies for a real-world application. <eos> we analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the museum of science in boston, in order to build a realistic model of user behavior when interacting with these characters. <eos> a simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using rl. <eos> our learned policy outperforms two baselines ( including the original dialogue policy that was used for collecting the corpus ) in a simulation setting.
a robust system that understands route instructions should be able to process instructions generated naturally by humans. <eos> also desirable would be the ability to handle repairs and other modifications to existing instructions. <eos> to this end, we collected a corpus of spoken instructions ( and modified instructions ) produced by subjects provided with an origin and a destination. <eos> we found that instructions could be classified into four categories, depending on their intent such as imperative, feedback, or meta comment. <eos> we asked a different set of subjects to follow these instructions to determine the usefulness and comprehensibility of individual instructions. <eos> finally, we constructed a semantic grammar and evaluated its coverage. <eos> to determine whether instructiongiving forms a predictable sub-language, we tested the grammar on three corpora collected by others and determined that this was largely the case. <eos> our work suggests that predictable sub-languages may exist for well-defined tasks. <eos> index terms : robot navigation, spoken instructions
we provide a systematic study of previously proposed features for implicit discourse relation identification, identifying new feature combinations that optimize f1-score. <eos> the resulting classifiers achieve the best f1-scores to date for the four top-level discourse relation classes of the penn discourse tree bank : comparison, contingency, expansion, and temporal. <eos> we further identify factors for feature extraction that can have a major impact on performance and determine that some features originally proposed for the task no longer provide performance gains in light of more powerful, recently discovered features. <eos> our results constitute a new set of baselines for future studies of implicit discourse relation identification.
developing sophisticated turn-taking behavior is necessary for next-generation dialogue systems. <eos> however, incorporating real users into the development cycle is expensive and current simulation techniques are inadequate. <eos> as a foundation for advancing turn-taking behavior, we present a temporal simulator that models the interaction between the user and the system, including speech, voice activity detection, and incremental speech recognition. <eos> we describe the details of the simulator and demonstrate it on a sample domain.
in this work we study the effectiveness of speaker adaptation for dialogue act recognition in multiparty meetings. <eos> first, we analyze idiosyncracy in dialogue verbal acts by qualitatively studying the differences and conflicts among speakers and by quantitively comparing speaker-specific models. <eos> based on these observations, we propose a new approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers ? <eos> data. <eos> our experiments on a realworld meeting dataset show that with even only 200 speaker-specific annotated dialogue acts, the performances on dialogue act recognition are significantly improved when compared to several baseline algorithms. <eos> to our knowledge, this work is the first 1 to tackle this promising research direction of speaker adaptation for dialogue act recogntion.
this work investigates to what degree speakers with different verbal intelligence may adapt to each other. <eos> the work is based on a corpus consisting of 100 descriptions of a short film ( monologues ), 56 discussions about the same topic ( dialogues ), and verbal intelligence scores of the test participants. <eos> adaptation between two dialogue partners was measured using cross-referencing, proportion of ? i ?, ? you ? <eos> and ? we ? <eos> words, between-subject correlation and similarity of texts. <eos> it was shown that lower verbal intelligence speakers repeated more nouns and adjectives from the other and used the same linguistic categories more often than higher verbal intelligence speakers. <eos> in dialogues between strangers, participants with higher verbal intelligence showed a greater level of adaptation.
we present a mixed initiative conversational dialogue system designed to address primarily mental health care concerns related to military deployment. <eos> it is supported by a new information-state based dialogue manager, flores ( forward-looking, reward seeking dialogue manager ), that allows both advanced, flexible, mixed initiative interaction, and efficient policy creation by domain experts. <eos> to easily reach its target population this dialogue system is accessible as a web application.
to enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. <eos> in particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. <eos> our empirical results have shown that, even when computer vision algorithms produce many errors ( e.g. <eos> 84.7 % of the objects in the environment are mis-recognized ), our approach can still achieve 66 % accuracy in referential grounding. <eos> these results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction.
a coherently related group of sentences may be referred to as a discourse. <eos> in this paper we address the problem of parsing coherence relations as defined in the penn discourse tree bank ( pdtb ). <eos> a good model for discourse structure analysis needs to account both for local dependencies at the token-level and for global dependencies and statistics. <eos> we present techniques on using inter-sentential or sentence-level ( global ), data-driven, nongrammatical features in the task of parsing discourse. <eos> the parser model follows up previous approach based on using tokenlevel ( local ) features with conditional random fields for shallow discourse parsing, which is lacking in structural knowledge of discourse. <eos> the parser adopts a twostage approach where first the local constraints are applied and then global constraints are used on a reduced weighted search space ( n-best ). <eos> in the latter stage we experiment with different rerankers trained on the first stage n-best parses, which are generated using lexico-syntactic local features. <eos> the two-stage parser yields significant improvements over the best performing model of discourse parser on the pdtb corpus.
this paper presents a discriminative reranking model for the discourse segmentation task, the first step in a discourse parsing system. <eos> our model exploits subtree features to rerank nbest outputs of a base segmenter, which uses syntactic and lexical features in a crf framework. <eos> experimental results on the rst discourse treebank corpus show that our model outperforms existing discourse segmenters in both settings that use gold standard penn treebank parse trees and stanford parse trees.
many modern spoken dialog systems use probabilistic graphical models to update their belief over the concepts under discussion, increasing robustness in the face of noisy input. <eos> however, such models are ill-suited to probabilistic reasoning about spatial relationships between entities. <eos> in particular, a car navigation system that infers users ? <eos> intended destination using nearby landmarks as descriptions must be able to use distance measures as a factor in inference. <eos> in this paper, we describe a belief tracking system for a location identification task that combines a semantic belief tracker for categorical concepts based on the dpot framework ( raux and ma, 2011 ) with a kernel density estimator that incorporates landmark evidence from multiple turns and landmark hypotheses, into a posterior probability over candidate locations. <eos> we evaluate our approach on a corpus of destination setting dialogs and show that it significantly outperforms a deterministic baseline.
probabilistic models such as bayesian networks are now in widespread use in spoken dialogue systems, but their scalability to complex interaction domains remains a challenge. <eos> one central limitation is that the state space of such models grows exponentially with the problem size, which makes parameter estimation increasingly difficult, especially for domains where only limited training data is available. <eos> in this paper, we show how to capture the underlying structure of a dialogue domain in terms of probabilistic rules operating on the dialogue state. <eos> the probabilistic rules are associated with a small, compact set of parameters that can be directly estimated from data. <eos> we argue that the introduction of this abstraction mechanism yields probabilistic models that are easier to learn and generalise better than their unstructured counterparts. <eos> we empirically demonstrate the benefits of such an approach learning a dialogue policy for a human-robot interaction domain based on a wizard-of-oz data set.
this paper proposes the use of unsupervised approaches to improve components of partition-based belief tracking systems. <eos> the proposed method adopts a dynamic bayesian network to learn the user action model directly from a machine-transcribed dialog corpus. <eos> it also addresses confidence score calibration to improve the observation model in a unsupervised manner using dialog-level grounding information. <eos> to verify the effectiveness of the proposed method, we applied it to the let ? s go domain ( raux et al, 2005 ). <eos> overall system performance for several comparative models were measured. <eos> the results show that the proposed method can learn an effective user action model without human intervention. <eos> in addition, the calibrated confidence score was verified by demonstrating the positive influence on the user action model learning process and on overall system performance.
models of dialog state are important, both scientifically and practically, but today ? s best build strongly on tradition. <eos> this paper presents a new way to identify the important dimensions of dialog state, more bottomup and empirical than previous approaches. <eos> specifically, we applied principal component analysis to a large number of low-level prosodic features to find the most important dimensions of variation. <eos> the top 20 out of 76 dimensions accounted for 81 % of the variance, and each of these dimensions clearly related to dialog states and activities, including turn taking, topic structure, grounding, empathy, cognitive processes, attitude and rhetorical structure.
addressee identification is an element of all language-based interactions, and is critical for turn-taking. <eos> we examine the particular problem of identifying when each child playing an interactive game in a small group is speaking to an animated character. <eos> after analyzing child and adult behavior, we explore a family of machine learning models to integrate audio and visual features with temporal group interactions and limited, task-independent language. <eos> the best model performs identification about 20 % better than the model that uses the audio-visual features of the child alone.
we evaluate a wizard-of-oz spoken dialogue system that adapts to multiple user affective states in real-time : user disengagement and uncertainty. <eos> we compare this version with the prior version of our system, which only adapts to user uncertainty. <eos> our analysis investigates how iteratively adding new affect adaptation to an existing affect-adaptive system impacts global and local performance. <eos> we find a significant increase in motivation for users who most frequently received the disengagement adaptation. <eos> moreover, responding to disengagement breaks its negative correlations with task success and user satisfaction, reduces uncertainty levels, and reduces the likelihood of continued disengagement.
we propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from twitter and real-time crowdsourcing. <eos> instead of using complex dialog management, our system replies with the utterance from the database that is most similar to the user input. <eos> we also propose a realtime crowdsourcing framework for handling the case in which there is no adequate response in the database.
we present a model for automatically predicting information status labels for german referring expressions. <eos> we train a crf on manually annotated phrases, and predict a fine-grained set of labels. <eos> we achieve an accuracy score of 69.56 % on our most detailed label set, 76.62 % when gold standard coreference is available.
this paper proposes a probabilistic approach to the resolution of referring expressions for task-oriented dialogue systems. <eos> the approach resolves descriptions, anaphora, and deixis in a unified manner. <eos> in this approach, the notion of reference domains serves an important role to handle context-dependent attributes of entities and references to sets. <eos> the evaluation with the rex-j corpus shows promising results.
ambiguous or open-ended requests to a dialogue system result in more complex dialogues. <eos> we present a semantic-specificity metric to gauge this complexity for dialogue systems that access a relational database. <eos> an experiment where a simulated user makes requests to a dialogue system shows that semantic specificity correlates with dialogue length.
huang hsin-hsi chen department of computer science and department of computer science and information engineering, information engineering, national taiwan university, taipei, taiwan national taiwan university, taipei, taiwan hhhuang @ nlg.csie.ntu.edu.tw hhchen @ csie.ntu.edu.tw abstract unlike in english, the sentence boundaries in chinese are fuzzy and not well-defined. <eos> as a result, chinese sentences tend to be long and consist of complex discourse relations. <eos> in this paper, we focus on two important relations, contingency and comparison, which occur often inside a sentence. <eos> we construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure. <eos> a learning based model is evaluated with various features. <eos> experimental results show our model achieves accuracies of 81.63 % in the task of relation labeling and 74.8 % in the task of relation structure prediction.
this paper presents an analysis of how the level of performance achievable by an nlu module can affect the optimal modular design of a dialogue system. <eos> we present an evaluation that shows how nlu accuracy levels impact the overall performance of a system that includes an nlu module and a rule-based dialogue policy. <eos> we contrast these performance levels with the performance of a direct classification design that omits a separate nlu module. <eos> we conclude with a discussion of the potential for a hybrid architecture incorporating the strengths of both approaches.
the goal of this paper is to present a first step toward integrating incremental speech recognition ( isr ) and partially-observable markov decision process ( pomdp ) based dialogue systems. <eos> the former provides support for advanced turn-taking behavior while the other increases the semantic accuracy of speech recognition results. <eos> we present an incremental interaction manager that supports the use of isr with strictly turn-based dialogue managers. <eos> we then show that using a pomdp-based dialogue manager with isr substantially improves the semantic accuracy of the incremental results.
during conversations, addressees produce conversational acts ? verbal and nonverbal backchannels ? that facilitate turn-taking, acknowledge speakership, and communicate common ground without disrupting the speaker ? s speech. <eos> these acts play a key role in achieving fluent conversations. <eos> therefore, gaining a deeper understanding of how these acts interact with speaker behaviors in shaping conversations might offer key insights into the design of technologies such as computer-mediated communication systems and embodied conversational agents. <eos> in this paper, we explore how a regression-based approach might offer such insights into modeling predictive relationships between speaker behaviors and addressee backchannels in a storytelling scenario. <eos> our results reveal speaker eye contact as a significant predictor of verbal, nonverbal, and bimodal backchannels and utterance boundaries as predictors of nonverbal and bimodal backchannels.
with the aim of investigating how humans understand each other through language and gestures, this paper focuses on how people understand incomplete sentences. <eos> we trained a system based on interrupted but resumed sentences, in order to find plausible completions for incomplete sentences. <eos> our promising results are based on multi-modal features.
participants in a conversation are normally receptive to their surroundings and their interlocutors, even while they are speaking and can, if necessary, adapt their ongoing utterance. <eos> typical dialogue systems are not receptive and can not adapt while uttering. <eos> we present combinable components for incremental natural language generation and incremental speech synthesis and demonstrate the flexibility they can achieve with an example system that adapts to a listener ? s acoustic understanding problems by pausing, repeating and possibly rephrasing problematic parts of an utterance. <eos> in an evaluation, this system was rated as significantly more natural than two systems representing the current state of the art that either ignore the interrupting event or just pause ; it also has a lower response time.
we present a novel unsupervised framework for focused meeting summarization that views the problem as an instance of relation extraction. <eos> we adapt an existing in-domain relation learner ( chen et al, 2011 ) by exploiting a set of task-specific constraints and features. <eos> we evaluate the approach on a decision summarization task and show that it outperforms unsupervised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method. <eos> moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard rouge score.
we present work on understanding natural language in a situated domain, that is, language that possibly refers to visually present entities, in an incremental, word-by-word fashion. <eos> such type of understanding is required in conversational systems that need to act immediately on language input, such as multi-modal systems or dialogue systems for robots. <eos> we explore a set of models specified as markov logic networks, and show that a model that has access to information about the visual context of an utterance, its discourse context, as well as the linguistic structure of the utterance performs best. <eos> we explore its incremental properties, and also its use in a joint parsing and understanding module. <eos> we conclude that mlns offer a promising framework for specifying such models in a general, possibly domain-independent way.
children acquire mental state verbs ( msvs ) much later than other, lower-frequency, words. <eos> one factor proposed to contribute to this delay is that children must learn various semantic and syntactic cues that draw attention to the difficult-to-observe mental content of a scene. <eos> we develop a novel computational approach that enables us to explore the role of such cues, and show that our model can replicate aspects of the developmental trajectory of msv acquisition.
for a given concrete noun concept, humans are usually able to cite properties ( e.g., elephant is animal, car has wheels ) of that concept ; cognitive psychologists have theorised that such properties are fundamental to understanding the abstract mental representation of concepts in the brain. <eos> consequently, the ability to automatically extract such properties would be of enormous benefit to the field of experimental psychology. <eos> this paper investigates the use of semi-supervised learning and support vector machines to automatically extract concept-relation-feature triples from two large corpora ( wikipedia and ukwac ) for concrete noun concepts. <eos> previous approaches have relied on manually-generated rules and hand-crafted resources such as wordnet ; our method requires neither yet achieves better performance than these prior approaches, measured both by comparison with a property norm-derived gold standard as well as direct human evaluation. <eos> our technique performs particularly well on extracting features relevant to a given concept, and suggests a number of promising areas for future focus.
some of the most robust effects of linguistic variables on eye movements in reading are those of word length. <eos> their leading explanation states that they are caused by visual acuity limitations on word recognition. <eos> however, bicknell ( 2011 ) presented data showing that a model of eye movement control in reading that includes visual acuity limitations and models the process of word identification from visual input ( bicknell & levy, 2010 ) does not produce humanlike word length effects, providing evidence against the visual acuity account. <eos> here, we argue that uncertainty about word length in early word identification can drive word length effects. <eos> we present an extension of bicknell and levy ? s model that incorporates word length uncertainty, and show that it produces more humanlike word length effects.
we describe a computational framework for language learning and parsing in which dynamical systems navigate on fractal sets. <eos> we explore the predictions of the framework in an artificial grammar task in which humans and recurrent neural networks are trained on a language with recursive structure. <eos> the results provide evidence for the claim of the dynamical systems models that grammatical systems continuously metamorphose during learning. <eos> the present perspective permits structural comparison between the recursive representations in symbolic and neural network models.
probabilistic context-free grammars ( pcfgs ) are a popular cognitive model of syntax ( jurafsky, 1996 ). <eos> these can be formulated to be sensitive to human working memory constraints by application of a right-corner transform ( schuler, 2009 ). <eos> one side-effect of the transform is that it guarantees at most a single expansion ( push ) and at most a single reduction ( pop ) during a syntactic parse. <eos> the primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser. <eos> this yields a simpler structure that more closely resembles existing simple recurrent network models of sentence comprehension.
experimental evidence demonstrates that syntactic structure influences human online sentence processing behavior. <eos> despite this evidence, open questions remain : which type of syntactic structure best explains observed behavior ? hierarchical or sequential, and lexicalized or unlexicalized ? <eos> recently, frank and bod ( 2011 ) find that unlexicalized sequential models predict reading times better than unlexicalized hierarchical models, relative to a baseline prediction model that takes wordlevel factors into account. <eos> they conclude that the human parser is insensitive to hierarchical syntactic structure. <eos> we investigate these claims and find a picture more complicated than the one they present. <eos> first, we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of frank and bod ( 2011 ) eliminates all differences in accuracy between those unlexicalized sequential and hierarchical models. <eos> second, we show that lexicalizing the hierarchical models used in frank and bod ( 2011 ) significantly improves prediction accuracy relative to the unlexicalized versions. <eos> third, we show that using stateof-the-art lexicalized hierarchical models further improves prediction accuracy. <eos> our results demonstrate that the claim of frank and bod ( 2011 ) that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization matters for prediction accuracy.
logical metonymies ( the student finished the beer ) represent a challenge to compositionality since they involve semantic content not overtly realized in the sentence ( covert events ? <eos> drinking the beer ). <eos> we present a contrastive study of two classes of computational models for logical metonymy in german, namely a probabilistic and a distributional, similarity-based model. <eos> these are built using the sdewac corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context. <eos> the similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models.
in the last two decades, information-seeking spoken dialog systems ( sds ) have moved from research prototypes to real-life commercial applications. <eos> still, dialog systems are limited by the scale, complexity of the task and coverage of knowledge required by problemsolving machines or mobile personal assistants. <eos> future spoken interaction are required to be multilingual, understand and act on large scale knowledge bases in all its forms ( from structured to unstructured ). <eos> the web research community have striven to build large scale and open multilingual resources ( e.g. <eos> wikipedia ) and knowledge bases ( e.g. <eos> yago ). <eos> we argue that a ) it is crucial to leverage this massive amount of web lightly structured knowledge and b ) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community.
we argue that standardized metrics and automatic evaluation tools are necessary for speeding up knowledge generation and development processes for dialog systems. <eos>
there has been a lot of interest for user simulation in the field of spoken dialogue systems during the last decades. <eos> user simulation was first proposed to assess the performance of sds before a public release. <eos> since the late 90 ? s, user simulation is also used for dialogue management optimisation. <eos> in this position paper, we focus on statistical methods for user simulation, their main advantages and drawbacks. <eos> we initiate a reflection about the utility of such methods and give some insights of what their future should be.
a sketch of dialogue systems as long-term adaptive, conversational agents. <eos>
spoken dialog systems frameworks fill a crucial role in the spoken dialog systems community by providing resources to lower barriers to entry. <eos> however, different user groups have different requirements and expectations for such systems. <eos> here, we consider the particular needs for spoken dialog systems toolkits within an instructional setting. <eos> we discuss the challenges for existing systems in meeting these needs and propose strategies to overcome them.
belief tracking is a promising technique for adding robustness to spoken dialog systems, but current research is fractured across different teams, techniques, and domains. <eos> this paper amplifies past informal discussions ( raux, 2011 ) to call for a belief tracking challenge task, based on the spoken dialog challenge corpus ( black et al, 2011 ). <eos> benefits, limitations, evaluation design issues, and next steps are presented.
we herein introduce our project of realizing a framework for the development of a spoken dialogue system based on collaboratively constructed semantic resources. <eos> we demonstrate that a semantic web-oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations, which are caused by the previous relational database-based approach, in the development process of spoken dialogue systems. <eos> in addition, we show that the proposed framework enables multilingual spoken dialogue system development due to clear separation of model, view and controller components.
we describe the 2012 release of our ? incremental processing toolkit ? <eos> ( inprotk ) 1, which combines a powerful and extensible architecture for incremental processing with components for incremental speech recognition and, new to this release, incremental speech synthesis. <eos> these components work fairly domainindependently ; we also provide example implementations of higher-level components such as natural language understanding and dialogue management that are somewhat more tied to a particular domain. <eos> we offer this release of the toolkit to foster research in this new and exciting area, which promises to help increase the naturalness of behaviours that can be modelled in such systems.
ion-based framework for spoken language understanding and action selection in situated interaction david cohen ian lane carnegie mellon university carnegie mellon university nasa research park nasa research park moffett field, ca moffett field, ca david.cohen @ sv.cmu.edu lane @ cs.cmu.edu abstract this paper introduces a simulation-based framework for performing action selection and understanding for interactive agents. <eos> by simulating the objects and actions relevant to an interaction, an agent can semantically ground natural language and interact consid-erately and on its own initiative in situated environments. <eos> the framework proposed in this paper leverages models of the environ-ment, user and system to predict possible fu-ture world states via simulation. <eos> it leverages understanding of spoken language and multi-modal input to estimate the state of the ongo-ing interaction and select actions based on the utility of future outcomes in the simulated world. <eos> in this paper we introduce this frame-work and demonstrate its effectiveness for in-car navigation.
in a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding ( slu ) is a crucial component aiming at capturing the key semantic components of utterances. <eos> building a robust slu system is a challenging task due to variability in the usage of language, need for labeled data, and requirements to expand to new domains ( movies, travel, finance, etc. ). <eos> in this paper, we survey recent research on bootstrapping or improving slu systems by using information mined or extracted from web search query logs, which include ( natural language ) queries entered by users as well as the links ( web sites ) they click on. <eos> we focus on learning methods that help unveiling hidden information in search query logs via implicit crowd-sourcing.
developing interactive robots is an extremely challenging task which requires a broad range of expertise across diverse disciplines, including, robotic planning, spoken language understanding, belief tracking and action management. <eos> while there has been a boom in recent years in the development of reusable components for robotic systems within common architectures, such as the robot operating system ( ros ), little emphasis has been placed on developing components for humanrobot-interaction. <eos> in this paper we introduce hritk ( the human-robot-interaction toolkit ), a framework, consisting of messaging protocols, core-components, and development tools for rapidly building speech-centric interactive systems within the ros environment. <eos> the proposed toolkit was specifically designed for extensibility, ease of use, and rapid development, allowing developers to quickly incorporate speech interaction into existing projects.
information about the quality of a spoken dialogue system ( sds ) is usually used only for comparing sdss with each other or manually improving the dialogue strategy. <eos> this information, however, provides a means for inherently improving the dialogue performance by adapting the dialogue manager during the interaction accordingly. <eos> for a quality metric to be suitable, it must suffice certain conditions. <eos> therefore, we address requirements for the quality metric and, additionally, present approaches for quality-adaptive dialogue management.
the frame-semantic parsing task is challenging for supervised techniques, even for those few languages where relatively large amounts of labeled data are available. <eos> in this preliminary work, we consider unsupervised induction of frame-semantic representations. <eos> an existing state-of-the-art bayesian model for propbank-style unsupervised semantic role induction ( titov and klementiev, 2012 ) is extended to jointly induce semantic frames and their roles. <eos> we evaluate the model performance both quantitatively and qualitatively by comparing the induced representation against framenet annotations.
in our experiment, we evaluate the transferability of frames from swedish to finnish in parallel corpora. <eos> we evaluate both the theoretical possibility of transferring frames and the possibility of performing it using available lexical resources. <eos> we add the frame information to an extract of the swedish side of the kotus and jrc-acquis corpora using an automatic frame labeler and copy it to the finnish side. <eos> we focus on evaluating the results to get an estimation on how often the parallel sentences can be said to express the same frame. <eos> this sheds light on the questions : are the same situations in the two languages expressed using different frames, i.e. <eos> are the frames transferable even in theory ? <eos> how well can the frame information of running text be transferred from one language to another ?
we show that orthographic cues can be helpful for unsupervised parsing. <eos> in the penn treebank, transitions between upper- and lowercase tokens tend to align with the boundaries of base ( english ) noun phrases. <eos> such signals can be used as partial bracketing constraints to train a grammar inducer : in our experiments, directed dependency accuracy increased by 2.2 % ( average over 14 languages having case information ). <eos> combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attaining state-of-the-art levels for many languages.
we provide a model that extends the splitmerge framework of petrov et al ( 2006 ) to jointly learn latent annotations and tree substitution grammars ( tsgs ). <eos> we then conduct a variety of experiments with this model, first inducing grammars on a portion of the penn treebank and the korean treebank 2.0, and next experimenting with grammar refinement from a single nonterminal and from the universal part of speech tagset. <eos> we present qualitative analysis showing promising signs across all experiments that our combined approach successfully provides for greater flexibility in grammar induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches.
for many nlp tasks, em-trained hmms are the common models. <eos> however, in order to escape local maxima and find the best model, we need to start with a good initial model. <eos> researchers suggested repeated random restarts or constraints that guide the model evolution. <eos> neither approach is ideal. <eos> restarts are time-intensive, and most constraint-based approaches require serious re-engineering or external solvers. <eos> in this paper we measure the effectiveness of very limited initial constraints : specifically, annotations of a small number of words in the training data. <eos> we vary the amount and distribution of initial partial annotations, and compare the results to unsupervised and supervised approaches. <eos> we find that partial annotations improve accuracy and can reduce the need for random restarts, which speeds up training time considerably.
some of the most used models for statistical word alignment are the ibm models. <eos> although these models generate acceptable alignments, they do not exploit the rich information found in lexical resources, and as such have no reasonable means to choose better translations for specific senses. <eos> we try to address this issue by extending the ibm hmm model with an extra hidden layer which represents the senses a word can take, allowing similar words to share similar output distributions. <eos> we test a preliminary version of this model on english-french data. <eos> we compare different ways of generating senses and assess the quality of the alignments relative to the ibm hmm model, as well as the generated sense probabilities, in order to gauge the usefulness in word sense disambiguation.
as linguistic models incorporate more subtle nuances of language and its structure, standard inference techniques can fall behind. <eos> often, such models are tightly coupled such that they defy clever dynamic programming tricks. <eos> however, sequential monte carlo ( smc ) approaches, i.e. <eos> particle filters, are well suited to approximating such models, resolving their multi-modal nature at the cost of generating additional samples. <eos> we implement two particle filters, which jointly sample either sentences or word types, and incorporate them into a gibbs sampler for part-of-speech ( pos ) inference. <eos> we analyze the behavior of the particle filters, and compare them to a block sentence sampler, a local token sampler, and a heuristic sampler, which constrains inference to a single pos per word type. <eos> our findings show that particle filters can closely approximate a difficult or even intractable sampler quickly. <eos> however, we found that high posterior likelihood do not necessarily correspond to better many-to-one accuracy. <eos> the results suggest that the approach has potential and more advanced particle filters are likely to lead to stronger performance.
in this paper, we study direct transfer methods for multilingual named entity recognition. <eos> specifically, we extend the method recently proposed by ta ? ckstro ? m et al ( 2012 ), which is based on cross-lingual word cluster features. <eos> first, we show that by using multiple source languages, combined with self-training for target language adaptation, we can achieve significant improvements compared to using only single source direct transfer. <eos> second, we investigate how the direct transfer system fares against a supervised target language system and conclude that between 8,000 and 16,000 word tokens need to be annotated in each target language to match the best direct transfer system. <eos> finally, we show that we can significantly improve target language performance, even after annotating up to 64,000 tokens in the target language, by simply concatenating source and target language annotations.
this paper presents the results of the pascal challenge on grammar induction, a competition in which competitors sought to predict part-of-speech and dependency syntax from text. <eos> although many previous competitions have featured dependency grammars or partsof-speech, these were invariably framed as supervised learning and/or domain adaption. <eos> this is the first challenge to evaluate unsupervised induction systems, a sub-field of syntax which is rapidly becoming very popular. <eos> our challenge made use of a 10 different treebanks annotated in a range of different linguistic formalisms and covering 9 languages. <eos> we provide an overview of the approaches taken by the participants, and evaluate their results on each dataset using a range of different evaluation metrics.
this paper describes a system for unsupervised dependency parsing based on gibbs sampling algorithm. <eos> the novel approach introduces a fertility model and reducibility model, which assumes that dependent words can be removed from a sentence without violating its syntactic correctness.
our system consists of a simple, em-based induction algorithm ( bisk and hockenmaier, 2012 ), which induces a language-specific combinatory categorial grammar ( ccg ) and lexicon based on a small number of linguistic principles, e.g. <eos> that verbs may be the roots of sentences and can take nouns as arguments.
we propose an unsupervised approach to pos tagging where first we associate each word type with a probability distribution over word classes using latent dirichlet allocation. <eos> then we create a hierarchical clustering of the word types : we use an agglomerative clustering algorithm where the distance between clusters is defined as the jensenshannon divergence between the probability distributions over classes associated with each word-type. <eos> when assigning pos tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag. <eos> this simple labeler outperforms a baseline based on brown clusters on 9 out of 10 datasets.
in this paper we describe our participating system for the dependency induction track of the pascal challenge on grammar induction. <eos> our system incorporates two types of inductive biases : the sparsity bias and the unambiguity bias. <eos> the sparsity bias favors a grammar with fewer grammar rules. <eos> the unambiguity bias favors a grammar that leads to unambiguous parses, which is motivated by the observation that natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is very small. <eos> we introduce our approach to combining these two types of biases and discuss the system implementation. <eos> our experiments show that both types of inductive biases are beneficial to grammar induction.
a key challenge for dialogue-based intelligent tutoring systems lies in selecting follow-up questions that are not only context relevant but also encourage self-expression and stimulate learning. <eos> this paper presents an approach to ranking candidate questions for a given dialogue context and introduces an evaluation framework for this task. <eos> we learn to rank using judgments collected from expert human tutors, and we show that adding features derived from a rich, multi-layer dialogue act representation improves system performance over baseline lexical and syntactic features to a level in agreement with the judges. <eos> the experimental results highlight the important factors in modeling the questioning process. <eos> this work provides a framework for future work in automatic question generation and it represents a step toward the larger goal of directly learning tutorial dialogue policies directly from human examples.
we present initial steps towards an interactive essay writing tutor that improves science knowledge by analyzing student essays for misconceptions and recommending science webpages that help correct those misconceptions. <eos> we describe the five components in this system : identifying core science concepts, determining appropriate pedagogical sequences for the science concepts, identifying student misconceptions in essays, aligning student misconceptions to science concepts, and recommending webpages to address misconceptions. <eos> we provide initial models and evaluations of the models for each component.
the save science project is an attempt to address the shortcomings of current assessments of science. <eos> the project has developed two virtual worlds that each have a mystery or natural phenomenon requiring scientific explanation ; by recording students ? <eos> behavior as they investigate the mystery, these worlds can be used to assess their understanding of the scientific method. <eos> currently, however, the scoring of the assessment depends either on manual grading of students ? <eos> written responses, or, on multiple choice questions. <eos> this paper presents an automated grader that can combine with save science ? s virtual worlds to provide a cheap mechanism for assessments of the ability to apply scientific methodology. <eos> in experiments on over 300 middle school students, our best automated grader improves by over 50 % relative to the closest system from previous work in predicting grades supplied by human judges.
to date, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts. <eos> we present the first systematic analysis of several methods for assessing coherence under the framework of automated assessment ( aa ) of learner free-text responses. <eos> we examine the predictive power of different coherence models by measuring the effect on performance when combined with an aa system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner ? s level of attainment. <eos> additionally, we identify new techniques that outperform previously developed ones and improve on the best published result for aa on a publically-available dataset of english learner free-text examination scripts.
to date, most work in grammatical error correction has focused on targeting specific error types. <eos> we present a probe study into whether we can use round-trip translations obtained from google translate via 8 different pivot languages for whole-sentence grammatical error correction. <eos> we develop a novel alignment algorithm for combining multiple round-trip translations into a lattice using the terp machine translation metric. <eos> we further implement six different methods for extracting whole-sentence corrections from the lattice. <eos> our preliminary experiments yield fairly satisfactory results but leave significant room for improvement. <eos> most importantly, though, they make it clear the methods we propose have strong potential and require further study.
incorrect usage of prepositions and determiners constitute the most common types of errors made by non-native speakers of english. <eos> it is not surprising, then, that there has been a significant amount of work directed towards the automated detection and correction of such errors. <eos> however, to date, the use of different data sets and different task definitions has made it difficult to compare work on the topic. <eos> this paper reports on the hoo 2012 shared task on error detection and correction in the use of prepositions and determiners, where systems developed by 14 teams from around the world were evaluated on the same previously unseen errorful text.
we describe a study aimed at measuring the use of factual information in test-taker essays and assessing its effectiveness for predicting essay scores. <eos> we found medium correlations with the proposed measures, that remained significant after the effect of essay length was factored out. <eos> the correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. <eos> implications for development of automated essay scoring systems are discussed.
we report two new approaches for building scoring models used by automated speech scoring systems. <eos> first, we introduce the cumulative logit model ( clm ), which has been widely used in modeling categorical outcomes in statistics. <eos> on a large set of responses to an english proficiency test, we systematically compare the clm with two other scoring models that have been widely used, i.e., linear regression and decision trees. <eos> our experiments suggest that the clm has advantages in its scoring performance and its robustness to limited-sized training data. <eos> second, we propose a novel way to utilize human rating processes in automated speech scoring. <eos> applying accurate human ratings on a small set of responses can improve the whole scoring system ? s performance while meeting cost and score-reporting time requirements. <eos> we find that the scoring difficulty of each speech response, which could be modeled by the degree to which it challenged human raters, could provide a way to select an optimal set of responses for the application of human scoring. <eos> in a simulation, we show that focusing on challenging responses can achieve a larger scoring performance improvement than simply applying human scoring on the same number of randomly selected responses.
ontology for improved automated content scoring of spontaneous non-native speech miao chen klaus zechner school of information studies educational testing service syracuse university 660 rosedale road syracuse, ny 13244, usa princeton, nj 08541, usa mchen14 @ syr.edu kzechner @ ets.org abstract this paper presents an exploration into auto-mated content scoring of non-native sponta-neous speech using ontology-based information to enhance a vector space ap-proach. <eos> we use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two co-sine-similarity-based features, previously used in the context of automated essay scoring. <eos> we use two ontology-facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in wordnet : ( 1 ) extending word vectors with semantic con-cepts from the wordnet ontology ( synsets ) ; and ( 2 ) using a reasoning approach for esti-mating the concept weights of concepts not present in the set of training responses by ex-ploiting the hierarchical structure of wordnet. <eos> furthermore, we compare features computed from human transcriptions of spoken respons-es with features based on output from an au-tomatic speech recognizer. <eos> we find that ( 1 ) for one of the two features, both ontologically based approaches improve average feature correlations with human scores, and that ( 2 ) the correlations for both features decrease on-ly marginally when moving from human speech transcriptions to speech recognizer output.
we develop a system for predicting the level of language learners, using only a small amount of targeted language data. <eos> in particular, we focus on learners of hebrew and predict level based on restricted placement exam exercises. <eos> as with many language teaching situations, a major problem is data sparsity, which we account for in our feature selection, learning algorithm, and in the setup. <eos> specifically, we define a two-phase classification process, isolating individual errors and linguistic constructions which are then aggregated into a second phase ; such a two-step process allows for easy integration of other exercises and features in the future. <eos> the aggregation of information also allows us to smooth over sparse features.
in this paper we present a new spell-checking system that utilizes contextual information for automatic correction of non-word misspellings. <eos> the system is evaluated with a large corpus of essays written by native and nonnative speakers of english to the writing prompts of high-stakes standardized tests ( toefl ? <eos> and gre ? ). <eos> we also present comparative evaluations with aspell and the speller from microsoft office 2007. <eos> using context-informed re-ranking of candidate suggestions, our system exhibits superior errorcorrection results overall and also corrects errors generated by non-native english writers with almost same rate of success as it does for writers who are native english speakers.
prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text. <eos> to date such fragments have been extracted from derivations of bayesianinduced tree substitution grammars ( tsgs ). <eos> evaluating on discriminative coarse and fine grammaticality classification tasks, we show that a simple, deterministic, count-based approach to fragment identification performs on par with the more complicated grammars of post ( 2011 ). <eos> this represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain.
accuracy of content have not been fully utilized in the previous studies on automated speaking assessment. <eos> compared to writing tests, responses in speaking tests are noisy ( due to recognition errors ), full of incomplete sentences, and short. <eos> to handle these challenges for doing content-scoring in speaking tests, we propose two new methods based on information extraction ( ie ) and machine learning. <eos> compared to using an ordinary content-scoring method based on vector analysis, which is widely used for scoring written essays, our proposed methods provided content features with higher correlations to human holistic scores.
this paper describes a novel arabic reading enhancement tool ( aret ) for classroom use, which has been built using corpus-based natural language processing in combination with expert linguistic annotation. <eos> the nlp techniques include a widely used morphological analyzer for modern standard arabic to provide word-level grammatical details, and a relational database index of corpus texts to provide word concordances. <eos> aret also makes use of a commercial arabic text-to-speech ( tts ) system to add a speech layer ( with male and female voices ) to the al-kitaab language textbook resources. <eos> the system generates test questions and distractors, offering teachers and students an interesting computer-aided language learning tool. <eos> we describe the background and the motivation behind the building of aret, presenting the various components and the method used to build the tools.
this paper describes and evaluates dqgen, which automatically generates multiple choice cloze questions to test a child ? s comprehension while reading a given text. <eos> unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it. <eos> we evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types. <eos> the results, errors, and judges ? <eos> comments reveal limitations and suggest how to address some of them.
grammar exercises for language learning fall into two distinct classes : those that are based on ? real life sentences ? <eos> extracted from existing documents or from the web ; and those that seek to facilitate language acquisition by presenting the learner with exercises whose syntax is as simple as possible and whose vocabulary is restricted to that contained in the textbook being used. <eos> in this paper, we introduce a framework ( called gramex ) which permits generating the second type of grammar exercises. <eos> using generation techniques, we show that a grammar can be used to semi-automatically generate grammar exercises which target a specific learning goal ; are made of short, simple sentences ; and whose vocabulary is restricted to that used in a given textbook.
we investigate the problem of readability assessment using a range of lexical and syntactic features and study their impact on predicting the grade level of texts. <eos> as empirical basis, we combined two web-based text sources, weekly reader and bbc bitesize, targeting different age groups, to cover a broad range of school grades. <eos> on the conceptual side, we explore the use of lexical and syntactic measures originally designed to measure language development in the production of second language learners. <eos> we show that the developmental measures from second language acquisition ( sla ) research when combined with traditional readability features such as word length and sentence length provide a good indication of text readability across different grades. <eos> the resulting classifiers significantly outperform the previous approaches on readability classification, reaching a classification accuracy of 93.3 %.
this paper presents an interactive analytic tool for educational peer-review analysis. <eos> it employs data visualization at multiple levels of granularity, and provides automated analytic support using clustering and natural language processing. <eos> this tool helps instructors discover interesting patterns in writing performance that are reflected through peer reviews.
this study presents a method that assesses esl learners ? <eos> vocabulary usage to improve an automated scoring system of spontaneous speech responses by non-native english speakers. <eos> focusing on vocabulary sophistication, we estimate the difficulty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difficulty level of the vocabulary usage across the responses ( vocabulary profile ). <eos> three different classes of features were generated based on the words in a spoken response : coverage-related, average word rank and the average word frequency and the extent to which they influence human-assigned language proficiency scores was studied. <eos> among these three types of features, the average word frequency showed the most predictive power. <eos> we then explored the impact of vocabulary profile features in an automated speech scoring context, with particular focus on the impact of two factors : genre of reference corpora and the characteristics of item-types. <eos> the contribution of the current study lies in the use of vocabulary profile as a measure of lexical sophistication for spoken language assessment, an aspect heretofore unexplored in the context of automated speech scoring.
a number of different research subfields are concerned with the automatic assessment of student answers to comprehension questions, from language learning contexts to computer science exams. <eos> they share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. <eos> this paper has the intention of fostering synergy between the different research strands. <eos> it discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. <eos> to that end, we present results with the comic-en content assessment system ( meurers et al, 2011a ) on the dataset published by mohler et al ( 2011 ) and outline what was necessary to perform this comparison. <eos> we conclude with a general discussion on comparability and evaluation of short answer assessment systems.
and correction of preposition and determiner errors in english : hoo 2012 pinaki bhaskar aniruddha ghosh santanu pal sivaji bandyopadhyay department of computer science and engineering, jadavpur university 188, raja s. c. mallick road kolkata ? <eos> 700032, india pinaki.bhaskar @ gmail.com arghyaonline @ gnail.com santanu.pal.ju @ gmail.com sivaji_cse_ju @ yahoo.com abstract this paper reports on our work in the hoo 2012 shared task. <eos> the task is to automatically detect, recognize and correct the errors in the use of prepositions and determiners in a set of given test documents in english. <eos> for that, we have developed a hybrid system of an n-gram statistical model along with some rule-based techniques. <eos> the system has been trained on the hoo shared task ? s training datasets and run on the test set given. <eos> we have submitted one run, which has demonstrated an f-score of 7.1, 6.46 and 2.58 for detection, recognition and correction respectively before revision and f-score of 8.22, 7.59 and 3.16 for detec-tion, recognition and correction respectively after revision.
we extend our n-gram-based data-driven prediction approach from the helping our own ( hoo ) 2011 shared task ( boyd and meurers, 2011 ) to identify determiner and preposition errors in non-native english essays from the cambridge learner corpus fce dataset ( yannakoudakis et al, 2011 ) as part of the hoo 2012 shared task. <eos> our system focuses on three error categories : missing determiner, incorrect determiner, and incorrect preposition. <eos> approximately two-thirds of the errors annotated in hoo 2012 training and test data fall into these three categories. <eos> to improve our approach, we developed a missing determiner detector and incorporated word clustering ( brown et al, 1992 ) into the n-gram prediction approach.
this paper describes the submission of the national university of singapore ( nus ) to the hoo 2012 shared task. <eos> our system uses a pipeline of confidence-weighted linear classifiers to correct determiner and preposition errors. <eos> our system achieves the highest correction f1 score on the official test set among all 14 participating teams, based on gold-standard edits both before and after revision.
this paper describes the system has been developed for the hoo 2012 shared task. <eos> the task was to correct determiner and preposition errors. <eos> i explore the possibility of learning error correcting rules from the given manually annotated data using features such as word length and word endings only. <eos> furthermore, i employ error correction ranking based on the ratio of the sentence probabilities using original and corrected language models. <eos> our system has been ranked for the ninth position out of thirteen teams. <eos> the best result was achieved in correcting missing prepositions, which was ranked for the sixth position.
some grammatical error detection methods, including the ones currently used by the educational testing service ? s e-rater system ( attali and burstein, 2006 ), are tuned for precision because of the perceived high cost of false positives ( i.e., marking fluent english as ungrammatical ). <eos> precision, however, is not optimal for all tasks, particularly the hoo 2012 shared task on grammatical errors, which uses f-score for evaluation. <eos> in this paper, we extend e-rater ? s preposition and determiner error detection modules with a largescale n-gram method ( bergsma et al, 2009 ) that complements the existing rule-based and classifier-based methods. <eos> on the hoo 2012 shared task, the hybrid method performed better than its component methods in terms of f-score, and it was competitive with submissions from other hoo 2012 participants.
previous work on automated error recognition and correction of texts written by learners of english as a second language has demonstrated experimentally that training classifiers on error-annotated esl text generally outperforms training on native text alone and that adaptation of error correction models to the native language ( l1 ) of the writer improves performance. <eos> nevertheless, most extant models have poor precision, particularly when attempting error correction, and this limits their usefulness in practical applications requiring feedback. <eos> we experiment with various feature types, varying quantities of error-corrected data, and generic versus l1-specific adaptation to typical errors using na ? <eos> ? ve bayes ( nb ) classifiers and develop one model which maximizes precision. <eos> we report and discuss the results for 8 models, 5 trained on the hoo data and 3 ( partly ) on the full error-coded cambridge learner corpus, from which the hoo data is drawn.
in this paper, we describe the korea university system that participated in the hoo 2012 shared task on the correction of preposition and determiner errors in non-native speaker texts. <eos> we focus our work on training the system on a large collection of error-tagged texts provided by the hoo 2012 shared task organizers and incrementally applying several methods to achieve better performance.
this is the report for the cngl ilt team entry to the hoo 2012 shared task. <eos> a naivebayes-based classifier was used in the task which involved error detection and correction in esl exam scripts. <eos> the features we use include n-grams of words and pos tags together with features based on the external google ngrams corpus. <eos> our system placed 11th out of 14 teams for the detection and recognition tasks and 11th out of 13 teams for the correction task based on f-score for both preposition and determiner errors.
in this paper we describe the technical implementation of our system that participated in the helping our own 2012 shared task ( hoo-2012 ). <eos> the system employs a number of preprocessing steps and machine learning classifiers for correction of determiner and preposition errors in non-native english texts. <eos> we use maximum entropy classifiers trained on the provided hoo-2012 development data and a large high-quality english text collection. <eos> the system proposes a number of highlyprobable corrections, which are evaluated by a language model and compared with the original text. <eos> a number of deterministic rules are used to increase the precision and recall of the system. <eos> our system is ranked among the three best performing hoo-2012 systems with a precision of 31.15 %, recall of 22.08 % and f1score of 25.84 % for correction of determiner and preposition errors combined.
we describe the university of illinois ( ui ) system that participated in the helping our own ( hoo ) 2012 shared task, which focuses on correcting preposition and determiner errors made by non-native english speakers. <eos> the task consisted of three metrics : detection, recognition, and correction, and measured performance before and after additional revisions to the test data were made. <eos> out of 14 teams that participated, our system scored first in detection and recognition and second in correction before the revisions ; and first in detection and second in the other metrics after revisions. <eos> we describe our underlying approach, which relates to our previous work in this area, and propose an improvement to the earlier method, error inflation, which results in significant gains in performance.
this paper describes the nara institute of science and technology ( naist ) error correction system in the helping our own ( hoo ) 2012 shared task. <eos> our system targets preposition and determiner errors with spelling correction as a pre-processing step. <eos> the result shows that spelling correction improves the detection, correction, and recognition fscores for preposition errors. <eos> with regard to preposition error correction, f-scores were not improved when using the training set with correction of all but preposition errors. <eos> as for determiner error correction, there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an np were removed. <eos> our system ranked third in preposition and fourth in determiner error corrections.
we describe the valkuil.net team entry for the hoo 2012 shared task. <eos> our systems consists of four memory-based classifiers that generate correction suggestions for middle positions in small text windows of two words to the left and to the right. <eos> trained on the google 1tb 5gram corpus, the first two classifiers determine the presence of a determiner or a preposition between all words in a text in which the actual determiners and prepositions are masked. <eos> the second pair of classifiers determines which is the most likely correction given a masked determiner or preposition. <eos> the hyperparameters that govern the classifiers are optimized on the shared task training data. <eos> we point out a number of obvious improvements to boost the medium-level scores attained by the system.
in this paper, we describe the ukp lab system participating in the hoo 2012 shared task on preposition and determiner error correction. <eos> our focus was to implement a highly flexible and modular system which can be easily augmented by other researchers. <eos> the system might be used to provide a level playground for subsequent shared tasks and enable further progress in this important research field on top of the state of the art identified by the shared task.
the growth of open-access technical publications and other open-domain textual information sources means that there is an increasing amount of online technical material that is in principle available to all, but in practice, incomprehensible to most. <eos> we propose to address the task of helping readers comprehend complex technical material, by using statistical methods to model the ? prerequisite structure ? <eos> of a corpus ? <eos> i.e., the semantic impact of documents on an individual reader ? s state of knowledge. <eos> experimental results using wikipedia as the corpus suggest that this task can be approached by crowdsourcing the production of ground-truth labels regarding prerequisite structure, and then generalizing these labels using a learned classifier which combines signals of various sorts. <eos> the features that we consider relate pairs of pages by analyzing not only textual features of the pages, but also how the containing corpora is connected and created.
to support vocabulary acquisition and reading comprehension in a second language, we have developed a system to display senseappropriate examples to learners for difficult words. <eos> we describe the construction of the system, incorporating word sense disambiguation, and an experiment we conducted testing it on a group of 60 learners of english as a second language ( esl ). <eos> we show that sensespecific information in an intelligent reading system helps learners in their vocabulary acquisition, even if the sense information contains some noise from automatic processing. <eos> we also show that it helps learners, to some extent, with their reading comprehension.
there is a rise in interest in the evaluation of meaning in real-life applications, e.g., for assessing the content of short answers. <eos> the approaches typically use a combination of shallow and deep representations, but little use is made of the semantic formalisms created by theoretical linguists to represent meaning. <eos> in this paper, we explore the use of the underspecified semantic formalism lrs, which combines the capability of precisely representing semantic distinctions with the robustness and modularity needed to represent meaning in real-life applications. <eos> we show that a content-assessment approach built on lrs outperforms a previous approach on the creg data set, a freely available corpus of answers to reading comprehension exercises by learners of german. <eos> the use of such a formalism also readily supports the integration of notions building on semantic distinctions, such as the information structuring in discourse, which we show to be useful for content assessment.
the main aim of this work is to perform sentiment analysis on urdu blog data. <eos> we use the method of structural correspondence learning ( scl ) to transfer sentiment analysis learning from urdu newswire data to urdu blog data. <eos> the pivots needed to transfer learning from newswire domain to blog domain is not trivial as urdu blog data, unlike newswire data is written in latin script and exhibits codemixing and code-switching behavior. <eos> we consider two oracles to generate the pivots. <eos> 1. <eos> transliteration oracle, to accommodate script variation and spelling variation and 2. <eos> translation oracle, to accommodate code-switching and code-mixing behavior. <eos> in order to identify strong candidates for translation, we propose a novel part-of-speech tagging method that helps select words based on pos categories that strongly reflect code-mixing behavior. <eos> we validate our approach against a supervised learning method and show that the performance of our proposed approach is comparable.
haul lehrman cecilia ovesdotter alm rub ? n a. proa ? o rochester institute of technology michael.lehrman @ alum.rit.edu coagla @ rit.edu rpmeie @ rit.edu abstract improving mental wellness with preventive measures can help people at risk of experiencing mental health conditions such as depression or post-traumatic stress disorder. <eos> we describe an encouraging study on how automatic analysis of short written texts based on relevant linguistic text features can be used to identify whether the authors of such texts are experiencing distress. <eos> such a computational model can be useful in developing an early warning system able to analyze writing samples for signs of mental distress. <eos> this could serve as a red flag, signaling when someone might need a professional assessment by a clinician. <eos> this paper reports on classification of distressed and non-distressed short, written excerpts from relevant web forums, using features automatically extracted from input text. <eos> varying the value of k in k-fold cross-validation shows that both coarse-grained and fine-grained automatic classification of affect states are generally 20 % more accurate in detecting affect state than randomly assigning a distress label to a text. <eos> the study also compares the importance of bundled linguistic super-factors with a 2k factorial model. <eos> analyzing the importance of different linguistic features for this task indicates main effects of affect word list matches, pronouns, and parts of speech in the predictive model. <eos> excerpt length contributed to interaction effects.
we present an approach to detecting hate speech in online text, where hate speech is defined as abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation. <eos> while hate speech against any group may exhibit some common characteristics, we have observed that hatred against each different group is typically characterized by the use of a small set of high frequency stereotypical words ; however, such words may be used in either a positive or a negative sense, making our task similar to that of words sense disambiguation. <eos> in this paper we describe our definition of hate speech, the collection and annotation of our hate speech corpus, and a mechanism for detecting some commonly used methods of evading common ? dirty word ? <eos> filters. <eos> we describe pilot classification experiments in which we classify anti-semitic speech reaching an accuracy 94 %, precision of 68 % and recall at 60 %, for an f1 measure of.6375.
it has long been established that there is a correlation between the dialog behavior of a participant and how influential he or she is perceived to be by other discourse participants. <eos> in this paper we explore the characteristics of communication that make someone an opinion leader and develop a machine learning based approach for the automatic identification of discourse participants that are likely to be influencers in online communication. <eos> our approach relies on identification of three types of conversational behavior : persuasion, agreement/disagreement, and dialog patterns.
what makes a tweet worth sharing ? <eos> we study the content of tweets to uncover linguistic tendencies of shared microblog posts ( retweets ), by examining surface linguistic features, deeper parse-based features and twitterspecific conventions in tweet content. <eos> we show how these features correlate with a functional classification of tweets, thereby categorizing people ? s writing styles based on their different intentions on twitter. <eos> we find that both linguistic features and functional classification contribute to re-tweeting. <eos> our work shows that opinion tweets favor originality and pithiness and that update tweets favor direct statements of a tweeter ? s current activity. <eos> judicious use of # hashtags also helps to encourage retweeting.
in this paper, we look at the problem of robust detection of a very productive class of asian style emoticons, known as facemarks or kaomoji. <eos> we demonstrate the frequency and productivity of these sequences in social media such as twitter. <eos> previous approaches to detection and analysis of kaomoji have placed limits on the range of phenomena that could be detected with their method, and have looked at largely monolingual evaluation sets ( e.g., japanese blogs ). <eos> we find that these emoticons occur broadly in many languages, hence our approach is language agnostic. <eos> rather than relying on regular expressions over a predefined set of likely tokens, we build weighted context-free grammars that reward graphical affinity and symmetry within whatever symbols are used to construct the emoticon.
social media services such as twitter offer an immense volume of real-world linguistic data. <eos> we explore the use of twitter to obtain authentic user-generated text in low-resource languages such as nepali, urdu, and ukrainian. <eos> automatic language identification ( lid ) can be used to extract language-specific data from twitter, but it is unclear how well lid performs on short, informal texts in low-resource languages. <eos> we address this question by annotating and releasing a large collection of tweets in nine languages, focusing on confusable languages using the cyrillic, arabic, and devanagari scripts. <eos> this is the first publiclyavailable collection of lid-annotated tweets in non-latin scripts, and should become a standard evaluation set for lid systems. <eos> we also advance the state-of-the-art by evaluating new, highly-accurate lid systems, trained both on our new corpus and on standard materials only. <eos> both types of systems achieve a huge performance improvement over the existing state-of-the-art, correctly classifying around 98 % of our gold standard tweets. <eos> we provide a detailed analysis showing how the accuracy of our systems vary along certain dimensions, such as the tweet-length and the amount of in- and out-of-domain training data.
regardless of language, the standard character set for text messages ( sms ) and many other social media platforms is the roman alphabet. <eos> there are romanization conventions for some character sets, but they are used inconsistently in informal text, such as sms. <eos> in this work, we convert informal, romanized urdu messages into the native arabic script and normalize non-standard sms language. <eos> doing so prepares the messages for existing downstream processing tools, such as machine translation, which are typically trained on well-formed, native script text. <eos> our model combines information at the word and character levels, allowing it to handle out-of-vocabulary items. <eos> compared with a baseline deterministic approach, our system reduces both word and character error rate by over 50 %.
in this paper we present the results of the analysis of a parallel corpus of original and simplified texts in spanish, gathered for the purpose of developing an automatic simplification system for this language. <eos> the system is intended for individuals with cognitive disabilities who experience difficulties reading and interpreting informative texts. <eos> we here concentrate on lexical simplification operations applied by human editors on the basis of which we derive a set of rules to be implemented automatically. <eos> we have so far addressed the issue of lexical units substitution, with special attention to reporting verbs and adjectives of nationality ; insertion of definitions ; simplification of numerical expressions ; and simplification of named entities.
while there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations. <eos> in this paper, we compare two offline methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall. <eos> these methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues. <eos> we find, most importantly, that the two correlate. <eos> magnitude estimation can be run on the web without supervision, and the results can be analysed automatically. <eos> the sentence recall methodology is more resource intensive, but allows us to tease apart the fluency and comprehension issues that arise.
generally, people with dyslexia are poor readers but strong visual thinkers. <eos> the use of graphical schemes for helping text comprehension is recommended in education manuals. <eos> this study explores the relation between text readability and the visual conceptual schemes which aim to make the text more clear for these specific target readers. <eos> our results are based on a user study for spanish native speakers through a group of twenty three dyslexic users and a control group of similar size. <eos> the data collected from our study combines qualitative data from questionnaires and quantitative data from tests carried out using eye tracking. <eos> the findings suggest that graphical schemes may help to improve readability for dyslexics but are, unexpectedly, counterproductive for understandability.
lexicons of word difficulty are useful for various educational applications, including readability classification and text simplification. <eos> in this work, we explore automatic creation of these lexicons using methods which go beyond simple term frequency, but without relying on age-graded texts. <eos> in particular, we derive information for each word type from the readability of the web documents they appear in and the words they co-occur with, linearly combining these various features. <eos> we show the efficacy of this approach by comparing our lexicon with an existing coarse-grained, low-coverage resource and a new crowdsourced annotation.
although many approaches have been presented to compute and predict readability of documents in different languages, the information provided by readability systems often fail to show in a clear and understandable way how difficult a document is and which aspects contribute to content readability. <eos> we address this issue by presenting a system that, for a given document in italian, provides not only a list of readability indices inspired by cohmetrix, but also a graphical representation of the difficulty of the text compared to the three levels of italian compulsory education, namely elementary, middle and high-school level. <eos> we believe that this kind of representation makes readability assessment more intuitive, especially for educators who may not be familiar with readability predictions via supervised classification. <eos> in addition, we present the first available system for readability assessment of italian inspired by coh-metrix.
readability formulas are methods used to match texts with the readers ? <eos> reading level. <eos> several methodological paradigms have previously been investigated in the field. <eos> the most popular paradigm dates several decades back and gave rise to well known readability formulas such as the flesch formula ( among several others ). <eos> this paper compares this approach ( henceforth ? classic ? ) <eos> with an emerging paradigm which uses sophisticated nlpenabled features and machine learning techniques. <eos> our experiments, carried on a corpus of texts for french as a foreign language, yield four main results : ( 1 ) the new readability formula performed better than the ? classic ? <eos> formula ; ( 2 ) ? non-classic ? <eos> features were slightly more informative than ? classic ? <eos> features ; ( 3 ) modern machine learning algorithms did not improve the explanatory power of our readability model, but allowed to better classify new observations ; and ( 4 ) combining ? classic ? <eos> and ? non-classic ? <eos> features resulted in a significant gain in performance.
early primary children ? s literature poses some interesting challenges for automated readability assessment : for example, teachers often use fine-grained reading leveling systems for determining appropriate books for children to read ( many current systems approach readability assessment at a coarser whole grade level ). <eos> in previous work ( ma et al, 2012 ), we suggested that the fine-grained assessment task can be approached using a ranking methodology, and incorporating features that correspond to the visual layout of the page improves performance. <eos> however, the previous methodology for using ? found ? <eos> text ( e.g., scanning in a book from the library ) requires human annotation of the text regions and correction of the ocr text. <eos> in this work, we ask whether the annotation process can be automated, and also experiment with richer syntactic features found in the literature that can be automatically derived from either the humancorrected or raw ocr text. <eos> we find that automated visual and text feature extraction work reasonably well and can allow for scaling to larger datasets, but that in our particular experiments the use of syntactic features adds little to the performance of the system, contrary to previous findings.
most tools and resources developed for natural language processing of arabic are designed for modern standard arabic ( msa ) and perform terribly on arabic dialects, such as egyptian arabic. <eos> egyptian arabic differs from msa phonologically, morphologically and lexically and has no standardized orthography. <eos> we present a linguistically accurate, large-scale morphological analyzer for egyptian arabic. <eos> the analyzer extends an existing resource, the egyptian colloquial arabic lexicon, and follows the part-of-speech guidelines used by the linguistic data consortium for egyptian arabic. <eos> it accepts multiple orthographic variants and normalizes them to a conventional orthography.
hindi is an indian language which is relatively rich in morphology. <eos> a few morphological analyzers of this language have been developed. <eos> however, they give only inflectional analysis of the language. <eos> in this paper, we present our hindi derivational morphological analyzer. <eos> our algorithm upgrades an existing inflectional analyzer to a derivational analyzer and primarily achieves two goals. <eos> first, it successfully incorporates derivational analysis in the inflectional analyzer. <eos> second, it also increases the coverage of the inflectional analysis of the existing inflectional analyzer.
sed approach for adaptive tokenization jianqiang ma dale gerdemann department of linguistics university of t ? bingen department of linguistics university of t ? bingen wilhelmstr. <eos> 19, t ? bingen, 72074, germany wilhelmstr. <eos> 19, t ? bingen, 72074, germany jma @ sfs.uni-tuebingen.de dg @ sfs.uni-tuebingen.de abstract fast re-training of word segmentation models is required for adapting to new resources or domains in nlp of many asian languages without word delimiters. <eos> the traditional tokenization model is efficient but inaccurate. <eos> this paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations, the dependencies of which are also taken into account. <eos> the model has a good oov recognition ability, which improves the overall performance significantly. <eos> the training is a linear time phrase extraction and mle procedure, while the decoding is via dynamic programming based algorithms.
languages are constantly evolving through their users due to the need to communicate more efficiently. <eos> under this hypothesis, we formulate unsupervised word segmentation as a regularized compression process. <eos> we reduce this process to an optimization problem, and propose a greedy inclusion solution. <eos> preliminary test results on the bernstein-ratner corpus and bakeoff-2005 show that the our method is comparable to the state-of-the-art in terms of effectiveness and efficiency.
this paper describes a small experiment to test a rule-based approach to unknown word recognition in arabic. <eos> the morphological complexity of arabic presents its challenges to a variety of nlp applications, but it can also be viewed as an advantage, if we can tap into the complex linguistic knowledge associated with these complex forms. <eos> in particular, the derived forms of verbs can be analysed and an educated guess at the likely meaning of a derived form can be predicted, based on the meaning of a known form and the relationship between the known form and the unknown one. <eos> the performance of the approach is tested on the nemlar written arabic corpus.
this paper first defines the conditions under which copying and deletion processes are subsequential : specifically this is the case when the process is bounded in the right ways. <eos> then, if we analyze metathesis as the composition of copying and deletion, it can be shown that the set of attested metathesis patterns fall into the subsequential or reverse subsequential classes. <eos> the implications of bounded copying are extended to partial reduplication, which is also shown to be either subsequential or reverse subsequential.
we show that a class of cases that has been previously studied in terms of learning of abstract phonological underlying representations ( urs ) can be handled by a learner that chooses urs from a contextually conditioned distribution over observed surface representations. <eos> we implement such a learner in a maximum entropy version of optimality theory, in which ur learning is an instance of semisupervised learning. <eos> our objective function incorporates a term aimed to ensure generalization, independently required for phonotactic learning in optimality theory, and does not have a bias for single urs for morphemes. <eos> this learner is successful on a test language provided by tesar ( 2006 ) as a challenge for ur learning. <eos> we also provide successful results on learning of a toy case modeled on french vowel alternations, which have also been previously analyzed in terms of abstract urs. <eos> this case includes lexically conditioned variation, an aspect of the data that can not be handled by abstract urs, showing that in this respect our approach is more general.
this paper presents a memoryless categorization learner that predicts differences in category complexity found in several psycholinguistic and psychological experiments. <eos> in particular, this learner predicts the order of difficulty of learning simple boolean categories, including the advantage of conjunctive categories over the disjunctive ones ( an advantage that is not typically modeled by the statistical approaches ). <eos> it also models the effect of labeling ( positive and negative labels vs. positive labels of two different kinds ) on category complexity. <eos> this effect has implications for the differences between learning a single category ( e.g., a phonological class of segments ) vs. a set of non-overlapping categories ( e.g., affixes in a morphological paradigm ).
narrative recall tasks are widely used in neuropsychological evaluation protocols in order to detect symptoms of disorders such as autism, language impairment, and dementia. <eos> in this paper, we propose a graph-based method commonly used in information retrieval to improve word-level alignments in order to align a source narrative to narrative retellings elicited in a clinical setting. <eos> from these alignments, we automatically extract narrative recall scores which can then be used for diagnostic screening. <eos> the significant reduction in alignment error rate ( aer ) afforded by the graph-based method results in improved automatic scoring and diagnostic classification. <eos> the approach described here is general enough to be applied to almost any narrative recall scenario, and the reductions in aer achieved in this work attest to the potential utility of this graph-based method for enhancing multilingual word alignment and alignment of comparable corpora for more standard nlp tasks.
we describe an open information extraction system for biomedical text based on nell ( the never-ending language learner ) ( carlson et al, 2010 ), a system designed for extraction from web text. <eos> nell uses a coupled semi-supervised bootstrapping approach to learn new facts from text, given an initial ontology and a small number of ? seeds ? <eos> for each ontology category. <eos> in contrast to previous applications of nell, in our task the initial ontology and seeds are automatically derived from existing resources. <eos> we show that nell ? s bootstrapping algorithm is susceptible to ambiguous seeds, which are frequent in the biomedical domain. <eos> using nell to extract facts from biomedical text quickly leads to semantic drift. <eos> to address this problem, we introduce a method for assessing seed quality, based on a larger corpus of data derived from the web. <eos> in our method, seed quality is assessed at each iteration of the bootstrapping process. <eos> experimental results show significant improvements over nell ? s original bootstrapping algorithm on two types of tasks : learning terms from biomedical categories, and named-entity recognition for biomedical entities using a learned lexicon.
the identification of semantically similar linguistic expressions despite their formal difference is an important task within nlp applications ( information retrieval and extraction, terminology structuring... ) we propose to detect the semantic relatedness between biomedical terms from the pharmacovigilance area. <eos> two approaches are exploited : semantic distance within structured resources and terminology structuring methods applied to a raw list of terms. <eos> we compare these methods and study their complementarity. <eos> the results are evaluated against the reference pharmacovigilance data and manually by an expert.
we investigate the task of assigning medical events in clinical narratives to discrete time-bins. <eos> the time-bins are defined to capture when a medical event occurs relative to the hospital admission date in each clinical narrative. <eos> we model the problem as a sequence tagging task using conditional random fields. <eos> we extract a combination of lexical, section-based and temporal features from medical events in each clinical narrative. <eos> the sequence tagging system outperforms a system that does not utilize any sequence information modeled using a maximum entropy classifier. <eos> we present results with both handtagged as well as automatically extracted features. <eos> we observe over 8 % improvement in overall tagging accuracy with the inclusion of sequence information.
the growth of digital clinical data has raised questions as to how best to leverage this data to aid the world of healthcare. <eos> promising application areas include information retrieval and question-answering systems. <eos> such systems require an in-depth understanding of the texts that are processed. <eos> one aspect of this understanding is knowing if a medical condition outlined in a patient record is recent, or if it occurred in the past. <eos> as well as this, patient records often discuss other individuals such as family members. <eos> this presents a second problem - determining if a medical condition is experienced by the patient described in the report or some other individual. <eos> in this paper, we investigate the suitability of a machine learning ( ml ) based system for resolving these tasks on a previously unexplored collection of patient history and physical examination reports. <eos> our results show that our novel score-based feature approach outperforms the standard linguistic and contextual features described in the related literature. <eos> specifically, near-perfect performance is achieved in resolving if a patient experienced a condition. <eos> while for the task of establishing when a patient experienced a condition, our ml system significantly outperforms the context system ( 87 % versus 69 % f-score, respectively ).
we present an algorithm for extracting abbreviation definitions from biomedical text. <eos> our approach is based on an alignment hmm, matching abbreviations and their definitions. <eos> we report 98 % precision and 93 % recall on a standard data set, and 95 % precision and 91 % recall on an additional test set. <eos> our results show an improvement over previously reported methods and our model has several advantages. <eos> our model : ( 1 ) is simpler and faster than a comparable alignment-based abbreviation extractor ; ( 2 ) is naturally generalizable to specific types of abbreviations, e.g., abbreviations of chemical formulas ; ( 3 ) is trained on a set of unlabeled examples ; and ( 4 ) associates a probability with each predicted definition. <eos> using the abbreviation alignment model we were able to extract over 1.4 million abbreviations from a corpus of 200k full-text pubmed papers, including 455,844 unique definitions.
in the english clinical and biomedical text domains, negation and certainty usage are two well-studied phenomena. <eos> however, few studies have made an in-depth characterization of uncertainties expressed in a clinical setting, and compared this between different annotation efforts. <eos> this preliminary, qualitative study attempts to 1 ) create a clinical uncertainty and negation taxonomy, 2 ) develop a translation map to convert annotation labels from an english schema into a swedish schema, and 3 ) characterize and compare two data sets using this taxonomy. <eos> we define a clinical uncertainty and negation taxonomy and a translation map for converting annotation labels between two schemas and report observed similarities and differences between the two data sets.
stepwise approach for de-identifying person names in clinical documents oscar ferr ? ndez1,2, brett r. south1,2, shuying shen1,2, st ? phane m. meystre1,2 1 department of biomedical informatics, university of utah, salt lake city, utah, usa 2 ideas center slcva healthcare system, salt lake city, utah, usa oscar.ferrandez @ utah.edu, { brett.south, shuying.shen, stephane.meystre } @ hsc.utah.edu abstract as electronic health records are growing ex-ponentially along with large quantities of un-structured clinical information that could be used for research purposes, protecting patient privacy becomes a challenge that needs to be met. <eos> in this paper, we present a novel hybrid system designed to improve the current strate-gies used for person names de-identification. <eos> to overcome this task, our system comprises several components designed to accomplish two separate goals : 1 ) achieve the highest re-call ( no patient data can be exposed ) ; and 2 ) create methods to filter out false positives. <eos> as a result, our system reached 92.6 % f2-measure when de-identifying person names in veteran ? s health administration clinical notes, and considerably outperformed other existing ? out-of-the-box ? <eos> de-identification or named entity recognition systems.
active learning can lower the cost of annotation for some natural language processing tasks by using a classifier to select informative instances to send to human annotators. <eos> it has worked well in cases where the training instances are selected one at a time and require minimal context for annotation. <eos> however, coreference annotations often require some context and the traditional active learning approach may not be feasible. <eos> in this work we explore various active learning methods for coreference resolution that fit more realistically into coreference annotation workflows.
recent efforts in biomolecular event extraction have mainly focused on core event types involving genes and proteins, such as gene expression, protein-protein interactions, and protein catabolism. <eos> the bionlp ? 11 shared task extended the event extraction approach to sub-protein events and relations in the epigenetics and post-translational modifications ( epi ) and protein relations ( rel ) tasks. <eos> in this study, we apply the turku event extraction system, the best-performing system for these tasks, to all pubmed abstracts and all available pmc full-text articles, extracting 1.4m epi events and 2.2m rel relations from 21m abstracts and 372k articles. <eos> we introduce several entity normalization algorithms for genes, proteins, protein complexes and protein components, aiming to uniquely identify these biological entities. <eos> this normalization effort allows direct mapping of the extracted events and relations with posttranslational modifications from uniprot, epigenetics from pubmeth, functional domains from interpro and macromolecular structures from pdb. <eos> the extraction of such detailed protein information provides a unique text mining dataset, offering the opportunity to further deepen the information provided by existing pubmed-scale event extraction efforts. <eos> the methods and data introduced in this study are freely available from bionlp.utu.fi.
the latest discoveries on diseases and their diagnosis/treatment are mostly disseminated in the form of scientific publications. <eos> however, with the rapid growth of the biomedical literature and a high level of variation and ambiguity in disease names, the task of retrieving disease-related articles becomes increasingly challenging using the traditional keywordbased approach. <eos> an important first step for any disease-related information extraction task in the biomedical literature is the disease mention recognition task. <eos> however, despite the strong interest, there has not been enough work done on disease name identification, perhaps because of the difficulty in obtaining adequate corpora. <eos> towards this aim, we created a large-scale disease corpus consisting of 6900 disease mentions in 793 pubmed citations, derived from an earlier corpus. <eos> our corpus contains rich annotations, was developed by a team of 12 annotators ( two people per annotation ) and covers all sentences in a pubmed abstract. <eos> disease mentions are categorized into specific disease, disease class, composite mention and modifier categories. <eos> when used as the gold standard data for a state-of-the-art machine-learning approach, significantly higher performance can be found on our corpus than the previous one. <eos> such characteristics make this disease name corpus a valuable resource for mining disease-related information from biomedical text. <eos> the ncbi corpus is available for download at http : //www.ncbi.nlm.nih.gov/cbbresearch/fe llows/dogan/disease.html.
event extraction is a major focus of recent work in biomedical information extraction. <eos> despite substantial advances, many challenges still remain for reliable automatic extraction of events from text. <eos> we introduce a new biomedical event extraction resource consisting of analyses automatically created by systems participating in the recent bionlp shared task ( st ) 2011. <eos> in providing for the first time the outputs of a broad set of state-ofthe-art event extraction systems, this resource opens many new opportunities for studying aspects of event extraction, from the identification of common errors to the study of effective approaches to combining the strengths of systems. <eos> we demonstrate these opportunities through a multi-system analysis on three bionlp st 2011 main tasks, focusing on events that none of the systems can successfully extract. <eos> we further argue for new perspectives to the performance evaluation of domain event extraction systems, considering a document-level, ? off-the-page ? <eos> representation and evaluation to complement the mentionlevel evaluations pursued in most recent work.
the acquisition of semantic resources and relations is an important task for several applications, such as query expansion, information retrieval and extraction, machine translation. <eos> however, their validity should also be computed and indicated, especially for automatic systems and applications. <eos> we exploit the compositionality based methods for the acquisition of synonymy relations and of indicators of these synonyms. <eos> we then apply pagerank-derived algorithm to the obtained semantic graph in order to filter out the acquired synonyms. <eos> evaluation performed with two independent experts indicates that the quality of synonyms is systematically improved by 10 to 15 % after their filtering.
in this paper we explore the applicability of existing coreference resolution systems to a biomedical genre : radiology reports. <eos> analysis revealed that, due to the idiosyncrasies of the domain, both the formulation of the problem of coreference resolution and its solution need significant domain adaptation work. <eos> we reformulated the task and developed an unsupervised algorithm based on heuristics for coreference resolution in radiology reports. <eos> the algorithm is shown to perform well on a test dataset of 150 manually annotated radiology reports.
the goal of this work is to apply nlp techniques to the field of bionlp in order to gain a better insight into the field and show connections and trends that might not otherwise be apparent. <eos> the data we analyzed was the proceedings from last decade of bionlp workshops. <eos> our findings reveal the prominent research problems and techniques in the field, their progression over time, the approaches that researchers are using to solve those problems, insightful ways to categorize works in the field, and the prominent researchers and groups whose works are influencing the field.
manually annotating clinical document corpora to generate reference standards for natural language processing ( nlp ) sys-tems or machine learning ( ml ) is a time-consuming and labor-intensive endeavor. <eos> although a variety of open source annota-tion tools currently exist, there is a clear opportunity to develop new tools and assess functionalities that introduce efficiencies into the process of generating reference standards. <eos> these features include : man-agement of document corpora and batch as-signment, integration of machine-assisted verification functions, semi-automated cu-ration of annotated information, and sup-port of machine-assisted pre-annotation. <eos> the goals of reducing annotator workload and improving the quality of reference standards are important considerations for development of new tools. <eos> an infrastruc-ture is also needed that will support large-scale but secure annotation of sensitive clinical data as well as crowdsourcing which has proven successful for a variety of annotation tasks. <eos> we introduce the ex-tensible human oracle suite of tools ( ehost ) http : //code.google.com/p/ehost that provides such functionalities that when coupled with server integration offer an end-to-end solution to carry out small or large scale as well as crowd sourced anno-tation projects.
eer, bensiin borukhov, michael crivaro, michael shafir, attapol thamrongrattanarit { mmeteer, bborukhov, mcrivaro, mshafir, tet } @ brandeis.edu department of computer science brandeis university waltham, ma 02453, usa abstract the application of natural language process-ing ( nlp ) in the biology and medical domain crosses many fields from healthcare informa-tion to bioinformatics to nlp itself. <eos> in order to make sense of how these fields relate and intersect, we have created ? medlingmap ? <eos> ( www.medlingmap.org ) which is a compila-tion of references with a multi-faceted index. <eos> the initial focus has been creating the infra-structure and populating it with references an-notated with facets such as topic, resources used ( ontologies, tools, corpora ), and organi-zations. <eos> simultaneously we are applying nlp techniques to the text to find clusters, key terms and other relationships. <eos> the goal for this paper is to introduce medlingmap to the community and show how it can be a power-ful tool for research and exploration in the field.
many genetic epidemiological studies of human diseases have multiple variables related to any given phenotype, resulting from different definitions and multiple measurements or subsets of data. <eos> manually mapping and harmonizing these phenotypes is a timeconsuming process that may still miss the most appropriate variables. <eos> previously, a supervised learning algorithm was proposed for this problem. <eos> that algorithm learns to determine whether a pair of phenotypes is in the same class. <eos> though that algorithm accomplished satisfying f-scores, the need to manually label training examples becomes a bottleneck to improve its coverage. <eos> herein we present a novel active learning solution to solve this challenging phenotype-mapping problem. <eos> active learning will make phenotype mapping more efficient and improve its accuracy.
block-lda is a topic modeling approach to perform data fusion between entity-annotated text documents and graphs with entity-entity links. <eos> we evaluate block-lda in the yeast biology domain by jointly modeling pubmed r ? <eos> articles and yeast protein-protein interaction networks. <eos> the topic coherence of the emergent topics and the ability of the model to retrieve relevant scientific articles and proteins related to the topic are compared to that of a text-only approach that does not make use of the protein-protein interaction matrix. <eos> evaluation of the results by biologists show that the joint modeling results in better topic coherence and improves retrieval performance in the task of identifying top related papers and proteins.
this paper presents a machine learning approach that selects and, more generally, ranks sentences containing clear relations between genes and terms that are related to them. <eos> this is treated as a binary classification task, where preference judgments are used to learn how to choose a sentence from a pair of sentences. <eos> features to capture how the relationship is described textually, as well as how central the relationship is in the sentence, are used in the learning process. <eos> simplification of complex sentences into simple structures is also applied for the extraction of the features. <eos> we show that such simplification improves the results by up to 13 %. <eos> we conducted three different evaluations and we found that the system significantly outperforms the baselines.
the relationship between small molecules and proteins has attracted attention from the biomedical research community. <eos> in this paper a text mining method of extracting smallmolecule and protein pairs from natural text is presented, based on a semi-supervised machine learning approach. <eos> the technique has been applied to the complete collection of medline abstracts and pairs were extracted and evaluated. <eos> the results show the feasibility of the bootstrapping system, which will subsequently be further investigated and improved.
evidence based medicine ( ebm ) is the practice of using the knowledge gained from the best medical evidence to make decisions in the effective care of patients. <eos> this medical evidence is extracted from medical documents such as research papers. <eos> the increasing number of available medical documents has imposed a challenge to identify the appropriate evidence and to access the quality of the evidence. <eos> in this paper, we present an approach for the automatic grading of evidence using the dataset provided by the 2011 australian language technology association ( alta ) shared task competition. <eos> with the feature sets extracted from publication types, medical subject headings ( mesh ), title, and body of the abstracts, we obtain a 73.77 % grading accuracy with a stacking based approach, a considerable improvement over previous work.
gene name identification is a fundamental step to solve more complicated text mining problems such as gene normalization and protein-protein interactions. <eos> however, state-ofthe-art name identification methods are not yet sufficient for use in a fully automated system. <eos> in this regard, a relaxed task, gene/protein sentence identification, may serve more effectively for manually searching and browsing biomedical literature. <eos> in this paper, we set up a new task, gene/protein sentence classification and propose an ensemble approach for addressing this problem. <eos> wellknown named entity tools use similar goldstandard sets for training and testing, which results in relatively poor performance for unknown sets. <eos> we here explore how to combine diverse high-precision gene identifiers for more robust performance. <eos> the experimental results show that the proposed approach outperforms banner as a stand-alone classifier for newly annotated sets as well as previous gold-standard sets.
datasets that answer difficult clinical questions are expensive in part due to the need for medical expertise and patient informed consent. <eos> we investigate the effect of small sample size on the performance of a text categorization algorithm. <eos> we show how to determine whether the dataset is large enough to train support vector machines. <eos> since it is not possible to cover all aspects of sample size calculation in one manuscript, we focus on how certain types of data relate to certain properties of support vector machines. <eos> we show that normal vectors of decision hyperplanes can be used for assessing reliability and internal cross-validation can be used for assessing stability of small sample data.
there has been an active development of corpora and annotations in the bionlp community. <eos> as those resources accumulate, a new issue arises about the reusability. <eos> as a solution to improve the reusability of corpora and annotations, we present pubannotation, a persistent and sharable repository, where various corpora and annotations can be stored together in a stable and comparable way. <eos> as a position paper, it explains the motivation and the core concepts of the repository and presents a prototype repository as a proof-of-concept.
the package insert ( aka drug product label ) is the only publicly-available source of information on drug-drug interactions ( ddis ) for some drugs, especially newer ones. <eos> thus, an automated method for identifying ddis in drug package inserts would be a potentially important complement to methods for identifying ddis from other sources such as the scientific literature. <eos> to develop such an algorithm, we created a corpus of federal drug administration approved drug package insert statements that have been manually annotated for pharmacokinetic ddis by a pharmacist and a drug information expert. <eos> we then evaluated three different machine learning algorithms for their ability to 1 ) identify pharmacokinetic ddis in the package insert corpus and 2 ) classify pharmacokinetic ddi statements by their modality ( i.e., whether they report a ddi or no interaction between drug pairs ). <eos> experiments found that a support vector machine algorithm performed best on both tasks with an f-measure of 0.859 for pharmacokinetic ddi identification and 0.949 for modality assignment. <eos> we also found that the use of syntactic information is very helpful for addressing the problem of sentences containing both interacting and non-interacting pairs of drugs.
publications that report genotype-drug interaction findings, as well as manually curated databases such as drugbank and pharmgkb are essential to advancing pharmacogenomics, a relatively new area merging pharmacology and genomic research. <eos> natural language processing ( nlp ) methods can be very useful for automatically extracting knowledge such as gene-drug interactions, offering researchers immediate access to published findings, and allowing curators a shortcut for their work. <eos> we present a corpus of gene-drug interactions for evaluating and training systems to extract those interactions. <eos> the corpus includes 551 sentences that have a mention of a drug and a gene from about 600 journals found to be relevant to pharmacogenomics through an analysis of gene-drug relationships in the pharmgkb knowledgebase. <eos> we evaluated basic approaches to automatic extraction, including gene and drug cooccurrence, co-occurrence plus interaction terms, and a linguistic pattern-based method. <eos> the linguistic pattern method had the highest precision ( 96.61 % ) but lowest recall ( 7.30 % ), for an f-score of 13.57 %. <eos> basic co-occurrence yields 68.99 % precision, with the addition of an interaction term precision increases slightly ( 69.60 % ), though not as much as could be expected. <eos> co-occurrence is a reasonable baseline method, with pattern-based being a promising approach if enough patterns can be generated to address recall. <eos> the corpus is available at http : //diego.asu.edu/index.php/projects
a preliminary work on symptom name recognition from free-text clinical records ( fcrs ) of traditional chinese medicine ( tcm ) is depicted in this paper. <eos> this problem is viewed as labeling each character in fcrs of tcm with a pre-defined tag ( ? b-syc ?, ? i-syc ? <eos> or ? osyc ? ) <eos> to indicate the character ? s role ( a beginning, inside or outside part of a symptom name ). <eos> the task is handled by conditional random fields ( crfs ) based on two types of features. <eos> the symptom name recognition fmeasure can reach up to 62.829 % with recognition rate 93.403 % and recognition error rate 52.665 % under our experiment settings. <eos> the feasibility and effectiveness of the methods and reasonable features are verified, and several interesting and helpful results are shown. <eos> a detailed analysis for recognizing symptom names from fcrs of tcm is presented through analyzing labeling results of crfs.
the most accurate approaches to word sense disambiguation ( wsd ) for biomedical documents are based on supervised learning. <eos> however, these require manually labeled training examples which are expensive to create and consequently supervised wsd systems are normally limited to disambiguating a small set of ambiguous terms. <eos> an alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones. <eos> this paper describes a large scale wsd system based on automatically labeled examples generated using information from the umls metathesaurus. <eos> the labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised ( unlike some previous approaches ). <eos> the system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the umls metathesaurus.
when only a small amount of manually annotated data is available, application of a bootstrapping method is often considered to compensate for the lack of sufcient training material for a machine-learning method. <eos> the paper reports a series of experimental results of bootstrapping for protein name recognition. <eos> the results show that the performance changes signicantly according to the choice of text collection where the training samples to bootstrap, and that an improvement can be obtained only with a well chosen text collection.
this paper discusses successes and failures of computational linguistics techniques in the study of how inter-event time intervals in a story affect the narrator ? s use of different types of referring expressions. <eos> the success story shows that a conditional frequency distribution analysis of proper nouns and pronouns yields results that are consistent with our previous results ? <eos> based on manual coding ? <eos> that the narrator ? s choice of referring expression depends on the amount of time that elapsed between events in a story. <eos> unfortunately, the less successful story indicates that state-of-the-art coreference resolution systems fail to achieve high accuracy for this genre of discourse. <eos> fine-grained analyses of these failures provide insight into the limitations of current coreference resolution systems, and ways of improving them.
what makes a poem beautiful ? <eos> we use computational methods to compare the stylistic and content features employed by awardwinning poets and amateur poets. <eos> building upon existing techniques designed to quantitatively analyze style and affect in texts, we examined elements of poetic craft such as diction, sound devices, emotive language, and imagery. <eos> results showed that the most important indicator of high-quality poetry we could detect was the frequency of references to concrete objects. <eos> this result highlights the influence of imagism in contemporary professional poetry, and suggests that concreteness may be one of the most appealing features of poetry to the modern aesthetic. <eos> we also report on other features that characterize high-quality poetry and argue that methods from computational linguistics may provide important insights into the analysis of beauty in verbal art.
the identification of stylistic inconsistency is a challenging task relevant to a number of genres, including literature. <eos> in this work, we carry out stylistic segmentation of a well-known poem, the waste land by t.s. <eos> eliot, which is traditionally analyzed in terms of numerous voices which appear throughout the text. <eos> our method, adapted from work in topic segmentation and plagiarism detection, predicts breaks based on a curve of stylistic change which combines information from a diverse set of features, most notably co-occurrence in larger corpora via reduced-dimensionality vectors. <eos> we show that this extrinsic information is more useful than ( within-text ) distributional features. <eos> we achieve well above baseline performance on both artificial mixed-style texts and the waste land itself.
electronic versions of literary works abound on the internet and the rapid dissemination of electronic readers will make electronic books more and more common. <eos> it is often the case that literary works exist in more than one language, suggesting that, if properly aligned, they could be turned into useful resources for many practical applications, such as writing and language learning aids, translation studies, or data-based machine translation. <eos> to be of any use, these bilingual works need to be aligned as precisely as possible, a notoriously difficult task. <eos> in this paper, we revisit the problem of sentence alignment for literary works and explore the performance of a new, multi-pass, approach based on a combination of systems. <eos> experiments conducted on excerpts of ten masterpieces of the french and english literature show that our approach significantly outperforms two open source tools.
this study explores the use of function words for authorship attribution in modern chinese ( c-fwaa ). <eos> this study consists of three tasks : ( 1 ) examine the c-fwaa effectiveness in three genres : novel, essay, and blog ; ( 2 ) compare the strength of function words as both genre and authorship indicators, and explore the genre interference on c-fwaa ; ( 3 ) examine whether c-fwaa is sensitive to the time periods when the texts were written.
simple text classification algorithms perform remarkably well when used for detecting famous quotes in literary or philosophical text, with f-scores approaching 95 %. <eos> we compare the task to topic classification, polarity classification and authorship attribution.
we present a method of authorship attribution and stylometry that exploits hierarchical information in phrase-structures. <eos> contrary to much previous work in stylometry, we focus on content words rather than function words. <eos> texts are parsed to obtain phrase-structures, and compared with texts to be analyzed. <eos> an efficient tree kernel method identifies common tree fragments among data of known authors and unknown texts. <eos> these fragments are then used to identify authors and characterize their styles. <eos> our experiments show that the structural information from fragments provides complementary information to the baseline trigram model.
we compare four methods for transcribing early printed texts. <eos> our comparison is through a case-study of digitizing an eighteenthcentury french novel for a new critical edition : the 1784 lettres ta ? <eos> ? tiennes by jose ? phine de monbart. <eos> we provide a detailed error analysis of transcription by optical character recognition ( ocr ), non-expert humans, and expert humans and weigh each technique based on accuracy, speed, cost and the need for scholarly overhead. <eos> our findings are relevant to 18th-century french scholars as well as the entire community of scholars working to preserve, present, and revitalize interest in literature published before the digital age.
readers suffering from information overload have often turned to collections of pithy and famous quotations. <eos> while research on largescale analysis of text reuse has found effective methods for detecting widely disseminated and famous quotations, this paper explores the complementary problem of detecting, from internal evidence alone, which phrases are quotable. <eos> these quotable phrases are memorable and succinct statements that people are likely to find useful outside of their original context. <eos> we evaluate quotable phrase extraction using a large digital library and demonstrate that an integration of lexical and shallow syntactic features results in a reliable extraction process. <eos> a study using a reddit community of quote enthusiasts as well as a simple corpus analysis further demonstrate the practical applications of our work.
the quran is a significant religious text written in a unique literary style, close to very poetic language in nature. <eos> accordingly it is significantly richer and more complex than the newswire style used in the previously released arabic propbank ( zaghouani et al, 2010 ; diab et al, 2008 ). <eos> we present preliminary work on the creation of a unique arabic proposition repository for quranic arabic. <eos> we annotate the semantic roles for the 50 most frequent verbs in the quranic arabic dependency treebank ( qatb ) ( dukes and buckwalter 2010 ). <eos> the quranic arabic propbank ( qapb ) will be a unique new resource of its kind for the arabic nlp research community as it will allow for interesting insights into the semantic use of classical arabic, poetic literary arabic, as well as significant religious texts. <eos> moreover, on a pragmatic level qapb will add approximately 810 new verbs to the existing arabic propbank ( apb ). <eos> in this pilot experiment, we leverage our knowledge and experience from our involvement in the apb project. <eos> all the qapb annotations will be made freely available for research purposes.
topic modeling of fashion trends were analyzed using the mallet toolkit. <eos> harper ? s bazaar magazines from 1860-1899 were used ( freely available online ). <eos> this resulted in 20 topics with 4 characterizing words each. <eos> trends over time were analyzed in several different ways using 100-topics and 20-topics.
we present a network analysis of a literary text, alice in wonderland. <eos> we build novel types of networks in which links between characters are different types of social events. <eos> we show that analyzing networks based on these social events gives us insight into the roles of characters in the story. <eos> also, static network analysis has limitations which become apparent from our analysis. <eos> we propose the use of dynamic network analysis to overcome these limitations.
automatic evaluation has greatly facilitated system development in summarization. <eos> at the same time, the use of automatic evaluation has been viewed with mistrust by many, as its accuracy and correct application are not well understood. <eos> in this paper we provide an assessment of the automatic evaluations used for multi-document summarization of news. <eos> we outline our recommendations about how any evaluation, manual or automatic, should be used to find statistically significant differences between summarization systems. <eos> we identify the reference automatic evaluation metrics ? <eos> rouge 1 and 2 ? that appear to best emulate human pyramid and responsiveness scores on four years of nist evaluations. <eos> we then demonstrate the accuracy of these metrics in reproducing human judgements about the relative content quality of pairs of systems and present an empirical assessment of the relationship between statistically significant differences between systems according to manual evaluations, and the difference according to automatic evaluations. <eos> finally, we present a case study of how new metrics should be compared to the reference evaluation, as we search for even more accurate automatic measures.
numerous nlp tasks rely on clustering or community detection algorithms. <eos> for many of these tasks, the solutions are disjoint, and the relevant evaluation metrics assume nonoverlapping clusters. <eos> in contrast, the relatively recent task of abstractive community detection ( acd ) results in overlapping clusters of sentences. <eos> acd is a sub-task of an abstractive summarization system and represents a twostep process. <eos> in the first step, we classify sentence pairs according to whether the sentences should be realized by a common abstractive sentence. <eos> this results in an undirected graph with sentences as nodes and predicted abstractive links as edges. <eos> the second step is to identify communities within the graph, where each community corresponds to an abstractive sentence to be generated. <eos> in this paper, we describe how the omega index, a metric for comparing non-disjoint clustering solutions, can be used as a summarization evaluation metric for this task. <eos> we use the omega index to compare and contrast several community detection algorithms.
the multilingual summarization pilot task at tac ? 11 opened a lot of problems we are facing when we try to evaluate summary quality in different languages. <eos> the additional language dimension greatly increases annotation costs. <eos> for the tac pilot task english articles were first translated to other 6 languages, model summaries were written and submitted system summaries were evaluated. <eos> we start with the discussion whether rouge can produce system rankings similar to those received from manual summary scoring by measuring their correlation. <eos> we study then three ways of projecting summaries to a different language : projection through sentence alignment in the case of parallel corpora, simple summary translation and summarizing machine translated articles. <eos> building such summaries gives opportunity to run additional experiments and reinforce the evaluation. <eos> later, we investigate whether an evaluation based on machine translated models can perform close to an evaluation based on original models.
the development of summarization systems requires reliable similarity ( evaluation ) measures that compare system outputs with human references. <eos> a reliable measure should have correspondence with human judgements. <eos> however, the reliability of measures depends on the test collection in which the measure is meta-evaluated ; for this reason, it has not yet been possible to reliably establish which are the best evaluation measures for automatic summarization. <eos> in this paper, we propose an unsupervised method called heterogeneitybased ranking ( hbr ) that combines summarization evaluation measures without requiring human assessments. <eos> our empirical results indicate that hbr achieves a similar correspondence with human assessments than the best single measure for every observed corpus. <eos> in addition, hbr results are more robust across topics than single measures.
today, automatic evaluation metrics such as rouge have become the de-facto mode of evaluating an automatic summarization system. <eos> however, based on the duc and the tac evaluation results, ( conroy and schlesinger, 2008 ; dang and owczarzak, 2008 ) showed that the performance gap between humangenerated summaries and system-generated summaries is clearly visible in manual evaluations but is often not reflected in automated evaluations using rouge scores. <eos> in this paper, we present our own experiments in comparing the results of manual evaluations versus automatic evaluations using our own text summarizer : blogsum. <eos> we have evaluated blogsum-generated summary content using rouge and compared the results with the original candidate list ( olist ). <eos> the t-test results showed that there is no significant difference between blogsum-generated summaries and olist summaries. <eos> however, two manual evaluations for content using two different datasets show that blogsum performed significantly better than olist. <eos> a manual evaluation of summary coherence also shows that blogsum performs significantly better than olist. <eos> these results agree with previous work and show the need for a better automated summary evaluation metric rather than the standard rouge metric.
in spite of their well known limitations, most notably their use of very local contexts, n-gram language models remain an essential component of many natural language processing applications, such as automatic speech recognition or statistical machine translation. <eos> this paper investigates the potential of language models using larger context windows comprising up to the 9 previous words. <eos> this study is made possible by the development of several novel neural network language model architectures, which can easily fare with such large context windows. <eos> we experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n-gram assumption is statistically reasonable as long as n is sufficiently high, and that efforts should be focused on improving the estimation procedures for such large models.
language models play an important role in large vocabulary speech recognition and statistical machine translation systems. <eos> the dominant approach since several decades are back-off language models. <eos> some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. <eos> lately, this tendency has changed and recent works concentrate on data selection. <eos> continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. <eos> this paper presents an experimental comparison of all these approaches on a large statistical machine translation task. <eos> we also describe an open-source implementation to train and use continuous space language models ( cslm ) for such large tasks. <eos> we describe an efficient implementation of the cslm using graphical processing units from nvidia. <eos> by these means, we are able to train an cslm on more than 500 million words in 20 hours. <eos> this cslm provides an improvement of up to 1.8 bleu points with respect to the best back-off language model that we were able to build.
in recent years, neural network language models ( nnlms ) have shown success in both peplexity and word error rate ( wer ) compared to conventional n-gram language models. <eos> most nnlms are trained with one hidden layer. <eos> deep neural networks ( dnns ) with more hidden layers have been shown to capture higher-level discriminative information about input features, and thus produce better networks. <eos> motivated by the success of dnns in acoustic modeling, we explore deep neural network language models ( dnn lms ) in this paper. <eos> results on a wall street journal ( wsj ) task demonstrate that dnn lms offer improvements over a single hidden layer nnlm. <eos> furthermore, our preliminary results are competitive with a model m language model, considered to be one of the current state-of-the-art techniques for language modeling.
in this paper, we describe a new, publicly available corpus intended to stimulate research into language modeling techniques which are sensitive to overall sentence coherence. <eos> the task uses the scholastic aptitude test ? s sentence completion format. <eos> the test set consists of 1040 sentences, each of which is missing a content word. <eos> the goal is to select the correct replacement from amongst five alternates. <eos> in general, all of the options are syntactically valid, and reasonable with respect to local n-gram statistics. <eos> the set was generated by using an n-gram language model to generate a long list of likely words, given the immediate context. <eos> these options were then hand-groomed, to identify four decoys which are globally incoherent, yet syntactically correct. <eos> to ensure the right to public distribution, all the data is derived from out-of-copyright materials from project gutenberg. <eos> the test sentences were derived from five of conan doyle ? s sherlock holmes novels, and we provide a large set of nineteenth and early twentieth century texts as training material.
modeling of foreign entity names is an important unsolved problem in morpheme-based modeling that is common in morphologically rich languages. <eos> in this paper we present an unsupervised vocabulary adaptation method for morph-based speech recognition. <eos> foreign word candidates are detected automatically from in-domain text through the use of letter n-gram perplexity. <eos> over-segmented foreign entity names are restored to their base forms in the morph-segmented in-domain text for easier and more reliable modeling and recognition. <eos> the adapted pronunciation rules are finally generated with a trainable grapheme-tophoneme converter. <eos> in asr performance the unsupervised method almost matches the ability of supervised adaptation in correctly recognizing foreign entity names.
we present a distributed framework for largescale discriminative language models that can be integrated within a large vocabulary continuous speech recognition ( lvcsr ) system using lattice rescoring. <eos> we intentionally use a weakened acoustic model in a baseline lvcsr system to generate candidate hypotheses for voice-search data ; this allows us to utilize large amounts of unsupervised data to train our models. <eos> we propose an efficient and scalable mapreduce framework that uses a perceptron-style distributed training strategy to handle these large amounts of data. <eos> we report small but significant improvements in recognition accuracies on a standard voice-search data set using our discriminative reranking model. <eos> we also provide an analysis of the various parameters of our models including model size, types of features, size of partitions in the mapreduce framework with the help of supporting experiments.
statistical language models used in deployed systems for speech recognition, machine translation and other human language technologies are almost exclusively n-gram models. <eos> they are regarded as linguistically na ? <eos> ? ve, but estimating them from any amount of text, large or small, is straightforward. <eos> furthermore, they have doggedly matched or outperformed numerous competing proposals for syntactically well-motivated models. <eos> this unusual resilience of n-grams, as well as their weaknesses, are examined here. <eos> it is demonstrated that n-grams are good word-predictors, even linguistically speaking, in a large majority of word-positions, and it is suggested that to improve over n-grams, one must explore syntax-aware ( or other ) language models that focus on positions where n-grams are weak.
this paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. <eos> from this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments.
in order for robots to effectively understand natural language commands, they must be able to acquire a large vocabulary of meaning representations that can be mapped to perceptual features in the external world. <eos> previous approaches to learning these grounded meaning representations require detailed annotations at training time. <eos> in this paper, we present an approach which is capable of jointly learning a policy for following natural language commands such as ? pick up the tire pallet, ? <eos> as well as a mapping between specific phrases in the language and aspects of the external world ; for example the mapping between the words ? the tire pallet ? <eos> and a specific object in the environment. <eos> we assume the action policy takes a parametric form that factors based on the structure of the language, based on the g3 framework and use stochastic gradient ascent to optimize policy parameters. <eos> our preliminary evaluation demonstrates the effectiveness of the model on a corpus of ? pick up ? <eos> commands given to a robotic forklift by untrained users.
this paper describes a demonstration of the winktalk system, which is a speech synthesis platform using expressive synthetic voices. <eos> with the help of a webcamera and facial expression analysis, the system allows the user to control the expressive features of the synthetic speech for a particular utterance with their facial expressions. <eos> based on a personalised mapping between three expressive synthetic voices and the users facial expressions, the system selects a voice that matches their face at the moment of sending a message. <eos> the winktalk system is an early research prototype that aims to demonstrate that facial expressions can be used as a more intuitive control over expressive speech synthesis than manual selection of voice types, thereby contributing to an improved communication experience for users of speech generating devices.
this paper presents a method for an aac system to predict a whole response given features of the previous utterance from the interlocutor. <eos> it uses a large corpus of scripted dialogs, computes a variety of lexical, syntactic and whole phrase features for the previous utterance, and predicts features that the response should have, using an entropy-based measure. <eos> we evaluate the system on a held-out portion of the corpus. <eos> we find that for about 3.5 % of cases in the held-out corpus, we are able to predict a response, and among those, over half are either exact or at least reasonable substitutes for the actual response. <eos> we also present some results on keystroke savings. <eos> finally we compare our approach to a state-of-the-art chatbot, and show ( not surprisingly ) that a system like ours, tuned for a particular style of conversation, outperforms one that is not. <eos> predicting possible responses automatically by mining a corpus of dialogues is a novel contribution to the literature on whole utterance-based methods in aac. <eos> also useful, we believe, is our estimate that about 3.5-4.0 % of utterances in dialogs are in principle predictable given previous context.
ristensson school of computer science university of st andrews pok @ st-andrews.ac.uk abstract it is well documented that people with severe speech and physical impairments ( sspi ) often experience literacy difficulties, which hinder them from effectively using orthographic-based aac systems for communication. <eos> to address this problem, phoneme-based aac systems have been proposed, which enable users to access a set of spoken phonemes and combine phonemes into speech output. <eos> in this paper we investigate how prediction tech-niques can be applied to improve user perfor-mance of such systems. <eos> we have developed a phoneme-based prediction system, which sup-ports single phoneme prediction and pho-neme-based word prediction using statistical language models generated using a crowdsourced aac-like corpus. <eos> we incorpo-rated our prediction system into a hypothetical 12-key reduced phoneme keyboard. <eos> a compu-tational experiment showed that our prediction system led to 56.3 % average keystroke sav-ings.
most icon-based augmentative and alternative communication ( aac ) devices require users to formulate messages in syntactic order in order to produce syntactic utterances. <eos> reliance on syntactic ordering, however, may not be appropriate for individuals with limited or emerging literacy skills. <eos> some of these users may benefit from unordered message formulation accompanied by automatic message expansion to generate syntactically correct messages. <eos> facilitating communication via unordered message formulation, however, requires new methods of prediction. <eos> this paper describes a novel approach to word prediction using semantic grams, or ? sem-grams, ? <eos> which provide relational information about message components regardless of word order. <eos> performance of four word-level prediction algorithms, two based on sem-grams and two based on n-grams, were compared on a corpus of informal blogs. <eos> results showed that sem-grams yield accurate word prediction, but lack prediction coverage. <eos> hybrid methods that combine n-gram and sem-gram approaches may be viable for unordered prediction in aac.
the number of people with dementia of the alzheimer 's type ( dat ) continues to grow. <eos> one of the significant impacts of this disease is a decline in the ability to communicate using natural language. <eos> this decline in language facility often results in decreased social interaction and life satisfaction for persons with dat and their caregivers. <eos> one possible strategy to lessen the effects of this loss of language facility is for the unaffected conversational partner ( facilitator ) to `` co-construct '' short autobiographical stories from the life of the dataffected conversational partner ( storyteller ). <eos> it has been observed that a skilled conversational partner can facilitate co-constructed narrative with individuals who have mild to moderate dat. <eos> developing a computational model of this type of co-constructed narrative would enable assistive technology to be developed that can monitor a conversation between a storyteller and facilitator. <eos> this technology could provide context-sensitive suggestions to an unskilled facilitator to help maintain the flow of conversation. <eos> this paper describes a framework in which the necessary computational model of co-constructed narrative can be developed. <eos> an analysis of the fundamental elements of such a model will be presented.
currently, health care costs associated with aging at home can be prohibitive if individuals require continual or periodic supervision or assistance because of alzheimer ? s disease. <eos> these costs, normally associated with human caregivers, can be mitigated to some extent given automated systems that mimic some of their functions. <eos> in this paper, we present inaugural work towards producing a generic automated system that assists individuals with alzheimer ? s to complete daily tasks using verbal communication. <eos> here, we show how to improve rates of correct speech recognition by preprocessing acoustic noise and by modifying the vocabulary according to the task. <eos> we conclude by outlining current directions of research including specialized grammars and automatic detection of confusion.
tactile maps are important substitutes for visual maps for blind and visually impaired people and the efficiency of tactile-map reading can largely be improved by giving assisting utterances that make use of spatial language. <eos> in this paper, we elaborate earlier ideas for a system that generates such utterances and present a prototype implementation based on a semantic conceptualization of the movements that the map user performs. <eos> a worked example shows the plausibility of the solution and the output that the prototype generates given input derived from experimental data.
erfauth department of computer science queens college and graduate center city university of new york ( cuny ) 65-30 kissena blvd, flushing, ny 11367 matt @ cs.qc.cuny.edu abstract american sign language ( asl ) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low english literacy. <eos> the synthesis com-ponent of current asl animation generation and scripting systems have limited handling of the many asl verb signs whose movement path is inflected to indicate 3d locations in the signing space associated with discourse refer-ents. <eos> using motion-capture data recorded from human signers, we model how the mo-tion-paths of verb signs vary based on the lo-cation of their subject and object. <eos> this model yields a lexicon for asl verb signs that is pa-rameterized on the 3d locations of the verb ? s arguments ; such a lexicon enables more real-istic and understandable asl animations. <eos> a new model presented in this paper, based on identifying the principal movement vector of the hands, shows improvement in modeling asl verb signs, including when trained on movement data from a different human signer.
this paper addresses the problem of automatic text simplification. <eos> automatic text simplifications aims at reducing the reading difficulty for people with cognitive disability, among other target groups. <eos> we describe an automatic text simplification system for spanish which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts. <eos> our system is integrated in a service architecture which includes a web service and mobile applications.
human assessment is often considered the gold standard in evaluation of translation systems. <eos> but in order for the evaluation to be meaningful, the rankings obtained from human assessment must be consistent and repeatable. <eos> recent analysis by bojar et al. <eos> ( 2011 ) raised several concerns about the rankings derived from human assessments of english-czech translation systems in the 2010 workshop on machine translation. <eos> we extend their analysis to all of the ranking tasks from 2010 and 2011, and show through an extension of their reasoning that the ranking is naturally cast as an instance of finding the minimum feedback arc set in a tournament, a wellknown np-complete problem. <eos> all instances of this problem in the workshop data are efficiently solvable, but in some cases the rankings it produces are surprisingly different from the ones previously published. <eos> this leads to strong caveats and recommendations for both producers and consumers of these rankings.
this paper presents the results of the wmt12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. <eos> we conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. <eos> we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. <eos> we introduced a new quality estimation task this year, and evaluated submissions from 11 teams.
textual similarity for mt evaluation julio castillo ? ? <eos> paula estrella ? <eos> ? famaf, unc, argentina ? <eos> ? utn-frc, argentina jotacastillo @ gmail.com pestrella @ famaf.unc.edu.ar abstract this paper describes the system used for our participation in the wmt12 machine transla-tion evaluation shared task. <eos> we also present a new approach to machine translation evaluation based on the recently defined task semantic textual similarity. <eos> this problem is addressed using a textual entail-ment engine entirely based on wordnet se-mantic features. <eos> we described results for the spanish-english, czech-english and german-english language pairs according to our submission on the eight workshop on statistical machine translation. <eos> our first experiments reports a competitive score to system level.
a recent paper described a new machine translation evaluation metric, amber. <eos> this paper describes two changes to amber. <eos> the first one is incorporation of a new ordering penalty ; the second one is the use of the downhill simplex algorithm to tune the weights for the components of amber. <eos> we tested the impact of the two changes, using data from the wmt metrics task. <eos> each of the changes by itself improved the performance of amber, and the two together yielded even greater improvement, which in some cases was more than additive. <eos> the new version of amber clearly outperforms bleu in terms of correlation with human judgment.
we investigate the use of error classification results for automatic evaluation of machine translation output. <eos> five basic error classes are taken into account : morphological errors, syntactic ( reordering ) errors, missing words, extra words and lexical errors. <eos> in addition, linear combinations of these categories are investigated. <eos> correlations between the class error rates and human judgments are calculated on the data of the third, fourth, fifth and sixth shared tasks of the statistical machine translation workshop. <eos> machine translation outputs in five different european languages are used : english, spanish, french, german and czech. <eos> the results show that the following combinations are the most promising : the sum of all class error rates, the weighted sum optimised for translation into english and the weighted sum optimised for translation from english.
this paper describes stanford university ? s submission to the shared evaluation task of wmt 2012. <eos> our proposed metric ( spede ) computes probabilistic edit distance as predictions of translation quality. <eos> we learn weighted edit distance in a probabilistic finite state machine ( pfsm ) model, where state transitions correspond to edit operations. <eos> while standard edit distance models can not capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pfsm model. <eos> our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features. <eos> evaluated on two different prediction tasks across a diverse set of datasets, our methods achieve state-of-the-art correlation with human judgments.
we describe a submission to the wmt12 quality estimation task, including an extensive machine learning experimentation. <eos> data were augmented with features from linguistic analysis and statistical features from the smt search graph. <eos> several feature selection algorithms were employed. <eos> the quality estimation problem was addressed both as a regression task and as a discretised classification task, but the latter did not generalise well on the unseen testset. <eos> the most successful regression methods had an rmse of 0.86 and were trained with a feature set given by correlation-based feature selection. <eos> indications that rmse is not always sufficient for measuring performance were observed.
in this paper we introduce a number of new features for quality estimation in machine translation that were developed for the wmt 2012 quality estimation shared task. <eos> we find that very simple features such as indicators of certain characters are able to outperform complex features that aim to model the connection between two languages.
this paper describes a study on the contribution of linguistically-informed features to the task of quality estimation for machine translation at sentence level. <eos> a standard regression algorithm is used to build models using a combination of linguistic and non-linguistic features extracted from the input text and its machine translation. <eos> experiments with englishspanish translations show that linguistic features, although informative on their own, are not yet able to outperform shallower features based on statistics from the input text, its translation and additional corpora. <eos> however, further analysis suggests that linguistic information is actually useful but needs to be carefully combined with other features in order to produce better results.
this is a description of the submissions made by the pattern recognition and human language technology group ( prhlt ) of the universitat polite`cnica de vale`ncia to the quality estimation task of the seventh workshop on statistical machine translation ( wmt12 ). <eos> we focus on two different issues : how to effectively combine subsequence-level features into sentence-level features, and how to select the most adequate subset of features. <eos> results showed that an adequate selection of a subset of highly discriminative features can improve efficiency and performance of the quality estimation system.
this paper describes uppsala university ? s submissions to the quality estimation ( qe ) shared task at wmt 2012. <eos> we present a qe system based on support vector machine regression, using a number of explicitly defined features extracted from the machine translation input, output and models in combination with tree kernels over constituency and dependency parse trees for the input and output sentences. <eos> we confirm earlier results suggesting that tree kernels can be a useful tool for qe system construction especially in the early stages of system design.
in this paper we present the system we submitted to the wmt12 shared task on quality estimation. <eos> each translated sentence is given a score between 1 and 5. <eos> the score is obtained using several numerical or boolean features calculated according to the source and target sentences. <eos> we perform a linear regression of the feature space against scores in the range. <eos> to this end, we use a support vector machine. <eos> we experiment with two kernels : linear and radial basis function. <eos> in our submission we use the features from the shared task baseline system and our own features. <eos> this leads to 66 features. <eos> to deal with this large number of features, we propose an in-house feature selection algorithm. <eos> our results show that a lot of information is already present in baseline features, and that our feature selection algorithm discards features which are linearly correlated.
we present the approach we took for our participation to the wmt12 quality estimation shared task : our main goal is to achieve reasonably good results without appeal to supervised learning. <eos> we have used various similarity measures and also an external resource ( google n -grams ). <eos> details of results clarify the interest of such an approach.
in this paper, we describe the upc system that participated in the wmt 2012 shared task on quality estimation for machine translation. <eos> based on the empirical evidence that fluencyrelated features have a very high correlation with post-editing effort, we present a set of features for the assessment of quality estimation for machine translation designed around different kinds of n-gram language models, plus another set of features that model the quality of dependency parses automatically projected from source sentences to translations. <eos> we document the results obtained on the shared task dataset, obtained by combining the features that we designed with the baseline features provided by the task organizers.
we present a method we used for the quality estimation shared task of wmt 2012 involving ibm1 and language model scores calculated on morphemes and pos tags. <eos> the ibm1 scores calculated on morphemes and pos-4grams of the source sentence and obtained translation output are shown to be competitive with the classic evaluation metrics for ranking of translation systems. <eos> since these scores do not require any reference translations, they can be used as features for the quality estimation task presenting a connection between the source language and the obtained target language. <eos> in addition, target language model scores of morphemes and pos tags are investigated as estimates for the obtained target language quality.
this paper describes the features and the machine learning methods used by dublin city university ( dcu ) and symantec for the wmt 2012 quality estimation task. <eos> two sets of features are proposed : one constrained, i.e. <eos> respecting the data limitation suggested by the workshop organisers, and one unconstrained, i.e. <eos> using data or tools trained on data that was not provided by the workshop organisers. <eos> in total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data. <eos> in this paper, we focus on a subset of our feature set that we consider to be relatively novel : features based on a topic model built using the latent dirichlet allocation approach, and features based on source and target language syntax extracted using part-of-speech ( pos ) taggers and parsers. <eos> we evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.
we present in this paper the system submissions of the sdl language weaver team in the wmt 2012 quality estimation shared-task. <eos> our mt quality-prediction systems use machine learning techniques ( m5p regression-tree and svm-regression models ) and a feature-selection algorithm that has been designed to directly optimize towards the official metrics used in this shared-task. <eos> the resulting submissions placed 1st ( the m5p model ) and 2nd ( the svm model ), respectively, on both the ranking task and the scoring task, out of 11 participating teams.
we in this paper describe the regression system for our participation in the quality estimation task of wmt12. <eos> this paper focuses on exploiting special phrases, or word sequences, to estimate translation quality. <eos> several feature templates on this topic are put forward subsequently. <eos> we train a svm regression model for predicting the scores and numerical results show the effectiveness of our phrase indicators and method in both ranking and scoring tasks.
this paper describes our work with the data distributed for the wmt ? 12 confidence estimation shared task. <eos> our contribution is twofold : i ) we first present an analysis of the data which highlights the difficulty of the task and motivates our approach ; ii ) we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task.
this paper presents techniques for referencefree, automatic prediction of machine translation output quality at both sentence- and document-level. <eos> in addition to helping with document-level quality estimation, sentencelevel predictions are used for system selection, improving the quality of the output translations. <eos> we present three system selection techniques and perform evaluations that quantify the gains across multiple domains and language pairs.
we address two challenges for automatic machine translation evaluation : a ) avoiding the use of reference translations, and b ) focusing on adequacy estimation. <eos> from an economic perspective, getting rid of costly hand-crafted reference translations ( a ) permits to alleviate the main bottleneck in mt evaluation. <eos> from a system evaluation perspective, pushing semantics into mt ( b ) is a necessity in order to complement the shallow methods currently used overcoming their limitations. <eos> casting the problem as a cross-lingual textual entailment application, we experiment with different benchmarks and evaluation settings. <eos> our method shows high correlation with human judgements and good results on all datasets without relying on reference translations.
post-editing performed by translators is an increasingly common use of machine translated texts. <eos> while high quality mt may increase productivity, post-editing poor translations can be a frustrating task which requires more effort than translating from scratch. <eos> for this reason, estimating whether machine translations are of sufficient quality to be used for post-editing and finding means to reduce post-editing effort are an important field of study. <eos> post-editing effort consists of different aspects, of which temporal effort, or the time spent on post-editing, is the most visible and involves not only the technical effort needed to perform the editing, but also the cognitive effort required to detect and plan necessary corrections. <eos> cognitive effort is difficult to examine directly, but ways to reduce the cognitive effort in particular may prove valuable in reducing the frustration associated with postediting work. <eos> in this paper, we describe an experiment aimed at studying the relationship between technical post-editing effort and cognitive post-editing effort by comparing cases where the edit distance and a manual score reflecting perceived effort differ. <eos> we present results of an error analysis performed on such sentences and discuss the clues they may provide about edits requiring great cognitive effort compared to the technical effort, on one hand, or little cognitive effort, on the other.
confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. <eos> the hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. <eos> this paper describes a systematic comparison of five well known hypothesis alignment algorithms for mt system combination via confusion network decoding. <eos> controlled experiments using identical pre-processing, decoding, and weight tuning methods on standard system combination evaluation sets are presented. <eos> translation quality is assessed using case insensitive bleu scores and bootstrapping is used to establish statistical significance of the score differences. <eos> all aligners yield significant bleu score gains over the best individual system included in the combination. <eos> incremental indirect hidden markov model and a novel incremental inversion transduction grammar with flexible matching consistently yield the best translation quality, though keeping all things equal, the differences between aligners are relatively small. <eos> ? the work reported in this paper was carried out while the authors were at raytheon bbn technologies and ? rwth aachen university.
the addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation ( pbsmt ) system. <eos> permutation parsers have been used to implement hierarchical re-ordering models ( galley and manning, 2008 ) and to enforce inversion transduction grammar ( itg ) constraints ( feng et al., 2010 ). <eos> we present a number of theoretical results regarding the use of permutation parsers in pbsmt. <eos> in particular, we show that an existing itg constraint ( zens et al, 2004 ) does not prevent all non-itg permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. <eos> experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output.
statistical phrase-based machine translation requires no linguistic information beyond word-aligned parallel corpora ( zens et al, 2002 ; koehn et al, 2003 ). <eos> unfortunately, this linguistic agnosticism often produces ungrammatical translations. <eos> syntax, or sentence structure, could provide guidance to phrasebased systems, but the ? non-constituent ? <eos> word strings that phrase-based decoders manipulate complicate the use of most recursive syntactic tools. <eos> we address these issues by using combinatory categorial grammar, or ccg, ( steedman, 2000 ), which has a much more flexible notion of constituency, thereby providing more labels for putative nonconstituent multiword translation phrases. <eos> using ccg parse charts, we train a syntactic analogue of a lexicalized reordering model by labelling phrase table entries with multiword labels and demonstrate significant improvements in translating between urdu and english, two language pairs with divergent sentence structure.
adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in ghkm ( galley et al, 2004 ), excludes potentially useful translation rules. <eos> samt ( zollmann and venugopal, 2006 ) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. <eos> we introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. <eos> we show that our labeling scheme performs comparably to samt on an urdu ? english translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast.
chiang ? s hierarchical phrase-based ( hpb ) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases ? <eos> phrases that contain sub-phrases. <eos> however, the original hpb model is prone to overgeneration due to lack of linguistic knowledge : the grammar may suggest more derivations than appropriate, many of which may lead to ungrammatical translations. <eos> on the other hand, limitations of glue grammar rules in the original hpb model may actually prevent systems from considering some reasonable derivations. <eos> this paper presents a simple but effective translation model, called the head-driven hpb ( hd-hpb ) model, which incorporates head information in translation rules to better capture syntax-driven information in a derivation. <eos> in addition, unlike the original glue rules, the hd-hpb model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space. <eos> an extensive set of experiments on chinese-english translation on four nist mt test sets, using both a small and a large training set, show that our hdhpb model consistently and statistically significantly outperforms chiang ? s model as well as a source side samt-style model.
we introduce the first fully automatic, fully semantic frame based mt evaluation metric, meant, that outperforms all other commonly used automatic metrics in correlating with human judgment on translation adequacy. <eos> recent work on hmeant, which is a human metric, indicates that machine translation can be better evaluated via semantic frames than other evaluation paradigms, requiring only minimal effort from monolingual humans to annotate and align semantic frames in the reference and machine translations. <eos> we propose a surprisingly effective occam ? s razor automation of hmeant that combines standard shallow semantic parsing with a simple maximum weighted bipartite matching algorithm for aligning semantic frames. <eos> the matching criterion is based on lexical similarity scoring of the semantic role fillers through a simple context vector model which can readily be trained using any publicly available large monolingual corpus. <eos> sentence level correlation analysis, following standard nist metricsmatr protocol, shows that this fully automated version of hmeant achieves significantly higher kendall correlation with human adequacy judgments than bleu, nist, meteor, per, cder, wer, or ter. <eos> furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. <eos> despite its high performance, fully automated meant is still able to preserve hmeant ? s virtues of simplicity, representational transparency, and inexpensiveness.
we introduce a taxonomy of factored phrasebased translation scenarios and conduct a range of experiments in this taxonomy. <eos> we point out several common pitfalls when designing factored setups. <eos> the paper also describes our wmt12 submissions cu-bojar and cu-poor-comb.
this paper describes the french-english translation system developed by the avenue research group at carnegie mellon university for the seventh workshop on statistical machine translation ( naacl wmt12 ). <eos> we present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.
one of the most notable recent improvements of the tectomt english-to-czech translation is a systematic and theoretically supported revision of formemes ? the annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the prague tectogrammatics theory. <eos> our modifications aim at reducing data sparsity, increasing consistency across languages and widening the usage area of this markup. <eos> formemes can be used not only in mt, but in various other nlp tasks.
rmiga, carlos.henriquez, adolfo.hernandezjose.marino, enric.monte, jose.fonollosa } @ upc.eduabstract this paper describes the upc participation in the wmt 12 evaluation campaign. <eos> all systems presented are based on standard phrasebased moses systems. <eos> variations adopted several improvement techniques such as morphology simplification and generation and domain adaptation. <eos> the morphology simplification overcomes the data sparsity problem when translating into morphologicallyrich languages such as spanish by translating first to a morphology-simplified language and secondly leave the morphology generation to an independent classification task. <eos> the domain adaptation approach improves the smt system by adding new translation units learned from mt-output and reference alignment. <eos> results depict an improvement on ter, meteor, nist and bleu scores compared to our baseline system, obtaining on the official test set more benefits from the domain adaptation approach than from the morphological generalization method.
we present joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. <eos> the main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, j-pro. <eos> we further present the extension of the thrax scfg grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases.
we present a variant of phrase-based smt that uses source-side parsing and a constituent reordering model based on word alignments in the word-aligned training corpus to predict hierarchical block-wise reordering of the input. <eos> multiple possible translation orders are represented compactly in a source order lattice. <eos> this source order lattice is then annotated with phrase-level translations to form a lattice of tokens in the target language. <eos> various feature functions are combined in a log-linear fashion to evaluate paths through that lattice.
we describe the systems developed by the team of the qatar computing research institute for the wmt12 shared translation task. <eos> we used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. <eos> the evaluation results show that we rank second in bleu and ter for spanish-english, and in the top tier for german-english.
this paper describes the statistical machine translation ( smt ) systems developed at rwth aachen university for the translation task of the naacl 2012 seventh workshop on statistical machine translation ( wmt 2012 ). <eos> we participated in the evaluation campaign for the french-english and german-english language pairs in both translation directions. <eos> both hierarchical and phrase-based smt systems are applied. <eos> a number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. <eos> by application of these methods we achieve considerable improvements over the respective baseline systems.
we describe a substitution-based system for hybrid machine translation ( mt ) that has been extended with machine learning components controlling its phrase selection. <eos> the approach is based on a rule-based mt ( rbmt ) system which creates template translations. <eos> based on the rule-based generation parse tree and target-to-target algnments, we identify the set of ? interesting ? <eos> translation candidates from one or more translation engines which could be substituted into our translation templates. <eos> the substitution process is either controlled by the output from a binary classifier trained on feature vectors from the different mt engines, or it is depending on weights for the decision factors, which have been tuned using mert. <eos> we are able to observe improvements in terms of bleu scores over a baseline version of the hybrid system.
we report on findings of exploiting large data sets for translation modeling, language modeling and tuning for the development of competitive machine translation systems for eight language pairs. <eos>
this paper describes the joint quaero submission to the wmt 2012 machine translation evaluation. <eos> four groups ( rwth aachen university, karlsruhe institute of technology, limsi-cnrs, and systran ) of the quaero project submitted a joint translation for the wmt german ? english task. <eos> each group translated the data sets with their own systems and finally the rwth system combination combined these translations in our final submission. <eos> experimental results show improvements of up to 1.7 points in bleu and 3.4 points in ter compared to the best single system.
this paper describes limsi ? s submissions to the shared translation task. <eos> we report results for french-english and german-english in both directions. <eos> our submissions use n-code, an open source system based on bilingual n-grams. <eos> in this approach, both the translation and target language models are estimated as conventional smoothed n-gram models ; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. <eos> experimental results show a significant and consistent bleu improvement of approximately 1 point for all conditions. <eos> we also report preliminary experiments using an ? on-the-fly ? <eos> translation model.
this paper describes the upm system for the spanish-english translation task at the naacl 2012 workshop on statistical machine translation. <eos> this system is based on moses. <eos> we have used all available free corpora, cleaning and deleting some repetitions. <eos> in this paper, we also propose a technique for selecting the sentences for tuning the system. <eos> this technique is based on the similarity with the sentences to translate. <eos> with our approach, we improve the bleu score from 28.37 % to 28.57 %. <eos> and as a result of the wmt12 challenge we have obtained a 31.80 % bleu with the 2012 test set. <eos> finally, we explain different experiments that we have carried out after the competition.
this paper describes the promt submission for the wmt12 shared translation task. <eos> we participated in two language pairs : englishfrench and english-spanish. <eos> the translations were made using the promt deephybrid engine, which is the first hybrid version of the promt system. <eos> we report on improvements over our baseline rbmt output both in terms of automatic evaluation metrics and linguistic analysis.
this paper describes the phrase-based smt systems developed for our participation in the wmt12 shared translation task. <eos> translations for english ? german and english ? french were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-of-speech ( pos ) and automatic cluster language models and discriminative word lexica. <eos> in addition, we explicitly handle out-of-vocabulary ( oov ) words in german, if we have translations for other morphological forms of the same stem. <eos> furthermore, we extended the pos-based reordering approach to also use information from syntactic trees.
we present an improved version of depfix ( marec ? ek et al, 2011 ), a system for automatic rule-based post-processing of englishto-czech mt outputs designed to increase their fluency. <eos> we enhanced the rule set used by the original depfix system and measured the performance of the individual rules. <eos> we also modified the dependency parser of mcdonald et al ( 2005 ) in two ways to adjust it for the parsing of mt outputs. <eos> we show that our system is able to improve the quality of the state-of-the-art mt systems.
this paper describes the development of french ? english and english ? french statistical machine translation systems for the 2012 wmt shared task evaluation. <eos> we developed phrase-based systems based on the moses decoder, trained on the provided data only. <eos> additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection.
we provide a few insights on data selection for machine translation. <eos> we evaluate the quality of the new czeng 1.0, a parallel data source used in wmt12. <eos> we describe a simple technique for reducing out-of-vocabulary rate after phrase extraction. <eos> we discuss the benefits of tuning towards multiple reference translations for english-czech language pair. <eos> we introduce a novel approach to data selection by full-text indexing and search : we select sentences similar to the test set from a large monolingual corpus and explore several options of incorporating them in a machine translation system. <eos> we show that this method can improve translation quality. <eos> finally, we describe our submitted system cu-tamch-boj.
we describe dfki ? s statistical based submission to the 2012 wmt evaluation. <eos> the submission is based on the freely available machine translation toolkit jane, which supports phrase-based and hierarchical phrase-based translation models. <eos> different setups have been tested and combined using a sentence selection method.
we developed a string-to-tree system for english ? german, achieving competitive results against a hierarchical model baseline. <eos> we provide details of our implementation of ghkm rule extraction and scope-3 parsing in the moses toolkit. <eos> we compare systems trained on the same data using different grammar extraction methods.
we describe our experiments with phrasebased machine translation for the wmt 2012 shared task. <eos> we trained one system for 14 translation directions between english or czech on one side and english, czech, german, spanish or french on the other side. <eos> we describe a set of results with different training data sizes and subsets.
recent work has established the efficacy of amazon ? s mechanical turk for constructing parallel corpora for machine translation research. <eos> we apply this to building a collection of parallel corpora between english and six languages from the indian subcontinent : bengali, hindi, malayalam, tamil, telugu, and urdu. <eos> these languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. <eos> we conduct a variety of baseline experiments and analysis, and release the data to the community.
microblogging services such as twitter have become popular media for real-time usercreated news reporting. <eos> such communication often happens in parallel in different languages, e.g., microblog posts related to the same events of the arab spring were written in arabic and in english. <eos> the goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic twitter translation, namely the lack of bilingual sentence pairs for training smt systems. <eos> we show that translation-based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrasebased smt pipeline. <eos> our method outperforms other approaches to domain adaptation for smt such as language model adaptation, meta-parameter tuning, or self-translation.
in statistical machine translation ( smt ), it is known that performance declines when the training data is in a different domain from the test data. <eos> nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data. <eos> in this paper, we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline ( alignment, phrase extraction, and phrase scoring ). <eos> through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words.
the new frontier of computer assisted translation technology is the effective integration of statistical mt within the translation workflow. <eos> in this respect, the smt ability of incrementally learning from the translations produced by users plays a central role. <eos> a still open problem is the evaluation of smt systems that evolve over time. <eos> in this paper, we propose a new metric for assessing the quality of an adaptive mt component that is derived from the theory of learning curves : the percentage slope.
smt typically models translation at the sentence level, ignoring wider document context. <eos> does this hurt the consistency of translated documents ? <eos> using a phrase-based smt system in various data conditions, we show that smt translates documents remarkably consistently, even without document knowledge. <eos> nevertheless, translation inconsistencies often indicate translation errors. <eos> however, unlike in human translation, these errors are rarely due to terminology inconsistency. <eos> they are more often symptoms of deeper issues with smt models instead.
in statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for chinese or morphological analysis for german. <eos> several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features. <eos> we introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence. <eos> it allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model. <eos> we argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously. <eos> on the newscommentary portion of the german ? english wmt 2011 task we can show moderate improvements of up to 0.6 % bleu over a stateof-the-art baseline system.
training the phrase table by force-aligning ( fa ) the training data with the reference translation has been shown to improve the phrasal translation quality while significantly reducing the phrase table size on medium sized tasks. <eos> we apply this procedure to several large-scale tasks, with the primary goal of reducing model sizes without sacrificing translation quality. <eos> to deal with the noise in the automatically crawled parallel training data, we introduce on-demand word deletions, insertions, and backoffs to achieve over 99 % successful alignment rate. <eos> we also add heuristics to avoid any increase in oov rates. <eos> we are able to reduce already heavily pruned baseline phrase tables by more than 50 % with little to no degradation in quality and occasionally slight improvement, without any increase in oovs. <eos> we further introduce two global scaling factors for re-estimation of the phrase table via posterior phrase alignment probabilities and a modified absolute discounting method that can be applied to fractional counts. <eos> index terms : phrasal machine translation, phrase training, phrase table pruning
minimum error rate training is often the preferred method for optimizing parameters of statistical machine translation systems. <eos> mert minimizes error rate by using a surrogate representation of the search space, such as n best lists or hypergraphs, which only offer an incomplete view of the search space. <eos> in our work, we instead minimize error rate directly by integrating the decoder into the minimizer. <eos> this approach yields two benefits. <eos> first, the function being optimized is the true error rate. <eos> second, it lets us optimize parameters of translations systems other than standard linear model features, such as distortion limit. <eos> since integrating the decoder into the minimizer is often too slow to be practical, we also exploit statistical significance tests to accelerate the search by quickly discarding unpromising models. <eos> experiments with a phrasebased system show that our approach is scalable, and that optimizing the parameters that mert can not handle brings improvements to translation results.
the introduction of large-margin based discriminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process. <eos> by removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features. <eos> however, these methods have not yet met with wide-spread adoption. <eos> this may be partly due to the perceived complexity of implementation, and partly due to the lack of standard methodology for applying these methods to mt. <eos> this papers aims to shed light on large-margin learning for mt, explicitly presenting the simple passive-aggressive algorithm which underlies many previous approaches, with direct application to mt, and empirically comparing several widespread optimization strategies.
the acl anthology network ( aan ) 1 is a comprehensive manually curated networked database of citations and collaborations in the field of computational linguistics. <eos> each citation edge in aan is associated with one or more citing sentences. <eos> a citing sentence is one that appears in a scientific article and contains an explicit reference to another article. <eos> in this paper, we shed the light on the usefulness of aan citing sentences for understanding research trends and summarizing previous discoveries and contributions. <eos> we also propose and motivate several different uses and applications of citing sentences.
we develop a people-centered computational history of science that tracks authors over topics and apply it to the history of computational linguistics. <eos> we present four findings in this paper. <eos> first, we identify the topical subfields authors work on by assigning automatically generated topics to each paper in the acl anthology from 1980 to 2008. <eos> next, we identify four distinct research epochs where the pattern of topical overlaps are stable and different from other eras : an early nlp period from 1980 to 1988, the period of us government-sponsored muc and atis evaluations from 1989 to 1994, a transitory period until 2001, and a modern integration period from 2002 onwards. <eos> third, we analyze the flow of authors across topics to discern how some subfields flow into the next, forming different stages of acl research. <eos> we find that the government-sponsored bakeoffs brought new researchers to the field, and bridged early topics to modern probabilistic approaches. <eos> last, we identify steep increases in author retention during the bakeoff era and the modern era, suggesting two points at which the field became more integrated.
we present a joint probabilistic model of who cites whom in computational linguistics, and also of the words they use to do the citing. <eos> the model reveals latent factions, or groups of individuals whom we expect to collaborate more closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without. <eos> we conduct an exploratory data analysis on the acl anthology. <eos> we extend the model to reveal changes in some authors ? <eos> faction memberships over time.
studies of gender balance in academic computer science are typically based on statistics on enrollment and graduation. <eos> going beyond these coarse measures of gender participation, we conduct a fine-grained study of gender in the field of natural language processing. <eos> we use topic models ( latent dirichlet allocation ) to explore the research topics of men and women in the acl anthology network. <eos> we find that women publish more on dialog, discourse, and sentiment, while men publish more than women in parsing, formal semantics, and finite state models. <eos> to conduct our study we labeled the gender of authors in the acl anthology mostly manually, creating a useful resource for other gender studies. <eos> finally, our study of historical patterns in female participation shows that the proportion of women authors in computational linguistics has been continuously increasing, with approximately a 50 % increase in the three decades since 1980.
the paper reports on a comparative study of two approaches to extracting definitional sentences from a corpus of scholarly discourse : one based on bootstrapping lexico-syntactic patterns and another based on deep analysis. <eos> computational linguistics was used as the target domain and the acl anthology as the corpus. <eos> definitional sentences extracted for a set of well-defined concepts were rated by domain experts. <eos> results show that both methods extract high-quality definition sentences intended for automated glossary construction.
collocation is a well-known linguistic phenomenon which has a long history of research and use. <eos> in this study i employ collocation segmentation to extract terms from the large and complex acl anthology reference corpus, and also briefly research and describe the history of the acl. <eos> the results of the study show that until 1986, the most significant terms were related to formal/rule based methods. <eos> starting in 1987, terms related to statistical methods became more important. <eos> for instance, language model, similarity measure, text classification. <eos> in 1990, the terms penn treebank, mutual information, statistical parsing, bilingual corpus, and dependency tree became the most important, showing that newly released language resources appeared together with many new research areas in computational linguistics. <eos> although penn treebank was a significant term only temporarily in the early nineties, the corpus is still used by researchers today. <eos> the most recent significant terms are bleu score and semantic role labeling. <eos> while machine translation as a term is significant throughout the acl arc corpus, it is not significant for any particular time period. <eos> this shows that some terms can be significant globally while remaining insignificant at a local level.
with rapidly increasing community, a plethora of conferences related to natural language processing and easy access to their proceedings make it essential to check the integrity and novelty of the new submissions. <eos> this study aims to investigate the trends of text reuse in the acl submissions, if any. <eos> we carried a set of analyses on two spans of five years papers ( the past and the present ) of acl using a publicly available text reuse detection application to notice the behaviour. <eos> in our study, we found some strong reuse cases which can be an indicator to establish a clear policy to handle text reuse for the upcoming editions of acl. <eos> the results are anonymised.
the acl anthology was revamped in 2012 to its second major version, encompassing faceted navigation, social media use, as well as author- and reader-generated content and comments on published work as part of the revised frontend user interface. <eos> at the backend, the anthology was updated to incorporate its publication records into a database. <eos> we describe the acl anthology ? s previous legacy, redesign and revamp process and technologies, and its resulting functionality.
the acl 2012 contributed task is a community effort aiming to provide the full acl anthology as a high-quality corpus with rich markup, following the tei p5 guidelines ? <eos> a new resource dubbed the acl anthology corpus ( aac ). <eos> the goal of the task is threefold : ( a ) to provide a shared resource for experimentation on scientific text ; ( b ) to serve as a basis for advanced search over the acl anthology, based on textual content and citations ; and, by combining the aforementioned goals, ( c ) to present a showcase of the benefits of natural language processing to a broader audience. <eos> the contributed task extends the current anthology reference corpus ( arc ) both in size, quality, and by aiming to provide tools that allow the corpus to be automatically extended with new content ? be they scanned or born-digital.
extracting textual content and document structure from pdf presents a surprisingly ( depressingly, to some, in fact ) difficult challenge, owing to the purely display-oriented design of the pdf document standard. <eos> while a variety of lower-level pdf extraction toolkits exist, none fully support the recovery of original text ( in reading order ) and relevant structural elements, even for so-called borndigital pdfs, i.e. <eos> those prepared electronically using typesetting systems like latex, openoffice, and the like. <eos> this short paper summarizes a new tool for high-quality extraction of text and structure from pdfs, combining state-ofthe-art pdf parsing, font interpretation, layout analysis, and tei-compliant output of text and logical document markup. <eos> ?
we describe how paperxml, a logical document structure markup for scholarly articles, is generated on the basis of ocr tool outputs. <eos> paperxml has been initially developed for the acl anthology searchbench. <eos> the main purpose was to robustly provide uniform access to sentences in acl anthology papers from the past 46 years, ranging from scanned, typewriter-written conference and workshop proceedings papers, up to recent high-quality typeset, born-digital journal articles, with varying layouts. <eos> paperxml markup includes information on page and paragraph breaks, section headings, footnotes, tables, captions, boldface and italics character styles as well as bibliographic and publication metadata. <eos> the role of paperxml in the acl contributed task rediscovering 50 years of discoveries is to serve as fall-back source ( 1 ) for older, scanned papers ( mostly published before the year 2000 ), for which born-digital pdf sources are not available, ( 2 ) for borndigital pdf papers on which the pdfextract method failed, ( 3 ) for document parts where pdfextract does not output useful markup such as currently for tables. <eos> we sketch transformation of paperxml into the acl contributed task ? s tei p5 xml.
in this paper we describe our participation in the contributed task at acl special workshop 2012. <eos> we contribute to the goal of enriching the textual content of acl anthology by identifying the citation contexts in a paper and linking them to their corresponding references in the bibliography section. <eos> we use parscit, to process the bibliography of each paper. <eos> pattern matching heuristics are then used to connect the citations with their references. <eos> furthermore, we prepared a small evaluation dataset, to test the efficiency of our method. <eos> we achieved 95 % precision and 80 % recall on this dataset.
several approaches have been proposed for the automatic acquisition of multiword expressions from corpora. <eos> however, there is no agreement about which of them presents the best cost-benefit ratio, as they have been evaluated on distinct datasets and/or languages. <eos> to address this issue, we investigate these techniques analysing the following dimensions : expression type ( compound nouns, phrasal verbs ), language ( english, french ) and corpus size. <eos> results show that these techniques tend to extract similar candidate lists with high recall ( ? <eos> 80 % ) for nominals and high precision ( ? <eos> 70 % ) for verbals. <eos> the use of association measures for candidate filtering is useful but some of them are more onerous and not significantly better than raw counts. <eos> we finish with an evaluation of flexibility and an indication of which technique is recommended for each languagetype-size context.
in my thesis i propose a data-oriented study on how social power relations between participants manifest in the language and structure of online written dialogs. <eos> i propose that there are different types of power relations and they are different in the ways they are expressed and revealed in dialog and across different languages, genres and domains. <eos> so far, i have defined four types of power and annotated them in corporate email threads in english and found support that they in fact manifest differently in the threads. <eos> using dialog and language features, i have built a system to predict participants possessing these types of power within email threads. <eos> i intend to extend this system to other languages, genres and domains and to improve it ? s performance using deeper linguistic analysis.
in sentiment classification, unlabeled user reviews are often free to collect for new products, while sentiment labels are rare. <eos> in this case, active learning is often applied to build a high-quality classifier with as small amount of labeled instances as possible. <eos> however, when the labeled instances are insufficient, the performance of active learning is limited. <eos> in this paper, we aim at enhancing active learning by employing the labeled reviews from a different but related ( source ) domain. <eos> we propose a framework active vector rotation ( avr ), which adaptively utilizes the source domain data in the active learning procedure. <eos> thus, avr gets benefits from source domain when it is helpful, and avoids the negative affects when it is harmful. <eos> extensive experiments on toy data and review texts show our success, compared with other state-of-theart active learning approaches, as well as approaches with domain adaptation.
this paper describes a query classification system for a specialized domain. <eos> we take as a case study queries asked to a search engine of an art, cultural and history library and classify them against the library cataloguing categories. <eos> we show how click-through links, i.e., the links that a user clicks after submitting a query, can be exploited for extracting information useful to enrich the query as well as for creating the training set for a machine learning based classifier. <eos> moreover, we show how topic model can be exploited to further enrich the query with hidden topics induced from the library meta-data. <eos> the experimental evaluations show that this system considerably outperforms a matching and ranking classification approach, where queries ( and categories ) were also enriched with similar information.
ensembles combine knowledge from distinct machine learning approaches into a general flexible system. <eos> while supervised ensembles frequently show great benefit, unsupervised ensembles prove to be more challenging. <eos> we propose evaluating various unsupervised ensembles when applied to the unsupervised task of word sense induction with a framework for combining diverse feature spaces and clustering algorithms. <eos> we evaluate our system using standard shared tasks and also introduce new automated semantic evaluations and supervised baselines, both of which highlight the current limitations of existing word sense induction evaluations.
this work presents a text segmentation algorithm called topictiling. <eos> this algorithm is based on the well-known texttiling algorithm, and segments documents using the latent dirichlet allocation ( lda ) topic model. <eos> we show that using the mode topic id assigned during the inference method of lda, used to annotate unseen documents, improves performance by stabilizing the obtained topics. <eos> we show significant improvements over state of the art segmentation algorithms on two standard datasets. <eos> as an additional benefit, topictiling performs the segmentation in linear time and thus is computationally less expensive than other lda-based segmentation methods.
this paper presents recent work on a new method to automatically extract finegrained duration information for common verbs using a large corpus of twitter tweets. <eos> regular expressions were used to extract verbs and durations from each tweet in a corpus of more than 14 million tweets with 90.38 % precision covering 486 verb lemmas. <eos> descriptive statistics for each verb lemma were found as well as the most typical fine-grained duration measure. <eos> mean durations were compared with previous work by gusev et al ( 2011 ) and it was found that there is a small positive correlation.
the current debate regarding the data structure necessary to represent discourse structure, specifically whether tree-structure is sufficient to represent discourse structure or not, is mainly focused on written text. <eos> this paper reviews some of the major claims about the structure in discourse and proposes an investigation of discourse structure for simultaneous spoken turkish by focusing on tree-violations and exploring ways to explain them away by non-structural means.
this paper presents an open and flexible methodological framework for the automatic acquisition of multiword expressions ( mwes ) from monolingual textual corpora. <eos> this research is motivated by the importance of mwes for nlp applications. <eos> after briefly presenting the modules of the framework, the paper reports extrinsic evaluation results considering two applications : computer-aided lexicography and statistical machine translation. <eos> both applications can benefit from automatic mwe acquisition and the expressions acquired automatically from corpora can both speed up and improve their quality. <eos> the promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate mwe treatment into these and many other applications.
automatically constructing knowledge bases from online resources has become a crucial task in many research areas. <eos> most existing knowledge bases are built from english resources, while few efforts have been made for other languages. <eos> building knowledge bases for chinese is of great importance on its own right. <eos> however, simply adapting existing tools from english to chinese yields inferior results.in this paper, we propose to create chinese knowledge bases from online resources with less human involvement.this project will be formulated in a self-supervised framework which requires little manual work to extract knowledge facts from online encyclopedia resources in a probabilistic view.in addition, this framework will be able to update the constructed knowledge base with knowledge facts extracted from up-to-date newswire.currently, we have obtained encouraging results in our pilot experiments that extracting knowledge facts from infoboxes can achieve a high accuracy of around 95 %, which will be then used as training data for the extraction of plain webpages.
this paper investigates the impact on french dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis. <eos> a distributional thesaurus is created from a large text corpus and used for distributional clustering and wordnet automatic sense ranking. <eos> the standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class. <eos> we use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes : lemmas, clusters, or synsets. <eos> probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features. <eos> we obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the french treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.
in the last decade, substantial progress has been made in the induction of semantic relations from raw text, especially of hypernymy and meronymy in the english language and in the classification of noun-noun relations in compounds or other contexts. <eos> we investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for german, by first introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classification results using a general framework for supervised classification of lexical relations.
this paper introduces a novel unsupervised approach to semantic role induction that uses a generative bayesian model. <eos> to the best of our knowledge, it is the first model that jointly clusters syntactic verbs arguments into semantic roles, and also creates verbs classes according to the syntactic frames accepted by the verbs. <eos> the model is evaluated on french and english, outperforming, in both cases, a strong baseline. <eos> on english, it achieves results comparable to state-of-the-art unsupervised approaches to semantic role induction.
t of linguisticsversley @ sfs.uni-tuebingen.de verena henrichuniversity of tu ? bingendepartment of linguisticsverena.henrich @ uni-tuebingen.deabstract in many morphologically rich languages, conceptually independent morphemes are glued together to form a new word ( a compound ) with a meaning that is often at least in part predictable from the meanings of the contributing morphemes. <eos> assuming that most compounds express a subconcept of exactly one sense of its nominal head, we use compounds as a higher-quality alternative to simply using general second-order collocate terms in the task of word sense discrimination. <eos> we evaluate our approach using lexical entries from the german wordnet germanet ( henrich and hinrichs, 2010 ).
the paper presents a novel approach to extracting dependency information in morphologically rich languages using co-occurrence statistics based not only on lexical forms ( as in previously described collocation-based methods ), but also on morphosyntactic and wordnet-derived semantic properties of words. <eos> statistics generated from a corpus annotated only at the morphosyntactic level are used as features in a machine learning classifier which is able to detect which heads of groups found by a shallow parser are likely to be connected by an edge in the complete parse tree. <eos> the approach reaches the precision of 89 % and the recall of 65 %, with an extra 6 % recall, if only words present in the wordnet are considered.
although parsing performances have greatly improved in the last years, grammar inference from treebanks for morphologically rich languages, especially from small treebanks, is still a challenging task. <eos> in this paper we investigate how state-of-the-art parsing performances can be achieved on spanish, a language with a rich verbal morphology, with a non-lexicalized parser trained on a treebank containing only around 2,800 trees. <eos> we rely on accurate part-of-speech tagging and datadriven lemmatization to provide parsing models able to cope lexical data sparseness. <eos> providing state-of-the-art results on spanish, our methodology is applicable to other languages with high level of inflection.
deep linguistic grammars are able to provide rich and highly complex grammatical representations of sentences, capturing, for instance, long-distance dependencies and returning a semantic representation. <eos> these grammars lack robustness in the sense that they do not gracefully handle words missing from their lexicon. <eos> several approaches have been explored to handle this problem, many of which consist in pre-annotating the input to the grammar with shallow processing machine-learning tools. <eos> most of these tools, however, use features based on a fixed window of context, such as n-grams. <eos> we investigate whether the use of features that encode discrete structures, namely grammatical dependencies, can improve the performance of a machine learning classifier that assigns deep lexical types. <eos> in this paper we report on the design and evaluation of this classifier.
dependency parsing has been shown to improve nlp systems in certain languages and in many cases helps achieve state of the art results in nlp applications, in particular applications for free word order languages. <eos> morphologically rich languages are often short on training data or require much higher amounts of training data due to the increased size of their lexicon. <eos> this paper examines a new approach for addressing morphologically rich languages with little training data to start. <eos> using tamil as our test language, we create 9 dependency parse models with a limited amount of training data. <eos> using these models we train an svm classifier using only the model agreements as features. <eos> we use this svm classifier on an edge by edge decision to form an ensemble parse tree. <eos> using only model agreements as features allows this method to remain language independent and applicable to a wide range of morphologically rich languages. <eos> we show a statistically significant 5.44 % improvement over the average dependency model and a statistically significant 0.52 % improvement over the best individual system.
korean is a morphologically rich language in which grammatical functions are marked by inflections and affixes, and they can indicate grammatical relations such as subject, object, predicate, etc. <eos> a korean sentence could be thought as a sequence of eojeols. <eos> an eojeol is a word or its variant word form agglutinated with grammatical affixes, and eojeols are separated by white space as in english written texts. <eos> korean treebanks ( choi et al, 1994 ; han et al, 2002 ; korean language institute, 2012 ) use eojeol as their fundamental unit of analysis, thus representing an eojeol as a prepreterminal phrase inside the constituent tree. <eos> this eojeol-based annotating schema introduces various complexity to train the parser, for example an entity represented by a sequence of nouns will be annotated as two or more different noun phrases, depending on the number of spaces used. <eos> in this paper, we propose methods to transform eojeol-based korean treebanks into entity-based korean treebanks. <eos> the methods are applied to sejong treebank, which is the largest constituent treebank in korean, and the transformed treebank is used to train and test various probabilistic cfg parsers. <eos> the experimental result shows that the proposed transformation methods reduce ambiguity in the training corpus, increasing the overall f1 score up to about 9 %.
we present an architecture for parsing in two steps. <eos> a phrase-structure parser builds for each sentence an n-best list of analyses which are converted to dependency trees. <eos> these dependency structures are then rescored by a discriminative reranker. <eos> our method is language agnostic and enables the incorporation of additional information which are useful for the choice of the best parse candidate. <eos> we test our approach on the the penn treebank and the french treebank. <eos> evaluation shows a significative improvement on different parse metrics.
this contribution focuses on multimodal interaction techniques for a mobile communication and assistance system on a robot platform. <eos> the system comprises of acoustic, visual and haptic input modalities. <eos> feedback is given to the user by a graphical user interface and a speech synthesis system. <eos> by this, multimodal and natural communication with the robot system is possible.
this paper discusses the significance of the multimodal interaction in virtual environments ( ve ) and the criticalities involved in integration and coordination between modes during interaction. <eos> also, we present an architecture and design of the integration mechanism with respect to information access in second language learning. <eos> in this connection, we have conducted an experiential study on speech inputs to understand how far users ? <eos> experience of information can be considered to be supportive to this architecture.
the vasa project develops a multimodal assistive system mediated by a virtual agent that is intended to foster autonomy of communication and activity management in older people and people with disabilities. <eos> assistive systems intended for these user groups have to take their individual vulnerabilities into account. <eos> a variety of psychic, emotional as well as behavioral conditions can manifest at the same time. <eos> systems that fail to take them into account might not only fail at joint tasks, but also risk damage to their interlocutors. <eos> we identify important conditions and disorders and analyze their immediate consequences for the design of careful assistive systems.
due to the demographic changes, support by means of assistive systems will become inevitable for home care and in nursing homes. <eos> robot systems are promising solutions but their value has to be acknowledged by the patients and the care personnel. <eos> natural and intuitive human-machine interfaces are an essential feature to achieve acceptance of the users. <eos> therefore, automatic speech recognition ( asr ) is a promising modality for such assistive devices. <eos> however, noises produced during movement of robots can degrade the asr performances. <eos> this work focuses on noise reduction by a non-negative matrix factorization ( nmf ) approach to efficiently suppress non stationary noise produced by the sensors of an assisting robot system.
this paper introduces research within the aladin project, which aims to develop an assistive vocal interface for people with a physical impairment. <eos> in contrast to existing approaches, the vocal interface is self-learning, which means it can be used with any language, dialect, vocabulary and grammar. <eos> this paper describes the overall learning framework, and the two components that will provide vocabulary learning and grammar induction. <eos> in addition, the paper describes encouraging results of early implementations of these vocabulary and grammar learning components, applied to recorded sessions of a vocally guided card game, patience.
different bengali tts systems are already available on a resourceful platform such as a personal computer. <eos> however, porting these systems to a resource limited device such as a mobile phone is not an easy task. <eos> practical aspects including application size and processing time have to be concerned. <eos> this paper describes the implementation of a bengali speech synthesizer on a mobile device. <eos> for speech generation we used epoch synchronous non overlap add ( esnola ) based concatenative speech synthesis technique which uses the partnemes as the smallest signal units for concatenations.
this paper presents a novel approach in sentiment polarity detection on twitter posts, by extracting a vector of weighted nodes from the graph of wordnet. <eos> these weights are used on sentiwordnet to compute a final estimation of the polarity. <eos> therefore, the method proposes a non-supervised solution that is domain-independent. <eos> the evaluation over a generated corpus of tweets shows that this technique is promising.
twitter is a micro blogging website, where users can post messages in very short text called tweets. <eos> tweets contain user opinion and sentiment towards an object or person. <eos> this sentiment information is very useful in various aspects for business and governments. <eos> in this paper, we present a method which performs the task of tweet sentiment identification using a corpus of pre-annotated tweets. <eos> we present a sentiment scoring function which uses prior information to classify ( binary classification ) and weight various sentiment bearing words/phrases in tweets. <eos> using this scoring function we achieve classification accuracy of 87 % on stanford dataset and 88 % on mejaj dataset. <eos> using supervised machine learning approach, we achieve classification accuracy of 88 % on stanford dataset.
in this work, we present samar, a system for subjectivity and sentiment analysis ( ssa ) for arabic social media genres. <eos> we investigate : how to best represent lexical information ; whether standard features are useful ; how to treat arabic dialects ; and, whether genre specific features have a measurable impact on performance. <eos> our results suggest that we need individualized solutions for each domain and task, but that lemmatization is a feature in all the best approaches.
the classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach. <eos> we propose an approach based on the order of the words without using any syntactic and semantic information. <eos> it consists of building one probabilistic model for the positive and another one for the negative opinions. <eos> then the test opinions are compared to both models and a decision and confidence measure are calculated. <eos> in order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards. <eos> we present an accuracy above 81 % for spanish opinions in the financial products domain.
in the nlp field, there have been a lot of works which focus on the reviewer ? s point of view conducted on sentiment analyses, which ranges from trying to estimate the reviewer ? s score. <eos> however the reviews are used by the readers. <eos> the reviews that give a big influence to the readers should have the highest value, rather than the reviews to which was assigned the highest score by the writer. <eos> in this paper, we conducted the analyses using the reader ? s point of view. <eos> we asked 20 subjects to read 500 sentences in the reviews of rakuten travel and extracted the sentences that gave a big influence to the subjects. <eos> we analyze the influential sentences from the following two points of view, 1 ) targets and evaluations and 2 ) personal tastes. <eos> we found that ? room ?, ? service ?, ? meal ? <eos> and ? scenery ? <eos> are important targets which are items included in the reviews, and that ? features ? <eos> and ? human senses ? <eos> are important evaluations which express sentiment or explain targets. <eos> also we showed personal tastes appeared on ? meal ? <eos> and ? service ?.
the past years have shown a steady growth in interest in the natural language processing task of sentiment analysis. <eos> the research community in this field has actively proposed and improved methods to detect and classify the opinions and sentiments expressed in different types of text - from traditional press articles, to blogs, reviews, fora or tweets. <eos> a less explored aspect has remained, however, the issue of dealing with sentiment expressed in texts in languages other than english. <eos> to this aim, the present article deals with the problem of sentiment detection in three different languages - french, german and spanish - using three distinct machine translation ( mt ) systems - bing, google and moses. <eos> our extensive evaluation scenarios show that smt systems are mature enough to be reliably employed to obtain training data for languages other than english and that sentiment analysis systems can obtain comparable performances to the one obtained for english.
online debate forums provide a powerful communication platform for individual users to share information, exchange ideas and express opinions on a variety of topics. <eos> understanding people ? s opinions in such forums is an important task as its results can be used in many ways. <eos> it is, however, a challenging task because of the informal language use and the dynamic nature of online conversations. <eos> in this paper, we propose a new method for identifying participants ? <eos> agreement or disagreement on an issue by exploiting information contained in each of the posts. <eos> our proposed method first regards each post in its local context, then aggregates posts to estimate a participant ? s overall position. <eos> we have explored the use of sentiment, emotional and durational features to improve the accuracy of automatic agreement and disagreement classification. <eos> our experimental results have shown that aggregating local positions over posts yields better performance than nonaggregation baselines when identifying users ? <eos> global positions on an issue.
a set of words labelled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. <eos> no simple function of the labels on the individual words may capture the overall emotion of the sentence ; words are interrelated and they mutually influence their affectrelated interpretation. <eos> we present a method which enables us to take the contextual emotion of a word and the syntactic structure of the sentence into account to classify sentences by emotion classes. <eos> we show that this promising method outperforms both a method based on a bag-of-words representation and a system based only on the prior emotions of words. <eos> the goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important for this task.
current approaches to sentiment analysis assume that the sole discourse function of sentiment-bearing texts is expressivity. <eos> however, the persuasive discourse function also utilises expressive language. <eos> in this work, we present the results of training supervised classifiers on a new corpus of clinical texts that contain documents with an expressive discourse function, and we test the learned models on a subset of the same corpus containing persuasive texts. <eos> the results of this indicate that despite the difference in discourse function, the learned models perform favourably.
this paper presents a corpus targeting evaluative meaning as it pertains to descriptions of events. <eos> the corpus, political-ads is drawn from 141 television ads from the 2008 u.s. presidential race and contains 3945 nps and 1549 vps annotated for scalar sentiment from three different perspectives : the narrator, the annotator, and general society. <eos> we show that annotators can distinguish these perspectives reliably and that correlation between the annotator ? s own perspective and that of a generic individual is higher than those with the narrator. <eos> finally, as a sample application, we demonstrate that a simple compositional model built off of lexical resources outperforms a lexical baseline.
this paper presents our research on automatic annotation of a five-billion-word corpus of japanese blogs with information on affect and sentiment. <eos> we first perform a study in emotion blog corpora to discover that there has been no large scale emotion corpus available for the japanese language. <eos> we choose the largest blog corpus for the language and annotate it with the use of two systems for affect analysis : ml-ask for word- and sentence-level affect analysis and cao for detailed analysis of emoticons. <eos> the annotated information includes affective features like sentence subjectivity ( emotive/non-emotive ) or emotion classes ( joy, sadness, etc. <eos> ), useful in affect analysis. <eos> the annotations are also generalized on a 2-dimensional model of affect to obtain information on sentence valence/polarity ( positive/negative ) useful in sentiment analysis. <eos> the annotations are evaluated in several ways. <eos> firstly, on a test set of a thousand sentences extracted randomly and evaluated by over forty respondents. <eos> secondly, the statistics of annotations are compared to other existing emotion blog corpora. <eos> finally, the corpus is applied in several tasks, such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions.
evaluation often denotes a key issue in semantics- or subjectivity-related tasks. <eos> here we discuss the difficulties of evaluating opinionated keyphrase extraction. <eos> we present our method to reduce the subjectivity of the task and to alleviate the evaluation process and we also compare the results of human and machine-based evaluation.
current work on sentiment analysis is characterized by approaches with a pragmatic focus, which use shallow techniques in the interest of robustness but often rely on ad-hoc creation of data sets and methods. <eos> we argue that progress towards deep analysis depends on a ) enriching shallow representations with linguistically motivated, rich information, and b ) focussing different branches of research and combining ressources to create synergies with related work in nlp. <eos> in the paper, we propose sentiframenet, an extension to framenet, as a novel representation for sentiment analysis that is tailored to these aims.
according to previous work on pedophile psychology and cyberpedophilia, sentiments and emotions in texts could be a good clue to detect online sexual predation. <eos> in this paper, we have suggested a list of high-level features, including sentiment and emotion based ones, for detection of online sexual predation. <eos> in particular, since pedophiles are known to be emotionally unstable, we were interested in investigating if emotion-based features could help in their detection. <eos> we have used a corpus of predators ? <eos> chats with pseudo-victims downloaded from www.perverted-justice.com and two negative datasets of different nature : cybersex logs available online and the nps chat corpus. <eos> naive bayes classification based on the proposed features achieves accuracies of up to 94 % while baseline systems of word and character n-grams can only reach up to 72 %.
we explore filled pause usage in spontaneous medical narration. <eos> expert physicians viewed images of dermatological conditions and provided a description while working toward a diagnosis. <eos> the narratives were analyzed for differences in filled pauses used by attending ( experienced ) and resident ( in-training ) physicians and by male and female physicians. <eos> attending physicians described more and used more filled pauses than residents. <eos> no difference was found by speaker gender. <eos> acoustic speech features were examined for two types of filled pauses : nasal ( e.g. <eos> um ) and non-nasal ( e.g. <eos> uh ). <eos> nasal filled pauses were more often followed by longer silent pauses. <eos> scores capturing diagnostic correctness and diagnostic thoroughness for each narrative were compared against filled pauses. <eos> the number of filled and silent pauses trends upward as correctness scores increase, indicating a tentative relationship between filled pause usage and expertise. <eos> also, we report on a computational model for predicting types of filled pause.
in this paper, we propose to study the effects of negation and modality on opinion expressions. <eos> based on linguistic experiments informed by native speakers, we distill these effects according to the type of modality and negation. <eos> we show that each type has a specific effect on the opinion expression in its scope : both on the polarity and the strength for negation, and on the strength and/or the degree of certainty for modality. <eos> the empirical results reported in this paper provide a basis for future opinion analysis systems that have to compute the sentiment orientation at the sentence or at the clause level. <eos> the methodology we used for deriving this basis was applied for french but it can be easily instantiated for other languages like english.
in the medical domain, misdiagnoses and diagnostic uncertainty put lives at risk and incur substantial financial costs. <eos> clearly, medical reasoning and decision-making need to be better understood. <eos> we explore a possible link between linguistic expression and diagnostic correctness. <eos> we report on an unusual data set of spoken diagnostic narratives used to computationally model and predict diagnostic correctness based on automatically extracted and linguistically motivated features that capture physicians ? <eos> uncertainty. <eos> a multimodal data set was collected as dermatologists viewed images of skin conditions and explained their diagnostic process and observations aloud. <eos> we discuss experimentation and analysis in initial and secondary pilot studies. <eos> in both cases, we experimented with computational modeling using features from the acoustic-prosodic and lexical-structural linguistic modalities.
this paper describes a system for discriminating between factual and non-factual contexts, trained on weakly labeled data by taking advantage of information implicit in annotations of negated events. <eos> in addition to evaluating factuality detection in isolation, we also evaluate its impact on a system for event detection. <eos> the two components for factuality detection and event detection form part of a system for identifying negative factual events, or counterfacts, with top-ranked results in the *sem 2012 shared task.
in this paper we present an iterative methodology to improve classifier performance by incorporating linguistic knowledge, and propose a way to incorporate domain rules into the learning process. <eos> we applied the methodology to the tasks of hedge cue recognition and scope detection and obtained competitive results on a publicly available corpus.
we study two approaches to the marking of extra-propositional aspects of statements in text : the task-independent cue-and-scope representation considered in the conll-2010 shared task, and the tagged-event representation applied in several recent event extraction tasks. <eos> building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. <eos> we demonstrate the feasibility of our approach by constructing a method that uses cue-and-scope analyses together with a small set of features motivated by data analysis to predict event negation and speculation. <eos> evaluation on bionlp shared task 2011 data indicates the method to outperform the negation/speculation components of state-of-theart event extraction systems. <eos> the system and resources introduced in this work are publicly available for research purposes at : https : //github.com/ninjin/eepura
we explore training an automatic modality tagger. <eos> modality is the attitude that a speaker might have toward an event or state. <eos> one of the main hurdles for training a linguistic tagger is gathering training data. <eos> this is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. <eos> we investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to mechanical turk annotators for further annotation. <eos> we used the resulting set of training data to train a precise modality tagger using a multi-class svm that delivers good performance.
blanco & moldovan ( blanco and moldovan, 2011 ) have empirically demonstrated that negated sentences often convey implicit positive inferences, or focus, and that these inferences are both human annotatable and machine learnable. <eos> concentrating on their annotation process, this paper argues that the focusbased implicit positivity should be separated from concepts of scalar implicature and negraising, as well as the placement of stress. <eos> we show that a model making these distinctions clear and which incorporates the pragmatic notion of question under discussion yields ? <eos> rates above.80, but that it substantially deflates the rates of focus of negation in text.
understanding the ways in which participants in public discussions frame their arguments is important in understanding how public opinion is formed. <eos> in this paper, we adopt the position that it is time for more computationallyoriented research on problems involving framing. <eos> in the interests of furthering that goal, we propose the following specific, interesting and, we believe, relatively accessible question : in the controversy regarding the use of genetically-modified organisms ( gmos ) in agriculture, do pro- and anti-gmo articles differ in whether they choose to adopt a more ? scientific ? <eos> tone ? <eos> prior work on the rhetoric and sociology of science suggests that hedging may distinguish popular-science text from text written by professional scientists for their colleagues. <eos> we propose a detailed approach to studying whether hedge detection can be used to understanding scientific framing in the gmo debates, and provide corpora to facilitate this study. <eos> some of our preliminary analyses suggest that hedges occur less frequently in scientific discourse than in popular text, a finding that contradicts prior assertions in the literature. <eos> we hope that our initial work and data will encourage others to pursue this promising line of inquiry.
in this paper we investigate two distinct tasks. <eos> the first task involves detecting arguing subjectivity, a type of linguistic subjectivity on which relatively little work has yet to be done. <eos> the second task involves labeling instances of arguing subjectivity with argument tags reflecting the conceptual argument being made. <eos> we refer to these two tasks collectively as ? recognizing arguments ?. <eos> we develop a new annotation scheme and assemble a new annotated corpus to support our learning efforts. <eos> through our machine learning experiments, we investigate the utility of a sentiment lexicon, discourse parser, and semantic similarity measures with respect to recognizing arguments. <eos> by incorporating information gained from these resources, we outperform a unigram baseline by a significant margin. <eos> in addition, we explore a two-phase approach to recognizing arguments, with promising results.
the current paper presents a languageindependent methodology, which facilitates the creation of machine translation ( mt ) systems for various language pairs. <eos> this methodology is implemented in the presemt hybrid mt system. <eos> presemt has the lowest possible requirements on specialised resources and tools, given that for many languages ( especially less widely used ones ) only limited linguistic resources are available. <eos> in presemt, the main translation process comprises two phases. <eos> the first one, structure selection, determines the overall structure of a target language ( tl ) sentence, drawing on syntactic information from a small bilingual corpus. <eos> the second phase, translation equivalent selection, relies on models extracted solely from monolingual corpora to implement translation disambiguation, determine intra-phrase word order and handle functional words. <eos> this paper proposes extracting information for disambiguation from the monolingual corpus. <eos> experimental results indicate that such information substantially contributes in improving translation quality.
recognition of named entities ( nes ) is a difficult process in indian languages like hindi, telugu, etc., where sufficient gazetteers and annotated corpora are not available compared to english language. <eos> this paper details a novel clustering and co-occurrence based approach to map english nes with their equivalent representations from different languages recognized in a language-independent way. <eos> we have substituted the required language specific resources by the richly structured multilingual content of wikipedia. <eos> the approach includes clustering of highly similar wikipedia articles. <eos> then the nes in an english article are mapped with other language terms in interlinked articles based on co-occurrence frequencies. <eos> the cluster information and the term co-occurrences are considered in extracting the nes from non-english languages. <eos> hence, the english wikipedia is used to bootstrap the nes for other languages. <eos> through this approach, we have availed the structured, semi-structured and multilingual content of the wikipedia to a massive extent. <eos> experimental results suggest that the proposed approach yields promising results in rates of precision and recall.
morph length is one of the indicative feature that helps learning the morphology of languages, in particular agglutinative languages. <eos> in this paper, we introduce a simple unsupervised model for morphological segmentation and study how the knowledge of morph length affect the performance of the segmentation task under the bayesian framework. <eos> the model is based on ( goldwater et al., 2006 ) unigram word segmentation model and assumes a simple prior distribution over morph length. <eos> we experiment this model on two highly related and agglutinative languages namely tamil and telugu, and compare our results with the state of the art morfessor system. <eos> we show that, knowledge of morph length has a positive impact and provides competitive results in terms of overall performance.
in this paper we present a methodology for building comparable corpus, using multilingual ontologies of a scpecific domain. <eos> this resource can be exploited to foster research on multilingual corpus-based ontology learning, population and matching. <eos> the building resource process is exemplified by the construction of annotated comparable corpora in english, portuguese, and french. <eos> the corpora, from the conference organization domain, are built using the multilingual ontology concept labels as seeds for crawling relevant documents from the web through a search engine. <eos> using ontologies allows a better coverage of the domain. <eos> the main goal of this paper is to describe the design methodology followed by the creation of the corpora. <eos> we present a preliminary evaluation and discuss their characteristics and potential applications.
in this paper, we propose a novel human computation game for sentiment analysis. <eos> our game aims at annotating sentiments of a collection of text documents and simultaneously constructing a highly discriminative lexicon of positive and negative phrases. <eos> human computation games have been widely used in recent years to acquire human knowledge and use it to solve problems which are infeasible to solve by machine intelligence. <eos> we package the problems of lexicon construction and sentiment detection as a single human computation game. <eos> we compare the results obtained by the game with that of other well-known sentiment detection approaches. <eos> obtained results are promising and show improvements over traditional approaches.
this paper presents a game with a purpose for the construction of a portuguese lexicalsemantic network. <eos> the network creation is implicit, as players collaboratively create links between words while they have fun. <eos> we describe the principles and implementation of the platform. <eos> as this is an ongoing project, we discuss challenges and long-term goals.we present the current network in terms a quantitative and qualitative analysis, comparing it to other resources. <eos> finally, we describe our target applications.
in this paper, we propose the collaborative construction of language resources ( translation memories ) using a novel browser extension-based client-server architecture that allows translation ( or ? localisation ? ) <eos> of web content capturing and aligning source and target content produced by the ? power of the crowd ?. <eos> the architectural approach chosen enables collaborative, in-context, and realtime localisation of web content supported by the crowd and high-quality language resources. <eos> to the best of our knowledge, this is the only practical web content localisation methodology currently being proposed that incorporates the collaborative construction and use of tms. <eos> the approach also supports the building of resources such as parallel corpora ? <eos> resources that are still not available for many, and especially not for underserved languages.
taxonomies, such as library of congress subject headings and open directory project, are widely used to support browsing-style information access in document collections. <eos> we call them browsing taxonomies. <eos> most existing browsing taxonomies are manually constructed thus they could not easily adapt to arbitrary document collections. <eos> in this paper, we investigate both automatic and interactive techniques to derive taxonomies from scratch for arbitrary document collections. <eos> particular, we focus on encoding user feedback in taxonomy construction process to handle task-specification rising from a given document collection. <eos> we also addresses the problem of path inconsistency due to local relation recognition in existing taxonomy construction algorithms. <eos> the user studies strongly suggest that the proposed approach successfully resolve task specification and path inconsistency in taxonomy construction.
key to named entity recognition, the manual gazetteering of entity lists is a costly, errorprone process that often yields results that are incomplete and suffer from sampling bias. <eos> exploiting current sources of structured information, we propose a novel method for extending minimal seed lists into complete gazetteers. <eos> like previous approaches, we value wikipedia as a huge, well-curated, and relatively unbiased source of entities. <eos> however, in contrast to previous work, we exploit not only its content, but also its structure, as exposed in dbpedia. <eos> we extend gazetteers through wikipedia categories, carefully limiting the impact of noisy categorizations. <eos> the resulting gazetteers easily outperform previous approaches on named entity recognition.
recent work on textual entailment has shown a crucial role of knowledge to support entailment inferences. <eos> however, it has also been demonstrated that currently available entailment rules are still far from being optimal. <eos> we propose a methodology for the automatic acquisition of large scale context-rich entailment rules from wikipedia revisions, taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable. <eos> we report on rule acquisition experiments on wikipedia, showing that it enables the creation of an innovative ( i.e. <eos> acquired rules are not present in other available resources ) and good quality rule repository.
relational clustering has received much attention from researchers in the last decade. <eos> in this paper we present a parametric method that employs a combination of both hard and soft clustering. <eos> based on the corresponding markov chain of an affinity matrix, we simulate a probability distribution on the states by defining a conditional probability for each subpopulation of states. <eos> this probabilistic model would enable us to use expectation maximization for parameter estimation. <eos> the effectiveness of the proposed approach is demonstrated on several real datasets against spectral clustering methods.
most of the research on social networks has almost exclusively focused on positive links between entities. <eos> there are much more insights that we may gain by generalizing social networks to the signed case where both positive and negative edges are considered. <eos> one of the reasons why signed social networks have received less attention that networks based on positive links only is the lack of an explicit notion of negative relations in most social network applications. <eos> however, most such applications have text embedded in the social network. <eos> applying linguistic analysis techniques to this text enables us to identify both positive and negative interactions. <eos> in this work, we propose a new method to automatically construct a signed social network from text. <eos> the resulting networks have a polarity associated with every edge. <eos> edge polarity is a means for indicating a positive or negative affinity between two individuals. <eos> we apply the proposed method to a larger amount of online discussion posts. <eos> experiments show that the proposed method is capable of constructing networks from text with high accuracy. <eos> we also connect out analysis to social psychology theories of signed network, namely the structural balance theory.
twitter, a popular social networking service, enables its users to not only send messages but re-broadcast or retweet a message from another twitter user to their own followers. <eos> considering the number of times that a message is retweeted across twitter is a straightforward way to estimate how interesting it is. <eos> however, a considerable number of messages in twitter with high retweet counts are actually mundane posts by celebrities that are of interest to themselves and possibly their followers. <eos> in this paper, we leverage retweets as implicit relationships between twitter users and messages and address the problem of automatically finding messages in twitter that may be of potential interest to a wide audience by using link analysis methods that look at more than just the sheer number of retweets. <eos> experimental results on real world data demonstrate that the proposed method can achieve better performance than several baseline methods.
we learn graph-based similarity measures for the task of extracting word synonyms from a corpus of parsed text. <eos> a constrained graph walk variant that has been successfully applied in the past in similar settings is shown to outperform a state-of-the-art syntactic vectorbased approach on this task. <eos> further, we show that learning specialized similarity measures for different word types is advantageous.
this paper presents a graph-based method for all-word word sense disambiguation of biomedical texts using semantic relatedness as edge weight. <eos> semantic relatedness is derived from a term-topic co-occurrence matrix. <eos> the sense inventory is generated by the metamap program. <eos> word sense disambiguation is performed on a disambiguation graph via a vertex centrality measure. <eos> the proposed method achieves competitive performance on a benchmark dataset.
in this paper we present the sdoirmi text graph-based semi-supervised algorithm for the task for relation mention identification when the underlying concept mentions have already been identified and linked to an ontology. <eos> to overcome the lack of annotated data, we propose a labelling heuristic based on information extracted from the ontology. <eos> we evaluated the algorithm on the kdd09cma1 dataset using a leave-one-document-out framework and demonstrated an increase in f1 in performance over a co-occurrence based alltrue baseline algorithm. <eos> an extrinsic evaluation of the predictions suggests a worthwhile precision on the more confidently predicted additions to the ontology.
to be able to answer the question what causes tumors to shrink ?, one would require a large cause-effect relation repository. <eos> many efforts have been payed on is-a and part-of relation leaning, however few have focused on cause-effect learning. <eos> this paper describes an automated bootstrapping procedure which can learn and produce with minimal effort a causeeffect term repository. <eos> to filter out the erroneously extracted information, we incorporate graph-based methods. <eos> to evaluate the performance of the acquired causeeffect terms, we conduct three evaluations : ( 1 ) human-based, ( 2 ) comparison with existing knowledge bases and ( 3 ) application driven ( semeval-1 task 4 ) in which the goal is to identify the relation between pairs of nominals. <eos> the results show that the extractions at rank 1500 are 89 % accurate, they comprise 61 % from the terms used in the semeval-1 task 4 dataset and can be used in the future to produce additional training examples for the same task.
social tagging systems, which allow users to freely annotate online resources with tags, become popular in the web 2.0 era. <eos> in order to ease the annotation process, research on social tag recommendation has drawn much attention in recent years. <eos> modeling the social tagging behavior could better reflect the nature of this issue and improve the result of recommendation. <eos> in this paper, we proposed a novel approach for bringing the associative ability to model the social tagging behavior and then to enhance the performance of automatic tag recommendation. <eos> to simulate human tagging process, our approach ranks the candidate tags on a weighted digraph built by the semantic relationships among meaningful words in the summary and the corresponding tags for a given resource. <eos> the semantic relationships are learnt via a word alignment model in statistical machine translation on large datasets. <eos> experiments on real world datasets demonstrate that our method is effective, robust and language-independent compared with the stateof-the-art methods.
we integrate semantic information at two stages of the translation process of a state-ofthe-art smt system. <eos> a word sense disambiguation ( wsd ) classifier produces a probability distribution over the translation candidates of source words which is exploited in two ways. <eos> first, the probabilities serve to rerank a list of n-best translations produced by the system. <eos> second, the wsd predictions are used to build a supplementary language model for each sentence, aimed to favor translations that seem more adequate in this specific sentential context. <eos> both approaches lead to significant improvements in translation performance, highlighting the usefulness of source side disambiguation for smt.
in this paper, we present our linguisticallyenriched bulgarian-to-english statistical machine translation model, which takes a statistical machine translation ( smt ) system as backbone various linguistic features as factors. <eos> the motivation is to take advantages of both the robustness of the smt system and the rich linguistic knowledge from morphological analysis as well as the hand-crafted grammar resources. <eos> the automatic evaluation has shown promising results and our extensive manual analysis confirms the high quality of the translation the system delivers. <eos> the whole framework is also extensible for incorporating information provided by different sources.
this paper presents an approach to improving performance of statistical machine translation by automatically creating new training data for difficult to translate phenomena. <eos> in particular this contribution is targeted towards tackling the poor performance of a state-of-the-art system on negated sentences. <eos> the corpus expansion is achieved by high quality rephrasing of existing sentences to their negated counterparts making use of semantic transfer. <eos> the method is designed to work on both sides of the parallel corpus while preserving the alignment. <eos> our results show an overall improvement of 0.16 bleu points, with a statistically significant increase of 1.63 bleu points when tested on only negated test data.
hmeant ( lo and wu, 2011a ) is a manual mt evaluation technique that focuses on predicate-argument structure of the sentence. <eos> we relate hmeant to an established linguistic theory, highlighting the possibilities of reusing existing knowledge and resources for interpreting and automating hmeant. <eos> we apply hmeant to a new language, czech in particular, by evaluating a set of englishto-czech mt systems. <eos> hmeant proves to correlate with manual rankings at the sentence level better than a range of automatic metrics. <eos> however, the main contribution of this paper is the identification of several issues of hmeant annotation and our proposal on how to resolve them.
in this paper, we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation ( smt ), which pose problems to standard parsers due to their frequent ungrammaticality. <eos> we adapt the mst parser by exploiting additional features from the source language, and by introducing artificial grammatical errors in the parser training data, so that the training sentences resemble smt output. <eos> we evaluate the modified parser on depfix, a system that improves english-czech smt outputs using automatic rule-based corrections of grammatical mistakes which requires parsed smt output sentences as its input. <eos> both parser modifications led to improvements in bleu score ; their combination was evaluated manually, showing a statistically significant improvement of the translation quality.
we present an unsupervised approach to estimate the appropriate degree of contribution of each semantic role type for semantic translation evaluation, yielding a semantic mt evaluation metric whose correlation with human adequacy judgments is comparable to that of recent supervised approaches but without the high cost of a human-ranked training corpus. <eos> our new unsupervised estimation approach is motivated by an analysis showing that the weights learned from supervised training are distributed in a similar fashion to the relative frequencies of the semantic roles. <eos> empirical results show that even without a training corpus of human adequacy rankings against which to optimize correlation, using instead our relative frequency weighting scheme to approximate the importance of each semantic role type leads to a semantic mt evaluation metric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus. <eos> as a result, the cost of semantic mt evaluation is greatly reduced.
in statistical machine translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. <eos> linguistically motivated rules have been incorporated into chineseto-english ( wang et al, 2007 ) and englishto-japanese ( isozaki et al, 2010b ) translation with significant gains to the statistical translation system. <eos> here, we carry out a linguistic analysis of the chinese-to-japanese translation problem and propose one of the first reordering rules for this language pair. <eos> experimental results show substantially improvements ( from 20.70 to 23.17 bleu ) when head-finalization rules based on hpsg parses are used, and further gains ( to 24.14 bleu ) were obtained using more refined rules.
this paper presents two procedures for extracting transfer rules from parallel corpora for use in a rule-based japanese-english mt system. <eos> first a ? shallow ? <eos> method where the parallel corpus is lemmatized before it is aligned by a phrase aligner, and then a ? deep ? <eos> method where the parallel corpus is parsed by deep parsers before the resulting predicates are aligned by phrase aligners. <eos> in both procedures, the phrase tables produced by the phrase aligners are used to extract semantic transfer rules. <eos> the procedures were employed on a 10 million word japanese english parallel corpus and 190,000 semantic transfer rules were extracted.
weighted finite-state acceptors and transducers ( pereira and riley, 1997 ) are a critical technology for nlp and speech systems. <eos> they flexibly capture many kinds of stateful left-toright substitution, simple transducers can be composed into more complex ones, and they are em- trainable. <eos> they are unable to handle long-range syntactic movement, but tree acceptors and transducers address this weakness ( knight and graehl, 2005 ). <eos> tree automata have been profitably used in syntaxbased mt systems. <eos> still, strings and trees are both weak at representing linguistic structure involving semantics and reference ( ? who did what to whom ? ). <eos> feature structures provide an attractive, well-studied, standard format ( shieber, 1986 ; rounds and kasper, 1986 ), which we can view computationally as directed acyclic graphs. <eos> in this paper, we develop probabilistic acceptors and transducers for feature structures, demonstrate them on linguistic problems, and lay down a foundation for semantics-based mt.
in this article we investigate the translation of terms from english into german and vice versa in the isolation of an ontology vocabulary. <eos> for this study we built new domainspecific resources from the translation search engine linguee and from the online encyclopedia wikipedia. <eos> we learned that a domainspecific resource produces better results than a bigger, but more general one. <eos> the first finding of our research is that the vocabulary and the structure of the parallel corpus are important. <eos> by integrating the multilingual knowledge base wikipedia, we further improved the translation wrt. <eos> the domain-specific resources, whereby some translation evaluation metrics outperformed the results of google translate. <eos> this finding leads us to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-specific terms.
verb plays a crucial role of specifying the action or function performed in a sentence. <eos> in translating english to morphologically richer language like hindi, the organization and the order of verbal constructs contributes to the fluency of the language. <eos> mere statistical methods of machine translation are not sufficient enough to consider this aspect. <eos> identification of verb parts in a sentence is essential for its understanding and they constitute as if they are a single entity. <eos> considering them as a single entity improves the translation of the verbal construct and thus the overall quality of the translation. <eos> the paper describes a strategy for pre-processing and for identification of verb parts in source and target language corpora. <eos> the steps taken towards reducing sparsity further helped in improving the translation results.
in japanese, particularly, spoken japanese, subjective, objective and possessive cases are very often omitted. <eos> such japanese sentences are often translated by japanese-english statistical machine translation to the english sentence whose subjective, objective and possessive cases are omitted, and it causes to decrease the quality of translation. <eos> we performed experiments of j-e phrase based translation using japanese sentence, whose omitted pronouns are complemented by human. <eos> we introduced ? antecedent f-measure ? <eos> as a score for measuring quality of the translated english. <eos> as a result, we found that it improves the scores of antecedent f-measure while the bleu scores were almost unchanged. <eos> every effectiveness of the zero pronoun resolution differs depending on the type and case of each zero pronoun.
comparisons play a critical role in scientific communication by allowing an author to situate their work in the context of earlier research problems, experimental approaches, and results. <eos> our goal is to identify comparison claims automatically from full-text scientific articles. <eos> in this paper, we introduce a set of semantic and syntactic features that characterize a sentence and then demonstrate how those features can be used in three different classifiers : na ? ve bayes ( nb ), a support vector machine ( svm ) and a bayesian network ( bn ). <eos> experiments were conducted on 122 full-text toxicology articles containing 14,157 sentences, of which 1,735 ( 12.25 % ) were comparisons. <eos> experiments show an f1 score of 0.71, 0.69, and 0.74 on the development set and 0.76, 0.65, and 0.74 on a validation set for the nb, svm and bn, respectively.
key knowledge components of biological research papers are conveyed by structurally and rhetorically salient sentences that summarize the main findings of a particular experiment. <eos> in this article we define such sentences as claimed knowledge updates ( ckus ), and propose using them in text mining tasks. <eos> we provide evidence that ckus convey the most important new factual information, and thus demonstrate that rhetorical salience is a systematic discourse structure indicator in biology articles along with structural salience. <eos> we assume that ckus can be detected automatically with state-ofthe-art text analysis tools, and suggest some applications for presenting ckus in knowledge bases and scientific browsing interfaces.
sentiment analysis of citations in scientific papers is a new and interesting problem which can open up many exciting new applications in bibliometrics. <eos> current research assumes that using just the citation sentence is enough for detecting sentiment. <eos> in this paper, we show that this approach misses much of the existing sentiment. <eos> we present a new corpus in which all mentions of a cited paper have been annotated. <eos> we explore methods to automatically identify these mentions and show that the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment.
anatomical entities such as kidney, muscle and blood are central to much of biomedical scientific discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts. <eos> although a number of resources and methods addressing aspects of the task have been introduced, there have so far been no annotated corpora for training and evaluating systems for broad-coverage, open-domain anatomical entity mention detection. <eos> we introduce the anem corpus, a domain- and species-independent resource manually annotated for anatomical entity mentions using a fine-grained classification system. <eos> the corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature. <eos> we demonstrate the use of the corpus through an evaluation of the broad-coverage metamap tagger and a crf-based system trained on the corpus data, considering also a combination of these two methods. <eos> the combined system demonstrates a promising level of performance, approaching 80 % f-score for mention detection for a relaxed matching criterion. <eos> the corpus and other introduced resources are available under open licences from http : // www.nactem.ac.uk/anatomy/.
ay perspective on scientific discourse annotation for knowledge extraction maria liakata aberystwyth university, uk / embl-ebi, uk liakata @ ebi.ac.uk paul thompson university of manchester, uk paul.thompson @ manchester.ac.uk anita de waard elsevier labs, usa / uil-ots, universiteit utrecht, nl a.dewaard @ elsevier.com raheel nawaz university of manchester, uk raheel.nawaz @ cs.man.ac.uk henk pander maat uil-ots, universiteit utrecht, nl h.l.w.pandermaat @ uu.nl sophia ananiadou university of manchester, uk sophia.ananiadou @ manchester.ac.uk abstract this paper presents a three-way perspective on the annotation of discourse in scientific literature. <eos> we use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. <eos> one scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. <eos> we present our analysis of the comparison, and a discussion of the contributions of each scheme.
waard henk pander maat elsevier labs utrecht institute of linguistics jericho, vt, usa utrecht, the netherlands a.dewaard @ elsevier.com h.l.w.pandermaat @ uu.nl abstract we propose a model for knowledge attribution and epistemic evaluation in scientific discourse, consisting of three dimensions with different values : source ( author, other, unknown ) ; value ( unknown, possible, probable, presumed true ) and basis ( reasoning, data, other ). <eos> based on a literature review, we investigate four linguistic features that mark different types epistemic evaluation ( modal auxiliary verbs, adverbs/adjectives, reporting verbs and references ). <eos> a corpus study on two biology papers indicates the usefulness of this model, and suggest some typical trends. <eos> in particular, we find that matrix clauses with a reporting verb of the form ? these results suggest ?, are the predominant feature indicating knowledge attribution in scientific text.
this report documents the machine transliteration shared task conducted as a part of the named entities workshop ( news 2012 ), an acl 2012 workshop. <eos> the shared task features machine transliteration of proper names from english to 11 languages and from 3 languages to english. <eos> in total, 14 tasks are provided. <eos> 7 teams participated in the evaluations. <eos> finally, 57 standard and 1 non-standard runs are submitted, where diverse transliteration methodologies are explored and reported on the evaluation data. <eos> we report the results with 4 performance metrics. <eos> we believe that the shared task has successfully achieved its objective by providing a common benchmarking platform for the research community to evaluate the state-of-the-art technologies that benefit the future research and development.
we present a new approach to named-entity recognition that jointly learns to identify named-entities in parallel text. <eos> the system generates seed candidates through local, cross-language edit likelihood and then bootstraps to make broad predictions across both languages, optimizing combined contextual, word-shape and alignment models. <eos> it is completely unsupervised, with no manually labeled items, no external resources, only using parallel text that does not need to be easily alignable. <eos> the results are strong, with f > 0.85 for purely unsupervised namedentity recognition across languages, compared to just f = 0.35 on the same data for supervised cross-domain named-entity recognition within a language. <eos> a combination of unsupervised and supervised methods increases the accuracy to f = 0.88. <eos> we conclude that we have found a viable new strategy for unsupervised named-entity recognition across lowresource languages and for domain-adaptation within high-resource languages.
transliteration has been usually recognized by spelling-based supervised models. <eos> however, a single model can not deal with mixture of words with different origins, such as ? get ? <eos> in ? piaget ? <eos> and ? target ?. <eos> li et al ( 2007 ) propose a class transliteration method, which explicitly models the source language origins and switches them to address this issue. <eos> in contrast to their model which requires an explicitly tagged training corpus with language origins, hagiwara and sekine ( 2011 ) have proposed the latent class transliteration model, which models language origins as latent classes and train the transliteration table via the em algorithm. <eos> however, this model, which can be formulated as unigram mixture, is prone to overfitting since it is based on maximum likelihood estimation. <eos> we propose a novel latent semantic transliteration model based on dirichlet mixture, where a dirichlet mixture prior is introduced to mitigate the overfitting problem. <eos> we have shown that the proposed method considerably outperform the conventional transliteration models.
supervised named entity recognizers require large amounts of annotated text. <eos> since manual annotation is a highly costly procedure, reducing the annotation cost is essential. <eos> we present a fully automatic method to build ne annotated corpora from wikipedia. <eos> in contrast to recent work, we apply a new method, which maps the dbpedia classes into conll ne types. <eos> since our method is mainly languageindependent, we used it to generate corpora for english and hungarian. <eos> the corpora are freely available.
this paper describes our syllable-based phrase transliteration system for the news 2012 shared task on english-chinese track and its back. <eos> grapheme-based transliteration maps the character ( s ) in the source side to the target character ( s ) directly. <eos> however, character-based segmentation on english side will cause ambiguity in alignment step. <eos> in this paper we utilize phrase-based model to solve machine transliteration with the mapping between chinese characters and english syllables rather than english characters. <eos> two heuristic rulebased syllable segmentation algorithms are applied. <eos> this transliteration model also incorporates three phonetic features to enhance discriminative ability for phrase. <eos> the primary system achieved 0.330 on chinese-english and 0.177 on english-chinese in terms of top-1 accuracy.
in this paper, we describe our approach to english-to-korean transliteration task in news 2012. <eos> our system mainly consists of two components : an letter-to-phoneme alignment with m2m-aligner, and transliteration training model directl-p. we construct different parameter settings to train several transliteration models. <eos> then, we use two reranking methods to select the best transliteration among the prediction results from the different models. <eos> one re-ranking method is based on the co-occurrence of the transliteration pair in the web corpora. <eos> the other one is the jlis-reranking method which is based on the features from the alignment results. <eos> our standard and non-standard runs achieves 0.398 and 0.458 in top-1 accuracy in the generation task.
we developed a machine transliteration system combining mpaligner ( an improvement of m2m-aligner ), directl+, and some japanesespecific heuristics for the purpose of news 2012. <eos> our results show that mpaligner is greatly better than m2m-aligner, and the japanese-specific heuristics are effective for jnjk and enja tasks. <eos> while m2m-aligner is not good at long alignment, mpaligner performs well at longer alignment without any length limit. <eos> in jnjk and enja tasks, it is crucial to handle long alignment. <eos> an experimental result revealed that de-romanization, which is reverse operation of romanization, is crucial for jnjk task. <eos> in enja task, it is shown that mora is the best alignment unit for japanese language.
we consider the task of generating transliterated word forms. <eos> to allow for a wide range of interacting features, we use a conditional random field ( crf ) sequence labeling model. <eos> we then present two innovations : a training objective that optimizes toward any of a set of possible correct labels ( since more than one transliteration is often possible for a particular input ), and a k-best reranking stage to incorporate nonlocal features. <eos> this paper presents results on the arabic-english transliteration task of the news 2012 workshop.
we report the results of our transliteration experiments with language-specific adaptations in the context of two language pairs : english to chinese, and arabic to english. <eos> in particular, we investigate a syllable-based pinyin intermediate representation for chinese, and a letter mapping for arabic.
this work presents an english-to-chinese ( e2c ) machine transliteration system based on two-stage conditional random fields ( crf ) models with accessor variety ( av ) as an additional feature to approximate local context of the source language. <eos> experiment results show that two-stage crf method outperforms the one-stage opponent since the former costs less to encode more features and finer grained labels than the latter.
the conll-2012 shared task involved predicting coreference in english, chinese, and arabic, using the final version, v5.0, of the ontonotes corpus. <eos> it was a follow-on to the english-only task organized in 2011. <eos> until the creation of the ontonotes corpus, resources in this sub-field of language processing were limited to noun phrase coreference, often on a restricted set of entities, such as the ace entities. <eos> ontonotes provides a largescale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types, and covers multiple languages. <eos> ontonotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. <eos> this paper describes the ontonotes annotation ( coreference and other layers ) and then describes the parameters of the shared task including the format, pre-processing information, evaluation criteria, and presents and discusses the results achieved by the participating systems. <eos> the task of coreference has had a complex evaluation history. <eos> potentially many evaluation conditions, have, in the past, made it difficult to judge the improvement in new algorithms over previously reported results. <eos> having a standard test set and standard evaluation parameters, all based on a resource that provides multiple integrated annotation layers ( syntactic parses, semantic roles, word senses, named entities and coreference ) and in multiple languages could support joint modeling and help ground and energize ongoing research in the task of entity and event coreference.
we describe a machine learning system based on large margin structure perceptron for unrestricted coreference resolution that introduces two key modeling techniques : latent coreference trees and entropy guided feature induction. <eos> the proposed latent tree modeling turns the learning problem computationally feasible. <eos> additionally, using an automatic feature induction method, we are able to efficiently build nonlinear models and, hence, achieve high performances with a linear learning algorithm. <eos> our system is evaluated on the conll2012 shared task closed track, which comprises three languages : arabic, chinese and english. <eos> we apply the same system to all languages, except for minor adaptations on some language dependent features, like static lists of pronouns. <eos> our system achieves an official score of 58.69, the best one among all the competitors.
this paper describes our contribution to the conll 2012 shared task.1 we present a novel decoding algorithm for coreference resolution which is combined with a standard pair-wise coreference resolver in a stacking approach. <eos> the stacked decoders are evaluated on the three languages of the shared task. <eos> we obtain an official overall score of 58.25 which is the second highest in the shared task.
we describe our system for the conll-2012 shared task, which seeks to model coreference in ontonotes for english, chinese, and arabic. <eos> we adopt a hybrid approach to coreference resolution, which combines the strengths of rule-based methods and learningbased methods. <eos> our official combined score over all three languages is 56.35. <eos> in particular, our score on the chinese test set is the best among the participating teams.
this paper describes the structure of the lth coreference solver used in the closed track of the conll 2012 shared task ( pradhan et al, 2012 ). <eos> the solver core is a mention classifier that uses soon et al ( 2001 ) ? s algorithm and features extracted from the dependency graphs of the sentences. <eos> this system builds on bjo ? rkelund and nugues ( 2011 ) ? s solver that we extended so that it can be applied to the three languages of the task : english, chinese, and arabic. <eos> we designed a new mention detection module that removes pleonastic pronouns, prunes constituents, and recovers mentions when they do not match exactly a noun phrase. <eos> we carefully redesigned the features so that they reflect more complex linguistic phenomena as well as discourse properties. <eos> finally, we introduced a minimal cluster model grounded in the first mention of an entity. <eos> we optimized the feature sets for the three languages : we carried out an extensive evaluation of pairs of features and we complemented the single features with associations that improved the conll score. <eos> we obtained the respective scores of 59.57, 56.62, and 48.25 on english, chinese, and arabic on the development set, 59.36, 56.85, and 49.43 on the test set, and the combined official score of 55.21.
in this paper, we present our system description for the conll-2012 coreference resolution task on english, chinese and arabic. <eos> we investigate a projection-based model in which we first translate chinese and arabic into english, run a publicly available coreference system, and then use a new projection algorithm to map the coreferring entities back from english into mention candidates detected in the chinese and arabic source. <eos> we compare to a baseline that just runs the english coreference system on the supplied parses for chinese and arabic. <eos> because our method does not beat the baseline system on the development set, we submit outputs generated by the baseline system as our final submission.
this paper presents a mixed deterministic model for coreference resolution in the conll-2012 shared task. <eos> we separate the two main stages of our model, mention detection and coreference resolution, into several sub-tasks which are solved by machine learning method and deterministic rules based on multi-filters, such as lexical, syntactic, semantic, gender and number information. <eos> we participate in the closed track for english and chinese, and also submit an open result for chinese using tools to generate the required features. <eos> finally, we reach the average f1 scores 58.68, 60.69 and 61.02 on the english closed task, chinese closed and open tasks.
this paper describes our system participating in the conll-2012 shared task : modeling multilingual unrestricted coreference in ontonotes. <eos> maximum entropy models are used for our system as classifiers to determine the coreference relationship between every two mentions ( usually noun phrases and pronouns ) in each document. <eos> we exploit rich lexical, syntactic and semantic features for the system, and the final features are selected using a greedy forward and backward strategy from an initial feature set. <eos> our system participated in the closed track for both english and chinese languages.
the current work presents the participation of ubiu ( zhekova and ku ? bler, 2010 ) in the conll-2012 shared task : modeling multilingual unrestricted coreference in ontonotes ( pradhan et al, 2012 ). <eos> our system deals with all three languages : arabic, chinese and english. <eos> the system results show that ubiu works reliably across all three languages, reaching an average score of 40.57 for arabic, 46.12 for chinese, and 48.70 for english. <eos> for arabic and chinese, the system produces high precision, while for english, precision and recall are balanced, which leads to the highest results across languages.
we in this paper present the model for our participation ( bcmi ) in the conll-2012 shared task. <eos> this paper describes a pure rule-based method, which assembles different filters in a proper order. <eos> different filters handle different situations and the filtering strategies are designed manually. <eos> these filters are assigned to different ordered tiers from general to special cases. <eos> we participated in the chinese and english closed tracks, scored 51.83 and 59.24 respectively.
this paper presents hits ? <eos> coreference resolution system that participated in the conll2012 shared task on multilingual unrestricted coreference resolution. <eos> our system employs a simple multigraph representation of the relation between mentions in a document, where the nodes correspond to mentions and the edges correspond to relations between the mentions. <eos> entities are obtained via greedy clustering. <eos> we participated in the closed tasks for english and chinese. <eos> our system ranked second in the english closed task.
this paper describes a coreference resolution system for conll 2012 shared task developed by hlt_hitsz group, which incorporates rule-based and statistic-based techniques. <eos> the system performs coreference resolution through the mention pair classification and linking. <eos> for each detected mention pairs in the text, a decision tree ( dt ) based binary classifier is applied to determine whether they form a coreference. <eos> this classifier incorporates 51 and 61 selected features for english and chinese, respectively. <eos> meanwhile, a rule-based classifier is applied to recognize some specific types of coreference, especially the ones with long distances. <eos> the outputs of these two classifiers are merged. <eos> next, the recognized coreferences are linked to generate the final coreference chain. <eos> this system is evaluated on english and chinese sides ( closed track ), respectively. <eos> it achieves 0.5861 and 0.6003 f1 score on the development data of english and chinese, respectively. <eos> as for the test dataset, the achieved f1 scores are 0.5749 and 0.6508, respectively. <eos> this encouraging performance shows the effectiveness of our proposed coreference resolution system.
the conll-2012 shared task is an extension of the last year ? s coreference task. <eos> we participated in the closed track of the shared tasks in both years. <eos> in this paper, we present the improvements of illinois-coref system from last year. <eos> we focus on improving mention detection and pronoun coreference resolution, and present a new learning protocol. <eos> these new strategies boost the performance of the system by 5 % muc f1, 0.8 % bcub f1, and 1.7 % ceaf f1 on the ontonotes-5.0 development set.
this paper describes our coreference resolution system for the conll-2012 shared task. <eos> our system is based on the stanford ? s dcoref deterministic system which applies multiple sieves with the order from high precision to low precision to generate coreference chains. <eos> we introduce the newly added constraints and sieves and discuss the improvement on the original system. <eos> we evaluate the system using ontonotes data set and report our results of average f-score 58.25 in the closed track.
this paper describes the unitn/essex submission to the conll-2012 shared task on the multilingual coreference resolution. <eos> we have extended our conll-2011 submission, based on bart, to cover two additional languages, arabic and chinese. <eos> this paper focuses on adapting bart to new languages, discussing the problems we have encountered and the solutions adopted. <eos> in particular, we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language. <eos> we also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system.
coreference resolution, which aims at correctly linking meaningful expressions in text, is a much challenging problem in natural language processing community. <eos> this paper describes the multilingual coreference modeling system of web information processing group, henan university of technology, china, for the conll-2012 shared task ( closed track ). <eos> the system takes a supervised learning strategy, and consists of two cascaded components : one for detecting mentions, and the other for clustering mentions. <eos> to make the system applicable for multiple languages, generic syntactic and semantic features are used to model coreference in text. <eos> the system obtained combined official score 41.88 over three languages ( arabic, chinese, and english ) and ranked 7th among the 15 systems in the closed track.
we inspect the viability of finite-state spellchecking and contextless correction of nonword errors in three languages with a large degree of morphological variety. <eos> overviewing previous work, we conduct large-scale tests involving three languages ? <eos> english, finnish and greenlandic ? <eos> and a variety of error models and algorithms, including proposed improvements of our own. <eos> special reference is made to on-line three-way composition of the input, the error model and the language model. <eos> tests are run on real-world text acquired from freely available sources. <eos> we show that the finite-state approaches discussed are sufficiently fast for high-quality correction, even for greenlandic which, due to its morphological complexity, is a difficult task for non-finite-state approaches.
previous work for encoding optimality theory grammars as finite-state transducers has included two prominent approaches : the socalled ? counting ? <eos> method where constraint violations are counted and filtered out to some set limit of approximability in a finite-state system, and the ? matching ? <eos> method, where constraint violations in alternative strings are matched through violation alignment in order to remove suboptimal candidates. <eos> in this paper we extend the matching approach to show how not only markedness constraints, but also faithfulness constraints and the interaction of the two types of constraints can be captured by the matching method. <eos> this often produces exact and small fst representations for ot grammars which we illustrate with two practical example grammars. <eos> we also provide a new proof of nonregularity of simple ot grammars.
aalan and mohammed attia faculty of engineering & it, the british university in dubai khaled.shaalan @ buid.ac.ae mohammed.attia @ buid.ac.ae abstract a morphological analyser only recognizes words that it already knows in the lexical database. <eos> it needs, however, a way of sensing significant changes in the language in the form of newly borrowed or coined words with high frequency. <eos> we develop a finite-state morphological guesser in a pipelined methodology for extracting unknown words, lemmatizing them, and giving them a priority weight for inclusion in a lexicon. <eos> the processing is performed on a large contemporary corpus of 1,089,111,204 words and passed through a machine-learning-based annotation tool. <eos> our method is tested on a manually-annotated gold standard of 1,310 forms and yields good results despite the complexity of the task. <eos> our work shows the usability of a highly non-deterministic finite state guesser in a practical and complex application.
this paper introduces a two-way urdu ? <eos> roman transliterator based solely on a nonprobabilistic finite state transducer that solves the encountered scriptural issues via a particular architectural design in combination with a set of restrictions. <eos> in order to deal with the enormous amount of overgenerations caused by inherent properties of the urdu script, the transliterator depends on a set of phonological and orthographic restrictions and a word list ; additionally, a default component is implemented to allow for unknown entities to be transliterated, thus ensuring a large degree of flexibility in addition to robustness.
the integration of semantic properties into morphological analyzers can significantly enhance the performance of any tool that uses their output as input, e.g., for derivation or for syntactic parsing. <eos> in this paper will be presented my approach to the integration of aspectually relevant properties of verbs into a morphological analyzer for english.
this paper presents dagger, a toolkit for finite-state automata that operate on directed acyclic graphs ( dags ). <eos> the work is based on a model introduced by ( kamimura and slutzki, 1981 ; kamimura and slutzki, 1982 ), with a few changes to make the automata more applicable to natural language processing. <eos> available algorithms include membership checking in bottom-up dag acceptors, transduction of dags to trees ( bottom-up dag-to-tree transducers ), k-best generation and basic operations such as union and intersection.
this paper introduces a new open source, wfst-based toolkit for grapheme-tophoneme conversion. <eos> the toolkit is efficient, accurate and currently supports a range of features including em sequence alignment and several decoding techniques novel in the context of g2p. <eos> experimental results show that a combination rnnlm system outperforms all previous reported results on several standard g2p test sets. <eos> preliminary experiments applying lattice minimum bayes-risk decoding to g2p conversion are also provided. <eos> the toolkit is implemented using openfst.
kleene is a high-level programming language, based on the openfst library, for constructing and manipulating finite-state acceptors and transducers. <eos> users can program using regular expressions, alternation-rule syntax and right-linear phrase-structure grammars ; and kleene provides variables, lists, functions and familiar program-control syntax. <eos> kleene has been approved by sap ag for release as free, open-source code under the apache license, version 2.0, and will be available by august 2012 for downloading from http : // www.kleene-lang.org. <eos> the design, implementation, development status and future plans for the language are discussed.
bac, miikka silfverberg, and anssi yli-jyr ? <eos> university of helsinki department of modern languages unioninkatu 40 a fi-00014 helsingin yliopisto, finland { senka.drobac, miikka.silfverberg, anssi.yli-jyra } @ helsinki.fi abstract we explain the implementation of replace rules with the.r-glc. <eos> operator and preference relations. <eos> our modular approach combines various preference constraints to form differ-ent replace rules. <eos> in addition to describing the method, we present illustrative examples.
this paper presents an application of finitestate transducers to the domain of medicine. <eos> the objective is to assign disease codes to each diagnostic term in the medical records generated by the basque health hospital system. <eos> as a starting point, a set of manually coded medical records were collected in order to code new medical records on the basis of this set of positive samples. <eos> since the texts are written in natural language by doctors, the same diagnostic term might show alternative forms. <eos> hence, trying to code a new medical record by exact matching the samples in the set is not always feasible due to sparsity of data. <eos> in an attempt to increase the coverage of the data, our work centered on applying a set of finite-state transducers that helped the matching process between the positive samples and a set of new entries. <eos> that is, these transducers allowed not only exact matching but also approximate matching. <eos> while there are related works in languages such as english, this work presents the first results on automatic assignment of disease codes to medical records written in spanish.
this paper presents the current status of development of a finite state transducer grammar for the verbal-chain transfer module in matxin, a rule based machine translation system between spanish and basque. <eos> due to the distance between spanish and basque, the verbal-chain transfer is a very complex module in the overall system. <eos> the grammar is compiled with foma, an open-source finitestate toolkit, and yields a translation execution time of 2000 verb chains/second.
in this paper we describe a conversion of the buckwalter morphological analyzer for arabic, originally written as a perl-script, into a pure finite-state morphological analyzer. <eos> representing a morphological analyzer as a finite-state transducer ( fst ) confers many advantages over running a procedural affix-matching algorithm. <eos> apart from application speed, an fst representation immediately offers various possibilities to flexibly modify a grammar. <eos> in the case of arabic, this is illustrated through the addition of the ability to correctly parse partially vocalized forms without overgeneration, something not possible in the original analyzer, as well as to serve both as an analyzer and a generator.
in this work, we describe a methodology based on the stochastic finite state transducers paradigm for spoken language understanding ( slu ) for obtaining concept graphs from word graphs. <eos> in the edges of these concept graphs, both semantic and lexical information are represented. <eos> this makes these graphs a very useful representation of the information for slu. <eos> the best path in these concept graphs provides the best sequence of concepts.
a finite-state approach to temporal ontology for natural language text is described under which intervals ( of the real line ) paired with event descriptions are encoded as strings. <eos> the approach is applied to an interval temporal logic linked to timeml, a standard mark-up language for time and events, for which various finite-state mechanisms are proposed.
this paper presents a finite-state approach to phrase-based statistical machine translation where a log-linear modelling framework is implemented by means of an on-the-fly composition of weighted finite-state transducers. <eos> moses, a well-known state-of-the-art system, is used as a machine translation reference in order to validate our results by comparison. <eos> experiments on the ted corpus achieve a similar performance to that yielded by moses.
speech translation can be tackled by means of the so-called decoupled approach : a speech recognition system followed by a text translation system. <eos> the major drawback of this two-pass decoding approach lies in the fact that the translation system has to cope with the errors derived from the speech recognition system. <eos> there is hardly any cooperation between the acoustic and the translation knowledge sources. <eos> there is a line of research focusing on alternatives to implement speech translation efficiently : ranging from semi-decoupled to tightly integrated approaches. <eos> the goal of integration is to make acoustic and translation models cooperate in the underlying decision problem. <eos> that is, the translation is built by virtue of the joint action of both models. <eos> as a side-advantage of the integrated approaches, the translation is obtained in a single-pass decoding strategy. <eos> the aim of this paper is to assess the quality of the hypotheses explored within different speech translation approaches. <eos> evidence of the performance is given through experimental results on a limited-domain task.
this work complements a parallel paper of a new finite-state dependency parser architecture ( yli-jyra ?, 2012 ) by a proposal for a linguistically elaborated morphology-syntax interface and its finite-state implementation. <eos> the proposed interface extends gaifman ? s ( 1965 ) classical dependency rule formalism by separating lexical word forms and morphological categories from syntactic categories. <eos> the separation lets the linguist take advantage of the morphological features in order to reduce the number of dependency rules and to make them lexically selective. <eos> in addition, the relative functional specificity of parse trees gives rise to a measure of parse quality. <eos> by filtering worse parses out from the parse forest using finite-state techniques, the best parses are saved. <eos> finally, we present a synthesis of strict grammar parsing and robust text parsing by connecting fragmental parses into trees with additional linear successor links.
minimum error rate training ( mert ) is a method for training the parameters of a loglinear model. <eos> one advantage of this method of training is that it can use the large number of hypotheses encoded in a translation lattice as training data. <eos> we demonstrate that the mert line optimisation can be modelled as computing the shortest distance in a weighted finite-state transducer using a tropical polynomial semiring.
while automatic metrics of translation quality are invaluable for machine translation research, deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality. <eos> we show that word sense disambiguation ( wsd ) can be used to evaluate the quality of machine translation lexical choice, by applying a standard phrase-based smt system on the semeval2010 cross-lingual wsd task. <eos> this case study reveals that the smt system does not perform as well as a wsd system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the wsd system.
the statistical machine translation ( smt ) system heavily depends on the sentence aligned parallel corpus and the target language model. <eos> this paper points out some of the core issues on switching a language script and its repercussion in the phrase based statistical machine translation system development. <eos> the present task reports on the outcome of englishmanipuri language pair phrase based smt task on two aspects ? <eos> a ) manipuri using bengali script, b ) manipuri using transliterated meetei mayek script. <eos> two independent views on bengali script based smt and transliterated meitei mayek based smt systems of the training data and language models are presented and compared. <eos> the impact of various language models is commendable in such scenario. <eos> the bleu and nist score shows that bengali script based phrase based smt ( pbsmt ) outperforms over the meetei mayek based english to manipuri smt system. <eos> however, subjective evaluation shows slight variation against the automatic scores.
selecting a set of nonterminals for the synchronous cfgs underlying the hierarchical phrase-based models is usually done on the basis of a monolingual resource ( like a syntactic parser ). <eos> however, a standard bilingual resource like word alignments is itself rich with reordering patterns that, if clustered somehow, might provide labels of different ( possibly complementary ) nature to monolingual labels. <eos> in this paper we explore a first version of this idea based on a hierarchical decomposition of word alignments into recursive tree representations. <eos> we identify five clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these five as nonterminal labels for the hiero productions. <eos> although this is our first non-optimized instantiation of the idea, our experiments show competitive performance with the hiero baseline, exemplifying certain merits of this novel approach.
in this paper, we empirically investigate the impact of critical configuration parameters in the popular cube pruning algorithm for decoding in hierarchical statistical machine translation. <eos> specifically, we study how the choice of the k-best generation size affects translation quality and resource requirements in hierarchical search. <eos> we furthermore examine the influence of two different granularities of hypothesis recombination. <eos> our experiments are conducted on the large-scale chinese ? english and arabic ? english nist translation tasks. <eos> besides standard hierarchical grammars, we also explore search with restricted recursion depth of hierarchical rules based on shallow-1 grammars.
we describe a novel approach to combining lexicalized, pos-based and syntactic treebased word reordering in a phrase-based machine translation system. <eos> our results show that each of the presented reordering methods leads to improved translation quality on its own. <eos> the strengths however can be combined to achieve further improvements. <eos> we present experiments on german-english and germanfrench translation. <eos> we report improvements of 0.7 bleu points by adding tree-based and lexicalized reordering. <eos> up to 1.1 bleu points can be gained by pos and tree-based reordering over a baseline with lexicalized reordering. <eos> a human analysis, comparing subjective translation quality as well as a detailed error analysis show the impact of our presented tree-based rules in terms of improved sentence quality and reduction of errors related to missing verbs and verb positions.
we show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone. <eos> previous approaches have relied on incrementally building larger rules by chunking smaller rules bottomup ; we introduce a complementary top-down model that incrementally builds shorter rules by segmenting larger rules. <eos> specifically, we combine iteratively chunked rules from saers et al ( 2012 ) with our new iteratively segmented rules. <eos> these integrate seamlessly because both stay strictly within a pure transduction grammar framework inducing under matching models during both training and testing ? instead of decoding under a completely different model architecture than what is assumed during the training phases, which violates an elementary principle of machine learning and statistics. <eos> to be able to drive induction top-down, we introduce a minimum description length objective that trades off maximum likelihood against model size. <eos> we show empirically that combining the more liberal rule chunking model with a more conservative rule segmentation model results in significantly better translations than either strategy in isolation.
deciding whether a synchronous grammar formalism generates a given word alignment ( the alignment coverage problem ) depends on finding an adequate instance grammar and then using it to parse the word alignment. <eos> but what does it mean to parse a word alignment by a synchronous grammar ? <eos> this is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments. <eos> this paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets ( graphs ) of translation equivalence units, one derived by a grammar instance and another defined by the word alignment. <eos> as a first sanity check, we report extensive coverage results for itg on automatic and manual alignments. <eos> even for the itg formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work.
we propose synchronous linear context-free rewriting systems as an extension to synchronous context-free grammars in which synchronized non-terminals span k ? <eos> 1 continuous blocks on each side of the bitext. <eos> such discontinuous constituents are required for inducing certain alignment configurations that occur relatively frequently in manually annotated parallel corpora and that can not be generated with less expressive grammar formalisms. <eos> as part of our investigations concerning the minimal k that is required for inducing manual alignments, we present a hierarchical aligner in form of a deduction system. <eos> we find that by restricting k to 2 on both sides, 100 % of the data can be covered.
this paper evaluates four metaphor identification systems on the 200,000 word vu amsterdam metaphor corpus, comparing results by genre and by sub-class of metaphor. <eos> the paper then compares the rate of agreement between the systems for each genre and sub-class. <eos> each of the identification systems is based, explicitly or implicitly, on a theory of metaphor which hypothesizes that certain properties are essential to metaphor-inlanguage. <eos> the goal of this paper is to see what the success or failure of these systems can tell us about the essential properties of metaphorin-language. <eos> the success of the identification systems varies significantly across genres and sub-classes of metaphor. <eos> at the same time, the different systems achieve similar success rates on each even though they show low agreement among themselves. <eos> this is taken to be evidence that there are several sub-types of metaphor-in-language and that the ideal metaphor identification system will first define these sub-types and then model the linguistic properties which can distinguish these sub-types from one another and from nonmetaphors.
this article discusses metaphor annotation in a corpus of argumentative essays written by test-takers during a standardized examination for graduate school admission. <eos> the quality of argumentation being the focus of the project, we developed a metaphor annotation protocol that targets metaphors that are relevant for the writer ? s arguments. <eos> the reliability of the protocol is ? =0.58, on a set of 116 essays ( the total of about 30k content-word tokens ). <eos> we found a moderate-to-strong correlation ( r=0.51-0.57 ) between the percentage of metaphorically used words in an essay and the writing quality score. <eos> we also describe encouraging findings regarding the potential of metaphor identification to contribute to automated scoring of essays.
metaphor is a pervasive feature of human language that enables us to conceptualize and communicate abstract concepts using more concrete terminology. <eos> unfortunately, it is also a feature that serves to confound a computer ? s ability to comprehend natural human language. <eos> we present a method to detect linguistic metaphors by inducing a domainaware semantic signature for a given text and compare this signature against a large index of known metaphors. <eos> by training a suite of binary classifiers using the results of several semantic signature-based rankings of the index, we are able to detect linguistic metaphors in unstructured text at a significantly higher precision as compared to several baseline approaches.
we present the csf - common semantic features method for metaphor detection. <eos> this method has two distinguishing characteristics : it is cross-lingual and it does not rely on the availability of extensive manually-compiled lexical resources in target languages other than english. <eos> a metaphor detecting classifier is trained on english samples and then applied to the target language. <eos> the method includes procedures for obtaining semantic features from sentences in the target language. <eos> our experiments with russian and english sentences show comparable results, supporting our hypothesis that a csf-based classifier can be applied across languages. <eos> we obtain state-ofthe-art performance in both languages.
a metaphor is a figure of speech that refers to one concept in terms of another, as in ? he is such a sweet person ?. <eos> metaphors are ubiquitous and they present nlp with a range of challenges for wsd, ie, etc. <eos> identifying metaphors is thus an important step in language understanding. <eos> however, since almost any word can serve as a metaphor, they are impossible to list. <eos> to identify metaphorical use, we assume that it results in unusual semantic patterns between the metaphor and its dependencies. <eos> to identify these cases, we use svms with tree-kernels on a balanced corpus of 3872 instances, created by bootstrapping from available metaphor lists.1 we outperform two baselines, a sequential and a vectorbased approach, and achieve an f1-score of 0.75.
we aim to investigate cross-cultural patterns of thought through cross-linguistic investigation of the use of metaphor. <eos> as a first step, we produce a system for locating instances of metaphor in english and spanish text. <eos> in contrast to previous work which relies on resources like syntactic parsing and wordnet, our system is based on lda topic modeling, enabling its application even to low-resource languages, and requires no labeled data. <eos> we achieve an f-score of 59 % for english.
traction of metaphors from novel data tomek strzalkowski1, george aaron broadwell1, sarah taylor2, laurie feldman1, boris yamrom1, samira shaikh1, ting liu1, kit cho1, umit boz1, ignacio cases1 and kyle el-liott3 1state university of new york 2sarah m. taylor consulting llc 3plessas experts university at albany 121 south oak st. network inc. albany ny usa 12222 falls church va usa 22046 herndon va 20171 tomek @ albany.edu talymail59 @ gmail.com kelliot @ plessas.net abstract this article describes our novel approach to the automated detection and analysis of meta-phors in text. <eos> we employ robust, quantitative language processing to implement a system prototype combined with sound social science methods for validation. <eos> we show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification. <eos> we use topical structure and tracking, an imageability score, and innova-tive methods to build an effective metaphor identification system that is fully automated and performs well over baseline.
this work presents the tentative version of the protocol designed for annotation of a russian metaphor corpus using the rapid annotation tool brat. <eos> the first part of the article is devoted to the procedure of `` shallow '' annotation in which metaphor-related words are identified according to a slightly modified version of the mipvu procedure. <eos> the paper presents the results of two reliability tests and the measures of inter-annotator agreement obtained in them. <eos> further on, the article gives a brief account of the linguistic problems that were encountered in adapting mipvu to russian. <eos> the rest of the first part describes the classes of metaphorrelated words and the rules of their annotation with brat. <eos> the examples of annotation show how the visualization functionalities of brat allow the researcher to describe the multifaceted nature of metaphor related words and the complexity of their relations. <eos> the second part of the paper speaks about the annotation of conceptual metaphors ( the `` deep '' annotation ), where formulations of conceptual metaphors are inferred from the basic and contextual meanings of metaphor-related words from the `` shallow '' annotation, which is expected to make the metaphor formulation process more controllable.
this paper introduces perspred, the first manually elaborated syntactic and semantic database for persian complex predicates ( cps ). <eos> beside their theoretical interest, persian cps constitute an important challenge in persian lexicography and for nlp. <eos> the first delivery, perspred 11, contains 700 cps, for which 22 fields of lexical, syntactic and semantic information are encoded. <eos> the semantic classification perspred provides allows to account for the productivity of these combinations in a way which does justice to their compositionality without overlooking their idiomaticity.
the paper describes a method for identifying and translating multiword expressions using a bi-directional dictionary. <eos> while a dictionarybased approach suffers from limited recall, precision is high ; hence it is best employed alongside an approach with complementing properties, such as an n-gram language model. <eos> we evaluate the method on data from the english-german translation part of the crosslingual word sense disambiguation task in the 2010 semantic evaluation exercise ( semeval ). <eos> the output of a baseline disambiguation system based on n-grams was substantially improved by matching the target words and their immediate contexts against compound and collocational words in a dictionary.
human ratings are an important source for evaluating computational models that predict compositionality, but like many data sets of human semantic judgements, are often fraught with uncertainty and noise. <eos> however, despite their importance, to our knowledge there has been no extensive look at the effects of cleansing methods on human rating data. <eos> this paper assesses two standard cleansing approaches on two sets of compositionality ratings for german noun-noun compounds, in their ability to produce compositionality ratings of higher consistency, while reducing data quantity. <eos> we find ( i ) that our ratings are highly robust against aggressive filtering ; ( ii ) z-score filtering fails to detect unreliable item ratings ; and ( iii ) minimum subject agreement is highly effective at detecting unreliable subjects.
this research focuses on determining semantic compositionality of word expressions using word space models ( wsms ). <eos> we discuss previous works employing wsms and present differences in the proposed approaches which include types of wsms, corpora, preprocessing techniques, methods for determining compositionality, and evaluation testbeds. <eos> we also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components. <eos> the vectors were obtained by latent semantic analysis ( lsa ) applied to the ukwac corpus. <eos> our results outperform those of all the participants in the distributional semantics and compositionality ( disco ) 2011 shared task.
while working on valency lexicons for czech and english, it was necessary to define treatment of multiword entities ( mwes ) with the verb as the central lexical unit. <eos> morphological, syntactic and semantic properties of such mwes had to be formally specified in order to create lexicon entries and use them in treebank annotation. <eos> such a formal specification has also been used for automated quality control of the annotation vs. the lexicon entries. <eos> we present a corpus-based study, concentrating on multilayer specification of verbal mwes, their properties in czech and english, and a comparison between the two languages using the parallel czech-english dependency treebank ( pcedt ). <eos> this comparison revealed interesting differences in the use of verbal mwes in translation ( discovering that such mwes are actually rarely translated as mwes, at least between czech and english ) as well as some inconsistencies in their annotation. <eos> adding mwe-based checks should thus result in better quality control of future treebank/lexicon annotation. <eos> since czech and english are typologically different languages, we believe that our findings will also contribute to a better understanding of verbal mwes and possibly their more unified treatment across languages. <eos> ? <eos> this work has been supported by the grant no. <eos> gpp406/13/03351p of the grant agency of the czech republic. <eos> the data used have been provided by the lindat/clarin infrastructural project lm2010013 supported by the msmt cr ( http : //lindat.cz ). <eos> ? <eos> authors ? <eos> full address : institute of formal and applied linguistics, charles university in prague, faculty of mathematics and physics, malostranske nam. <eos> 25, 11800 prague 1, czech republic
this paper presents a supervised machine learning approach that uses a machine learning algorithm called random forest for recognition of bengali noun-noun compounds as multiword expression ( mwe ) from bengali corpus. <eos> our proposed approach to mwe recognition has two steps : ( 1 ) extraction of candidate multi-word expressions using chunk information and various heuristic rules and ( 2 ) training the machine learning algorithm to recognize a candidate multi-word expression as multi-word expression or not. <eos> a variety of association measures, syntactic and linguistic clues are used as features for identifying mwes. <eos> the proposed system is tested on a bengali corpus for identifying noun-noun compound mwes from the corpus.
this paper presents an algorithm that allows the user to issue a query pattern, collects multi-word expressions ( mwes ) that match the pattern, and then ranks them in a uniform fashion. <eos> this is achieved by quantifying the strength of all possible relations between the tokens and their features in the mwes. <eos> the algorithm collects the frequency of morphological categories of the given pattern on a unified scale in order to choose the stable categories and their values. <eos> for every part of speech, and for all of its categories, we calculate a normalized kullback-leibler divergence between the category ? s distribution in the pattern and its distribution in the corpus overall. <eos> categories with the largest divergence are considered to be the most significant. <eos> the particular values of the categories are sorted according to a frequency ratio. <eos> as a result, we obtain morphosyntactic profiles of a given pattern, which includes the most stable category of the pattern, and their values.
high frequency can convert a word sequence into a multiword expression ( mwe ), i.e., a collocation. <eos> in this paper, we use collocations as well as syntactically-flexible, lexicalized phrases to analyze ? job specification documents ? <eos> ( a kind of corporate technical document ) for subsequent acquisition of automated knowledge elicitation. <eos> we propose the definition of structural and functional patterns of specific corporate documents by analyzing the contexts and sections in which the expression occurs. <eos> such patterns and its automated processing are the basis for identifying organizational domain knowledge and business information which is used later for the first instances of requirement elicitation processes in software engineering.
based on a lexicon of portuguese mwe, this presentation focuses on an ongoing work that aims at the creation of a typology that describes these expressions taking into account their semantic, syntactic and pragmatic properties. <eos> we also plan to annotate each mweentry in the mentioned lexicon according to the information obtained from that typology. <eos> our objective is to create a valuable resource, which will allow for the automatic identification mwe in running text and for a deeper understanding of these expressions in their context.
a challenging topic in portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se, which impacts nlp tasks such as syntactic parsing, semantic role labeling and machine translation. <eos> aiming to give a step forward towards the automatic disambiguation of se, our study focuses on the identification of pronominal verbs, which correspond to one of the six uses of se as a clitic pronoun, when se is considered a constitutive particle of the verb lemma to which it is bound, as a multiword unit. <eos> our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se. <eos> this process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task. <eos> the availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources, such as the portuguese versions of wordnet, propbank and verbnet. <eos> moreover, it will allow the revision of parsers and dictionaries already in use.
we deal with syntactic identification of occurrences of multiword expression ( mwe ) from an existing dictionary in a text corpus. <eos> the mwes we identify can be of arbitrary length and can be interrupted in the surface sentence. <eos> we analyse and compare three approaches based on linguistic analysis at a varying level, ranging from surface word order to deep syntax. <eos> the evaluation is conducted using two corpora : the prague dependency treebank and czech national corpus. <eos> we use the dictionary of multiword expressions semlex, that was compiled by annotating the prague dependency treebank and includes deep syntactic dependency trees of all mwes.
we present an experimental study of how different features help measuring the idiomaticity of noun+verb ( nv ) expressions in basque. <eos> after testing several techniques for quantifying the four basic properties of multiword expressions or mwes ( institutionalization, semantic non-compositionality, morphosyntactic fixedness and lexical fixedness ), we test different combinations of them for classification into idioms and collocations, using machine learning ( ml ) and feature selection. <eos> the results show the major role of distributional similarity, which measures compositionality, in the extraction and classification of mwes, especially, as expected, in the case of idioms. <eos> even though cooccurrence and some aspects of morphosyntactic flexibility contribute to this task in a more limited measure, ml experiments make benefit of these sources of knowledge, allowing to improve the results obtained using exclusively distributional similarity features.
the linguistic annotation of noun-verb complex predicates ( also termed as light verb constructions ) is challenging as these predicates are highly productive in hindi. <eos> for semantic role labelling, each argument of the noun-verb complex predicate must be given a role label. <eos> for complex predicates, frame files need to be created specifying the role labels for each noun-verb complex predicate. <eos> the creation of frame files is usually done manually, but we propose an automatic method to expedite this process. <eos> we use two resources for this method : hindi propbank frame files for simple verbs and the annotated hindi treebank. <eos> our method perfectly predicts 65 % of the roles in 3015 unique noun-verb combinations, with an additional 22 % partial predictions, giving us 87 % useful predictions to build our annotation resource.
grading is a primary cognitive operation that has an important expressive function. <eos> information on degree is grammatically relevant and constitutes what lazard ( 2006 ) calls a primary domain of grammaticalization : according to typological studies ( cuzzolin & lehmann, 2004 ), many languages of the world have in fact at their disposal multiple grammatical devices to express gradation. <eos> in italian, the class of superlativizing structures alternative to the morphological superlative is very rich and consists, among others, of adverbs of degree, focalizing adverbs and prototypical comparisons. <eos> this contribution deals with a particular analytic structure of superlative in italian that is still neglected in the literature. <eos> this is what we will call constructional intensifying adjectives ( cias ), adjectives which modify the intensity of other adjectives on the basis of regular semantic patterns, thus giving rise to multiword superlative constructions of the type : adjx+adjintens. <eos> a comparative quantitative corpus analysis demonstrates that this strategy, though paradigmatically limited, is nonetheless widely exploited : from a distributional point of view, some of these cias only combine with one or a few adjectives and form mwes that appear to be completely lexicalized, while some others modify wider classes of adjectives thus displaying a certain degree of productivity.
this paper reports our ongoing project for constructing an english multiword expression ( mwe ) dictionary and nlp tools based on the developed dictionary. <eos> we extracted functional mwes from the english part of wiktionary, annotated the penn treebank ( ptb ) with mwe information, and conducted pos tagging experiments. <eos> we report how the mwe annotation is done on ptb and the results of pos and mwe tagging experiments.
we explore improving parsing social media and other web data by altering the input data, namely by normalizing web text, and by revising output parses. <eos> we find that text normalization improves performance, though spell checking has more of a mixed impact. <eos> we also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model. <eos> the results also demonstrate that, more than the size of the training data, the goodness of fit of the data has a great impact on the parser.
does phonological variation get transcribed into social media text ? <eos> this paper investigates examples of the phonological variable of consonant cluster reduction in twitter. <eos> not only does this variable appear frequently, but it displays the same sensitivity to linguistic context as in spoken language. <eos> this suggests that when social media writing transcribes phonological properties of speech, it is not merely a case of inventing orthographic transcriptions. <eos> rather, social media displays influence from structural properties of the phonological system.
although the ideal length of summaries differs greatly from topic to topic on twitter, previous work has only generated summaries of a pre-fixed length. <eos> in this paper, we propose an event-graph based method using information extraction techniques that is able to create summaries of variable length for different topics. <eos> in particular, we extend the pageranklike ranking algorithm from previous work to partition event graphs and thereby detect finegrained aspects of the event to be summarized. <eos> our preliminary results show that summaries created by our method are more concise and news-worthy than sumbasic according to human judges. <eos> we also provide a brief survey of datasets and evaluation design used in previous work to highlight the need of developing a standard evaluation for automatic tweet summarization task.
more and more of the information on the web is dialogic, from facebook newsfeeds, to forum conversations, to comment threads on news articles. <eos> in contrast to traditional, monologic natural language processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for nlp. <eos> this paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue : sarcasm and nastiness. <eos> we explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall. <eos> the best performing classifier for the first phase achieves 54 % precision and 38 % recall for sarcastic utterances. <eos> we then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62 % and recall to 52 %. <eos> to further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. <eos> our first phase, using crowdsourced nasty indicators, achieves 58 % precision and 49 % recall, which increases to 75 % precision and 62 % recall when we bootstrap over the first level with generalized syntactic patterns.
ositioning : a new method for predicting opinion changes in conversation ching-sheng lin1, samira shaikh1, jennifer stromer-galley1,2, jennifer crowley1, tomek strzalkowski1,3, veena ravishankar1 1state university of new york - university at albany, ny 12222 usa 2syracuse university 3polish academy of sciences clin3 @ albany.edu, sshaikh @ albany.edu, tomek @ albany.edu abstract in this paper, we describe a novel approach to automatically detecting and tracking discus-sion dynamics in internet social media by fo-cusing on attitude modeling of topics. <eos> we characterize each participant ? s attitude to-wards topics as topical positioning, employ topical positioning map to represent the posi-tions of participants with respect to each other and track attitude shifts over time. <eos> we also discuss how we used participants ? <eos> attitudes towards system-detected meso-topics to re-flect their attitudes towards the overall topic of conversation. <eos> our approach can work across different types of social media, such as twitter discussion and online chat room. <eos> in this article, we show results on twitter data.
we perform a series of 3-class sentiment classification experiments on a set of 2,624 tweets produced during the run-up to the irish general elections in february 2011. <eos> even though tweets that have been labelled as sarcastic have been omitted from this set, it still represents a difficult test set and the highest accuracy we achieve is 61.6 % using supervised learning and a feature set consisting of subjectivity-lexicon-based scores, twitterspecific features and the top 1,000 most discriminative words. <eos> this is superior to various naive unsupervised approaches which use subjectivity lexicons to compute an overall sentiment score for a < tweet, political party > pair.
this paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in wikipedia. <eos> sockpuppets are fake accounts created by malicious users to bypass wikipedia ? s regulations. <eos> our dataset is composed of the comments made by the editors on the talk pages. <eos> to overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. <eos> we show that this approach is promising and that it can be a viable alternative to the current human process that wikipedia uses to resolve suspected sockpuppet cases.
we investigate the task of detecting reliable statements about food-health relationships from natural language texts. <eos> for that purpose, we created a specially annotated web corpus from forum entries discussing the healthiness of certain food items. <eos> we examine a set of task-specific features ( mostly ) based on linguistic insights that are instrumental in finding utterances that are commonly perceived as reliable. <eos> these features are incorporated in a supervised classifier and compared against standard features that are widely used for various tasks in natural language processing, such as bag of words, part-of speech and syntactic parse information.
while the automatic translation of tweets has already been investigated in different scenarios, we are not aware of any attempt to translate tweets created by government agencies. <eos> in this study, we report the experimental results we obtained when translating 12 twitter feeds published by agencies and organizations of the government of canada, using a state-ofthe art statistical machine translation ( smt ) engine as a black box translation device. <eos> we mine parallel web pages linked from the urls contained in english-french pairs of tweets in order to create tuning and training material. <eos> for a twitter feed that would have been otherwise difficult to translate, we report significant gains in translation quality using this strategy. <eos> furthermore, we give a detailed account of the problems we still face, such as hashtag translation as well as the generation of tweets of legal length.
this paper introduces gaf, a grounded annotation framework to represent events in a formal context that can represent information from both textual and extra-textual sources. <eos> gaf makes a clear distinction between mentions of events in text and their formal representation as instances in a semantic layer. <eos> instances are represented by rdf compliant uris that are shared across different research disciplines. <eos> this allows us to complete textual information with external sources and facilitates reasoning. <eos> the semantic layer can integrate any linguistic information and is compatible with previous event representations in nlp. <eos> through a use case on earthquakes in southeast asia, we demonstrate gaf flexibility and ability to reason over events with the aid of extra-linguistic resources.
this paper describes an approach for investigating the representation of events and their distribution in a corpus. <eos> we collect and analyze statistics about subject-verb-object triplets and their content, which helps us compare corpora belonging to the same domain but to different genre/text type. <eos> we argue that event structure is strongly related to the genre of the corpus, and propose statistical properties that are able to capture these genre differences. <eos> the results obtained can be used for the improvement of information extraction.
i present a set of functional requirements for a speculative tool informing users about events in historical discourse, in order to demonstrate what these requirements imply about how we should define and represent historical events. <eos> the functions include individuation, selection, and contextualization of events. <eos> i conclude that a tool providing these functions would need events to be defined and represented as features of discourses about the world rather than objectively existing things in the world.
understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text. <eos> our task is the identification of phenotypes, specifically, pneumonia, from clinical narratives. <eos> in this paper, we consider the importance of identifying the change of state for events, in particular, events that measure and compare multiple states across time. <eos> change of state is important to the clinical diagnosis of pneumonia ; in the example ? there are bibasilar opacities that are unchanged ?, the presence of bibasilar opacities alone may suggest pneumonia, but not when they are unchanged, which suggests the need to modify events with change of state information. <eos> our corpus is comprised of chest xray reports, where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas. <eos> we propose an annotation schema to capture this information as a tuple of < location, attribute, value, change-of-state, time-reference >.
we are interested in the task of image annotation using noisy natural text as training data. <eos> an image and its caption convey different information, but are generated by the same underlying concepts. <eos> in this paper, we learn latent mixtures of topics that generate image and product descriptions on shopping websites by adapting a topic model for multilingual data ( mimno et al, 2009 ). <eos> we use the trained model to annotate test images without corresponding text. <eos> we capture visual properties such as color, texture, shape, and orientation by computing low-level image features, and measure the contribution of each type of visual feature towards the accuracy of the model. <eos> our model significantly outperforms both a competitive baseline and a previous topic model-based system.
we present a holistic data-driven technique that generates natural-language descriptions for videos. <eos> we combine the output of state-ofthe-art object and activity detectors with ? realworld ? <eos> knowledge to select the most probable subject-verb-object triplet for describing a video. <eos> we show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification. <eos> unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. <eos> we evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61 % of the time.
we propose a method to learn succinct hierarchical linguistic descriptions of visual datasets, which allow for improved navigation efficiency in image collections. <eos> classic exploratory data analysis methods, such as agglomerative hierarchical clustering, only provide a means of obtaining a tree-structured partitioning of the data. <eos> this requires the user to go through the images first, in order to reveal the semantic relationship between the different nodes. <eos> on the other hand, in this work we propose to learn a hierarchy of linguistic descriptions, referred to as attributes, which allows for a textual description of the semantic content that is captured by the hierarchy. <eos> our approach is based on a generative model, which relates the attribute descriptions associated with each node, and the node assignments of the data instances, in a probabilistic fashion. <eos> we furthermore use a nonparametric bayesian prior, known as the tree-structured stick breaking process, which allows for the structure of the tree to be learned in an unsupervised fashion. <eos> we also propose appropriate performance measures, and demonstrate superior performance compared to other hierarchical clustering algorithms.
there are cultural barriers to collaborative effort between literary scholars and computational linguists. <eos> in this work, we discuss some of these problems in the context of our ongoing research project, an exploration of free indirect discourse in virginia woolf ? s to the lighthouse, ultimately arguing that the advantages of taking each field out of its ? comfort zone ? <eos> justifies the inherent difficulties.
this work presents a novel method for recognizing and extracting classical arabic poems found in textual sources. <eos> the method utilizes the basic classical arabic poem features such as structure, rhyme, writing style, and word usage. <eos> the proposed method achieves a precision of 96.94 % while keeping a high recall value at 92.24 %. <eos> the method was also used to build a prototype search engine for classical arabic poems.
scholars of chinese literature note that china ? s tumultuous literary history in the 20th century centered around the uncomfortable tensions between tradition and modernity. <eos> in this corpus study, we develop and automatically extract three features to show that the classical character of chinese poetry decreased across the century. <eos> we also find that taiwan poets constitute a surprising exception to the trend, demonstrating an unusually strong connection to classical diction in their work as late as the ? 50s and ? 60s.
this position paper argues the need for a comprehensive corpus of online book responses. <eos> responses to books ( in traditional reviews, book blogs, on booksellers ? <eos> sites, etc. ) <eos> are important for understanding how readers understand literature and how literary works become popular. <eos> a sufficiently large, varied and representative corpus of online responses to books will facilitate research into these processes. <eos> this corpus should include context information about the responses and should remain open to additional material. <eos> based on a pilot study for the creation of a corpus of dutch online book response, the paper shows how linguistic tools can find differences in word usage between responses from various sites. <eos> they can also reveal response type by clustering responses based on usage of either words or their pos-tags, and can show the sentiments expressed in the responses. <eos> lsa-based similarity between book fragments and response may be able to reveal the book fragments that most affected readers. <eos> the paper argues that a corpus of book responses can be an important instrument for research into reading behavior, reader response, book reviewing and literary appreciation.
t.s. <eos> eliot ? s modernist poem the waste land is often interpreted as collection of voices which appear multiple times throughout the text. <eos> here, we investigate whether we can automatically cluster existing segmentations of the text into coherent, expert-identified characters. <eos> we show that clustering the waste land is a fairly difficult task, though we can do much better than random baselines, particularly if we begin with a good initial segmentation.
this work performs some basic research upon topical poetry segmentation in a pilot study designed to test some initial assumptions and methodologies. <eos> nine segmentations of the poem titled kubla khan ( coleridge, 1816, pp. <eos> 55-58 ) are collected and analysed, producing low but comparable inter-coder agreement. <eos> analyses and discussions of these codings focus upon how to improve agreement and outline some initial results on the nature of topics in this poem.
we present coocviewer, a graphical analysis tool for the purpose of quantitative literary analysis, and demonstrate its use on a corpus of crime novels. <eos> the tool displays words, their significant co-occurrences, and contains a new visualization for significant concordances. <eos> contexts of words and co-occurrences can be displayed. <eos> after reviewing previous research and current challenges in the newly emerging field of quantitative literary research, we demonstrate how coocviewer allows comparative research on literary corpora in a project-specific study, and how we can confirm or enhance our hypotheses through quantitative literary analysis.
stylometric analysis of prose is typically limited to classification tasks such as authorship attribution. <eos> since the models used are typically black boxes, they give little insight into the stylistic differences they detect. <eos> in this paper, we characterize two prose genres syntactically : chick lit ( humorous novels on the challenges of being a modern-day urban female ) and high literature. <eos> first, we develop a top-down computational method based on existing literary-linguistic theory. <eos> using an off-the-shelf parser we obtain syntactic structures for a dutch corpus of novels and measure the distribution of sentence types in chick-lit and literary novels. <eos> the results show that literature contains more complex ( subordinating ) sentences than chick lit. <eos> secondly, a bottom-up analysis is made of specific morphological and syntactic features in both genres, based on the parser ? s output. <eos> this shows that the two genres can be distinguished along certain features. <eos> our results indicate that detailed insight into stylistic differences can be obtained by combining computational linguistic analysis with literary theory.
this paper discusses user study outcomes with teachers who used language musesm a webbased teacher professional development ( tpd ) application designed to enhance teachers ? <eos> linguistic awareness, and support teachers in the development of language-based instructional scaffolding ( support ) for their english language learners ( ell ). <eos> system development was grounded in literature that supports the notion that instruction incorporating language support for ells can improve their accessibility to content-area classroom texts ? in terms of access to content, and improvement of language skills. <eos> measurement outcomes of user piloting with teachers in a tpd setting indicated that application use increased teachers ' linguistic knowledge and awareness, and their ability to develop appropriate language-based instruction for ells. <eos> instruction developed during the pilot was informed by the application ? s linguistic analysis feedback, provided by natural language processing capabilities in language muse.
persons affected by autism spectrum disorders ( asd ) present impairments in social interaction. <eos> a significant percentile of them have inadequate reading comprehension skills. <eos> in the ongoing first project we build a multilingual tool called open book that helps the asd people to better understand the texts. <eos> the tool applies a series of automatic transformations to user documents to identify and remove the reading obstacles to comprehension. <eos> we focus on three semantic components : an image component that retrieves images for the concepts in the text, an idiom detection component and a topic model component. <eos> moreover, we present the personalization component that adapts the system output to user preferences.
one of the populations that often needs some form of help to read everyday documents is non-native speakers. <eos> this paper discusses aid at the word and word string levels and focuses on the possibility of using translation and simplification. <eos> seen from the perspective of the non-native as an ever-learning reader, we show how translation may be of more harm than help in understanding and retaining the meaning of a word while simplification holds promise. <eos> we conclude that if reading everyday documents can be considered as a learning activity as well as a practical necessity, then our study reinforces the arguments that defend the use of simplification to make documents that non-natives need to read more accessible.
we present a computational notion of lexical tightness that measures global cohesion of content words in a text. <eos> lexical tightness represents the degree to which a text tends to use words that are highly inter-associated in the language. <eos> we demonstrate the utility of this measure for estimating text complexity as measured by us school grade level designations of texts. <eos> lexical tightness strongly correlates with grade level in a collection of expertly rated reading materials. <eos> lexical tightness captures aspects of prose complexity that are not covered by classic readability indexes, especially for literary texts. <eos> we also present initial findings on the utility of this measure for automated estimation of complexity for poetry.
the purpose of this paper is to motivate and describe a system that simplifies numerical expression in texts, along with an evaluation study in which experts in numeracy and literacy assessed the outputs of this system. <eos> we have worked with a collection of newspaper articles with a significant number of numerical expressions. <eos> the results are discussed in comparison to conclusions obtained from a prior empirical survey.
many existing approaches for measuring text complexity tend to overestimate the complexity levels of informational texts while simultaneously underestimating the complexity levels of literary texts. <eos> we present a two-stage estimation technique that successfully addresses this problem. <eos> at stage 1, each text is classified into one or another of three possible genres : informational, literary or mixed. <eos> next, at stage 2, a complexity score is generated for each text by applying one or another of three possible prediction models : one optimized for application to informational texts, one optimized for application to literary texts, and one optimized for application to mixed texts. <eos> each model combines lexical, syntactic and discourse features, as appropriate, to best replicate human complexity judgments. <eos> we demonstrate that resulting text complexity predictions are both unbiased, and highly correlated with classifications provided by experienced educators.
we present a bootstrapping algorithm to automatically learn hashtags that convey emotion. <eos> using the bootstrapping framework, we learn lists of emotion hashtags from unlabeled tweets. <eos> our approach starts with a small number of seed hashtags for each emotion, which we use to automatically label tweets as initial training data. <eos> we then train emotion classifiers and use them to identify and score candidate emotion hashtags. <eos> we select the hashtags with the highest scores, use them to automatically harvest new tweets from twitter, and repeat the bootstrapping process. <eos> we show that the learned hashtag lists help to improve emotion classification performance compared to an n-gram classifier, obtaining 8 % microaverage and 9 % macro-average improvements in f-measure.
in this paper, we detail a method for domain specific, multi-category emotion recognition, based on human computation. <eos> we create an amazon mechanical turk1 task that elicits emotion labels and phrase-emotion associations from the participants. <eos> using the proposed method, we create an emotion lexicon, compatible with the 20 emotion categories of the geneva emotion wheel. <eos> gew is the first computational resource that can be used to assign emotion labels with such a high level of granularity. <eos> our emotion annotation method also produced a corpus of emotion labeled sports tweets. <eos> we compared the crossvalidated version of the lexicon with existing resources for both the positive/negative and multi-emotion classification problems. <eos> we show that the presented domain-targeted lexicon outperforms the existing general purpose ones in both settings. <eos> the performance gains are most pronounced for the fine-grained emotion classification, where we achieve an accuracy twice higher than the benchmark.2
the topic of sentiment analysis in text has been extensively studied in english for the past 30 years. <eos> an early, influential work by cynthia whissell, the dictionary of affect in language ( dal ), allows rating words along three dimensions : pleasantness, activation and imagery. <eos> given the lack of such tools in spanish, we decided to replicate whissell ? s work in that language. <eos> this paper describes the spanish dal, a knowledge base formed by more than 2500 words manually rated by humans along the same three dimensions. <eos> we evaluated its usefulness on two sentiment analysis tasks, which showed that the knowledge base managed to capture relevant information regarding the three affective dimensions.
to avoid a sarcastic message being understood in its unintended literal meaning, in microtexts such as messages on twitter.com sarcasm is often explicitly marked with the hashtag ? # sarcasm ?. <eos> we collected a training corpus of about 78 thousand dutch tweets with this hashtag. <eos> assuming that the human labeling is correct ( annotation of a sample indicates that about 85 % of these tweets are indeed sarcastic ), we train a machine learning classifier on the harvested examples, and apply it to a test set of a day ? s stream of 3.3 million dutch tweets. <eos> of the 135 explicitly marked tweets on this day, we detect 101 ( 75 % ) when we remove the hashtag. <eos> we annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag. <eos> 30 % of the top-250 ranked tweets are indeed sarcastic. <eos> analysis shows that sarcasm is often signalled by hyperbole, using intensifiers and exclamations ; in contrast, non-hyperbolic sarcastic messages often receive an explicit marker. <eos> we hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of nonverbal expressions that people employ in live interaction when conveying sarcasm.
nowadays a large number of opinion reviews are posted on the web. <eos> such reviews are a very important source of information for customers and companies. <eos> the former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients ? <eos> expectations. <eos> due to the economic importance of these reviews there is a growing trend to incorporate spam on such sites, and, as a consequence, to develop methods for opinion spam detection. <eos> in this paper we focus on the detection of deceptive opinion spam, which consists of fictitious opinions that have been deliberately written to sound authentic, in order to deceive the consumers. <eos> in particular we propose a method based on the pu-learning approach which learns only from a few positive examples and a set of unlabeled data. <eos> evaluation results in a corpus of hotel reviews demonstrate the appropriateness of the proposed method for real applications since it reached a f-measure of 0.84 in the detection of deceptive opinions using only 100 positive examples for training.
this paper describes a novel approach for sexual predator detection in chat conversations based on sequences of classifiers. <eos> the proposed approach divides documents into three parts, which, we hypothesize, correspond to the different stages that a predator employs when approaching a child. <eos> local classifiers are trained for each part of the documents and their outputs are combined by a chain strategy : predictions of a local classifier are used as extra inputs for the next local classifier. <eos> additionally, we propose a ring-based strategy, in which the chaining process is iterated several times, with the goal of further improving the performance of our method. <eos> we report experimental results on the corpus used in the first international competition on sexual predator identification ( pan ? 12 ). <eos> experimental results show that the proposed method outperforms a standard ( global ) classification technique for the different settings we consider ; besides the proposed method compares favorably with most methods evaluated in the pan ? 12 competition.
though much research has been conducted on subjectivity and sentiment analysis ( ssa ) during the last decade, little work has focused on arabic. <eos> in this work, we focus on ssa for both modern standard arabic ( msa ) news articles and dialectal arabic microblogs from twitter. <eos> we showcase some of the challenges associated with ssa on microblogs. <eos> we adopted a random graph walk approach to extend the arabic ssa lexicon using arabicenglish phrase tables, leading to improvements for ssa on arabic microblogs. <eos> we used different features for both subjectivity and sentiment classification including stemming, part-of-speech tagging, as well as tweet specific features. <eos> our classification features yield results that surpass arabic ssa results in the literature.
this article provides an in-depth research of machine learning methods for sentiment analysis of czech social media. <eos> whereas in english, chinese, or spanish this field has a long history and evaluation datasets for various domains are widely available, in case of czech language there has not yet been any systematical research conducted. <eos> we tackle this issue and establish a common ground for further research by providing a large humanannotated czech social media corpus. <eos> furthermore, we evaluate state-of-the-art supervised machine learning methods for sentiment analysis. <eos> we explore different pre-processing techniques and employ various features and classifiers. <eos> moreover, in addition to our newly created social media dataset, we also report results on other widely popular domains, such as movie and product reviews. <eos> we believe that this article will not only extend the current sentiment analysis research to another family of languages, but will also encourage competition which potentially leads to the production of high-end commercial solutions.
we discuss a tagging scheme to tag data for training information extraction models which can extract the features of a product/service and opinions about them from textual reviews, and which can be used across different domains with minimal adaptation. <eos> a simple tagging scheme results in a large number of domain dependent opinion phrases and impedes the usefulness of the trained models across domains. <eos> we show that by using minor modifications to this simple tagging scheme the number of domain dependent opinion phrases are reduced from 36 % to 17 %, which leads to models more useful across domains.
we compare the performance of two lexiconbased sentiment systems ? <eos> sentistrength ( thelwall et al, 2012 ) and so-cal ( taboada et al, 2011 ) ? <eos> on the two genres of newspaper text and tweets. <eos> while sentistrength has been geared specifically toward short social-media text, so-cal was built for general, longer text. <eos> after the initial comparison, we successively enrich the so-cal-based analysis with tweet-specific mechanisms and observe that in some cases, this improves the performance. <eos> a qualitative error analysis then identifies classes of typical problems the two systems have with tweets.
up until now most of the methods published for polarity classification are applied to english texts. <eos> however, other languages on the internet are becoming increasingly important. <eos> this paper presents a set of experiments on english and spanish product reviews. <eos> using a comparable corpus, a supervised method and two unsupervised methods have been assessed. <eos> furthermore, a list of spanish opinion words is presented as a valuable resource.
in this paper we propose a method that uses corpora where phrases are annotated as positive, negative, objective and neutral, to achieve new sentiment resources involving words dictionaries with their associated polarity. <eos> our method was created to build sentiment words inventories based on sentisemantic evidences obtained after exploring text with annotated sentiment polarity information. <eos> through this process a graph-based algorithm is used to obtain auto-balanced values that characterize sentiment polarities well used on sentiment analysis tasks. <eos> to assessment effectiveness of the obtained resource, sentiment classification was made, achieving objective instances over 80 %.
we describe twita, the first corpus of italian tweets, which is created via a completely automatic procedure, portable to any other language. <eos> we experiment with sentiment analysis on two datasets from twita : a generic collection and a topic-specific collection. <eos> the only resource we use is a polarity lexicon, which we obtain by automatically matching three existing resources thereby creating the first polarity database for italian. <eos> we observe that albeit shallow, our simple system captures polarity distinctions matching reasonably well the classification done by human judges, with differences in performance across polarity values and on the two sets.
in this work, we attempt to detect sentencelevel subjectivity by means of two supervised machine learning approaches : a fuzzy control system and adaptive neuro-fuzzy inference system. <eos> even though these methods are popular in pattern recognition, they have not been thoroughly investigated for subjectivity analysis. <eos> we present a novel ? pruned icf weighting coefficient, ? <eos> which improves the accuracy for subjectivity detection. <eos> our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any lexical knowledge. <eos> for this reason, these machine learning models can be applied to any language ; i.e., there is no lexical, grammatical, syntactical analysis used in the classification process.
sentiment analysis means to extract opinion of users from review documents. <eos> sentiment classification using machine learning ( ml ) methods faces the problem of high dimensionality of feature vector. <eos> therefore, a feature selection method is required to eliminate the irrelevant and noisy features from the feature vector for efficient working of ml algorithms. <eos> rough set theory based feature selection method finds the optimal feature subset by eliminating the redundant features. <eos> in this paper, rough set theory ( rst ) based feature selection method is applied for sentiment classification. <eos> a hybrid feature selection method based on rst and information gain ( ig ) is proposed for sentiment classification. <eos> proposed methods are evaluated on four standard datasets viz. <eos> movie review, product ( book, dvd and electronics ) review dataset. <eos> experimental results show that hybrid feature selection method outperforms than other feature selection methods for sentiment classification.
this paper presents a method for sentiment analysis specifically designed to work with twitter data ( tweets ), taking into account their structure, length and specific language. <eos> the approach employed makes it easily extendible to other languages and makes it able to process tweets in near real time. <eos> the main contributions of this work are : a ) the pre-processing of tweets to normalize the language and generalize the vocabulary employed to express sentiment ; b ) the use minimal linguistic processing, which makes the approach easily portable to other languages ; c ) the inclusion of higher order n-grams to spot modifications in the polarity of the sentiment expressed ; d ) the use of simple heuristics to select features to be employed ; e ) the application of supervised learning using a simple support vector machines linear classifier on a set of realistic data. <eos> we show that using the training models generated with the method described we can improve the sentiment classification performance, irrespective of the domain and distribution of the test sets.
we investigate the utility of linguistic features for automatically differentiating between children with varying combinations of two potentially comorbid neurodevelopmental disorders : autism spectrum disorder and specific language impairment. <eos> we find that certain manual codes for linguistic errors are useful for distinguishing between diagnostic groups. <eos> we investigate the relationship between coding detail and diagnostic classification performance, and find that a simple coding scheme is of high diagnostic utility. <eos> we propose a simple method to automate the pared down coding scheme, and find that these automatic codes are of diagnostic utility.
we describe the nus corpus of learner english ( nucle ), a large, fully annotated corpus of learner english that is freely available for research purposes. <eos> the goal of the corpus is to provide a large data resource for the development and evaluation of grammatical error correction systems. <eos> although nucle has been available for almost two years, there has been no reference paper that describes the corpus in detail. <eos> in this paper, we address this need. <eos> we describe the annotation schema and the data collection and annotation process of nucle. <eos> most importantly, we report on an unpublished study of annotator agreement for grammatical error correction. <eos> finally, we present statistics on the distribution of grammatical errors in the nucle corpus.
automated feedback on writing may be a useful complement to teacher comments in the process of learning a foreign language. <eos> this paper presents a self-assessment and tutoring system which combines an holistic score with detection and correction of frequent errors and furthermore provides a qualitative assessment of each individual sentence, thus making the language learner aware of potentially problematic areas rather than providing a panacea. <eos> the system has been tested by learners in a range of educational institutions, and their feedback has guided its development.
we present the first system developed for automated grading of high school essays written in swedish. <eos> the system uses standard text quality indicators and is able to compare vocabulary and grammar to large reference corpora of blog posts and newspaper articles. <eos> the system is evaluated on a corpus of 1 702 essays, each graded independently by the student ? s own teacher and also in a blind re-grading process by another teacher. <eos> we show that our system ? s performance is fair, given the low agreement between the two human graders, and furthermore show how it could improve efficiency in a practical setting where one seeks to identify incorrectly graded essays.
native language identification, or nli, is the task of automatically classifying the l1 of a writer based solely on his or her essay written in another language. <eos> this problem area has seen a spike in interest in recent years as it can have an impact on educational applications tailored towards non-native speakers of a language, as well as authorship profiling. <eos> while there has been a growing body of work in nli, it has been difficult to compare methodologies because of the different approaches to pre-processing the data, different sets of languages identified, and different splits of the data used. <eos> in this shared task, the first ever for native language identification, we sought to address the above issues by providing a large corpus designed specifically for nli, in addition to providing an environment for systems to be directly compared. <eos> in this paper, we report the results of the shared task. <eos> a total of 29 teams from around the world competed across three different sub-tasks.
vector space models ( vsm ) have been widely used in the language assessment field to provide measurements of students ? <eos> vocabulary choices and content relevancy. <eos> however, training reference vectors ( rv ) in a vsm requires a time-consuming and costly human scoring process. <eos> to address this limitation, we applied unsupervised learning methods to reduce or even eliminate the human scoring step required for training rvs. <eos> our experiments conducted on data from a non-native english speaking test suggest that the unsupervised topic clustering is better at selecting responses to train rvs than random selection. <eos> in addition, we conducted an experiment to totally eliminate the need of human scoring. <eos> instead of using human rated scores to train rvs, we used used the machine-predicted scores from an automated speaking assessment system for training rvs. <eos> we obtained vsm-derived features that show promisingly high correlations to human-holistic scores, indicating that the costly human scoring process can be eliminated. <eos> index terms : vector space model ( vsm ), speech assessment, unsupervised learning, document clustering
we developed an approach to predict the proficiency level of estonian language learners based on the cefr guidelines. <eos> we performed learner classification by studying morphosyntactic variation and lexical richness in texts produced by learners of estonian as a second language. <eos> we show that our features which exploit the rich morphology of estonian by focusing on the nominal case and verbal mood are useful predictors for this task. <eos> we also show that re-formulating the classification problem as a multi-stage cascaded classification improves the classification accuracy. <eos> finally, we also studied the effect of training data size on classification accuracy and found that more training data is beneficial in only some of the cases.
this paper presents and evaluates approaches to automatically score the content correctness of spoken responses in a new language test for teachers of english as a foreign language who are non-native speakers of english. <eos> most existing tests of english spoken proficiency elicit responses that are either very constrained ( e.g., reading a passage aloud ) or are of a predominantly spontaneous nature ( e.g., stating an opinion on an issue ). <eos> however, the assessment discussed in this paper focuses on essential speaking skills that english teachers need in order to be effective communicators in their classrooms and elicits mostly responses that fall in between these extremes and are moderately predictable. <eos> in order to automatically score the content accuracy of these spoken responses, we propose three categories of robust features, inspired from flexible text matching, n-grams, as well as string edit distance metrics. <eos> the experimental results indicate that even based on speech recognizer output, most of the feature correlations with human expert rater scores are in the range of r = 0.4 to r = 0.5, and further, that a scoring model for predicting human rater proficiency scores that includes our content features can significantly outperform a baseline without these features ( r = 0.56 vs. r = 0.33 ).
we present a system for automatically identifying the native language of a writer. <eos> we experiment with a large set of features and train them on a corpus of 9,900 essays written in english by speakers of 11 different languages. <eos> our system achieved an accuracy of 43 % on the test data, improved to 63 % with improved feature normalization. <eos> in this paper, we present the features used in our system, describe our experiments and provide an analysis of our results.
this paper describes the system developed for the nli 2013 shared task, requiring to identify a writer ? s native language by some text written in english. <eos> i explore the given manually annotated data using word features such as the length, endings and character trigrams. <eos> furthermore, i employ k-nn classification. <eos> modified tfidf is used to generate a stop-word list automatically. <eos> the distance between two documents is calculated combining n-grams of word lengths and endings, and character trigrams.
we decribe the submissions made by the national research council canada to the native language identification ( nli ) shared task. <eos> our submissions rely on a support vector machine classifier, various feature spaces using a variety of lexical, spelling, and syntactic features, and on a simple model combination strategy relying on a majority vote between classifiers. <eos> somewhat surprisingly, a classifier relying on purely lexical features performed very well and proved difficult to outperform significantly using various combinations of feature spaces. <eos> however, the combination of multiple predictors allowed to exploit their different strengths and provided a significant boost in performance.
this paper describes mitre ? s participation in the native language identification ( nli ) task at bea-8. <eos> our best effort performed at an accuracy of 82.6 % in the eleven-way nli task, placing it in a statistical tie with the best performing systems. <eos> we describe the variety of machine learning approaches that we explored, including winnow, language modeling, logistic regression and maximum-entropy models. <eos> our primary features were word and character n-grams. <eos> we also describe several ensemble methods that we employed for combining these base systems.
g classification accuracy in native language identification scott jarvis yves bestgen steve pepper ohio university universit ? <eos> catholique de louvain department of linguistic department of linguistics centre for english corpus linguistics aesthetic and literary studies athens, oh, usa louvain-la-neuve, belgium university of bergen, norway jarvis @ ohio.edu yves.bestgen @ uclouvain.be pepper.steve @ gmail.com abstract this paper reports our contribution to the 2013 nli shared task. <eos> the purpose of the task was to train a machine-learning system to identify the native-language affiliations of 1,100 texts written in english by nonnative speakers as part of a high-stakes test of gen-eral academic english proficiency. <eos> we trained our system on the new toefl11 corpus, which includes 11,000 essays written by nonnative speakers from 11 native-language backgrounds. <eos> our final system used an svm classifier with over 400,000 unique features consisting of lexical and pos n-grams occur-ring in at least two texts in the training set. <eos> our system identified the correct native-language affiliations of 83.6 % of the texts in the test set. <eos> this was the highest classification accuracy achieved in the 2013 nli shared task.
native language identification ( nli ), which tries to identify the native language ( l1 ) of a second language learner based on their writings, is helpful for advancing second language learning and authorship profiling in forensic linguistics. <eos> with the availability of relevant data resources, much work has been done to explore the native language of a foreign language learner. <eos> in this report, we present our system for the first shared task in native language identification ( nli ). <eos> we use a linear svm classifier and explore features of words, word and character n-grams, style, and metadata. <eos> our official system achieves accuracy of 0.773, which ranks it 18th among the 29 teams in the closed track.
our submission for this nli shared task used for the most part standard features found in recent work. <eos> our focus was instead on two other aspects of our system : at a high level, on possible ways of constructing ensembles of multiple classifiers ; and at a low level, on the granularity of part-of-speech tags used as features. <eos> we found that the choice of ensemble combination method did not lead to much difference in results, although exploiting the varying behaviours of linear versus logistic regression svm classifiers could be promising in future work ; but part-of-speech tagsets showed noticeable differences. <eos> we also note that the overall architecture, with its feature set and ensemble approach, had an accuracy of 83.1 % on the test set when trained on both the training data and development data supplied, close to the best result of the task. <eos> this suggests that basically throwing together all the features of previous work will achieve roughly the state of the art.
this paper describes the nara institute of science and technology ( naist ) native language identification ( nli ) system in the nli 2013 shared task. <eos> we apply feature selection using a measure based on frequency for the closed track and try capping and sampling data methods for the open tracks. <eos> our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2.
we apply support vector machines to differentiate between 11 native languages in the 2013 native language identification shared task. <eos> we expand a set of common language identification features to include cognate interference and spelling mistakes. <eos> our best results are obtained with a classifier which includes both the cognate and the misspelling features, as well as word unigrams, word bigrams, character bigrams, and syntax production rules.
tree substitution grammar rules form a large and expressive class of features capable of representing syntactic and lexical patterns that provide evidence of an author ? s native language. <eos> however, this class of features can be applied to any general constituent based model of grammar and previous work has done little to explore these options, relying primarily on the common penn treebank annotation standard. <eos> in this work we contrast the performance of syntactic features for native language indentification using five different formalisms. <eos> the use of different formalisms captures complementary information from second language data, and can be used in combination to yield classification performance superior to any formalism taken on its own.
native language identification ( nli ) is the task to determine the native language of the author based on an essay written in a second language. <eos> nli is often treated as a classification problem. <eos> in this paper, we use the toefl11 data set which consists of more data, in terms of the amount of essays and languages, and less biased across prompts, i.e., topics, of essays. <eos> we demonstrate that even using word level n-grams as features, and support vector machine ( svm ) as a classifier can yield nearly 80 % accuracy. <eos> we observe that the accuracy of a binary-based word level ngram representation ( ~80 % ) is much better than the performance of a frequency-based word level n-gram representation ( ~20 % ). <eos> notably, comparable results can be achieved without removing punctuation marks, suggesting a very simple baseline system for nli.
this paper investigates the use of promptbased content features for the automated assessment of spontaneous speech in a spoken language proficiency assessment. <eos> the results show that single highest performing promptbased content feature measures the number of unique lexical types that overlap with the listening materials and are not contained in either the reading materials or a sample response, with a correlation of r = 0.450 with holistic proficiency scores provided by humans. <eos> furthermore, linear regression scoring models that combine the proposed promptbased content features with additional spoken language proficiency features are shown to achieve competitive performance with scoring models using content features based on prescored responses.
we introduce a cognitive framework for measuring reading comprehension that includes the use of novel summary writing tasks. <eos> we derive nlp features from the holistic rubric used to score the summaries written by students for such tasks and use them to design a preliminary, automated scoring system. <eos> our results show that the automated approach performs well on summaries written by students for two different passages.
this paper reports on a study of interannotator agreement ( iaa ) for a dependency annotation scheme designed for learner english. <eos> reliably-annotated learner corpora are a necessary step for the development of pos tagging and parsing of learner language. <eos> in our study, three annotators marked several layers of annotation over different levels of learner texts, and they were able to obtain generally high agreement, especially after discussing the disagreements among themselves, without researcher intervention, illustrating the feasibility of the scheme. <eos> we pinpoint some of the problems in obtaining full agreement, including annotation scheme vagueness for certain learner innovations, interface design issues, and difficult syntactic constructions. <eos> in the process, we also develop ways to calculate agreements for sets of dependencies.
this paper reports on our work in the nli shared task 2013 on native language identification. <eos> the task is to automatically detect the native language of the toefl essays authors in a set of given test documents in english. <eos> the task was solved by a system that used the ppm compression algorithm based on an n-gram statistical model. <eos> we submitted four runs ; word-based ppmc algorithm with normalization and without, character-based ppmc algorithm with normalization and without. <eos> the worst result was obtained on training and testing data during the evaluation procedure using the character-based ppm method and normalization : accuracy = 31.9 % ; the best one was macroaverage f-measure = 0.708 with the word-based ppmc algorithm without normalization.
our efforts in the 2013 nli shared task focused on the potential benefits of external corpora. <eos> we show that including training data from multiple corpora is highly effective at robust, cross-corpus nli ( i.e. <eos> open-training task 1 ), particularly when some form of domain adaptation is also applied. <eos> this method can also be used to boost performance even when training data from the same corpus is available ( i.e. <eos> open-training task 2 ). <eos> however, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work.
we explore a range of features and ensembles for the task of native language identification as part of the nli shared task ( tetreault et al, 2013 ). <eos> starting with recurring word-based ngrams ( bykh and meurers, 2012 ), we tested different linguistic abstractions such as partof-speech, dependencies, and syntactic trees as features for nli. <eos> we also experimented with features encoding morphological properties, the nature of the realizations of particular lemmas, and several measures of complexity developed for proficiency and readability classification ( vajjala and meurers, 2012 ). <eos> employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2 % ( rank 5 ) in the closed task and 83.5 % ( rank 1 ) in the open-2 task. <eos> in the open-1 task, the word-based recurring ngrams outperformed the ensemble, yielding 38.5 % ( rank 2 ). <eos> overall, across all three tasks, our best accuracy of 83.5 % for the standard toefl11 test set came in second place.
in this paper, we describe our approach to native language identification and discuss the results we submitted as participants to the first nli shared task. <eos> by resorting to a wide set of general ? purpose features qualifying the lexical and grammatical structure of a text, rather than to ad hoc features specifically selected for the nli task, we achieved encouraging results, which show that the proposed approach is general ? purpose and portable across different tasks, domains and languages.
this paper presents a native language identification ( nli ) system based on tf-idf weighting schemes and using linear classifiers - support vector machines, logistic regressions and perceptrons. <eos> the system was one of the participants of the 2013 nli shared task in the closed-training track, achieving 0.814 overall accuracy for a set of 11 native languages. <eos> this accuracy was only 2.2 percentage points lower than the winner ? s performance. <eos> furthermore, with subsequent evaluations using 10-fold cross-validation ( as given by the organizers ) on the combined training and development data, the best average accuracy obtained is 0.8455 and the features that contributed to this accuracy are the tf-idf of the combined unigrams and bigrams of words.
this paper describes our approaches to native language identification ( nli ) for the nli shared task 2013. <eos> nli as a sub area of author profiling focuses on identifying the first language of an author given a text in his second language. <eos> researchers have reported several sets of features that have achieved relatively good performance in this task. <eos> the type of features used in such works are : lexical, syntactic and stylistic features, dependency parsers, psycholinguistic features and grammatical errors. <eos> in our approaches, we selected lexical and syntactic features based on n-grams of characters, words, penn treebank ( ptb ) and universal parts of speech ( pos ) tagsets, and perplexity values of character of n-grams to build four different models. <eos> we also combine all the four models using an ensemble based approach to get the final result. <eos> we evaluated our approach over a set of 11 native languages reaching 75 % accuracy.
our goal is to predict the first language ( l1 ) of english essays ? s authors with the help of the toefl11 corpus where l1, prompts ( topics ) and proficiency levels are provided. <eos> thus we approach this task as a classification task employing machine learning methods. <eos> out of key concepts of machine learning, we focus on feature engineering. <eos> we design features across all the l1 languages not making use of knowledge of prompt and proficiency level. <eos> during system development, we experimented with various techniques for feature filtering and combination optimized with respect to the notion of mutual information and information gain. <eos> we trained four different svm models and combined them through majority voting achieving accuracy 72.5 %.
we report on the performance of two different feature sets in the native language identification shared task ( tetreault et al, 2013 ). <eos> our feature sets were inspired by existing literature on native language identification and word networks. <eos> experiments show that word networks have competitive performance against the baseline feature set, which is a promising result. <eos> we also present a discussion of feature analysis based on information gain, and an overview on the performance of different word network features in the native language identification task.
this paper describes limsi ? s participation to the first shared task on native language identification. <eos> our submission uses a maximum entropy classifier, using as features character and chunk n-grams, spelling and grammatical mistakes, and lexical preferences. <eos> performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages.
this paper describes an effort to perform native language identification ( nli ) using machine learning on a large amount of lexical features. <eos> the features were collected from sequences and collocations of bare word forms, suffixes and character n-grams amounting to a feature set of several hundred thousand features. <eos> these features were used to train a linear support vector machine ( svm ) classifier for predicting the native language category.
this paper presents our approach to the 2013 native language identification shared task, which is based on machine learning methods that work at the character level. <eos> more precisely, we used several string kernels and a kernel based on local rank distance ( lrd ). <eos> actually, our best system was a kernel combination of string kernel and lrd. <eos> while string kernels have been used before in text analysis tasks, lrd is a distance measure designed to work on dna sequences. <eos> in this work, lrd is applied with success in native language identification. <eos> finally, the unibuc team ranked third in the closed nli shared task. <eos> this result is more impressive if we consider that our approach is language independent and linguistic theory neutral.
we show that it is possible to learn to identify, with high accuracy, the native language of english test takers from the content of the essays they write. <eos> our method uses standard text classification techniques based on multiclass logistic regression, combining individually weak indicators to predict the most probable native language from a set of 11 possibilities. <eos> we describe the various features used for classification, as well as the settings of the classifier that yielded the highest accuracy.
in automated speech assessment, adaptation of language models ( lms ) to test questions is important to achieve high recognition accuracy however, for large-scale language tests, the ordinary supervised training, which uses an expensive and time-consuming manual transcription process, is hard to utilize for lm adaptation. <eos> in this paper, several lm adaptation methods that require either no manual transcription process or just a small amount of transcriptions have been evaluated. <eos> our experiments suggest that these lm adaptation methods can allow us to obtain considerable recognition accuracy gain with no or low human transcription cost. <eos> index terms : language model adaptation, unsupervised training, web as a corpus
we present an experiment aimed at improving interpretation robustness of a tutorial dialogue system that relies on detailed semantic interpretation and dynamic natural language feedback generation. <eos> we show that we can improve overall interpretation quality by combining the output of a semantic interpreter with that of a statistical classifier trained on the subset of student utterances where semantic interpretation fails. <eos> this improves on a previous result which used a similar approach but trained the classifier on a substantially larger data set containing all student utterances. <eos> finally, we discuss how the labels from the statistical classifier can be integrated effectively with the dialogue system ? s existing error recovery policies.
we present a method for automatically detecting missing hyphens in english text. <eos> our method goes beyond a purely dictionary-based approach and also takes context into account. <eos> we evaluate our model on artificially generated data as well as naturally occurring learner text. <eos> our best-performing model achieves high precision and reasonable recall, making it suitable for inclusion in a system that gives feedback to language learners.
this paper discusses preliminary work investigating the application of machine translation ( mt ) metrics toward the evaluation of translations written by human novice ( student ) translators. <eos> we describe a study in which we apply the metric terp ( translation edit rate plus ) to a corpus of student-written translations from spanish to english and compare the judgments of terp against assessments provided by a translation instructor.
this research analyzed the clinical notes of epilepsy patients using techniques from corpus linguistics and machine learning and predicted which patients are candidates for neurosurgery, i.e. <eos> have intractable epilepsy, and which are not. <eos> information-theoretic and machine learning techniques are used to determine whether and how sets of clinic notes from patients with intractable and nonintractable epilepsy are different. <eos> the results show that it is possible to predict from an early stage of treatment which patients will fall into one of these two categories based only on text data. <eos> these results have broad implications for developing clinical decision support systems.
identification of complex clinical phenotypes among critically ill patients is a major challenge in clinical research. <eos> the overall research goal of our work is to develop automated approaches that accurately identify critical illness phenotypes to prevent the resource intensive manual abstraction approach. <eos> in this paper, we describe a text processing method that uses natural language processing ( nlp ) and supervised text classification methods to identify patients who are positive for acute lung injury ( ali ) based on the information available in free-text chest x-ray reports. <eos> to increase the classification performance we enhanced the baseline unigram representation with bigram and trigram features, enriched the n-gram features with assertion analysis, and applied statistical feature selection. <eos> we used 10-fold cross validation for evaluation and our best performing classifier achieved 81.70 % precision ( positive predictive value ), 75.59 % recall ( sensitivity ), 78.53 % f-score, 74.61 % negative predictive value, 76.80 % specificity in identifying patients with ali.
the clinical narrative contains a great deal of valuable information that is only understandable in a temporal context. <eos> events, time expressions, and temporal relations convey information about the time course of a patient ? s clinical record that must be understood for many applications of interest. <eos> in this paper, we focus on extracting information about how time expressions and events are related by narrative containers. <eos> we use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. <eos> our experiments show that using tree kernels in addition to standard feature kernels improves f1 classification for this task.
in order to integrate heterogeneous clinical information sources, semantically correlating information entities have to be linked. <eos> our discussions with radiologists revealed that anatomical entities with pathological findings are of particular interest when linking radiology text and images. <eos> previous research to identify pathological findings focused on simplistic approaches that recognize diseases or negated findings, but failed to establish a holistic approach. <eos> in this paper, we introduce our syntacto-semantic parsing approach to classify sentences in radiology reports as either pathological or non-pathological based on the findings they describe. <eos> although we operate with an incomplete, radlex-based linguistic resource, the obtained results show the effectiveness of our approach by identifying a recall value of 74.3 % for the classification task.
the various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as snomed ct. <eos> developing terminological resources manually is, however, prohibitively expensive and likely to result in low coverage, especially given the high variability of language use in clinical text. <eos> to support this process, distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms. <eos> in this paper, we exemplify the potential of our proposed method using the swedish version of snomed ct, which currently lacks synonyms. <eos> a medical expert inspects two thousand term pairs generated by two semantic spaces ? <eos> one of which models multiword terms in addition to single words ? <eos> for one hundred preferred terms of the semantic types disorder and finding.
in this paper, a new self ? training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically ? <eos> driven algorithm, ulisse. <eos> the method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines, which demonstrates its ability to capture both reliability of parses and domain ? <eos> specificity of linguistic constructions.
while interest in biomedical question answering has been growing, research in consumer health question answering remains relatively sparse. <eos> in this paper, we focus on the task of consumer health question understanding. <eos> we present a rule-based methodology that relies on lexical and syntactic information as well as anaphora/ellipsis resolution to construct structured representations of questions ( frames ). <eos> our results indicate the viability of our approach and demonstrate the important role played by anaphora and ellipsis in interpreting consumer health questions.
text mining methods for the biomedical domain have matured substantially and are currently being applied on a large scale to support a variety of applications in systems biology, pathway curation, data integration and gene summarization. <eos> community-wide challenges in the bionlp research field provide goldstandard datasets and rigorous evaluation criteria, allowing for a meaningful comparison between techniques as well as measuring progress within the field. <eos> however, such evaluations are typically conducted on relatively small training and test datasets. <eos> on a larger scale, systematic erratic behaviour may occur that severely influences hundreds of thousands of predictions. <eos> in this work, we perform a critical assessment of a large-scale text mining resource, identifying systematic errors and determining their underlying causes through semi-automated analyses and manual evaluations1.
it has long been realized that sublanguages are relevant to natural language processing and text mining. <eos> however, practical methods for recognizing or characterizing them have been lacking. <eos> this paper describes a publicly available set of tools for sublanguage recognition. <eos> closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model. <eos> scientific journal articles are compared to general english text, and it is shown that the journal articles fit the sublanguage model, while the general english text does not. <eos> a number of examples of implications of the sublanguage characteristics for natural language processing are pointed out. <eos> the software is made publicly available at [ edited for anonymization ].
rks derived from qualitative translations of bionlp shared task annotations ? juliane ? fluck1*, ? alexander ? klenner1, ? sumit ? madan1, ? sam ? ansari2, ? tamara ? bobic1,3, ? julia ? hoeng2, ? martin ? hofmann- ? <eos> ? apitius1,3, ? manuel ? c. <eos> ? peitsch2 ? 1fraunhofer ? institute ? for ? algorithms ? and ? scientific ? computing, ? schloss ? birlinghoven, ? sankt ? augustin, ? germany. <eos> ? 2philip ? morris ? international ? r & d, ? philip ? morris ? products ? s.a., ? quai ? jeanrenaud ? 5, ? 2000 ? neuch ? tel, ? switzerland. <eos> ? 3bonn- ? <eos> ? aachen ? international ? centre ? for ? information ? technology, ? dahlmannstr. <eos> ? 2, ? bonn, ? <eos> ? germany ? <eos> ? <eos> ? <eos> { jfluck, smadan, aklenner, tbobic, mhofmann-apitius } @ scai.fraunhofer.de, { sam.ansari, julia.hoeng, manuel.peitsch } @ pmi.com abstract interpreting the rapidly increasing amount of experimental data requires the availability and representation of biological knowledge in a computable form. <eos> the biological expres-sion language ( bel ) encodes the data in form of causal relationships, which describe the as-sociation between biological events. <eos> bel can successfully be applied to large data and sup-port causal reasoning and hypothesis genera-tion. <eos> with the rapid growth of biomedical litera-ture, automated methods are a crucial prereq-uisite for handling and encoding the available knowledge. <eos> the bionlp shared tasks support the development of such tools and provide a linguistically motivated format for the anno-tation of relations. <eos> on the other hand, bel statements and the corresponding evidence sentences might be a valuable resource for fu-ture bionlp shared task training data genera-tion. <eos> in this paper, we briefly introduce bel and investigate how far bionlp-shared task an-notations could be converted to bel state-ments and in such a way directly support bel statement generation. <eos> we present the first results of the automatic bel statement generation and emphasize the need for more training data that captures the underlying bio-logical meaning.
we present a set of new measures designed to reveal latent information of language use in children at the lexico-syntactic level. <eos> we used these metrics to analyze linguistic patterns in spontaneous narratives from children developing typically and children identified as having a language impairment. <eos> we observed significant differences in the z-scores of both populations for most of the metrics. <eos> these findings suggest we can use these metrics to aid in the task of language assessment in children.
sentence types typical to swedish clinical text were extracted by comparing sentence part-of-speech tag sequences in clinical and in standard swedish text. <eos> parsings by a syntactic dependency parser, trained on standard swedish, were manually analysed for the 33 sentence types most typical to clinical text. <eos> this analysis resulted in the identification of eight error types, and for two of these error types, preprocessing rules were constructed to improve the performance of the parser. <eos> for all but one of the ten sentence types affected by these two rules, the parsing was improved by pre-processing.
medline/pubmed contains structured abstracts that can provide argumentative labels. <eos> selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks. <eos> these abstracts make up less than one quarter of all the abstracts in medline/pubmed, so it is worthwhile to learn how to automatically label the non-structured ones. <eos> we have compared several machine learning algorithms trained on structured abstracts to identify argumentative labels. <eos> we have performed an intrinsic evaluation on predicting argumentative labels for non-structured abstracts and an extrinsic evaluation to predict argumentative labels on abstracts relevant to gene reference into function ( generif ) indexing. <eos> intrinsic evaluation shows that argumentative labels can be assigned effectively to structured abstracts. <eos> algorithms that model the argumentative structure seem to perform better than other algorithms. <eos> extrinsic results show that assigning argumentative labels to non-structured abstracts improves the performance on generif indexing. <eos> on the other hand, the algorithms that model the argumentative structure of the abstracts obtain lower performance in the extrinsic evaluation.
child language narratives are used for language analysis, measurement of language development, and the detection of language impairment. <eos> in this paper, we explore the use of latent dirichlet allocation ( lda ) for detecting topics from narratives, and use the topics derived from lda in two classification tasks : automatic prediction of coherence and language impairment. <eos> our experiments show lda is useful for detecting the topics that correspond to the narrative structure. <eos> we also observed improved performance for the automatic prediction of coherence and language impairment when we use features derived from the topic words provided by lda.
in this paper we take a fresh look at parallels between linguistics and biology. <eos> we expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues.
the genia event extraction task is organized for the third time, in bionlp shared task 2013. <eos> toward knowledge based construction, the task is modified in a number of points. <eos> as the final results, it received 12 submissions, among which 2 were withdrawn from the final report. <eos> this paper presents the task setting, data sets, and the final results with discussion for possible future directions.
we participate in the bionlp 2013 shared task with turku event extraction system ( tees ) version 2.1. <eos> tees is a support vector machine ( svm ) based text mining system for the extraction of events and relations from natural language texts. <eos> in version 2.1 we introduce an automated annotation scheme learning system, which derives task-specific event rules and constraints from the training data, and uses these to automatically adapt the system for new corpora with no additional programming required. <eos> tees 2.1 is shown to have good generalizability and good performance across the bionlp 2013 task corpora, achieving first place in four out of eight tasks.
during the past few years, several novel text mining algorithms have been developed in the context of the bionlp shared tasks on event extraction. <eos> these algorithms typically aim at extracting biomolecular interactions from text by inspecting only the context of one sentence. <eos> however, when humans interpret biomolecular research articles, they usually build upon extensive background knowledge of their favorite genes and pathways. <eos> to make such world knowledge available to a text mining algorithm, it could first be applied to all available literature to subsequently make a more informed decision on which predictions are consistent with the current known data. <eos> in this paper, we introduce our participation in the latest shared task using the largescale text mining resource evex which we previously implemented using state-ofthe-art algorithms, and which was applied to the whole of pubmed and pubmed central. <eos> we participated in the genia event extraction ( ge ) and gene regulation network ( grn ) tasks, ranking first in the former and fifth in the latter.
the genia event ( ge ) extraction task of the bionlp shared task addresses the extraction of biomedical events from the natural language text of the published literature. <eos> in our submission, we modified an existing system for learning of event patterns via dependency parse subgraphs to utilise a more accurate parser and significantly more, but noisier, training data. <eos> we explore the impact of these two aspects of the system and conclude that the change in parser limits recall to an extent that can not be offset by the large quantities of training data. <eos> however, our extensions of the system to extract modification events shows promise.
this paper describes the hds4nlp entry to the bionlp 2013 shared task on biomedical event extraction. <eos> this system is based on a pairwise model that transforms trigger classification in a simple multi-class problem in place of the usual multi-label problem. <eos> this model facilitates inference compared to global models while relying on richer information compared to usual pipeline approaches. <eos> the hds4nlp system ranked 6th on the genia task ( 43.03 % f-score ), and after fixing a bug discovered after the final submission, it outperforms the winner of this task ( with a f-score of 51.15 % ).
we present the design, preparation, results and analysis of the cancer genetics ( cg ) event extraction task, a main task of the bionlp shared task ( st ) 2013. <eos> the cg task is an information extraction task targeting the recognition of events in text, represented as structured n-ary associations of given physical entities. <eos> in addition to addressing the cancer domain, the cg task is differentiated from previous event extraction tasks in the bionlp st series in addressing a wide range of pathological processes and multiple levels of biological organization, ranging from the molecular through the cellular and organ levels up to whole organisms. <eos> final test set submissions were accepted from six teams. <eos> the highest-performing system achieved an fscore of 55.4 %. <eos> this level of performance is broadly comparable with the state of the art for established molecular-level extraction tasks, demonstrating that event extraction resources and methods generalize well to higher levels of biological organization and are applicable to the analysis of scientific texts on cancer. <eos> the cg task continues as an open challenge to all interested parties, with tools and resources available from http : //2013. <eos> bionlp-st.org/.
we present the pathway curation ( pc ) task, a main event extraction task of the bionlp shared task ( st ) 2013. <eos> the pc task concerns the automatic extraction of biomolecular reactions from text. <eos> the task setting, representation and semantics are defined with respect to pathway model standards and ontologies ( sbml, biopax, sbo ) and documents selected by relevance to specific model reactions. <eos> two bionlp st 2013 participants successfully completed the pc task. <eos> the highest achieved fscore, 52.8 %, indicates that event extraction is a promising approach to supporting pathway curation efforts. <eos> the pc task continues as an open challenge with data, resources and tools available from http : //2013.bionlp-st.org/
we participated in the bionlp 2013 shared tasks, addressing the genia ( ge ) and the cancer genetics ( cg ) event extraction tasks. <eos> our event extraction is based on the system we recently proposed for mining relations and events involving genes or proteins in the biomedical literature using a novel, approximate subgraph matching-based approach. <eos> in addition to handling the ge task involving 13 event types uniformly related to molecular biology, we generalized our system to address the cg task targeting a challenging set of 40 event types related to cancer biology with various arguments involving 18 kinds of biological entities. <eos> moreover, we attempted to integrate a distributional similarity model into our system to extend the graph matching scheme for more events. <eos> in addition, we evaluated the impact of using paths of all possible lengths among event participants as key contextual dependencies to extract potential events as compared to using only the shortest paths within the framework of our system. <eos> we achieved a 46.38 % f-score in the cg task and a 48.93 % f-score in the ge task, ranking 3rd and 4th respectively. <eos> the consistent performance confirms that our system generalizes well to various event extraction tasks and scales to handle a large number of event and entity types.
we tested a linguistically motivated rulebased system in the cancer genetics task of the bionlp13 shared task challenge. <eos> the performance of the system was very moderate, ranging from 52 % against the development set to 45 % against the test set. <eos> interestingly, the performance of the system did not change appreciably when using only entities tagged by the inbuilt tagger as compared to performance using the gold-tagged entities. <eos> the lack of an event anaphoric module, as well as problems in reducing events generated by a large trigger class to the task-specific event subset, were likely major contributory factors to the rather moderate performance.
this paper describes nactem entries for the cancer genetics ( cg ) and pathway curation ( pc ) tasks in the bionlp shared task 2013. <eos> we have applied a state-ofthe-art event extraction system eventmine to the tasks in two different settings : a single-corpus setting for the cg task and a stacking setting for the pc task. <eos> eventmine was applicable to the two tasks with simple task specific configuration, and it produced a reasonably high performance, positioning second in the cg task and first in the pc task.
this paper describes the technical contribution of the supporting resources provided for the bionlp shared task 2013. <eos> following the tradition of the previous two bionlp shared task events, the task organisers and several external groups sought to make system development easier for the task participants by providing automatically generated analyses using a variety of automated tools. <eos> providing analyses created by different tools that address the same task also enables extrinsic evaluation of the tools through the evaluation of their contributions to the event extraction task. <eos> such evaluation can improve understanding of the applicability and benefits of specific tools and representations. <eos> the supporting resources described in this paper will continue to be publicly available from the shared task homepage http : //2013.bionlp-st.org/
in this paper we present a biomedical event extraction system for the bionlp 2013 event extraction task. <eos> our system consists of two phases. <eos> in the learning phase, a dictionary and patterns are generated automatically from annotated events. <eos> in the extraction phase, the dictionary and obtained patterns are applied to extract events from input text. <eos> when evaluated on the genia event extraction task of the bionlp 2013 shared task, the system obtained the best results on strict matching and the third best on approximate span and recursive matching, with f-scores of 48.92 and 50.68, respectively. <eos> moreover, it has excellent performance in terms of speed.
we describe a system for extracting biomedical events among genes and proteins from biomedical literature, using the corpus from the bionlp ? 13 shared task on event extraction. <eos> the proposed system is characterized by a wide array of features based on dependency parse graphs and additional argument information in the second trigger detection. <eos> based on the uturku system which is the best one in the bionlp ? 09 shared task, we improve the performance of biomedical event extraction by reducing illegal events and false positives in the second trigger detection and the second argument detection. <eos> on the development set of bionlp ? 13, the system achieves an f-score of 50.96 % on the primary task. <eos> on the test set of bionlp ? 13, it achieves an f-score of 47.56 % on the primary task obtaining the 5th place in task 1, which is 1.78 percentage points higher than the baseline ( following the uturku system ), demonstrating that the proposed method is efficient.
we describe a biological event detection method implemented for the genia event extraction task of bionlp 2013. <eos> the method relies on syntactic dependency relations provided by a general nlp pipeline, supported by statistics derived from maximum entropy models for candidate trigger words, for potential arguments, and for argument frames.
in this paper we propose a system which uses hybrid methods that combine both rule-based and machine learning ( ml ) -based approaches to solve genia event extraction of bionlp shared task 2013. <eos> we apply uima1 framework to support coding. <eos> there are three main stages in model : pre-processing, trigger detection and biomedical event detection. <eos> we use dictionary and support vector machine classifier to detect event triggers. <eos> event detection is applied on syntactic patterns which are combined with features extracted for classification.
we describe our system to extract genia events that was developed for the bionlp 2013 shared task. <eos> our system uses a supervised information extraction platform based on support vector machines ( svm ) and separates the process of event classification into multiple stages. <eos> for each event type the svm parameters are adjusted and feature selection carried out. <eos> we find that this optimisation improves the performance of our approach. <eos> overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on f-measure ( strict matching ).
we describe a high precision system for extracting events of biomedical significance that was developed during the bionlp shared task 2013 and tested on the cancer genetics data set. <eos> the system achieved an f-score on the development data of 73.67 but was ranked 5th out of six with an f-score of 29.94 on the test data. <eos> however, precision was the second highest ranked on the task at 62.73. <eos> analysis suggests the need to continue to improve our system for complex events particularly taking into account cross-domain differences in argument distributions.
the bionlp shared task 2013 is organised to further advance the field of information extraction in biomedical texts. <eos> this paper describes our entry in the gene regulation network in bacteria ( grn ) part, for which our system finished in second place ( out of five ). <eos> to tackle this relation extraction task, we employ a basic support vector machine framework. <eos> we discuss our findings in constructing local and contextual features, that augment our precision with as much as 7.5 %. <eos> we touch upon the interaction type hierarchy inherent in the problem, and the importance of the evaluation procedure to encourage exploration of that structure.
in the perspective of annotating a text with respect to an ontology, we have participated in the subtask 1 of the bb bionlpst whose aim is to detect, in the text, bacteria habitats and associate to them one or several categories from the ontobiotope ontology provided for the task. <eos> we have used a rule-based machine learning algorithm ( whisk ) combined with a rule-based automatic ontology projection method and a rote learning technique. <eos> the combination of these three sources of rules leads to good results with a ser measure close to the winner and a best f-measure.
in this paper, we present the methods we used to extract bacteria and biotopes names and then to identify the relation between those entities while participating to the bionlp ? 13 bacteria and biotopes shared task. <eos> we used machine-learning based approaches for this task, namely a crf to extract bacteria and biotopes names and a simple matching algorithm to predict the relations. <eos> we achieved poor results : an ser of 0.66 in sub-task 1, and a 0.06 f-measure in both sub-tasks 2 and 3.
ssy, philippe bessi ? res, claire n ? dellec unit ? <eos> math ? matique, informatique et g ? nome institut national de la recherche agronomique ur1077, f78352 jouy-en-josas, france forename.name @ jouy.inra.fr abstract the goal of the genic regulation network task ( grn ) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium bacillus subtilis. <eos> it is an extension of the bi task of bionlp-st ? 11. <eos> the corpus is composed of sentences selected from publicly available pubmed scientific abstracts. <eos> the paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.
ssy1, wiktoria golik1, zorana ratkovic1,2, philippe bessi ? res1, claire n ? dellec1 1unit ? <eos> math ? matique, informatique et g ? nome mig inra ur1077 ? <eos> f-78352 jouy-en-josas ? <eos> france 2lattice umr 8094 cnrs, 1 rue maurice arnoux, f-92120 montrouge ? <eos> france forename.name @ jouy.inra.fr abstract this paper presents the bacteria biotope task of the bionlp shared task 2013, which follows bionlp-st-11. <eos> the bacteria biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the ontobiotope ontology. <eos> bacteria locations are crucial knowledge in biology for phenotype studies. <eos> the paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.
the absence of a comprehensive database of locations where bacteria live is an important obstacle for biologists to understand and study the interactions between bacteria and their habitats. <eos> this paper reports the results to a challenge, set forth by the bacteria biotopes task of the bionlp shared task 2013. <eos> two systems are explained : sub-task 1 system for identifying habitat mentions in unstructured biomedical text and normalizing them through the ontobiotope ontology and sub-task 2 system for extracting localization and partof relations between bacteria and habitats. <eos> both approaches rely on syntactic rules designed by considering the shallow linguistic analysis of the text. <eos> sub-task 2 system also makes use of discourse-based rules. <eos> the two systems achieve promising results on the shared task test data set.
published literature in molecular genetics may collectively provide much information on gene regulation networks. <eos> dedicated computational approaches are required to sip through large volumes of text and infer gene interactions. <eos> we propose a novel sieve-based relation extraction system that uses linear-chain conditional random fields and rules. <eos> also, we introduce a new skip-mention data representation to enable distant relation extraction using first-order models. <eos> to account for a variety of relation types, multiple models are inferred. <eos> the system was applied to the bionlp 2013 gene regulation network shared task. <eos> our approach was ranked first of five, with a slot error rate of 0.73.
this paper describes the information extraction techniques developed in the framework of the participation of irisatexmex to the following bionlp-st13 tasks : bacterial biotope subtasks 1 and 2, and graph regulation network. <eos> the approaches developed are general-purpose ones and do not rely on specialized preprocessing, nor specialized external data, and they are expected to work independently of the domain of the texts processed. <eos> they are classically based on machine learning techniques, but we put the emphasis on the use of similarity measures inherited from the information retrieval domain ( okapi-bm25 ( robertson et al, 1998 ), language modeling ( hiemstra, 1998 ) ). <eos> through the good results obtained for these tasks, we show that these simple settings are competitive provided that the representation and similarity chosen are well suited for the task.
statistical natural language generation from abstract meaning representations presupposes large corpora consisting of text ? meaning pairs. <eos> even though such corpora exist nowadays, or could be constructed using robust semantic parsing, the simple alignment between text and meaning representation is too coarse for developing robust ( statistical ) nlg systems. <eos> by reformatting semantic representations as graphs, fine-grained alignment can be obtained. <eos> given a precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule.
the increasing amount of machinereadable data available in the context of the semantic web creates a need for methods that transform such data into human-comprehensible text. <eos> in this paper we develop and evaluate a natural language generation ( nlg ) system that converts rdf data into natural language text based on an ontology and an associated ontology lexicon. <eos> while it follows a classical nlg pipeline, it diverges from most current nlg systems in that it exploits an ontology lexicon in order to capture context-specific lexicalisations of ontology concepts, and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-specific corpus. <eos> we apply the developed approach to the cooking domain, providing both an ontology and an ontology lexicon in lemon format. <eos> finally, we evaluate fluency and adequacy of the generated recipes with respect to two target audiences : cooking novices and advanced cooks.
in this paper we describe a natural language generation system which produces complex sentences from a biology knowledge base. <eos> the nlg system allows domain experts to discover errors in the knowledge base and generates certain parts of answers in response to users ? <eos> questions in an e-textbook application. <eos> the system allows domain experts to customise its lexical resources and to set parameters which influence syntactic constructions in generated sentences. <eos> the system is capable of dealing with certain types of incomplete inputs arising from a knowledge base which is constantly edited and includes a referring expression generation module which keeps track of discourse history. <eos> our referring expression module is available for download as the open source antfarm tool1.
we show that nakatsu & white ? s ( 2010 ) proposed enhancements to the sparky restaurant corpus ( src ; walker et al, 2007 ) for better expressing contrast do indeed make it possible to generate better texts, including ones that make effective and varied use of contrastive connectives and discourse adverbials. <eos> after first presenting a validation experiment for naturalness ratings of src texts gathered using amazon ? s mechanical turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. <eos> we conclude with a discussion of possible ways of improving the ranker in future work.
in this paper, we focus on the task of generating elliptic sentences. <eos> we extract from the data provided by the surface realisation ( sr ) task ( belz et al, 2011 ) 2398 input whose corresponding output sentence contain an ellipsis. <eos> we show that 9 % of the data contains an ellipsis and that both coverage and bleu score markedly decrease for elliptic input ( from 82.3 % coverage for non-elliptic sentences to 65.3 % for elliptic sentences and from 0.60 bleu score to 0.47 ). <eos> we argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the sr data. <eos> finally, we evaluate an existing surface realiser on the resulting dataset. <eos> we show that, after rewriting, the generator achieves a coverage of 76 % and a bleu score of 0.74 on the elliptical data.
we present an integer linear programming model of content selection, lexicalization, and aggregation that we developed for a system that generates texts from owl ontologies. <eos> unlike pipeline architectures, our model jointly considers the available choices in these three text generation stages, to avoid greedy decisions and produce more compact texts. <eos> experiments with two ontologies confirm that it leads to more compact texts, compared to a pipeline with the same components, with no deterioration in the perceived quality of the generated texts. <eos> we also present an approximation of our model, which allows longer texts to be generated efficiently.
planning-based approaches to reference provide a uniform treatment of linguistic decisions, from content selection to lexical choice. <eos> in this paper, we show how the issues of lexical ambiguity, vagueness, unspecific descriptions, ellipsis, and the interaction of subsective modifiers can be expressed using a belief-state planner modified to support context-dependent actions. <eos> because the number of distinct denotations it searches grows doublyexponentially with the size of the referential domain, we present representational and search strategies that make generation and interpretation tractable.
the netherlandsabstract when they introduced the graph-based algorithm ( gba ) for referring expression generation, krahmer et al ( 2003 ) flaunted the natural way in which it deals with relations between objects ; but this feature has never been tested empirically. <eos> we fill this gap in this paper, exploring referring expression generation from the perspective of the gba and focusing in particular on generating human-like expressions in visual scenes with spatial relations. <eos> we compare the original gba against a variant that we introduce to better reflect human reference, and find that although the original gba performs reasonably well, our new algorithm offers an even better match to human data ( 77.91 % dice ). <eos> further, it can be extended to capture speaker variation, reaching an 82.83 % dice overlap with human-produced expressions.
pointing gestures are pervasive in human referring actions, and are often combined with spoken descriptions. <eos> combining gesture and speech naturally to refer to objects is an essential task in multimodal nlg systems. <eos> however, the way gesture and speech should be combined in a referring act remains an open question. <eos> in particular, it is not clear whether, in planning a pointing gesture in conjunction with a description, an nlg system should seek to minimise the redundancy between them, e.g. <eos> by letting the pointing gesture indicate locative information, with other, nonlocative properties of a referent included in the description. <eos> this question has a bearing on whether the gestural and spoken parts of referring acts are planned separately or arise from a common underlying computational mechanism. <eos> this paper investigates this question empirically, using machine-learning techniques on a new corpus of dialogues involving multimodal references to objects. <eos> our results indicate that human pointing strategies interact with descriptive strategies. <eos> in particular, pointing gestures are strongly associated with the use of locative features in referring expressions.
in this overview paper we present the outcome of the first content selection challenge from open semantic web data, focusing mainly on the preparatory stages for defining the task and annotating the data. <eos> the task to perform was described in the challenge ? s call as follows : given a set of rdf triples containing facts about a celebrity, select those triples that are reflected in the target text ( i.e., a short biography about that celebrity ). <eos> from the initial nine expressions of interest, finally two participants submitted their systems for evaluation.
when instructors prepare learning materials for students, they frequently develop accompanying questions to guide learning. <eos> natural language processing technology can be used to automatically generate such questions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs. <eos> we introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning. <eos> while we have not yet incorporated the full learning context into our approach, our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning.
we describe a statistical natural language generation ( nlg ) method for summarisation of time-series data in the context of feedback generation for students. <eos> in this paper, we initially present a method for collecting time-series data from students ( e.g. <eos> marks, lectures attended ) and use example feedback from lecturers in a datadriven approach to content selection. <eos> we show a novel way of constructing a reward function for our reinforcement learning agent that is informed by the lecturers ? <eos> method of providing feedback. <eos> we evaluate our system with undergraduate students by comparing it to three baseline systems : a rule-based system, lecturerconstructed summaries and a brute force system. <eos> our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers. <eos> our findings suggest that the learning agent needs to take into account both the student and lecturers ? <eos> preferences.
this study is conducted in the area of multidocument summarization, and develops a literature review framework based on a deconstruction of human-written literature review sections in information science research papers. <eos> the first part of the study presents the results of a multi-level discourse analysis to investigate their discourse and content characteristics. <eos> these findings were incorporated into a framework for literature reviews, focusing on their macro-level document structure and the sentence-level templates, as well as the information summarization strategies. <eos> the second part of this study discusses insights from this analysis, and how the framework can be adapted to automatic summaries resembling human written literature reviews. <eos> summaries generated from a partial implementation are evaluated against human written summaries and assessors ? <eos> comments are discussed to formulate recommendations for future work.
we propose a novel end-to-end framework for abstractive meeting summarization. <eos> we cluster sentences in the input into communities and build an entailment graph over the sentence communities to identify and select the most relevant sentences. <eos> we then aggregate those selected sentences by means of a word graph model. <eos> we exploit a ranking strategy to select the best path in the word graph as an abstract sentence. <eos> despite not relying on the syntactic structure, our approach significantly outperforms previous models for meeting summarization in terms of informativeness. <eos> moreover, the longer sentences generated by our method are competitive with shorter sentences generated by the previous word graph model in terms of grammaticality.
this paper focuses on a subtask of natural language generation ( nlg ), voice selection, which decides whether a clause is realised in the active or passive voice according to its contextual information. <eos> automatic voice selection is essential for realising more sophisticated mt and summarisation systems, because it impacts the readability of generated texts. <eos> however, to the best of our knowledge, the nlg community has been less concerned with explicit voice selection. <eos> in this paper, we propose an automatic voice selection model based on various linguistic information, ranging from lexical to discourse information. <eos> our empirical evaluation using a manually annotated corpus in japanese demonstrates that the proposed model achieved 0.758 in f-score, outperforming the two baseline models.
the cross-disciplinary mime project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the first person on scene and ambulance clinicians. <eos> nlg is used to produce a textual handover report at any time, summarising data from novel medical sensors, as well as observations and actions recorded by the carer. <eos> we describe the mime project with a focus on the nlg algorithm and an initial evaluation of the generated reports.
we present the results from an elicitation experiment in which human speakers were asked to produced quantified referring expressions ( qres ), as in ? the crate with 10 apples ?, ? the crate with many apples ?, etc. <eos> these results suggest that some subtle contextual factors govern the choice between different types of qres, and that numerals are highly preferred for subitizable quantities despite the availability of coarser-grained expressions.
in this paper we present the preliminary work of a basque poetry generation system. <eos> basically, we have extracted the pos-tag sequences from some verse corpora and calculated the probability of each sequence. <eos> for the generation process we have defined 3 different experiments : based on a strophe from the corpora, we ( a ) replace each word with other according to its pos-tag and suffixes, ( b ) replace each noun and adjective with another equally inflected word and ( c ) replace only nouns with semantically related ones ( inflected ). <eos> finally we evaluate those strategies using a turing test-like evaluation.
we present first results of our project on the generation of contextually adequate greeting exchanges in video role playing games. <eos> to make greeting exchanges computable, an analysis of the factors influencing greeting behavior as well as the factors influencing greeting exchanges is given. <eos> based on the politeness model proposed by brown & levinson ( 1987 ) we develop a simple algorithm for the generation of greeting exchanges. <eos> an evaluation, comparing dialog from the video role playing game skyrim to dialog determined by our algorithm, shows that our algorithm is able to generate greeting exchanges that are contextually more adequate than those featured by skyrim.
this paper introduces the problem of generating descriptions of n-dimensional spatial data by decomposing it via modelbased clustering. <eos> i apply the approach to the error function of supervised classification algorithms, a practical problem that uses natural language generation for understanding the behaviour of a trained classifier. <eos> i demonstrate my system on a dataset taken from conll shared tasks.
we introduce gennext, an nlg system designed specifically to adapt quickly and easily to different domains. <eos> given a domain corpus of historical texts, gennext allows the user to generate a template bank organized by semantic concept via derived discourse representation structures in conjunction with general and domain-specific entity tags. <eos> based on various features collected from the training corpus, the system statistically learns template representations and document structure and produces well ? formed texts ( as evaluated by crowdsourced and expert evaluations ). <eos> in addition to domain adaptation, gennext ? s hybrid approach significantly reduces complexity as compared to traditional nlg systems by relying on templates ( consolidating micro-planning and surface realization ) and minimizing the need for domain experts. <eos> in this description, we provide details of gennext ? s theoretical perspective, architecture and evaluations of output.
c vaudry and guy lapalme rali-diro ? <eos> universit ? <eos> de montr ? al c.p. <eos> 6128, succ. <eos> centre-ville montr ? al, qu ? bec, canada, h3c 3j8 { vaudrypl, lapalme } @ iro.umontreal.ca abstract this paper describes simplenlg-enfr, an adaption of the english realisation engine simplenlg ( gatt and reiter, 2009 ) for bilin-gual english-french realisation. <eos> grammatical similarities between english and french that could be exploited and specifics of french that needed adaptation are discussed.
paraphrasing is expressing the same semantic content using different linguistic means. <eos> although previous work has addressed linguistic variations at different levels of language, paraphrasing in turkish has not been yet thoroughly studied. <eos> this paper presents the first study towards turkish paraphrase alignment. <eos> we perform an analysis of different types of paraphrases on a modest turkish paraphrase corpus and present preliminary results on that analysis from different standpoints. <eos> we also explore the impact of human interpretation of paraphrasing on the alignment of paraphrase sentence pairs.
this position paper presents an on-going work on a natural language generation framework that is particularly tailored for summary text generation from body area networks. <eos> we present an overview of the main challenges when considering this type of sensor devices used for at home monitoring of health parameters. <eos> this paper describes the first steps towards the implementation of a system which collects information from heart rate and respiration rate using a wearable sensor. <eos> the paper further outlines the direction for future work and in particular the challenges for nlg in this application domain.
we present the first prototype of a handover report generator developed for the mime ( managing information in medical emergencies ) project. <eos> nlg applications in the medical domain have been varied but most are deployed in clinical situations. <eos> we develop a mobile device for prehospital care which receives streamed sensor data and user input, and converts these into a handover report for paramedics.
this demo showcases thoughtland, an end-to-end system that takes training data and a selected machine learning model, produces a cloud of points via crossvalidation to approximate its error function, then uses model-based clustering to identify interesting components of the error function and natural language generation to produce an english text summarizing the error function. <eos>
this paper described uic-csc, the entry we submitted for the content selection challenge 2013. <eos> our model consists of heuristic rules based on co-occurrences of predicates in the training data.
we present the results of the wmt13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. <eos> this year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. <eos> an additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. <eos> the quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries.
this paper presents the results of the wmt13 metrics shared task. <eos> we asked participants of this task to score the outputs of the mt systems involved in wmt13 shared translation task. <eos> we collected scores of 16 metrics from 8 research groups. <eos> in addition to that we computed scores of 5 standard metrics such as bleu, wer, per as baselines. <eos> collected scores were evaluated in terms of system level correlation ( how well each metric ? s scores correlate with wmt13 official human scores ) and in terms of segment level correlation ( how often a metric agrees with humans in comparing two translations of a particular sentence ).
there has been a recent surge of interest in semantic machine translation, which standard automatic metrics struggle to evaluate. <eos> a family of measures called meant has been proposed which uses semantic role labels ( srl ) to overcome this problem. <eos> the human variant, hmeant, has largely been evaluated using correlation with human contrastive evaluations, the standard human evaluation metric for the wmt shared tasks. <eos> in this paper we claim that for a human metric to be useful, it needs to be evaluated on intrinsic properties. <eos> it needs to be reliable ; it needs to work across different language pairs ; and it needs to be lightweight. <eos> most importantly, however, a human metric must be discerning. <eos> we conclude that hmeant is a step in the right direction, but has some serious flaws. <eos> the reliance on verbs as heads of frames, and the assumption that annotators need minimal guidelines are particularly problematic.
this paper describes limsi ? s submissions to the shared wmt ? 13 translation task. <eos> we report results for french-english, german-english and spanish-english in both directions. <eos> our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. <eos> the main novelties of this year ? s participation are the following : our first participation to the spanish-english task ; experiments with source pre-ordering ; a tighter integration of continuous space language models using artificial text generation ( for german ) ; and the use of different tuning sets according to the original language of the text to be translated.
we describe the cmu systems submitted to the 2013 wmt shared task in machine translation. <eos> we participated in three language pairs, french ? english, russian ? <eos> english, and english ? russian. <eos> our particular innovations include : a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create ? synthetic translation options ? <eos> that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context.
we use feature decay algorithms ( fda ) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction. <eos> we develop parallel fda for solving computational scalability problems caused by the abundance of training data for smt models and lm models and still achieve smt performance that is on par with using all of the training data or better. <eos> parallel fda runs separate fda models on randomized subsets of the training data and combines the instance selections later. <eos> parallel fda can also be used for selecting the lm corpus based on the training set selected by parallel fda. <eos> the high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing smt systems. <eos> the relevancy of the selected lm corpus can reach up to 86 % reduction in the number of oov tokens and up to 74 % reduction in the perplexity. <eos> we perform smt experiments in all language pairs in the wmt13 translation task and obtain smt performance close to the top systems using significantly less resources for training and development.
we describe our experiments with phrase-based machine translation for the wmt 2013 shared task. <eos> we trained one system for 18 translation directions between english or czech on one side and english, czech, german, spanish, french or russian on the other side. <eos> we describe a set of results with different training data sizes and subsets. <eos> for the pairs containing russian, we describe a set of independent experiments with slightly different translation models.
this paper describes our wmt submissions cu-bojar and cu-depfix, the latter dubbed ? chimera ? <eos> because it combines on three diverse approaches : tectomt, a system with transfer at the deep syntactic level of representation, factored phrase-based translation using moses, and finally automatic rule-based correction of frequent grammatical and meaning errors. <eos> we do not use any off-the-shelf systemcombination method.
this paper describes the english-russian and russian-english statistical machine translation ( smt ) systems developed at yandex school of data analysis for the shared translation task of the acl 2013 eighth workshop on statistical machine translation. <eos> we adopted phrase-based smt approach and evaluated a number of different techniques, including data filtering, spelling correction, alignment of lemmatized word forms and transliteration. <eos> altogether they yielded +2.0 and +1.5 bleu improvement for ru-en and enru language pairs. <eos> we also report on the experiments that did not have any positive effect and provide an analysis of the problems we encountered during the development of our systems.
this paper describes the phrase-based smt systems developed for our participation in the wmt13 shared translation task. <eos> translations for english ? german and english ? french were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-ofspeech ( pos ) and automatic cluster language models and discriminative word lexica ( dwl ). <eos> in addition, we combined reordering models on different sentence abstraction levels.
this paper describes t ? ub ? itak-b ? ilgem statistical machine translation ( smt ) systems submitted to the eighth workshop on statistical machine translation ( wmt ) shared translation task for german-english language pair in both directions. <eos> we implement phrase-based smt systems with standard parameters. <eos> we present the results of using a big tuning data and the effect of averaging tuning weights of different seeds. <eos> additionally, we performed a linguistically motivated compound splitting in the germanto-english smt system.
this paper describes munich-edinburghstuttgart ? s submissions to the eighth workshop on statistical machine translation. <eos> we report results of the translation tasks from german, spanish, czech and russian into english and from english to german, spanish, czech, french and russian. <eos> the systems described in this paper use osm ( operation sequence model ). <eos> we explain different pre-/post-processing steps that we carried out for different language pairs. <eos> for german-english we used constituent parsing for reordering and compound splitting as preprocessing steps. <eos> for russian-english we transliterated the unknown words. <eos> the transliteration system is learned with the help of an unsupervised transliteration mining algorithm.
we present the system we developed to provide efficient large-scale feature-rich discriminative training for machine translation. <eos> we describe how we integrate with mapreduce using hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. <eos> we report our findings on german-english and russianenglish translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data.
this paper describes the talp participation in the wmt13 evaluation campaign. <eos> our participation is based on the combination of several statistical machine translation systems : based on standard phrasebased moses systems. <eos> variations include techniques such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. <eos> the results show a coherent improvement on ter, meteor, nist, and bleu scores when compared to our baseline system.
we present two english-to-czech systems that took part in the wmt 2013 shared task : tectomt and phrasefix. <eos> the former is a deep-syntactic transfer-based system, the latter is a more-or-less standard statistical post-editing ( spe ) applied on top of tectomt. <eos> in a brief survey, we put spe in context with other system combination techniques and evaluate spe vs. another simple system combination technique : using synthetic parallel data from tectomt to train a statistical mt system ( smt ). <eos> we confirm that phrasefix ( spe ) improves the output of tectomt, and we use this to analyze errors in tectomt. <eos> however, we also show that extending data for smt is more effective.
we describe the stanford university nlp group submission to the 2013 workshop on statistical machine translation shared task. <eos> we demonstrate the effectiveness of a new adaptive, online tuning algorithm that scales to large feature and tuning sets. <eos> for both english-french and english-german, the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods.
we describe the lia machine translation systems for the russian-english and english-russian translation tasks. <eos> various factored translation systems were built using moses to take into account the morphological complexity of russian and we experimented with the romanization of untranslated russian words.
this paper describes omnifluenttm translate ? <eos> a state-of-the-art hybrid mt system capable of high-quality, high-speed translations of text and speech. <eos> the system participated in the english-to-french and russian-to-english wmt evaluation tasks with competitive results. <eos> the features which contributed the most to high translation quality were training data sub-sampling methods, document-specific models, as well as rule-based morphological normalization for russian. <eos> the latter improved the baseline russian-to-english bleu score from 30.1 to 31.3 % on a heldout test set.
we propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order. <eos> this is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order, driven by a classifier trained on a parallel corpus. <eos> our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features.
this paper describes shallow semantically-informed hierarchical phrase-based smt ( hpbsmt ) and phrase-based smt ( pbsmt ) systems developed at dublin city university for participation in the translation task between en-es and es-en at the workshop on statistical machine translation ( wmt 13 ). <eos> the system uses pbsmt and hpbsmt decoders with multiple lms, but will run only one decoding path decided before starting translation. <eos> therefore the paper does not present a multi-engine system combination. <eos> we investigate three types of shallow semantics : ( i ) quality estimation ( qe ) score, ( ii ) genre id, and ( iii ) context id derived from context-dependent language models. <eos> our results show that the improvement is 0.8 points absolute ( bleu ) for en-es and 0.7 points for es-en compared to the standard pbsmt system ( single best system ). <eos> it is important to note that we developed this method when the standard ( confusion network-based ) system combination is ineffective such as in the case when the input is only two.
this paper describes the joint submission of the quaero project for the german ? english translation task of the acl 2013 eighth workshop on statistical machine translation ( wmt 2013 ). <eos> the submission was a system combination of the output of four different translation systems provided by rwth aachen university, karlsruhe institute of technology ( kit ), limsi-cnrs and systran software, inc. <eos> the translations were joined using the rwth ? s system combination approach. <eos> experimental results show improvements of up to 1.2 points in bleu and 1.2 points in ter compared to the best single translation.
this paper describes the statistical machine translation ( smt ) systems developed at rwth aachen university for the translation task of the acl 2013 eighth workshop on statistical machine translation ( wmt 2013 ). <eos> we participated in the evaluation campaign for the french-english and german-english language pairs in both translation directions. <eos> both hierarchical and phrase-based smt systems are applied. <eos> a number of different techniques are evaluated, including hierarchical phrase reordering, translation model interpolation, domain adaptation techniques, weighted phrase extraction, word class language model, continuous space language model and system combination. <eos> by application of these methods we achieve considerable improvements over the respective baseline systems.
this paper describes the university of cambridge submission to the eighth workshop on statistical machine translation. <eos> we report results for the russianenglish translation task. <eos> we use multiple segmentations for the russian input language. <eos> we employ the hadoop framework to extract rules. <eos> the decoder is hifst, a hierarchical phrase-based decoder implemented using weighted finitestate transducers. <eos> lattices are rescored with a higher order language model and minimum bayes-risk objective.
we describe improvements made over the past year to joshua, an open-source translation system for parsing-based machine translation. <eos> the main contributions this past year are significant improvements in both speed and usability of the grammar extraction and decoding steps. <eos> we have also rewritten the decoder to use a sparse feature representation, enabling training of large numbers of features with discriminative training methods.
this paper presents the experiments conducted by the machine translation group at dcu and prompsit language engineering for the wmt13 translation task. <eos> three language pairs are considered : spanishenglish and french-english in both directions and german-english in that direction. <eos> for the spanish-english pair, the use of linguistic information to select parallel data is investigated. <eos> for the frenchenglish pair, the usefulness of the small indomain parallel corpus is evaluated, compared to an out-of-domain parallel data sub-sampling method. <eos> finally, for the german-english system, we describe our work in addressing the long distance reordering problem and a system combination strategy.
this paper describes qcri-mes ? s submission on the english-russian dataset to the eighth workshop on statistical machine translation. <eos> we generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to giza++ and build a phrase-based machine translation system. <eos> for tuning, we use a variation of pro which provides better weights by optimizing bleu+1 at corpus-level. <eos> we transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. <eos> for the russian to english translation direction, we apply linguistically motivated pre-processing on the russian side of the data.
we describe the uppsala university system for wmt13, for english-to-german translation. <eos> we use the docent decoder, a local search decoder that translates at the document level. <eos> we add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to docent. <eos> we also investigate cleaning of the noisy common crawl corpus. <eos> we show that we can use alignment-based filtering for cleaning with good results. <eos> finally we investigate effects of corpus selection for recasing.
we present 5 systems of the munichedinburgh-stuttgart1 joint submissions to the 2013 smt shared task : fr-en, enfr, ru-en, de-en and en-de. <eos> the first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and de-en performs compound splitting. <eos> for our experiments, we use standard phrase-based moses systems and operation sequence models ( osm ).
supervised approaches to nlp tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. <eos> for some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications. <eos> in machine translation ( mt ) quality estimation ( qe ), for instance, using humanannotated data to train a binary classifier that discriminates between good ( useful for a post-editor ) and bad translations is not trivial. <eos> focusing on this binary task, we show that subjective human judgements can be effectively replaced with an automatic annotation procedure. <eos> to this aim, we compare binary classifiers trained on different data : the human-annotated dataset from the 7th workshop on statistical machine translation ( wmt-12 ), and an automatically labelled version of the same corpus. <eos> our results show that human labels are less suitable for the task.
many tasks in nlp and ir require efficient document similarity computations. <eos> beyond their common application to exploratory data analysis, latent variable topic models have been used to represent text in a low-dimensional space, independent of vocabulary, where documents may be compared. <eos> this paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other. <eos> we present ( 1 ) efficient, online inference for representing documents in several languages in a common topic space and ( 2 ) fast approximations for finding near neighbors in the probability simplex. <eos> empirical evaluations show that these methods are as accurate as ? and significantly faster than ? <eos> gibbs sampling and brute-force all-pairs search.
statistical machine translation ( smt ) performance suffers when models are trained on only small amounts of parallel data. <eos> the learned models typically have both low accuracy ( incorrect translations and feature scores ) and low coverage ( high out-of-vocabulary rates ). <eos> in this work, we use an additional data resource, comparable corpora, to improve both. <eos> beginning with a small bitext and corresponding phrase-based smt model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. <eos> then, we supplement the model ? s feature space with translation scores estimated over comparable corpora in order to improve accuracy. <eos> we observe improvements between 0.5 and 1.7 bleu translating tamil, telugu, bengali, malayalam, hindi, and urdu into english.
we propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation options ? phrasal translations that are generated by auxiliary translation and postediting processes ? to augment the default phrase inventory learned from parallel data. <eos> we apply our technique to the problem of producing english determiners when translating from russian and czech, languages that lack definiteness morphemes. <eos> our approach augments the english side of the phrase table using a classifier to predict where english articles might plausibly be added or removed, and then we decode as usual. <eos> doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier.
our field has seen significant improvements in the quality of machine translation systems over the past several years. <eos> the single biggest factor in this improvement has been the accumulation of ever larger stores of data. <eos> however, we now find ourselves the victims of our own success, in that it has become increasingly difficult to train on such large sets of data, due to limitations in memory, processing power, and ultimately, speed ( i.e., data to models takes an inordinate amount of time ). <eos> some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets ( denkowski et al, 2012a ; rarrick et al, 2011 ), ? domain adaptation ? <eos> to arrive at data more suited to the task at hand ( moore and lewis, 2010 ; axelrod et al, 2011 ), or by specifically focusing on data reduction by keeping only as much data as is needed for building models e.g., ( eck et al, 2005 ). <eos> this paper focuses on techniques related to the latter efforts. <eos> we have developed a very simple n-gram counting method that reduces the size of data sets dramatically, as much as 90 %, and is applicable independent of specific dev and test data. <eos> at the same time it reduces model sizes, improves training times, and, because it attempts to preserve contexts for all n-grams in a corpus, the cost in quality is minimal ( as measured by bleu ). <eos> further, unlike other methods created specifically for data reduction that have similar effects on the data, our method scales to very large data, up to tens to hundreds of millions of parallel sentences.
multi-task learning has been shown to be effective in various applications, including discriminative smt. <eos> we present an experimental evaluation of the question whether multi-task learning depends on a ? natural ? <eos> division of data into tasks that balance shared and individual knowledge, or whether its inherent regularization makes multi-task learning a broadly applicable remedy against overfitting. <eos> to investigate this question, we compare ? natural ? <eos> tasks defined as sections of the international patent classification versus ? random ? <eos> tasks defined as random shards in the context of patent smt. <eos> we find that both versions of multi-task learning improve equally well over independent and pooled baselines, and gain nearly 2 bleu points over standard mert tuning.
we present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario. <eos> with the introduction of a simple online feature, we are able to adapt the translation model on the fly to the corrections made by the translators. <eos> additionally, we do online adaption of the feature weights with a large margin algorithm. <eos> our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 bleu points absolute, and a standard incremental adaptation approach by 2 bleu points absolute.
we present an iterative technique to generate phrase tables for smt, which is based on force-aligning the training data with a modified translation decoder. <eos> different from previous work, we completely avoid the use of a word alignment or phrase extraction heuristics, moving towards a more principled phrase generation and probability estimation. <eos> during training, we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration. <eos> experiments are carried out on the iwslt 2011 arabic-english task, where we are able to reach moderate improvements on a state-of-the-art baseline with our training method. <eos> the resulting phrase table shows only a small overlap with the heuristically extracted one, which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics. <eos> by interpolating the heuristic and the trained phrase table, we can improve over the baseline by 0.5 % bleu and 0.5 % ter.
we present positive diversity tuning, a newmethod for tuningmachine translation models specifically for improved performance during system combination. <eos> system combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. <eos> we propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. <eos> the formulation of the positive diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. <eos> we find that individual systems tuned on the same data to positive diversity can be even more diverse than systems built using different data sets, while still obtaining good bleu scores. <eos> when these individual systems are used together for system combination, our approach allows for significant gains of 0.8 bleu even when the combination is performed using a small number of otherwise identical individual systems.
this paper describes a set of experiments on two sub-tasks of quality estimation of machine translation ( mt ) output. <eos> sentence-level ranking of alternative mt outputs is done with pairwise classifiers using logistic regression with blackbox features originating from pcfg parsing, language models and various counts. <eos> post-editing time prediction uses regression models, additionally fed with new elaborate features from the statistical mt decoding process. <eos> these seem to be better indicators of post-editing time than blackbox features. <eos> prior to training the models, feature scoring with relieff and information gain is used to choose feature sets of decent size and avoid computational complexity.
we describe the results of our submissions to the wmt13 shared task on quality estimation ( subtasks 1.1 and 1.3 ). <eos> our submissions use the framework of gaussian processes to investigate lightweight approaches for this problem. <eos> we focus on two approaches, one based on feature selection and another based on active learning. <eos> using only 25 ( out of 160 ) features, our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask, while the active learning model reached 2nd place in the scoring variant using only ? 25 % of the available instances for training. <eos> these results give evidence that gaussian processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive.
we introduce referential translation machines ( rtm ) for quality estimation of translation outputs. <eos> rtms are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for estimating the quality of translation outputs, judging the semantic similarity between text, and evaluating the quality of student answers. <eos> rtms achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation ( smt ) quality. <eos> rtms remove the need to access any smt system specific information or prior knowledge of the training data or models used when generating the translations. <eos> we develop novel techniques for solving all subtasks in the wmt13 quality estimation ( qe ) task ( qet 2013 ) based on individual rtm models. <eos> our results achieve improvements over last year ? s qe task results ( qet 2012 ), as well as our previous results, provide new features and techniques for qe, and rank 1st or 2nd in all of the subtasks.
in this paper we present the approach and system setup of the joint participation of fondazione bruno kessler and university of edinburgh in the wmt 2013 quality estimation shared-task. <eos> our submissions were focused on tasks whose aim was predicting sentence-level human-mediated translation edit rate and sentence-level post-editing time ( task 1.1 and 1.3, respectively ). <eos> we designed features that are built on resources such as automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. <eos> our models consistently overcome the baselines for both tasks and performed particularly well for task 1.3, ranking first among seven participants.
this paper describes the talp-upc participation in the wmt ? 13 shared task on quality estimation ( qe ). <eos> our participation is reduced to task 1.2 on system selection. <eos> we used a broad set of features ( 86 for german-to-english and 97 for english-to-spanish ) ranging from standard qe features to features based on pseudo-references and semantic similarity. <eos> we approached system selection by means of pairwise ranking decisions. <eos> for that, we learned random forest classifiers especially tailored for the problem. <eos> evaluation at development time showed considerably good results in a cross-validation experiment, with kendall ? s ? <eos> values around 0.30. <eos> the results on the test set dropped significantly, raising different discussions to be taken into account.
this paper is to introduce our participation in the wmt13 shared tasks on quality estimation for machine translation without using reference translations. <eos> we submitted the results for task 1.1 ( sentence-level quality estimation ), task 1.2 ( system selection ) and task 2 ( word-level quality estimation ). <eos> in task 1.1, we used an enhanced version of bleu metric without using reference translations to evaluate the translation quality. <eos> in task 1.2, we utilized a probability model na ? ve bayes ( nb ) as a classification algorithm with the features borrowed from the traditional evaluation metrics. <eos> in task 2, to take the contextual information into account, we employed a discriminative undirected probabilistic graphical model conditional random field ( crf ), in addition to the nb algorithm. <eos> the training experiments on the past wmt corpora showed that the designed methods of this paper yielded promising results especially the statistical models of crf and nb. <eos> the official results show that our crf model achieved the highest f-score 0.8297 in binary classification of task 2.
in this paper we present our entry to the wmt ? 13 shared task : quality estimation ( qe ) for machine translation ( mt ). <eos> we participated in the 1.1, 1.2 and 1.3 sub-tasks with our qe system trained on features from diverse information sources like mt decoder features, n-best lists, mono- and bi-lingual corpora and giza training models. <eos> our system shows competitive results in the workshop shared task.
in this paper we present the system we submitted to the wmt13 shared task on quality estimation. <eos> we participated in the task 1.1. <eos> each translated sentence is given a score between 0 and 1. <eos> the score is obtained by using several numerical or boolean features calculated according to the source and target sentences. <eos> we perform a linear regression of the feature space against scores in the range. <eos> to this end, we use a support vector machine with 66 features. <eos> in this paper, we propose to increase the size of the training corpus. <eos> for that, we use the post-edited and reference corpora during the training step. <eos> we assign a score to each sentence of these corpora. <eos> then, we tune these scores on a development corpus. <eos> this leads to an improvement of 10.5 % on the development corpus, in terms of mean average error, but achieves only a slight improvement on the test corpus.
this paper presents the lig ? s systems submitted for task 2 of wmt13 quality estimation campaign. <eos> this is a word confidence estimation ( wce ) task where each participant was asked to label each word in a translated text as a binary ( keep/change ) or multi-class ( keep/substitute/delete ) category. <eos> we integrate a number of features of various types ( system-based, lexical, syntactic and semantic ) into the conventional feature set, for our baseline classifier training. <eos> after the experiments with all features, we deploy a ? feature selection ? <eos> strategy to keep only the best performing ones. <eos> then, a method that combines multiple ? weak ? <eos> classifiers to build a strong ? composite ? <eos> classifier by taking advantage of their complementarity is presented and experimented. <eos> we then select the best systems for submission and present the official results obtained.
we describe the two systems submitted by the dcu-symantec team to task 1.1. of the wmt 2013 shared task on quality estimation for machine translation. <eos> task 1.1 involve estimating postediting effort for english-spanish translation pairs in the news domain. <eos> the two systems use a wide variety of features, of which the most effective are the word-alignment, n-gram frequency, language model, pos-tag-based and pseudoreferences ones. <eos> both systems perform at a similarly high level in the two tasks of scoring and ranking translations, although there is some evidence that the systems are over-fitting to the training data.
this paper describes the machine learning algorithm and the features used by limsi for the quality estimation shared task. <eos> our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output.
we describe terrorcat, a submission to this year ? s metrics shared task. <eos> it is a machine learning-based metric that is trained on manual ranking data from wmt shared tasks 2008 ? 2012. <eos> input features are generated by applying automatic translation error analysis to the translation hypotheses and calculating the error category frequency differences. <eos> we additionally experiment with adding quality estimation features in addition to the error analysis-based ones. <eos> when evaluated against wmt ? 2012 rankings, the systemlevel agreement is rather high for several language pairs.
this paper gives a detailed description of the act ( accuracy of connective translation ) metric, a reference-based metric that assesses only connective translations. <eos> act relies on automatic word-level alignment ( using giza++ ) between a source sentence and respectively the reference and candidate translations, along with other heuristics for comparing translations of discourse connectives. <eos> using a dictionary of equivalents, the translations are scored automatically or, for more accuracy, semi-automatically. <eos> the accuracy of the act metric was assessed by human judges on sample data for english/french, english/arabic, english/italian and english/german translations ; the act scores are within 2-5 % of human scores. <eos> the actual version of act is available only for a limited language pairs. <eos> consequently, we are participating only for the english/french and english/german language pairs. <eos> our hypothesis is that act metric scores increase with better translation quality in terms of human evaluation.
this paper is to describe our machine translation evaluation systems used for participation in the wmt13 shared metrics task. <eos> in the metrics task, we submitted two automatic mt evaluation systems nlepor_baseline and lepor_v3.1. <eos> nlepor_baseline is an n-gram based language independent mt evaluation metric employing the factors of modified sentence length penalty, position difference penalty, n-gram precision and n-gram recall. <eos> nlepor_baseline measures the similarity of the system output translations and the reference translations only on word sequences. <eos> lepor_v3.1 is a new version of lepor metric using the mathematical harmonic mean to group the factors and employing some linguistic features, such as the part-of-speech information. <eos> the evaluation results of wmt13 show lepor_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using pearson correlation criterion on english-to-other ( fr, de, es, cs, ru ) language pairs.
the linguistically transparentmeant and umeant metrics are tunable, simple yet highly effective, fully automatic approximation to the human hmeant mt evaluation metric which measures semantic frame similarity between mt output and reference translations. <eos> in this paper, we describe hkust ? s submission to the wmt 2013 metrics evaluation task, meant and umeant. <eos> meant is optimized by tuning a small number of weights ? one for each semantic role label ? so as to maximize correlation with human adequacy judgment on a development set. <eos> umeant is an unsupervised version where weights for each semantic role label are estimated via an inexpensive unsupervised approach, as opposed to meant ? s supervised method relying on more expensive grid search. <eos> in this paper, we present a battery of experiments for optimizing meant on different development sets to determine the set of weights that maximize meant ? s accuracy and stability. <eos> evaluated on test sets from the wmt 2012/2011 metrics evaluation, bothmeant and umeant achieve competitive correlations with human judgments using nothing more than a monolingual corpus and an automatic shallow semantic parser.
in this paper we describe our participation to the wmt13 shared task on quality estimation. <eos> the main originality of our approach is to include features originally designed to classify text according to some author ? s style. <eos> this implies the use of reference categories, which are meant to represent the quality of the mt output. <eos> preamble this paper describes the approach followed in the two systems that we submitted to subtask 1.3 of the wmt13 shared task on quality estimation, identified as tcd-dcu-cngl 1-3 svm1 and tcd-dcu-cngl 1-3 svm2. <eos> this approach was also used by the first author in his submissions to subtask 1.1, identified as tcd-cngl open and tcd-cngl restricted1. <eos> in the remaining of this paper we focus on subtask 1.3, but there is very little difference in the application of the approach to task 1.1.
in this paper, we propose a novel syntactic based mt evaluation metric which only employs the dependency information in the source side. <eos> experimental results show that our method achieves higher correlation with human judgments than bleu, ter, hwcm and meteor at both sentence and system level for all of the four language pairs in wmt 2010.
despite being closely related languages, german and english are characterized by important word order differences. <eos> longrange reordering of verbs, in particular, represents a real challenge for state-of-theart smt systems and is one of the main reasons why translation quality is often so poor in this language pair. <eos> in this work, we review several solutions to improve the accuracy of german-english word reordering while preserving the efficiency of phrase-based decoding. <eos> among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena. <eos> through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical smt.
we introduce a lexicalized reordering model for hierarchical phrase-based machine translation. <eos> the model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by tillmann ( 2004 ). <eos> while this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems ( koehn et al, 2007 ), it is however commonly not employed in hierarchical decoders. <eos> we describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. <eos> the model is empirically evaluated on the nist chinese ? english translation task. <eos> we achieve a significant improvement of +1.2 % bleu over a typical hierarchical baseline setup and an improvement of +0.7 % bleu over a syntax-augmented hierarchical setup. <eos> on a french ? german translation task, we obtain a gain of up to +0.4 % bleu.
this paper presents a dependencyconstrained hierarchical machine translation model that uses moses open-source toolkit for rule extraction and decoding. <eos> experiments are carried out for the german-english language pair in both directions for projective and non-projective dependencies. <eos> we examine effects on scfg size and automatic evaluation results when constraints are applied with respect to projective or non-projective dependency structures and on the source or target language side.
we present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. <eos> we also present a first implementation of that method along with experimental results shedding light on some fundamental issues. <eos> in hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model. <eos> we replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain. <eos> our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions. <eos> while the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework.
sentence alignment is an important step in the preparation of parallel data. <eos> most aligners do not perform very well when the input is a noisy, rather than a highlyparallel, document pair. <eos> evaluating aligners under noisy conditions would seem to require creating an evaluation dataset by manually annotating a noisy document for gold-standard alignments. <eos> such a costly process hinders our ability to evaluate an aligner under various types and levels of noise. <eos> in this paper, we propose a new evaluation framework for sentence aligners, which is particularly suitable for noisy-data evaluation. <eos> our approach is unique as it requires no manual labeling, instead relying on small parallel datasets ( already at the disposal of mt researchers ) to generate many evaluation datasets that mimic a variety of noisy conditions. <eos> we use our framework to perform a comprehensive comparison of three aligners under noisy conditions. <eos> furthermore, our framework facilitates the fine-tuning of a state-of-the-art sentence aligner, allowing us to substantially increase its recall rates by anywhere from 5 % to 14 % ( absolute ) across several language pairs.
we apply multi-rate hmms, a tree structured hmm model, to the word-alignment problem. <eos> multi-rate hmms allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion. <eos> this approach leads to better machine translation results than a morphemeaware model that does not explicitly model morpheme reordering.
we propose a novel unsupervised word alignment model based on the hidden markov tree ( hmt ) model. <eos> our model assumes that the alignment variables have a tree structure which is isomorphic to the target dependency tree and models the distortion probability based on the source dependency tree, thereby incorporating the syntactic structure from both sides of the parallel sentences. <eos> in english-japanese word alignment experiments, our model outperformed an ibm model 4 baseline by over 3 points alignment error rate. <eos> while our model was sensitive to posterior thresholds, it also showed a performance comparable to that of hmm alignment models.
the discriminative word lexicon ( dwl ) is a maximum-entropy model that predicts the target word probability given the source sentence words. <eos> we present two ways to extend a dwl to improve its ability to model the word translation probability in a phrase-based machine translation ( pbmt ) system. <eos> while dwls are able to model the global source information, they ignore the structure of the source and target sentence. <eos> we propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words. <eos> furthermore, as the standard dwl does not get any feedback from the mt system, we change the dwl training process to explicitly focus on addressing mt errors. <eos> by using these methods we are able to improve the translation performance by up to 0.8 bleu points compared to a system that uses a standard dwl.
for languages with complex morphologies, limited resources and tools, and/or lack of standard grammars, developing annotated resources can be a challenging task. <eos> annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. <eos> we present two methods for automatic correction and extension of morphological annotations, and demonstrate their success on three divergent egyptian arabic corpora.
this paper presents a method for part-ofspeech tagging of historical data and evaluates it on texts from different corpora of historical german ( 15th ? 18th century ). <eos> spelling normalization is used to preprocess the texts before applying a pos tagger trained on modern german corpora. <eos> using only 250 manually normalized tokens as training data, the tagging accuracy of a manuscript from the 15th century can be raised from 28.65 % to 74.89 %.
the recent success of statistical parsing methods has made treebanks become important resources for building good parsers. <eos> however, constructing highquality annotated treebanks is a challenging task. <eos> we utilized two publicly available parsers, berkeley and mst parsers, for feedback on improving the quality of part-of-speech tagging for the vietnamese treebank. <eos> analysis of the treebank and parsing errors revealed how problems with the vietnamese treebank influenced the parsing results and real difficulties of vietnamese parsing that required further improvements to existing parsing technologies.
when creating a new resource, preprocessing the source texts before annotation is both ubiquitous and obvious. <eos> how the preprocessing affects the annotation effort for various tasks is for the most part an open question, however. <eos> in this paper, we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline. <eos> we also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated.1
we explore the use of continuous rating scales for human evaluation in the context of machine translation evaluation, comparing two assessor-intrinsic qualitycontrol techniques that do not rely on agreement with expert judgments. <eos> experiments employing amazon ? s mechanical turk service show that quality-control techniques made possible by the use of the continuous scale show dramatic improvements to intra-annotator agreement of up to +0.101 in the kappa coefficient, with inter-annotator agreement increasing by up to +0.144 when additional standardization of scores is applied.
hierarchical or nested annotation of linguistic data often co-exists with simpler non-hierarchical or flat counterparts, a classic example being that of annotations used for parsing and chunking. <eos> in this work, we propose a general strategy for comparing across these two schemes of annotation using the concept of entailment that formalizes a correspondence between them. <eos> we use crowdsourcing to obtain query and sentence chunking and show that entailment can not only be used as an effective evaluation metric to assess the quality of annotations, but it can also be employed to filter out noisy annotations.
we introduce a framework for lightweight dependency syntax annotation. <eos> our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workflow. <eos> moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. <eos> we demonstrate the efficacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations.
the paper addresses the challenge of converting midt, an existing dependency ? <eos> based italian treebank resulting from the harmonization and merging of smaller resources, into the stanford dependencies annotation formalism, with the final aim of constructing a standard ? compliant resource for the italian language. <eos> achieved results include a methodology for converting treebank annotations belonging to the same dependency ? based family, the italian stanford dependency treebank ( isdt ), and an italian localization of the stanford dependency scheme.
huang chi-hsin yu tai-wei chang cong-kai lin hsin-hsi chen department of computer science and information engineering national taiwan university, taipei, taiwan { hhhuang, jsyu, twchang, cklin } @ nlg.csie.ntu.edu.tw ; hhchen @ ntu.edu.tw abstract discourse relation may entail sentiment in-formation. <eos> in this work, we annotate both discourse relation and sentiment information on a moderate-sized chinese corpus extracted from the clueweb09. <eos> based on the annota-tion, we investigate the association between the relation type and the sentiment polarity in chinese and interpret the data from various aspects. <eos> finally, we highlight some language phenomena and give some remarks.
there exist various different discourse annotation schemes that vary both in the perspectives of discourse structure considered and the granularity of textual units that are annotated. <eos> comparison and integration of multiple schemes have the potential to provide enhanced information. <eos> however, the differing formats of corpora and tools that contain or produce such schemes can be a barrier to their integration. <eos> u-compare is a graphical, uima-based workflow construction platform for combining interoperable natural language processing ( nlp ) resources, without the need for programming skills. <eos> in this paper, we present an extension of u-compare that allows the easy comparison, integration and visualisation of resources that contain or output annotations based on multiple discourse annotation schemes. <eos> the extension works by allowing the construction of parallel subworkflows for each scheme within a single u-compare workflow. <eos> the different types of discourse annotations produced by each sub-workflow can be either merged or visualised side-by-side for comparison. <eos> we demonstrate this new functionality by using it to compare annotations belonging to two different approaches to discourse analysis, namely discourse relations and functional discourse annotations. <eos> integrating these different annotation types within an interoperable environment allows us to study the correlations between different types of discourse and report on the new insights that this allows us to discover. <eos> ? the authors have contributed equally to the development of this work and production of the manuscript.
unstructured information management architecture ( uima ) has been gaining popularity in annotating text corpora. <eos> the architecture defines common data structures and interfaces to support interoperability of individual processing components working together in a uima application. <eos> the components exchange data by sharing common type systems ? schemata of data type structures ? which extend a generic, top-level type system built into uima. <eos> this flexibility in extending type systems has resulted in the development of repositories of components that share one or several type systems ; however, components coming from different repositories, and thus not sharing type systems, remain incompatible. <eos> commonly, this problem has been solved programmatically by implementing uima components that perform the alignment of two type systems, an arduous task that is impractical with a growing number of type systems. <eos> we alleviate this problem by introducing a conversion mechanism based on sparql, a query language for the data retrieval and manipulation of rdf graphs. <eos> we provide a uima component that serialises data coming from a source component into rdf, executes a user-defined, typeconversion query, and deserialises the updated graph into a target component. <eos> the proposed solution encourages ad hoc conversions, enables the usage of heterogeneous components, and facilitates highly customised uima applications.
this paper describes the importation of manually annotated sub-corpus ( masc ) data and annotations into the linguistic database annis, which allows users to visualize and query linguistically-annotated corpora. <eos> we outline the process of mapping masc ? s graf representation to annis ? s internal format relannis and demonstrate how the system provides access to multiple annotation layers in the corpus. <eos> this access provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications.
this paper discusses the problem of annotating coreference relations with generic expressions in a large scale corpus. <eos> we present and analyze some existing theories of genericity, compare them to the approaches to generics that are used in the state-of-the-art coreference annotation guidelines and discuss how coreference of generic expressions is processed in the manual annotation of the prague dependency treebank. <eos> after analyzing some typical problematic issues we propose some partial solutions that can be used to enhance the quality and consistency of the annotation.
anaphoric shell nouns such as this issue and this fact conceptually encapsulate complex pieces of information ( schmid, 2000 ). <eos> we examine the feasibility of annotating such anaphoric nouns using crowdsourcing. <eos> in particular, we present our methodology for reliably annotating antecedents of such anaphoric nouns and the challenges we faced in doing so. <eos> we also evaluated the quality of crowd annotation using experts. <eos> the results suggest that most of the crowd annotations were good enough to use as training data for resolving such anaphoric nouns.
various discourse theories have argued for data structures ranging from the simplest trees to the most complex chain graphs. <eos> this paper investigates the structure represented by the explicit connectives annotated in the multiplegenre turkish discourse bank ( tdb ). <eos> the dependencies that violate tree-constraints are analyzed. <eos> the effects of information structure in the surface form, which result in seemingly complex configurations with underlying simple dependencies, are introduced ; and the structural implications are discussed. <eos> the results indicate that our current approach to local discourse structure needs to accommodate properly contained arguments and relations, and partially overlapping as well as shared arguments ; deviating further from simple trees, but not as drastically as a chain graph structure would imply, since no genuine cases of structural crossing dependencies are attested in tdb.
in this paper, we present an annotation tool developed specifically for manual sentiment analysis of social media posts. <eos> the tool provides facilities for general and target based opinion marking on different type of posts ( i.e. <eos> comparative, ironic, conditional ) with a web based ui which supports synchronous annotation. <eos> it is also designed as a saas ( software as a service ). <eos> the tool ? s outstanding features are easy and fast annotation interface, detailed sentiment levels, multi-client support, easy to manage administrative modules and linguistic annotation capabilities.
we describe a new annotation scheme for formalizing relation structures in research papers. <eos> the scheme has been developed through the investigation of computer science papers. <eos> using the scheme, we are building a japanese corpus to help develop information extraction systems for digital libraries. <eos> we report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the ipsj journal.
semantically annotated corpora play an important role in natural language processing. <eos> this paper presents the results of a pilot study on building a sense-tagged parallel corpus, part of ongoing construction of aligned corpora for four languages ( english, chinese, japanese, and indonesian ) in four domains ( story, essay, news, and tourism ) from the ntu-multilingual corpus. <eos> each subcorpus is first sensetagged using a wordnet and then these synsets are linked. <eos> upon the completion of this project, all annotated corpora will be made freely available. <eos> the multilingual corpora are designed to not only provide data for nlp tasks like machine translation, but also to contribute to the study of translation shift and bilingual lexicography as well as the improvement of monolingual wordnets.
in this paper, we discuss our efforts to annotate nominals in the hindi treebank with the semantic property of animacy. <eos> although the treebank already encodes lexical information at a number of levels such as morph and part of speech, the addition of animacy information seems promising given its relevance to varied linguistic phenomena. <eos> the suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution, syntactic parsing, verb classification and argument differentiation.
automatic pre-annotation is often used to improve human annotation speed and accuracy. <eos> we address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. <eos> our study design includes two different corpora, three pre-annotation schemes linked to two annotation levels, both expert and novice annotators, a questionnaire-based subjective assessment and a corpus-based quantitative assessment. <eos> we observe that preannotation helps in all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome.
we describe abstract meaning representation ( amr ), a semantic representation language in which we are writing down the meanings of thousands of english sentences. <eos> we hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the penn treebank encouraged work on statistical parsing. <eos> this paper gives an overview of amr and tools associated with it.
this paper presents a case study of a difficult and important categorical annotation task ( word sense ) to demonstrate a probabilistic annotation model applied to crowdsourced data. <eos> it is argued that standard ( chance-adjusted ) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels. <eos> compared to conventional agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost.
we investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation. <eos> we show that it is possible to isolate a reliable subgroup of annotators, so that aspects of the difficulty of the underlying task can be studied. <eos> our task is to annotate the argumentative structure of short texts.
crowdsourcing, while ideally reducing both costs and the need for domain experts, is no all-purpose tool. <eos> we review how paraphrase recognition has benefited from crowdsourcing in the past and identify two problems in paraphrase acquisition and semantic similarity evaluation that can be solved by employing a smart crowdsourcing strategy. <eos> first, we employ the crowdflower platform to conduct an experiment on sub-sentential paraphrase acquisition with early exclusion of lowaccuracy crowdworkers. <eos> second, we compare two human intelligence task designs for evaluating phrase pairs on a semantic similarity scale. <eos> while the first experiment confirms our strategy successful at tackling the problem of missing gold in paraphrase generation, the results of the second experiment suggest that, for both semantic similarity evaluation on a continuous and a binary scale, querying crowdworkers for a semantic similarity value on a multi-grade scale yields better results than directly asking for a binary classification.
this paper presents an analysis of an annotator ? s behaviour during her/his annotation process for eliciting useful information for natural language processing ( nlp ) tasks. <eos> text annotation is essential for machine learning-based nlp where annotated texts are used for both training and evaluating supervised systems. <eos> since an annotator ? s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful information for nlp tasks, in particular semantic and discourse processing that require deeper language understanding. <eos> we conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in japanese texts. <eos> our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches.
in this paper we present the development of a corpus of french newswire texts annotated with enunciative and modal commitment information. <eos> the annotation scheme we propose is based on the detection of predicative cues referring to an enunciative and/or modal variation - and their scope at a sentence level. <eos> we describe how we have improved our annotation guideline by using the evaluation ( in terms of precision, recall and f-measure ) of a first round of annotation produced by two expert annotators and by our automatic annotation system.
despite many methods that effectively solve sentiment classification task for such widely used languages as english, there is no clear answer which methods are the most suitable for the languages that are substantially different. <eos> in this paper we attempt to solve internet comments sentiment classification task for lithuanian, using two classification approaches ? <eos> knowledge-based and supervised machine learning. <eos> we explore an influence of sentiment word dictionaries based on the different parts-of-speech ( adjectives, adverbs, nouns, and verbs ) for knowledge-based method ; different feature types ( bag-ofwords, lemmas, word n-grams, character n-grams ) for machine learning methods ; and pre-processing techniques ( emoticons replacement with sentiment words, diacritics replacement, etc. ) <eos> for both approaches. <eos> despite that supervised machine learning methods ( support vector machine and na ? <eos> ? ve bayes multinomial ) significantly outperform proposed knowledge-based method all obtained results are above baseline. <eos> the best accuracy 0.679 was achieved with na ? <eos> ? ve bayes multinomial and token unigrams plus bigrams, when pre-processing involved diacritics replacement.
in this paper we describe our experience in conducting the first open sentiment analysis evaluations in russian in 2011-2012. <eos> these initiatives took part within russian information retrieval seminar ( romip ), which is an annual trec-like competition in russian. <eos> several test and train collections were created for such tasks as sentiment classification in blogs and newswire, opinion retrieval. <eos> the paper describes the state of the art in sentiment analysis in russian, collection characteristics, track tasks and evaluation metrics.
aspect-oriented opinion mining aims to identify product aspects ( features of products ) about which opinion has been expressed in the text. <eos> we present an approach for aspect-oriented opinion mining from user reviews in croatian. <eos> we propose methods for acquiring a domain-specific opinion lexicon, linking opinion clues to product aspects, and predicting polarity and rating of reviews. <eos> we show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions.
frequently asked questions ( faq ) are an efficient way of communicating domainspecific information to the users. <eos> unlike general purpose retrieval engines, faq retrieval engines have to address the lexical gap between the query and the usually short answer. <eos> in this paper we describe the design and evaluation of a faq retrieval engine for croatian. <eos> we frame the task as a binary classification problem, and train a model to classify each faq as either relevant or not relevant for a given query. <eos> we use a variety of semantic textual similarity features, including term overlap and vector space features. <eos> we train and evaluate on a faq test collection built specifically for this purpose. <eos> our best-performing model reaches 0.47 of mean reciprocal rank, i.e., on average ranks the relevant answer among the top two returned answers.
we present an approach for natural language parsing in which dependency and constituency parses are acquired simultaneously. <eos> this leads to accurate parses represented in a specific way, richer than constituency or dependency tree. <eos> it also allows reducing parsing time complexity. <eos> within the proposed approach, we show how to treat some significant phenomena of the russian language and also perform a brief evaluation of the parser implementation, known as dictascope syntax.
we describe gpkex, a keyphrase extraction method based on genetic programming. <eos> we represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text. <eos> we apply and evaluate gpkex on croatian newspaper articles. <eos> we show that gpkex can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for croatian.
we investigate state-of-the-art statistical models for lemmatization and morphosyntactic tagging of croatian and serbian. <eos> the models stem from a new manually annotated setimes.hr corpus of croatian, based on the setimes parallel corpus. <eos> we train models on croatian text and evaluate them on samples of croatian and serbian from the setimes corpus and the two wikipedias. <eos> lemmatization accuracy for the two languages reaches 97.87 % and 96.30 %, while full morphosyntactic tagging accuracy using a 600-tag tagset peaks at 87.72 % and 85.56 %, respectively. <eos> part of speech tagging accuracies reach 97.13 % and 96.46 %. <eos> results indicate that more complex methods of croatian-toserbian annotation projection are not required on such dataset sizes for these particular tasks. <eos> the setimes.hr corpus, its resulting models and test sets are all made freely available.
we propose a language-independent word normalization method exemplified on modernizing historical slovene words. <eos> our method relies on character-based statistical machine translation and uses only shallow knowledge. <eos> we present the relevant lexicons and two experiments. <eos> in one, we use a lexicon of historical word ? <eos> contemporary word pairs and a list of contemporary words ; in the other, we only use a list of historical words and one of contemporary ones. <eos> we show that both methods produce significantly better results than the baseline.
the present paper introduces approach to improve english-russian sentence alignment, based on pos-tagging of automatically aligned ( by hunalign ) source and target texts. <eos> the initial hypothesis is tested on a corpus of bitexts. <eos> sequences of pos tags for each sentence ( exactly, nouns, adjectives, verbs and pronouns ) are processed as ? words ? <eos> and dameraulevenshtein distance between them is computed. <eos> this distance is then normalized by the length of the target sentence and is used as a threshold between supposedly mis-aligned and ? good ? <eos> sentence pairs. <eos> the experimental results show precision 0.81 and recall 0.8, which allows the method to be used as additional data source in parallel corpora alignment. <eos> at the same time, this leaves space for further improvement.
in this paper we present a corpus-based approach to automatic identification of false friends for slovene and croatian, a pair of closely related languages. <eos> by taking advantage of the lexical overlap between the two languages, we focus on measuring the difference in meaning between identically spelled words by using frequency and distributional information. <eos> we analyze the impact of corpora of different origin and size together with different association and similarity measures and compare them to a simple frequency-based baseline. <eos> with the best performing setting we obtain very good average precision of 0.973 and 0.883 on different gold standards. <eos> the presented approach works on non-parallel datasets, is knowledge-lean and language-independent, which makes it attractive for natural language processing tasks that often lack the lexical resources and can not afford to build them by hand.
the task of named entity recognition ( ner ) is to identify in text predefined units of information such as person names, organizations and locations. <eos> in this work, we address the problem of ner in estonian using supervised learning approach. <eos> we explore common issues related to building a ner system such as the usage of language-agnostic and languagespecific features, the representation of named entity tags, the required corpus size and the need for linguistic tools. <eos> for system training and evaluation purposes, we create a gold standard ner corpus. <eos> on this corpus, our crf-based system achieves an overall f1-score of 87 %.
this paper reports on some experiments aiming at tuning a rule-based ner system designed for detecting names in polish online news to the processing of targeted twitter streams. <eos> in particular, one explores whether the performance of the baseline ner system can be improved through the incremental application of knowledge-poor methods for name matching and guessing. <eos> we study various settings and combinations of the methods and present evaluation results on five corpora gathered from twitter, centred around major events and known individuals.
in the paper we discuss the problem of low recall for the named entity ( ne ) recognition task for polish. <eos> we discuss to what extent the recall of ne recognition can be improved by reducing the space of ne categories. <eos> we also present several extensions to the binary model which give an improvement of the recall. <eos> the extensions include : new features, application of external knowledge and post-processing. <eos> for the partial evaluation the final model obtained 90.02 % recall with 91.30 % precision on the corpus of economic news.
this paper describes a plug-in component to extend the puls information extraction framework to analyze russian-language text. <eos> puls is a comprehensive framework for information extraction ( ie ) that is used for analysis of news in several scenarios from english-language text and is primarily monolingual. <eos> although monolinguality is recognized as a serious limitation, building an ie system for a new language from the bottom up is very labor-intensive. <eos> thus, the objective of the present work is to explore whether the base framework can be extended to cover additional languages with limited effort, and to leverage the preexisting puls modules as far as possible, in order to accelerate the development process. <eos> the component for russian analysis is described and its performance is evaluated on two news-analysis scenarios : epidemic surveillance and cross-border security. <eos> the approach described in the paper can be generalized to a range of heavilyinflected languages.
in this paper we present a semi-automatic approach for acqusition of lexico-syntactic knowledge for event extraction in two slavic languages, namely bulgarian and czech. <eos> the method uses several weaklysupervised and unsupervised algorithms, based on distributional semantics. <eos> moreover, an intervention from a language expert is envisaged on different steps in the learning procedure, which increases its accuracy, with respect to unsupervised methods for lexical and grammar learning.
we propose a method for cross-language identification of semantic relations based on word similarity measurement and morphosemantic relations in wordnet. <eos> we transfer these relations to pairs of derivationally unrelated words and train a model for automatic classification of new instances of ( morpho ) semantic relations in context based on the existing ones and the general semantic classes of collocated verb and noun senses. <eos> our experiments are based on bulgarian-english parallel and comparable texts but the method is to a great extent language-independent and particularly suited to less-resourced languages, since it does not need parsed or semantically annotated data. <eos> the application of the method leads to an increase in the number of discovered semantic relations by 58.35 % and performs relatively consistently, with a small decrease in precision between the baseline ( based on morphosemantic relations identified in wordnet ) ? <eos> 0.774, and the extended method ( based on the data obtained through machine learning ) ? <eos> 0.721.
we propose a data-driven approach to enhance translation extraction from comparable corpora. <eos> instead of resorting to an external dictionary, we translate source vector features by using a cross-lingual word sense disambiguation method. <eos> the candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature. <eos> the translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. <eos> as a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods.
this paper presents a simple and effective method for automatic bilingual lexicon extraction from less-known language pairs. <eos> to do this, we bring in a bridge language named the pivot language and adopt information retrieval techniques combined with natural language processing techniques. <eos> moreover, we use a freely available word aligner : anymalign ( lardilleux et al, 2011 ) for constructing context vectors. <eos> unlike the previous works, we obtain context vectors via a pivot language. <eos> therefore, we do not require to translate context vectors by using a seed dictionary and improve the accuracy of low frequency word alignments that is weakness of statistical model by using anymalign. <eos> in this paper, experiments have been conducted on two different language pairs that are bi-directional koreanspanish and korean-french, respectively. <eos> the experimental results have demonstrated that our method for high-frequency words shows at least 76.3 and up to 87.2 % and for the lowfrequency words at least 43.3 % and up to 48.9 % within the top 20 ranking candidates, respectively.
this paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. <eos> we study of the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors. <eos> for this purpose, we augment the standard approach by a word sense disambiguation process relying on a wordnet-based semantic similarity measure. <eos> the aim of this process is to identify the translations that are more likely to give the best representation of words in the target language. <eos> on two specialized french-english comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach.
smoothing is a central issue in language modeling and a prior step in different natural language processing ( nlp ) tasks. <eos> however, less attention has been given to it for bilingual lexicon extraction from comparable corpora. <eos> if a first work to improve the extraction of low frequency words showed significant improvement while using distance-based averaging ( pekar et al, 2006 ), no investigation of the many smoothing techniques has been carried out so far. <eos> in this paper, we present a study of some widelyused smoothing algorithms for language n-gram modeling ( laplace, good-turing, kneser-ney... ). <eos> our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach ( fung, 1998 ) traditionally used for bilingual lexicon extraction. <eos> we show that using smoothing as a preprocessing step of the standard approach increases its performance significantly.
parallel sentences are crucial for statistical machine translation ( smt ). <eos> however, they are quite scarce for most language pairs, such as chinese ? japanese. <eos> many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. <eos> we extract chinese ? japanese parallel sentences from quasi ? comparable corpora, which are available in far larger quantities. <eos> the task is significantly more difficult than the extraction from noisy parallel or comparable corpora. <eos> we extend a previous study that treats parallel sentence identification as a binary classification problem. <eos> previous method of classifier training by the cartesian product is not practical, because it differs from the real process of parallel sentence extraction. <eos> we propose a novel classifier training method that simulates the real sentence extraction process. <eos> furthermore, we use linguistic knowledge of chinese character features. <eos> experimental results on quasi ? <eos> comparable corpora indicate that our proposed approach performs significantly better than the previous study.
this paper discusses a modular and opensource focused crawler ( ilsp-fc ) for the automatic acquisition of domain-specific monolingual and bilingual corpora from the web. <eos> besides describing the main modules integrated in the crawler ( dealing with page fetching, normalization, cleaning, text classification, de-duplication and document pair detection ), we evaluate several of the system functionalities in an experiment for the acquisition of pairs of parallel documents in german and italian for the `` health & safety at work '' domain.
we present a study on linguistic contrast and commonality in english scientific discourse on the basis of a monolingually comparable corpus. <eos> the focus is on selected scientific disciplines at the boundaries to computer science ( computational linguistics, bioinformatics, digital construction, microelectronics ). <eos> the data basis is the english scientific text corpus ( scitex ) which covers a time range of roughly thirty years ( 1970/80s to early 2000s ). <eos> in particular, we investigate the disciplinary diversification/relatedness of scientific research articles in terms of register. <eos> our results are relevant for research on multilingually comparable corpora as used in machine translation and related research, since they shed new light on the notion of ? comparablity ?.
in this article, we present an automated approach of extracting english-bengali parallel fragments of text from comparable corpora created using wikipedia documents. <eos> our approach exploits the multilingualism of wikipedia. <eos> the most important fact is that this approach does not need any domain specific corpus. <eos> we have been able to improve the bleu score of an existing domain specific englishbengali machine translation system by 11.14 %.
this paper presents a comparable translation corpus created to investigate translation variation phenomena in terms of contrasts between languages, text types and translation methods ( machine vs. computer-aided vs. human ). <eos> these phenomena are reflected in linguistic features of translated texts belonging to different registers and produced with different translation methods. <eos> for their analysis, we combine methods derived from translation studies, language variation and machine translation, concentrating especially on textual and lexico-grammatical variation. <eos> to our knowledge, none of the existing corpora can provide comparable resources for a comprehensive analysis of variation across text types and translation methods. <eos> therefore, the corpus resources created, as well as our analysis results will find application in different research areas, such as translation studies, machine translation, and others.
c elizabeth a. lennon winter mason jeffrey v. nickerson stevens institute of technology center for decision technologies castle point on hudson, hoboken, nj usa { ygenc, elennon, wmason, jnickerson } @ stevens.edu abstract tools and techniques that automate the inter-pretation of multilingual corpora are useful on many fronts ; scholars, as an example, could use such tools to more readily pinpoint relevant articles from journals in a wide vari-ety of languages. <eos> this work describes tech-niques to build and characterize ontologies using collaborative knowledge bases, e.g., wikipedia. <eos> these ontologies can then be used to search and classify texts. <eos> originally devel-oped for monolingual corpora, we extend the approach to multilingual texts and test the methods with mandarin scientific abstracts. <eos> the presented techniques provide a novel and efficient mechanism to obtain contextually rich ontologies and measure document rele-vancy within multilingual corpora.
we present a novel method to recognise semantic equivalents of biomedical terms in language pairs. <eos> we hypothesise that biomedical term are formed by semantically similar textual units across languages. <eos> based on this hypothesis, we employ a random forest ( rf ) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples. <eos> we apply our method on two language pairs : one that uses the same character set and another with a different script, english-french and englishchinese, respectively. <eos> we show that english-french pairs of terms are highly transliterated in contrast to the englishchinese pairs. <eos> nonetheless, our method performs robustly on both cases. <eos> we evaluate rf against a state-of-the-art alignment method, giza++, and we report a statistically significant improvement. <eos> finally, we compare rf against support vector machines and analyse our results.
multilingual sentiment analysis attracts increased attention as the massive growth of multilingual web contents. <eos> this conducts to study opinions across different languages by comparing the underlying messages written by different people having different opinions. <eos> in this paper, we propose sentiment based comparability measures ( scm ) to compare opinions in multilingual comparable articles without translating source/target into the same language. <eos> this will allow media trackers ( journalists ) to automatically detect public opinion split across huge multilingual web contents. <eos> to develop scm, we need either to get or to build parallel sentiment corpora. <eos> because this kind of corpora are not available, we decided to build them. <eos> for that, we propose a new method to automatically label parallel corpora with sentiment classes. <eos> then we use the extracted parallel sentiment corpora to develop multilingual sentiment analysis system. <eos> experimental results show that, the proposed measure can capture differences in terms of opinions. <eos> the results also show that comparable articles variate in their objectivity and positivity.
previous attempts in extracting parallel data from wikipedia were restricted by the monotonicity constraint of the alignment algorithm used for matching possible candidates. <eos> this paper proposes a method for exploiting wikipedia articles without worrying about the position of the sentences in the text. <eos> the algorithm ranks the candidate sentence pairs by means of a customized metric, which combines different similarity criteria. <eos> moreover, we limit the search space to a specific topical domain, since our final goal is to use the extracted data in a domain-specific statistical machine translation ( smt ) setting. <eos> the precision estimates show that the extracted sentence pairs are clearly semantically equivalent. <eos> the smt experiments, however, show that the extracted data is not refined enough to improve a strong in-domain smt system. <eos> nevertheless, it is good enough to boost the performance of an out-of-domain system trained on sizable amounts of data.
we present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. <eos> sentential paraphrases are extracted from a comparable corpus of temporally and topically related messages on twitter which often express semantically identical information through distinct surface forms. <eos> we demonstrate the utility of this new resource on the task of paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art paraphrase and normalization systems 1.
focusing on a systematic latent semantic analysis ( lsa ) and machine learning ( ml ) approach, this research contributes to the development of a methodology for the automatic compilation of comparable collections of documents. <eos> its originality lies within the delineation of relevant comparability characteristics of similar documents in line with an established definition of comparable corpora. <eos> these innovative characteristics are used to build a lsa vector-based representation of the texts. <eos> in accordance with this new reduced in dimensionality document space, an unsupervised machine learning algorithm gathers similar texts into comparable clusters. <eos> on a monolingual collection of less than 100 documents, the proposed approach assigns comparable documents to different comparable corpora with high confidence.
this paper presents an efficient approach to finding more bilingual webpage pairs with high credibility via link analysis, using little prior knowledge or heuristics. <eos> it extends from a previous algorithm that takes the number of bilingual url pairs that a key ( i.e., a url pairing pattern ) can match as the objective function to search for the best set of keys yielding the greatest number of webpage pairs within targeted bilingual websites. <eos> enhanced algorithms are proposed to match more bilingual webpages following the credibility based on statistical analysis of the link relationship of the seed websites available. <eos> with about 12,800 seed websites as test set, the enhanced algorithms improve precision over baseline by more than 5 %, from 94.06 % to 99.40 %, and hence find above 20 % more true bilingual url pairs, illustrating that significantly more bilingual webpages with high credibility can be mined with the help of the link analysis.
cross-linguistic studies on unsupervised word segmentation have consistently shown that english is easier to segment than other languages. <eos> in this paper, we propose an explanation of this finding based on the notion of segmentation ambiguity. <eos> we show that english has a very low segmentation ambiguity compared to japanese and that this difference correlates with the segmentation performance in a unigram model. <eos> we suggest that segmentation ambiguity is linked to a trade-off between syllable structure complexity and word length distribution.
computational work in the past decade has produced several models accounting for phonetic category learning from distributional and lexical cues. <eos> however, there have been no computational proposals for how people might use another powerful learning mechanism : generalization from learned to analogous distinctions ( e.g., from /b/ ? /p/ to /g/ ? /k/ ). <eos> here, we present a new simple model of generalization in phonetic category learning, formalized in a hierarchical bayesian framework. <eos> the model captures our proposal that linguistic knowledge includes the possibility that category types in a language ( such as voiced and voiceless ) can be shared across sound classes ( such as labial and velar ), thus naturally leading to generalization. <eos> we present two sets of simulations that reproduce key features of human performance in behavioral experiments, and we discuss the model ? s implications and directions for future research.
recent work in computational psycholinguistics shows that morpheme lexica can be acquired in an unsupervised manner from a corpus of words by selecting the lexicon that best balances productivity and reuse ( e.g. <eos> goldwater et al. <eos> ( 2009 ) and others ). <eos> in this paper, we extend such work to the problem of acquiring non-concatenative morphology, proposing a simple model of morphology that can handle both concatenative and non-concatenative morphology and applying bayesian inference on two datasets of arabic and english verbs to acquire lexica. <eos> we show that our approach successfully extracts the non-contiguous triliteral root from arabic verb stems.
we use a set of enriched n-gram models to track grammaticality judgements for different sorts of passive sentences in english. <eos> we construct these models by specifying scoring functions to map the log probabilities ( logprobs ) of an n-gram model for a test set of sentences onto scores which depend on properties of the string related to the parameters of the model. <eos> we test our models on classification tasks for different kinds of passive sentences. <eos> our experiments indicate that our n-gram models achieve high accuracy in identifying ill-formed passives in which ill-formedness depends on local relations within the n-gram frame, but they are far less successful in detecting non-local relations that produce unacceptability in other types of passive construction. <eos> we take these results to indicate some of the strengths and the limitations of word and lexical class n-gram models as candidate representations of speakers ? <eos> grammatical knowledge.
reading experiments using naturalistic stimuli have shown unanticipated facilitations for completing center embeddings when frequency effects are factored out. <eos> to eliminate possible confounds due to surface structure, this paper introduces a processing model based on deep syntactic dependencies. <eos> results on eye-tracking data indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface embeddings.
there are few computational models of second language acquisition ( sla ). <eos> at the same time, many questions in the field of sla remain unanswered. <eos> in particular, sla patterns are difficult to study due to the large amount of variation between human learners. <eos> we present a computational model of second language construction learning that allows manipulating specific parameters such as age of onset and amount of exposure. <eos> we use the model to study general developmental patterns of sla and two specific effects sometimes found in empirical studies : construction priming and a facilitatory effect of skewed frequencies in the input. <eos> our simulations replicate the expected sla patterns as well as the two effects. <eos> our model can be used in further studies of various sla phenomena.
we augment an existing tag-based incremental syntactic formalism, pltag, with a semantic component designed to support the simultaneous modeling effects of thematic fit as well as syntactic and semantic predictions. <eos> pltag is a psycholinguistically-motivated formalism which extends the standard tag operations with a prediction and verification mechanism and has experimental support as a model of syntactic processing difficulty. <eos> we focus on the problem of formally modelling semantic role prediction in the context of an incremental parse and describe a flexible neo-davidsonian formalism and composition procedure to accompany a pltag parse. <eos> to this end, we also provide a means of augmenting the pltag lexicon with semantic annotation. <eos> to illustrate this, we run through an experimentally-relevant model case, wherein the resolution of semantic role ambiguities influences the resolution of syntactic ambiguities and vice versa.
this paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments. <eos> the tasks at issue are ( i ) identification of consistent primes based on their semantic relatedness to the target and ( ii ) correlation of semantic relatedness with latency times. <eos> we also provide an evaluation of the impact of specific model parameters on the prediction of priming. <eos> to the best of our knowledge, this is the first systematic evaluation of a wide range of dsm parameters in all possible combinations. <eos> an important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming.
an increasing body of empirical evidence suggests that concreteness is a fundamental dimension of semantic representation. <eos> by implementing both a vector space model and a latent dirichlet allocation ( lda ) model, we explore the extent to which concreteness is reflected in the distributional patterns in corpora. <eos> in one experiment, we show that that vector space models can be tailored to better model semantic domains of particular degrees of concreteness. <eos> in a second experiment, we show that the quality of the representations of abstract words in lda models can be improved by supplementing the training data with information on the physical properties of concrete concepts. <eos> we conclude by discussing the implications for computational systems and also for how concrete and abstract concepts are represented in the mind
discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text. <eos> using the penn discourse treebank, we investigate what information relevant to inferring discourse relations is conveyed by discourse connectives, and whether the specificity of discourse relations reflects general cognitive biases for establishing coherence. <eos> we also propose an approach to measure the effect of a discourse marker on sense identification according to the different levels of a relation sense hierarchy. <eos> this will open a way to the computational modeling of discourse processing.
we describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning. <eos> working in an inherently incremental framework, dynamic syntax, we show how words can be associated with probabilistic procedures for the incremental projection of meaning, providing a grammar which can be used directly in incremental probabilistic parsing and generation. <eos> we test this on child-directed utterances from the childes corpus, and show that it results in good coverage and semantic accuracy, without requiring annotation at the word level or any independent notion of syntax.
cultural heritage collections usually organise sets of items into exhibitions or guided tours. <eos> these items are often accompanied by text that describes the theme and topic of the exhibition and provides background context and details of connections with other items. <eos> the paths project brings the idea of guided tours to digital library collections where a tool to create virtual paths are used to assist with navigation and provide guides on particular subjects and topics. <eos> in this paper we characterise and analyse paths of items created by users of our online system. <eos> the analysis highlights that most users spend time selecting items relevant to their chosen topic, but few users took time to add background information to the paths. <eos> in order to address this, we conducted preliminary investigations to test whether wikipedia can be used to automatically add background text for sequences of items. <eos> in the future we would like to explore the automatic creation of full paths.
language transformation can be defined as translating between diachronically distinct language variants. <eos> we investigate the transformation of middle dutch into modern dutch by means of machine translation. <eos> we demonstrate that by using character overlap the performance of the machine translation process can be improved for this task.
differences between large-scale historical population archives and small decentralized databases can be used to improve data quality and record connectedness in both types of databases. <eos> a parser is developed to account for differences in syntax and data representation models. <eos> a matching procedure is described to discover records from different databases referring to the same historical event. <eos> the problem of verification without reliable benchmark data is addressed by matching on a subset of record attributes and measuring support for the match using a different subset of attributes. <eos> an application of the matching procedure for comparison of family trees is discussed. <eos> a visualization tool is described to present an interactive overview of comparison results.
cross-period ( diachronic ) thesaurus construction aims to enable potential users to search for modern terms and obtain semantically related terms from earlier periods in history. <eos> this is a complex task not previously addressed computationally. <eos> in this paper we introduce a semi-automatic iterative query expansion ( qe ) scheme for supporting cross-period thesaurus construction. <eos> we demonstrate the empirical benefit of our scheme for a jewish crossperiod thesaurus and evaluate its impact on recall and on the effectiveness of lexicographer manual effort.
we present an extension of the dualist tool that enables social scientists to engage directly with large twitter datasets. <eos> our approach supports collaborative construction of classifiers and associated gold standard data sets. <eos> the tool can be used to build classifier cascades that decomposes tweet streams, and provide analysis of targeted conversations. <eos> a central concern is to provide an environment in which social science researchers can rapidly develop an informed sense of what the datasets look like. <eos> the intent is that they develop, not only an informed view as to how the data could be fruitfully analysed, but also how feasible it is to analyse it in that way.
in our paper, we present a computational morphology for old and middle hungarian used in two research projects that aim at creating morphologically annotated corpora of old and middle hungarian. <eos> in addition, we present the web-based disambiguation tool used in the semi-automatic disambiguation of the annotations and the structured corpus query tool that has a unique but very useful feature of making corrections to the annotation in the query results possible.
in this paper we describe an application of language technology to policy formulation, where it can support policy makers assess the acceptance of a yet-unpublished policy before the policy enters public consultation. <eos> one of the key concepts is that instead of relying on thematic similarity, we extract arguments expressed in support or opposition of positions that are general statements that are, themselves, consistent with the policy or not. <eos> the focus of this paper in this overall pipeline, is identifying arguments in text : we present and empirically evaluate the hypothesis that verbal tense and mood are good indicators of arguments that have not been explored in the relevant literature.
we develop a pipeline consisting of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. <eos> our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneficial assistance for the political scientists and new application challenges for nlp. <eos> it is of particular importance to find a ? common language ? <eos> between the different disciplines. <eos> therefore, we use an interactive web-interface which is easily usable by non-experts. <eos> it interfaces an active learning algorithm which is complemented by the nlp pipeline to provide a rich feature selection. <eos> political scientists are thus enabled to use their own intuitions to find custom concepts.
manually assigned keywords provide a valuable means for accessing large document collections. <eos> they can serve as a shallow document summary and enable more efficient retrieval and aggregation of information. <eos> in this paper we investigate keywords in the context of the dutch folktale database, a large collection of stories including fairy tales, jokes and urban legends. <eos> we carry out a quantitative and qualitative analysis of the keywords in the collection. <eos> up to 80 % of the assigned keywords ( or a minor variation ) appear in the text itself. <eos> human annotators show moderate to substantial agreement in their judgment of keywords. <eos> finally, we evaluate a learning to rank approach to extract and rank keyword candidates. <eos> we conclude that this is a promising approach to automate this time intensive task.
we propose to bring together two kinds of linguistic resources ? interlinear glossed text ( igt ) and a language-independent precision grammar resource ? to automatically create precision grammars in the context of language documentation. <eos> this paper takes the first steps in that direction by extracting major-constituent word order and case system properties from igt for a diverse sample of languages.
in this paper, we argue that comparable collections of historical written resources can help overcoming typical challenges posed by heritage texts enhancing spelling normalization, pos-tagging and subsequent diachronic linguistic analyses. <eos> thus, we present a comparable corpus of historical german recipes and show how such a comparable text collection together with the application of innovative mt inspired strategies allow us ( i ) to address the word form normalization problem and ( ii ) to automatically generate a diachronic dictionary of spelling variants. <eos> such a diachronic dictionary can be used both for spelling normalization and for extracting new ? translation ? <eos> ( word formation/change ) rules for diachronic spelling variants. <eos> moreover, our approach can be applied virtually to any diachronic collection of texts regardless of the time span they represent. <eos> a first evaluation shows that our approach compares well with state-of-art approaches.
we present current work dealing with the integration of a multilingual thesaurus for social sciences in a nlp framework for supporting knowledge-driven information extraction in the field of social sciences. <eos> we describe the various steps that lead to a running ie system : lexicalization of the labels of the thesaurus and semi-automatic generation of domain specific ie grammars, with their subsequent implementation in a finite state engine. <eos> finally, we outline the actual field of application of the ie system : analysis of social media for recognition of relevant topics in the context of elections.
applying machine translation ( mt ) to literary texts involves the same domain shift challenges that arise for any sublanguage ( e.g. <eos> medical or scientific ). <eos> however, it also introduces additional challenges. <eos> one focus in the discussion of translation theory in the humanities has been on the human translator ? s role in staying faithful to an original text versus adapting it to make it more familiar to readers. <eos> in contrast to other domains, one objective in literary translation is to preserve the experience of reading a text when moving to the target language. <eos> we use existing mt systems to translate samples of french literature into english. <eos> we then use qualitative analysis grounded in translation theory and real example outputs in order to address what makes literary translation particularly hard and the potential role of the machine in it.
in this paper we look at a task at border of natural language processing, historical linguistics and the study of language development, namely that of identifying the time when a text was written. <eos> we use machine learning classification using lexical, word ending and dictionary-based features, with linear support vector machines and random forests. <eos> we find that lexical features are the most helpful.
as the amount of cultural data available on the semantic web is expanding, the demand of accessing this data in multiple languages is increasing. <eos> previous work on multilingual access to cultural heritage information has shown that at least two different problems must be dealt with when mapping from ontologies to natural language : ( 1 ) mapping multilingual metadata to interoperable knowledge sources ; ( 2 ) assigning multilingual knowledge to cultural data. <eos> this paper presents our effort to deal with these problems. <eos> we describe our experiences with processing museum data extracted from two distinct sources, harmonizing this data and making its content accessible in natural language. <eos> we extend prior work in two ways. <eos> first, we present a grammar-based system that is designed to generate coherent texts from semantic web ontologies in 15 languages. <eos> second, we describe how this multilingual system is exploited to form queries using the standard query language sparql. <eos> the generation and retrieval system builds on w3c standards and is available for further research.
a current increasing trend in machine translation is to combine data-driven and rule-based techniques. <eos> such combinations typically involve the hybridization of different paradigms such as, for instance, the introduction of linguistic knowledge into statistical paradigms, the incorporation of data-driven components into rulebased paradigms, or the pre- and postprocessing of either sort of translation system outputs. <eos> aiming at bringing together researchers and practitioners from the different multidisciplinary areas working in these directions, as well as at creating a brainstorming and discussion venue for hybrid translation approaches, the hytra initiative was born. <eos> this paper gives an overview of the second workshop on hybrid approaches to translation ( hytra 2013 ) concerning its motivation, contents and outcomes.
we explore the selection of training data for language models using perplexity. <eos> we introduce three novel models that make use of linguistic information and evaluate them on three different corpora and two languages. <eos> in four out of the six scenarios a linguistically motivated method outperforms the purely statistical state-of-theart approach. <eos> finally, a method which combines surface forms and the linguistically motivated methods outperforms the baseline in all the scenarios, selecting data whose perplexity is between 3.49 % and 8.17 % ( depending on the corpus and language ) lower than that of the baseline.
we have implemented a rule-based prototype of a spanish-to-cuzco quechua mt system enhanced through the addition of statistical components. <eos> the greatest difficulty during the translation process is to generate the correct quechua verb form in subordinated clauses. <eos> the prototype has several rules that decide which verb form should be used in a given context. <eos> however, matching the context in order to apply the correct rule depends crucially on the parsing quality of the spanish input. <eos> as the form of the subordinated verb depends heavily on the conjunction in the subordinated spanish clause and the semantics of the main verb, we extracted this information from two treebanks and trained different classifiers on this data. <eos> we tested the best classifier on a set of 4 texts, increasing the correct subordinated verb forms from 80 % to 89 %.
dependency parsers are almost ubiquitously evaluated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures. <eos> the structures may have more complexity due to their coordination structure or attachment rules. <eos> as dependency parses are basic structures in which other systems are built upon, it would seem more reasonable to judge these parsers down the nlp pipeline. <eos> we show results from 7 individual parsers, including dependency and constituent parsers, and 3 ensemble parsing techniques with their overall effect on a machine translation system, treex, for english to czech translation. <eos> we show that parsers ? <eos> uas scores are more correlated to the nist evaluation metric than to the bleu metric, however we see increases in both metrics.
chinese and japanese have a different sentence structure. <eos> reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. <eos> however, chinese has a loose word order, and chinese parsers that extract the phrase structure do not perform well. <eos> we propose a framework where only pos tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. <eos> we show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods.
reordering is pre-processing stage for statistical machine translation ( smt ) system where the words of the source sentence are reordered as per the syntax of the target language. <eos> we are proposing a rich set of rules for better reordering. <eos> the idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase based smt system. <eos> reordering also helps the decoding process and hence improving the machine translation quality. <eos> we have observed significant improvements in the translation quality by using our approach over the baseline smt. <eos> we have used bleu, nist, multi-reference word error rate, multi-reference position independent error rate for judging the improvements. <eos> we have exploited open source smt toolkit moses to develop the system.
phrase-based statistical machine translation systems can generate translations of reasonable quality in the case of language pairs with similar structure and word order. <eos> however, if the languages are more distant from a grammatical point of view, the quality of translations is much behind the expectations, since the baseline translation system can not cope with long distance reordering of words and the mapping of word internal grammatical structures. <eos> in our paper, we present a method that tries to overcome these problems in the case of english-hungarian translation by applying reordering rules prior to the translation process and by creating morpheme-based and factored models. <eos> although automatic evaluation scores do not reliably reflect the improvement in all cases, human evaluation of our systems shows that readability and accuracy of the translations were improved both by reordering and applying richer models.
we explore the intersection of rule-based and statistical approaches in machine translation, with a particular focus on past and current work here at microsoft research. <eos> until about ten years ago, the only machine translation systems worth using were rule-based and linguistically-informed. <eos> along came statistical approaches, which use large corpora to directly guide translations toward expressions people would actually say. <eos> rather than making local decisions when writing and conditioning rules, goodness of translation was modeled numerically and free parameters were selected to optimize that goodness. <eos> this led to huge improvements in translation quality as more and more data was consumed. <eos> by necessity, the pendulum is swinging towards the inclusion of linguistic features in mt systems. <eos> we describe some of our statistical and non-statistical attempts to incorporate linguistic insights into machine translation systems, showing what is currently working well, and what isn ? t. <eos> we also look at trade-offs in using linguistic knowledge ( ? rules ? ) <eos> in pre- or post-processing by language pair, with a particular eye on the return on investment as training data increases in size.
we present a minimalist, unsupervised learning model that induces relatively clean phrasal inversion transduction grammars by employing the minimum description length principle to drive search over a space defined by two opposing extreme types of itgs. <eos> in comparison to most current smt approaches, the model learns a very parsimonious phrase translation lexicons that provide an obvious basis for generalization to abstract translation schemas. <eos> to do this, the model maintains internal consistency by avoiding use of mismatched or unrelated models, such as word alignments or probabilities from ibm models. <eos> the model introduces a novel strategy for avoiding the pitfalls of premature pruning in chunking approaches, by incrementally splitting an itgwhile using a second itg to guide this search.
this paper presents a hybrid approach to the enhancement of english to arabic statistical machine translation quality. <eos> machine translation has been defined as the process that utilizes computer software to translate text from one natural language to another. <eos> arabic, as a morphologically rich language, is a highly flexional language, in that the same root can lead to various forms according to its context. <eos> statistical machine translation ( smt ) engines often show poor syntax processing especially when the language used is morphologically rich such as arabic. <eos> in this paper, to overcome these shortcomings, we describe our hybrid approach which integrates knowledge of the arabic language into statistical machine translation. <eos> in this framework, we propose the use of a featured language model sflm ( sma ? li et al., 2004 ) to be able to integrate syntactic and grammatical knowledge about each word. <eos> in this paper, we first discuss some challenges in translating from english to arabic and we explore various techniques to improve performance on this task. <eos> we apply a morphological segmentation step for arabic words and we present our hybrid approach by identifying morpho-syntactic class of each segmented word to build up our statistical feature language model. <eos> we propose the scheme for recombining the segmented arabic word, and describe their effect on translation.
this paper presents the methods which are based on the part-of-speech ( pos ) and auto alignment information to improve the quality of machine translation result and the word alignment. <eos> we utilize different types of pos tag to restructure source sentences and use an alignment-based reordering method to improve the alignment. <eos> after applying the reordering method, we use two phrase tables in the decoding part to keep the translation performance. <eos> our experiments on korean-chinese show that our methods can improve the alignment and translation results. <eos> since the proposed approach reduces the size of the phrase table, multi-tables are considered. <eos> the combination of all these methods together would get the best translation result.
since the tunisian revolution, tunisian dialect ( td ) used in daily life, has became progressively used and represented in interviews, news and debate programs instead of modern standard arabic ( msa ). <eos> this situation has important negative consequences for natural language processing ( nlp ) : since the spoken dialects are not officially written and do not have standard orthography, it is very costly to obtain adequate corpora to use for training nlp tools. <eos> furthermore, there are almost no parallel corpora involving td and msa. <eos> in this paper, we describe the creation of tunisian dialect text corpus as well as a method for building a bilingual dictionary, in order to create language model for speech recognition system for the tunisian broadcast news. <eos> so, we use explicit knowledge about the relation between td and msa.
this paper proposes a hybrid word alignment model for phrase-based statistical machine translation ( pb-smt ). <eos> the proposed hybrid alignment model provides most informative alignment links which are offered by both unsupervised and semi-supervised word alignment models. <eos> two unsupervised word alignment models ( giza++ and berkeley aligner ) and a rule based aligner are combined together. <eos> the rule based aligner only aligns named entities ( nes ) and chunks. <eos> the nes are aligned through transliteration using a joint source-channel model. <eos> chunks are aligned employing a bootstrapping approach by translating the source chunks into the target language using a baseline pb-smt system and subsequently validating the target chunks using a fuzzy matching technique against the target corpus. <eos> all the experiments are carried out after single-tokenizing the multi-word nes. <eos> our best system provided significant improvements over the baseline as measured by bleu.
we present initial work on an inexpensive approach for building largevocabulary lexical selection modules for hybrid rbmt systems by framing lexical selection as a sequence labeling problem. <eos> we submit that maximum entropy markov models ( memms ) are a sensible formalism for this problem, due to their ability to take into account many features of the source text, and show how we can build a combination memm/hmm system that allows mt system implementors flexibility regarding which words have their lexical choices modeled with classifiers. <eos> we present initial results showing successful use of this system both in translating english to spanish and spanish to guarani.
in the context of a hybrid french-toenglish smt system for translating online forum posts, we present two methods for addressing the common problem of homophone confusions in colloquial written language. <eos> the first is based on hand-coded rules ; the second on weighted graphs derived from a large-scale pronunciation resource, with weights trained from a small bicorpus of domain language. <eos> with automatic evaluation, the weighted graph method yields an improvement of about +0.63 bleu points, while the rulebased method scores about the same as the baseline. <eos> on contrastive manual evaluation, both methods give highly significant improvements ( p < 0.0001 ) and score about equally when compared against each other.
resource limitation is challenging for crossdomain adaption. <eos> this paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results, and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline mt system. <eos> the adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant.
the present article provides a comprehensive review of the work carried out on developing presemt, a hybrid language-independent machine translation ( mt ) methodology. <eos> this methodology has been designed to facilitate rapid creation of mt systems for unconstrained language pairs, setting the lowest possible requirements on specialised resources and tools. <eos> given the limited availability of resources for many languages, only a very small bilingual corpus is required, while language modelling is performed by sampling a large target language ( tl ) monolingual corpus. <eos> the article summarises implementation decisions, using the greek-english language pair as a test case. <eos> evaluation results are reported, for both objective and subjective metrics. <eos> finally, main error sources are identified and directions are described to improve this hybrid mt methodology.
in this paper, we introduce a syntax-based sentence simplifier that models simplification using a probabilistic synchronous tree substitution grammar ( stsg ). <eos> to improve the stsg model specificity we utilize a multi-level backoff model with additional syntactic annotations that allow for better discrimination over previous stsg formulations. <eos> we compare our approach to t3 ( cohn and lapata, 2009 ), a recent stsg implementation, as well as two state-of-the-art phrase-based sentence simplifiers on a corpus of aligned sentences from english and simple english wikipedia. <eos> our new approach performs significantly better than t3, similarly to human simplifications for both simplicity and fluency, and better than the phrasebased simplifiers for most of the evaluation metrics.
in this paper we report our experiments in creating a parallel corpus using german/simple german documents from the web. <eos> we require parallel data to build a statistical machine translation ( smt ) system that translates from german into simple german. <eos> parallel data for smt systems needs to be aligned at the sentence level. <eos> we applied an existing monolingual sentence alignment algorithm. <eos> we show the limits of the algorithm with respect to the language and domain of our data and suggest ways of circumventing them.
ign language center, bldg. <eos> 420, room 119 monterey, ca 93944, usatamas.g.marius.civ @ mail.milabstract in this paper, we introduce a new baseline for language-independent text difficulty assessment applied to the interagency language roundtable ( ilr ) proficiency scale. <eos> we demonstrate that reading level assessment is a discriminative problem that is best-suited for regression. <eos> our baseline uses z-normalized shallow length features and tf-log weighted vectors on bag-of-words for arabic, dari, english, and pashto. <eos> we compare support vector machines and the margin-infused relaxed algorithm measured by mean squared error. <eos> we provide an analysis of which features are most predictive of a given level.
the paper discusses the main issues regarding the reading skills and comprehension proficiency in written bulgarian of people with communication difficulties, and deaf people, in particular. <eos> we consider several key components of text comprehension which pose a challenge for deaf readers and propose a rule-based system for automatic modification of bulgarian texts intended to facilitate comprehension by deaf people, to assist education, etc. <eos> in order to demonstrate the benefits of such a system and to evaluate its performance, we have carried out a study among a group of deaf people who use bulgarian sign language ( bulsl ) as their primary language ( primary bulsl users ), which compares the comprehensibility of original texts and their modified versions. <eos> the results shows a considerable improvement in readability when using modified texts, but at the same time demonstrates that the level of comprehension is still low, and that a complex set of modifications will have to be implemented to attain satisfactory results.
comma placements in chinese text are relatively arbitrary although there are some syntactic guidelines for them. <eos> in this research, we attempt to improve the readability of text by optimizing comma placements through integration of linguistic features of text and gaze features of readers. <eos> we design a comma predictor for general chinese text based on conditional random field models with linguistic features. <eos> after that, we build a rule-based filter for categorizing commas in text according to their contribution to readability based on the analysis of gazes of people reading text with and without commas. <eos> the experimental results show that our predictor reproduces the comma distribution in the penn chinese treebank with 78.41 in f1-score and commas chosen by our filter smoothen certain gaze behaviors.
an increasing range of features is being used for automatic readability classification. <eos> the impact of the features typically is evaluated using reference corpora containing graded reading material. <eos> but how do the readability models and the features they are based on perform on real-world web texts ? <eos> in this paper, we want to take a step towards understanding this aspect on the basis of a broad range of lexical and syntactic features and several web datasets we collected. <eos> applying our models to web search results, we find that the average reading level of the retrieved web documents is relatively high. <eos> at the same time, documents at a wide range of reading levels are identified and even among the top-10 search results one finds documents at the lower levels, supporting the potential usefulness of readability ranking for the web. <eos> finally, we report on generalization experiments showing that the features we used generalize well across different web sources.
the task of identifying complex words ( cws ) is important for lexical simplification, however it is often carried out with no evaluation of success. <eos> there is no basis for comparison of current techniques and, prior to this work, there has been no standard corpus or evaluation technique for the cw identification task. <eos> this paper addresses these shortcomings with a new corpus for evaluating a system ? s performance in identifying cws. <eos> simple wikipedia edit histories were mined for instances of single word lexical simplifications. <eos> the corpus contains 731 sentences, each with one annotated cw. <eos> this paper describes the method used to produce the cw corpus and presents the results of evaluation, showing its validity.
in this paper we report the results of a pilot study of basing readability prediction on training data annotated with reading time. <eos> although reading time is known to be a good metric for predicting readability, previous work has mainly focused on annotating the training data with subjective readability scores usually on a 1 to 5 scale. <eos> instead of the subjective assessments of complexity, we use the more objective measure of reading time. <eos> we create and evaluate a predictor using the binary classification problem ; the predictor identifies the better of two documents correctly with 68.55 % accuracy. <eos> we also report a comparison of predictors based on reading time and on readability scores.
we present three ways of inducing probability distributions on derivation trees produced by minimalist grammars, and give their maximum likelihood estimators. <eos> we argue that a parameterization based on locally normalized log-linear models balances competing requirements for modeling expressiveness and computational tractability.
adjuncts are characteristically optional, but many, such as adverbs and adjectives, are strictly ordered. <eos> in minimalist grammars ( mgs ), it is straightforward to account for optionality or ordering, but not both. <eos> i present an extension of mgs, mgs with adjunction, which accounts for optionality and ordering simply by keeping track of two pieces of information at once : the original category of the adjoined-to phrase, and the category of the adjunct most recently adjoined. <eos> by imposing a partial order on the categories, the adjoin operation can require that higher adjuncts precede lower adjuncts, but not vice versa, deriving order.
we study the complexity of uniform membership for linear context-free rewriting systems, i.e., the problem where we are given a string w and a grammar g and are asked whether w ? <eos> l ( g ). <eos> in particular, we use parameterized complexity theory to investigate how the complexity depends on various parameters. <eos> while we focus primarily on rank and fan-out, derivation length is also considered.
timelines interpreting interval temporal logic formulas are segmented into strings which serve as semantic representations for tense and aspect. <eos> the strings have bounded but refinable granularity, suitable for analyzing ( im ) perfectivity, durativity, telicity, and various relations including branching.
this paper develops a compositional vector-based semantics of relative pronouns within a categorical framework. <eos> frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. <eos> we develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics.
attested and ? pathological ? <eos> vowel harmony patterns are studied in the context of subclasses of regular functions. <eos> the analysis suggests that the computational complexity of phonology can be reduced from regular to weakly deterministic.
this paper shows how factored finitestate representations of subregular language classes are identifiable in the limit from positive data by learners which are polytime iterative and optimal. <eos> these representations are motivated in two ways. <eos> first, the size of this representation for a given regular language can be exponentially smaller than the size of the minimal deterministic acceptor recognizing the language. <eos> second, these representations ( including the exponentially smaller ones ) describe actual formal languages which successfully model natural language phenomenon, notably in the subfield of phonology.
in this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the a ? <eos> algorithm proposed in corlett and penn ( 2010 ) for deciphering letter-substitution ciphers. <eos> we argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language, and we develop a principled way of incorporating entropy into our complexity analysis. <eos> specifically, we find that the low entropy of natural languages can allow us, with high probability, to bound the depth of the heuristic values expanded in the search. <eos> this leads to a novel probabilistic bound on search depth in these tasks.
the consistency method has been established as the standard strategy for extracting high quality translation rules in statistical machine translation ( smt ). <eos> however, no attention has been drawn to why this method is successful, other than empirical evidence. <eos> using concepts from graph theory, we identify the relation between consistency and components of graphs that represent word-aligned sentence pairs. <eos> it can be shown that phrase pairs of interest to smt form a sigma-algebra generated by components of such graphs. <eos> this construction is generalized by allowing segmented sentence pairs, which in turn gives rise to a phrase-based generative model. <eos> a by-product of this model is a derivation of probability mass functions for random partitions. <eos> these are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence.
this document overviews the strategy, effort and aftermath of the multiling 2013 multilingual summarization data collection. <eos> we describe how the data contributors of multiling collected and generated a multilingual multi-document summarization corpus on 10 different languages : arabic, chinese, czech, english, french, greek, hebrew, hindi, romanian and spanish. <eos> we discuss the rationale behind the main decisions of the collection, the methodology used to generate the multilingual corpus, as well as challenges and problems faced per language. <eos> this paper overviews the work on arabic, chinese, english, greek, and romanian languages. <eos> a second part, covering the remaining languages, is available as a distinct paper in the multiling 2013 proceedings.
this document overviews the strategy, effort and aftermath of the multiling 2013 multilingual summarization data collection. <eos> we describe how the data contributors of multiling collected and generated a multilingual multi-document summarization corpus on 10 different languages : arabic, chinese, czech, english, french, greek, hebrew, hindi, romanian and spanish. <eos> we discuss the rationale behind the main decisions of the collection, the methodology used to generate the multilingual corpus, as well as challenges and problems faced per language. <eos> this paper overviews the work on czech, hebrew and spanish languages.
the multiling 2013 workshop of acl 2013 posed a multi-lingual, multidocument summarization task to the summarization community, aiming to quantify and measure the performance of multi-lingual, multi-document summarization systems across languages. <eos> the task was to create a 240 ? 250 word summary from 10 news articles, describing a given topic. <eos> the texts of each topic were provided in 10 languages ( arabic, chinese, czech, english, french, greek, hebrew, hindi, romanian, spanish ) and each participant generated summaries for at least 2 languages. <eos> the evaluation of the summaries was performed using automatic and manual processes. <eos> the participating systems submitted over 15 runs, some providing summaries across all languages. <eos> an automatic evaluation task was also added to this year ? s set of tasks. <eos> the evaluation task meant to determine whether automatic measures of evaluation can function well in the multi-lingual domain. <eos> this paper provides a brief description related to the data of both tasks, the evaluation methodology, as well as an overview of participation and corresponding results.
the 2013 association for computational linguistics multiling pilot posed a task to measure the performance of multilingual, single-document, summarization systems using a dataset derived from many wikipedias. <eos> the objective of the pilot was to assess automatic summarization of multilingual text documents outside the news domain and the potential of using wikipedia articles for such research. <eos> this report describes the pilot task, the dataset, the methods used to evaluate the submitted summaries, and the overall performance of each participant ? s system.
this report provides a description of the methods applied in cist system participating acl multiling 2013. <eos> summarization is based on sentence extraction. <eos> hlda topic model is adopted for multilingual multi-document modeling. <eos> various features are combined to evaluate and extract candidate summary sentences.
in this paper we present a linear model for the problem of text summarization, where a summary preserves the information coverage as much as possible in comparison to the original document set. <eos> we reduce the problem of finding the best summary to the problem of finding the point on a convex polytope closest to the given hyperplane, and solve it efficiently with the help of fractional linear programming. <eos> we supply here an overview of our system, titled poly2, that participated in the multiling contest at acl 2013.
the paper describes our participation in the multi-document summarization task of multiling-2013. <eos> the community initiative was born as a pilot task for the text analysis conference in 2011. <eos> this year the corpus was extended by new three languages and another five topics, covering in total 15 topics in 10 languages. <eos> our summariser is based on latent semantic analysis and it is in principle language independent. <eos> its results on the multiling-2011 corpus were promising. <eos> the generated summaries were ranked first in several languages based on various metrics. <eos> the summariser with minor changes was run on the updated 2013 corpus. <eos> although we do not have the manual evaluation results yet the rouge-2 score indicates good results again. <eos> the summariser produced best summaries in 6 from 10 considered languages according to the rouge-2 metric.
in this paper we show the results of our participation in the multiling 2013 summarisation tasks. <eos> we participated with single-document and multi-document corpus-based summarisers for both arabic and english languages. <eos> the summarisers used word frequency lists and log likelihood calculations to generate single and multi document summaries. <eos> the single and multi summaries generated by our systems were evaluated by arabic and english native speaker participants and by different automatic evaluation metrics, rouge, autosummeng, memog and npower. <eos> we compare our results to other systems that participated in the same tracks on both arabic and english languages. <eos> our single-document summarisers performed particularly well in the automatic evaluation with our english singledocument summariser performing better on average than the results of the other participants. <eos> our arabic multi-document summariser performed well in the human evaluation ranking second.
this paper describes the architecture of uaic1 ? s summarization system participating at multiling ? <eos> 2013. <eos> the architecture includes language independent text processing modules, but also modules that are adapted for one language or another. <eos> in our experiments, the languages under consideration are bulgarian, german, greek, english, and romanian. <eos> our method exploits the cohesion and coherence properties of texts to build discourse structures. <eos> the output of the parsing process is used to extract general summaries.
we present vector space semantic parsing ( vssp ), a framework for learning compositional models of vector space semantics. <eos> our framework uses combinatory categorial grammar ( ccg ) to define a correspondence between syntactic categories and semantic representations, which are vectors and functions on vectors. <eos> the complete correspondence is a direct consequence of minimal assumptions about the semantic representations of basic syntactic categories ( e.g., nouns are vectors ), and ccg ? s tight coupling of syntax and semantics. <eos> furthermore, this correspondence permits nonuniform semantic representations and more expressive composition operations than previous work. <eos> vssp builds a ccg semantic parser respecting this correspondence ; this semantic parser parses text into lambda calculus formulas that evaluate to vector space representations. <eos> in these formulas, the meanings of words are represented by parameters that can be trained in a task-specific fashion. <eos> we present experiments using noun-verbnoun and adverb-adjective-noun phrases which demonstrate that vssp can learn composition operations that rnn ( socher et al, 2011 ) and mv-rnn ( socher et al, 2012 ) can not.
in this paper, we address the problem of how to use semantics to improve syntactic parsing, by using a hybrid reranking method : a k-best list generated by a symbolic parser is reranked based on parsecorrectness scores given by a compositional, connectionist classifier. <eos> this classifier uses a recursive neural network to construct vector representations for phrases in a candidate parse tree in order to classify it as syntactically correct or not. <eos> tested on the wsj23, our method achieved a statistically significant improvement of 0.20 % on f-score ( 2 % error reduction ) and 0.95 % on exact match, compared with the state-ofthe-art berkeley parser. <eos> this result shows that vector-based compositional semantics can be usefully applied in syntactic parsing, and demonstrates the benefits of combining the symbolic and connectionist approaches.
in this paper we present a novel approach ( sdsm ) that incorporates structure in distributional semantics. <eos> sdsm represents meaning as relation specific distributions over syntactic neighborhoods. <eos> we empirically show that the model can effectively represent the semantics of single words and provides significant advantages when dealing with phrasal units that involve word composition. <eos> in particular, we demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches for composing distributional semantic representations on an artificial task of verb sense disambiguation and a real-world application of judging event coreference.
we present a letter-based encoding for words in continuous space language models. <eos> we represent the words completely by letter n-grams instead of using the word index. <eos> this way, similar words will automatically have a similar representation. <eos> with this we hope to better generalize to unknown or rare words and to also capture morphological information. <eos> we show their influence in the task of machine translation using continuous space language models based on restricted boltzmann machines. <eos> we evaluate the translation quality as well as the training time on a german-to-english translation task of ted and university lectures as well as on the news translation task translating from english to german. <eos> using our new approach a gain in bleu score by up to 0.4 points can be achieved.
classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors. <eos> in this paper, we explore an alternative path to use syntax in feature spaces : the distributed representation ? parsers ? <eos> ( drp ). <eos> the core of the idea is straightforward : drps directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations. <eos> results show that drps produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information.
in recent years, there has been widespread interest in compositional distributional semantic models ( cdsms ), that derive meaning representations for phrases from their parts. <eos> we present an evaluation of alternative cdsms under truly comparable conditions. <eos> in particular, we extend the idea of baroni and zamparelli ( 2010 ) and guevara ( 2010 ) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature, so that all models can be tested under the same training conditions. <eos> the linguistically motivated functional model of baroni and zamparelli ( 2010 ) and coecke et al ( 2010 ) emerges as the winner in all our tests.
we introduce a new 50-dimensional embedding obtained by spectral clustering of a graph describing the conceptual structure of the lexicon. <eos> we use the embedding directly to investigate sets of antonymic pairs, and indirectly to argue that function application in cvsms requires not just vectors but two transformations ( corresponding to subject and object ) as well.
this paper presents a comparative study of 5 different types of word space models ( wsms ) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions. <eos> many combinations of wsms and measures have never been applied to the task before. <eos> the study follows biemann and giesbrecht ( 2011 ) who attempted to find a list of expressions for which the compositionality assumption ? <eos> the meaning of an expression is determined by the meaning of its constituents and their combination ? <eos> does not hold. <eos> our results are very promising and can be appreciated by those interested in wsms, compositionality, and/or relevant evaluation methods.
with the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. <eos> in this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. <eos> as a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models.
most distributional models of word similarity represent a word type by a single vector of contextual features, even though, words commonly have more than one sense. <eos> the multiple senses can be captured by employing several vectors per word in a multi-prototype distributional model, prototypes that can be obtained by first constructing all the context vectors for the word and then clustering similar vectors to create sense vectors. <eos> storing and clustering context vectors can be expensive though. <eos> as an alternative, we introduce multi-sense random indexing, which performs on-the-fly ( incremental ) clustering. <eos> to evaluate the method, a number of measures for word similarity are proposed, both contextual and non-contextual, including new measures based on optimal alignment of word senses. <eos> experimental results on the task of predicting semantic textual similarity do, however, not show a systematic difference between singleprototype and multi-prototype models.
we present a novel compositional, generative model for vector space representations of meaning. <eos> this model reformulates earlier tensor-based approaches to vector space semantics as a top-down process, and provides efficient algorithms for transformation from natural language to vectors and from vectors to natural language. <eos> we describe procedures for estimating the parameters of the model from positive examples of similar phrases, and from distributional representations, then use these procedures to obtain similarity judgments for a set of adjective-noun pairs. <eos> the model ? s estimation of the similarity of these pairs correlates well with human annotations, demonstrating a substantial improvement over several existing compositional approaches in both settings.
while words in documents are generally treated as discrete entities, they can be embedded in a euclidean space which reflects an a priori notion of similarity between them. <eos> in such a case, a text document can be viewed as a bag-ofembedded-words ( boew ) : a set of realvalued vectors. <eos> we propose a novel document representation based on such continuous word embeddings. <eos> it consists in non-linearly mapping the wordembeddings in a higher-dimensional space and in aggregating them into a documentlevel representation. <eos> we report retrieval and clustering experiments in the case where the word-embeddings are computed from standard topic models showing significant improvements with respect to the original topic models.
we develop a recursive neural network ( rnn ) to extract answers to arbitrary natural language questions from supporting sentences, by training on a crowdsourced data set ( to be released upon presentation ). <eos> the rnn defines feature representations at every node of the parse trees of questions and supporting sentences, when applied recursively, starting with token vectors from a neural probabilistic language model. <eos> in contrast to prior work, we fix neither the types of the questions nor the forms of the answers ; the system classifies tokens to match a substring chosen by the question ? s author. <eos> our classifier decides to follow each parse tree node of a support sentence or not, by classifying its rnn embedding together with those of its siblings and the root node of the question, until reaching the tokens it selects as the answer. <eos> a novel co-training task for the rnn, on subtree recognition, boosts performance, along with a scheme to consistently handle words that are not well-represented in the language model. <eos> on our data set, we surpass an open source system epitomizing a classic ? pattern bootstrapping ? <eos> approach to question answering.
the compositionality of meaning extends beyond the single sentence. <eos> just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. <eos> we introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. <eos> the sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. <eos> the discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. <eos> the discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. <eos> without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment.
e for infocomm research, 1 fusionpolis way, singaporejaw97 @ georgetown.edu { rembanchs, hli } @ i2r.a-star.edu.sgabstract we present a new approach to dialogue processing in terms of ? meaning units ?. <eos> in our annotation task, we asked speakers of english and chinese to mark boundaries where they could construct the maximal concept using minimal words. <eos> we compared english data across genres ( news, literature, and policy ). <eos> we analyzed the agreement for annotators using a state-ofthe-art segmentation similarity algorithm and compared annotations with a random baseline. <eos> we found that annotators are able to identify meaning units systematically, even though they may disagree on the quantity and position of units. <eos> our analysis includes an examination of phrase structure for annotated units using constituency parses.
a number of approaches have been taken to improve lexical consistency in statistical machine translation. <eos> however, little has been written on the subject of where and when to encourage consistency. <eos> i present an analysis of human authored translations, focussing on words belonging to different parts-of-speech across a number of different genres.
explicit discourse connectives in a source language text are not always translated to comparable words or phrases in the target language. <eos> the paper provides a corpus analysis and a method for semi-automatic detection of such cases. <eos> results show that discourse connectives are not translated into comparable forms ( or even any form at all ), in up to 18 % of human reference translations from english to french or german. <eos> in machine translation, this happens much less frequently ( up to 8 % only ). <eos> work in progress aims to capture this natural implicitation of discourse connectives in current statistical machine translation models.
we present a suggestive finding regarding the loss of associative texture in the process of machine translation, using comparisons between ( a ) original and backtranslated texts, ( b ) reference and system translations, and ( c ) better and worse mt systems. <eos> we represent the amount of association in a text using word association profile ? <eos> a distribution of pointwise mutual information between all pairs of content word types in a text. <eos> we use the average of the distribution, which we term lexical tightness, as a single measure of the amount of association in a text. <eos> we show that the lexical tightness of humancomposed texts is higher than that of the machine translated materials ; human references are tighter than machine translations, and better mt systems produce lexically tighter translations. <eos> while the phenomenon of the loss of associative texture has been theoretically predicted by translation scholars, we present a measure capable of quantifying the extent of this phenomenon.
the correct translation of verb tenses ensures that the temporal ordering of events in the source text is maintained in the target text. <eos> this paper assesses the utility of automatically labeling english simple past verbs with a binary discursive feature, narrative vs. non-narrative, for statistical machine translation ( smt ) into french. <eos> the narrativity feature, which helps deciding which of the french past tenses is a correct translation of the english simple past, can be assigned with about 70 % accuracy ( f1 ). <eos> the narrativity feature improves smt by about 0.2 bleu points when a factored smt system is trained and tested on automatically labeled english-french data. <eos> more importantly, manual evaluation shows that verb tense translation and verb choice are improved by respectively 9.7 % and 3.4 % ( absolute ), leading to an overall improvement of verb translation of 17 % ( relative ).
the paper presents machine translation experiments from english to czech with a large amount of manually annotated discourse connectives. <eos> the gold-standard discourse relation annotation leads to better translation performance in ranges of 4 ? 60 % for some ambiguous english connectives and helps to find correct syntactical constructs in czech for less ambiguous connectives. <eos> automatic scoring confirms the stability of the newly built discourseaware translation systems. <eos> error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful.
we present a novel approach to the translation of the english personal pronoun it to czech. <eos> we conduct a linguistic analysis on how the distinct categories of it are usually mapped to their czech counterparts. <eos> armed with these observations, we design a discriminative translation model of it, which is then integrated into the tectomt deep syntax mt framework. <eos> features in the model take advantage of rich syntactic annotation tectomt is based on, external tools for anaphoricity resolution, lexical co-occurrence frequencies measured on a large parallel corpus and gold coreference annotation. <eos> even though the new model for it exhibits no improvement in terms of bleu, manual evaluation shows that it outperforms the original solution in 8.5 % sentences containing it.
we present an approach to feature weight optimization for document-level decoding. <eos> this is an essential task for enabling future development of discourse-level statistical machine translation, as it allows easy integration of discourse features in the decoding process. <eos> we extend the framework of sentence-level feature weight optimization to the document-level. <eos> we show experimentally that we can get competitive and relatively stable results when using a standard set of features, and that this framework also allows us to optimize documentlevel features, which can be used to model discourse phenomena.
this paper describes the process of composing problems that are suitable for competitions in linguistics. <eos> the type of problems described is ? rosetta stone ? <eos> ? a bilingual problem where typically one of the languages is unknown, and the other is the native language of the person solving the problem. <eos> the process includes selecting phenomena, composing and arranging the data and assignments in order to illustrate the phenomena, and verifying the solvability and complexity of the problem.
the paper is focused on self-contained linguistic problems based on text corpora. <eos> we argue that corpus-based problems differ from traditional linguistic problems because they make it possible to represent language variation. <eos> furthermore, they often require basic statistical thinking from the students. <eos> the practical value of using data obtained from text corpora for teaching linguistics through linguistic problems is shown.
multilinguality has been an essential feature of the international linguistic olympiad since its conception. <eos> although deemed most desirable, the production of a problem set in several parallel versions and the verification of their equivalence is a time-consuming and errorprone task. <eos> this paper tells about the efforts to develop tools and methods which increase its efficiency and reliability.
the australian computational and linguistics olympiad ( ozclo ) started in 2008 in only two locations and has since grown to a nationwide competition with almost 1500 high school students participating in 2013. <eos> an australian team has participated in the international linguistics olympiad ( ilo ) every year since 2009. <eos> this paper describes how the competition is run ( with a regional first round and a final national round ) and the organisation of the competition ( a national steering committee and local organising committees for each region ) and discusses the particular challenges faced by australia ( timing of the competition and distance between the major population centres ). <eos> one major factor in the growth and success of ozclo has been the introduction of the online competition, allowing participation of students from rural and remote country areas. <eos> the organisation relies on the good-will and volunteer work of university and school staff but the strong interest among students and teachers shows that ozclo is responding to a demand for linguistic challenges.
what is it that we want to achieve with our linguistic olympiads and how do the contests vary in different countries ? <eos> the swedish olympiad has been running for 7 years now and is primarily focused on public outreach - spreading linguistics to secondary school students. <eos> the contest involves not only a test but also lectures, school visits and teaching material. <eos> the effort is put on promoting the interest of linguistics to students through fun material and good contact with teachers of languages. <eos> this presentation contains an overview of the swedish version of olympiads in linguistics as well as some concrete examples of workshop material on linguistic problems for secondary school students.
we present the concept of a correspondence seminar as a way to complement and support one-time contests, especially olympiads. <eos> we evaluate specific payoffs of this way of teaching linguistics, and compare its nature to that of a linguistics olympiad. <eos> we believe that the correspondence seminar is a great way to introduce talented high school students to linguistics.
in this paper we present a choreography that explains the process of supervised machine learning. <eos> we present how a perceptron ( in its dual form ) uses convolution kernels to learn to differentiate between two categories of objects. <eos> convolution kernels such as string kernels and tree kernels are widely used in natural language processing ( nlp ) applications. <eos> however, the baggage associated with learning the theory behind convolution kernels, which extends beyond graduate linear algebra, makes the adoption of this technology intrinsically difficult. <eos> the main challenge in creating this choreography was that we were required to represent these mathematical equations at their meaning level before we could translate them into the language of movement. <eos> by orchestrating such a choreography, we believe, we have obviated the need for people to posses advanced math background in order to appreciate the core ideas of using convolution kernels in a supervised learning setting.
data-driven research in linguistics typically involves the processes of data annotation, data visualization and identification of relevant patterns. <eos> we describe our experience in incorporating these processes at an undergraduate course on language information technology. <eos> students collectively annotated the syntactic structures of a set of classical chinese poems ; the resulting treebank was put on a platform for corpus search and visualization ; finally, using this platform, students investigated research questions about the text of the treebank.
we present in the paper our experience of involving the students of the department of theoretical and computational linguistics of the moscow state university into full-cycle activities of preparing and evaluating the results of the nlp evaluation forums, held in 2010 and 2012 in russia. <eos> the forum of 2010 started as a new initiative and was the first independent evaluation of morphology parsers for russian in russia. <eos> at the same time the forum campaign has been a source of a successful academic course which resulted in a closeknit student team, strong enough to implement the two-year research for the second forum on syntax, held in 2012. <eos> the new forum of anaphora ( to be held in 2014 ) is now prepared mostly by students.
we present an open-source virtual manipulative for conditional log-linear models. <eos> this web-based interactive visualization lets the user tune the probabilities of various shapes ? which grow and shrink accordingly ? by dragging sliders that correspond to feature weights. <eos> the visualization displays a regularized training objective ; it supports gradient ascent by optionally displaying gradients on the sliders and providing ? step ? <eos> and ? solve ? <eos> buttons. <eos> the user can sample parameters and datasets of different sizes and compare their own parameters to the truth. <eos> our website, http : //cs.jhu.edu/ ? jason/ tutorials/loglin/, guides the user through a series of interactive lessons and provides auxiliary readings, explanations, practice problems and resources.
in this paper we discuss our experience of teaching basic natural language processing ( nlp ) and machine learning ( ml ) in an introductory course to information science. <eos> we discuss the challenges we faced while incorporating nlp and ml to the curriculum followed by a presentation of how we met these challenges. <eos> the overall response ( of students ) to the inclusion of this new topic to the curriculum has been positive. <eos> students this semester are pursuing nlp/ml projects, formulating their own tasks ( some of which are novel and presented towards the end of the paper ), collecting and annotating data and building models for their task.
this paper describes a seminar course designed by ibm and columbia university on the topic of semantic technologies, in particular as used in ibm watsontm ? <eos> a large scale question answering system which famously won at jeopardy ! <eos> r ? <eos> against two human grand champions. <eos> it was first offered at columbia university during the 2013 spring semester, and will be offered at other institutions starting in the fall semester. <eos> we describe the course ? s first successful run and its unique features : a class centered around a specific industrial technology ; a large-scale class project which student teams can choose to participate in and which serves as the basis for an open source project that will continue to grow each time the course is offered ; publishable papers, demos and start-up ideas ; evidence that the course can be self-evaluating, which makes it potentially appropriate for an online setting ; and a unique model where a large company trains instructors and contributes to creating educational material at no charge to qualifying institutions.
active learning and domain adaptation are both important tools for reducing labeling effort to learn a good supervised model in a target domain. <eos> in this paper, we investigate the problem of online active learning within a new active domain adaptation setting : there are insufficient labeled data in both source and target domains, but it is cheaper to query labels in the source domain than in the target domain. <eos> given a total budget, we develop two costsensitive online active learning methods, a multi-view uncertainty-based method and a multi-view disagreement-based method, to query the most informative instances from the two domains, aiming to learn a good prediction model in the target domain. <eos> empirical studies on the tasks of cross-domain sentiment classification of amazon product reviews demonstrate the efficacy of the proposed methods on reducing labeling cost.
within the natural language processing ( nlp ) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new nlp systems and technologies. <eos> this paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions ( sp ). <eos> the analysis has revealed three elements that are central to the success of the sp method : ( 1 ) bounds on cohen ? s kappa agreement between successively trained models impose bounds on differences in f-measure performance of the models ; ( 2 ) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time ; and ( 3 ) good ( low variance ) sample estimates of kappa between successive models can be obtained. <eos> proofs of relationships between the level of kappa agreement and the difference in performance between consecutive models are presented. <eos> specifically, if the kappa agreement between two models exceeds a threshold t ( where t > 0 ), then the difference in f-measure performance between those models is bounded above by 4 ( 1 ? t ) t in all cases. <eos> if precisionof the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4 ( 1 ? t ) ( p+1 ) t.
we design a new co-occurrence based word association measure by incorporating the concept of significant cooccurrence in the popular word association measure pointwise mutual information ( pmi ). <eos> by extensive experiments with a large number of publicly available datasets we show that the newly introduced measure performs better than other co-occurrence based measures and despite being resource-light, compares well with the best known resource-heavy distributional similarity and knowledge based word association measures. <eos> we investigate the source of this performance improvement and find that of the two types of significant co-occurrence - corpus-level and document-level, the concept of corpus level significance combined with the use of document counts in place of word counts is responsible for all the performance gains observed. <eos> the concept of document level significance is not helpful for pmi adaptation.
we discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. <eos> our focus is on a lowresource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. <eos> the current state-of-art methods 1 ) exploit both the annotated and unannotated data in a semi-supervised manner, and 2 ) learn morph lexicons and subsequently uncover segmentations by generating the most likely morph sequences. <eos> in contrast, we discuss 1 ) employing only the annotated data in a supervised manner, while entirely ignoring the unannotated data, and 2 ) directly learning to predict morph boundaries given their local sub-string contexts instead of learning the morph lexicons. <eos> specifically, we employ conditional random fields, a popular discriminative log-linear model for segmentation. <eos> we present experiments on two data sets comprising five diverse languages. <eos> we show that the fully supervised boundary prediction approach outperforms the state-of-art semi-supervised morph lexicon approaches on all languages when using the same annotated data sets.
we present a flexible formulation of semisupervised learning for structured models, which seamlessly incorporates graphbased and more general supervision by extending the posterior regularization ( pr ) framework. <eos> our extension allows for any regularizer that is a convex, differentiable function of the appropriate marginals. <eos> we show that surprisingly, non-linearity of such regularization does not increase the complexity of learning, provided we use multiplicative updates of the structured exponentiated gradient algorithm. <eos> we illustrate the extended framework by learning conditional random fields ( crfs ) with quadratic penalties arising from a graph laplacian. <eos> on sequential prediction tasks of handwriting recognition and part-ofspeech ( pos ) tagging, our method makes significant gains over strong baselines.
this paper proposes a boosting algorithm that uses a semi-markov perceptron. <eos> the training algorithm repeats the training of a semi-markov model and the update of the weights of training samples. <eos> in the boosting, training samples that are incorrectly segmented or labeled have large weights. <eos> such training samples are aggressively learned in the training of the semi-markov perceptron because the weights are used as the learning ratios. <eos> we evaluate our training method with noun phrase chunking, text chunking and extended named entity recognition. <eos> the experimental results show that our method achieves better accuracy than a semi-markov perceptron and a semi-markov conditional random fields.
we derive a spectral algorithm for learning the parameters of a refinement hmm. <eos> this method is simple, efficient, and can be applied to a wide range of supervised sequence labeling tasks. <eos> like other spectral methods, it avoids the problem of local optima and provides a consistent estimate of the parameters. <eos> our experiments on a phoneme recognition task show that when equipped with informative feature functions, it performs significantly better than a supervised hmm and competitively with em.
sentence compression techniques often assemble output sentences using fragments of lexical sequences such as ngrams or units of syntactic structure such as edges from a dependency tree representation. <eos> we present a novel approach for discriminative sentence compression that unifies these notions and jointly produces sequential and syntactic representations for output text, leveraging a compact integer linear programming formulation to maintain structural integrity. <eos> our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-theart approaches. <eos> experiments on corpora featuring human-generated compressions demonstrate a 13-15 % relative gain in 4gram accuracy over a well-studied language model-based compression system.
this paper proposes passage reranking models that ( i ) do not require manual feature engineering and ( ii ) greatly preserve accuracy, when changing application domain. <eos> their main characteristic is the use of relational semantic structures representing questions and their answer passages. <eos> the relations are established using information from automatic classifiers, i.e., question category ( qc ) and focus classifiers ( fc ) and named entity recognizers ( ner ). <eos> this way ( i ) effective structural relational patterns can be automatically learned with kernel machines ; and ( ii ) structures are more invariant w.r.t. <eos> different domains, thus fostering adaptability.
in most previous research on distributional semantics, vector space models ( vsms ) of words are built either from topical information ( e.g., documents in which a word is present ), or from syntactic/semantic types of words ( e.g., dependency parse links of a word in sentences ), but not both. <eos> in this paper, we explore the utility of combining these two representations to build vsm for the task of semantic composition of adjective-noun phrases. <eos> through extensive experiments on benchmark datasets, we find that even though a type-based vsm is effective for semantic composition, it is often outperformed by a vsm built using a combination of topic- and type-based statistics. <eos> we also introduce a new evaluation task wherein we predict the composed vector representation of a phrase from the brain activity of a human subject reading that phrase. <eos> we exploit a large syntactically parsed corpus of 16 billion tokens to build our vsms, with vectors for both phrases and words, and make them publicly available.
in this paper, we propose a new method for semantic class induction. <eos> first, we introduce a generative model of sentences, based on dependency trees and which takes into account homonymy. <eos> our model can thus be seen as a generalization of brown clustering. <eos> second, we describe an efficient algorithm to perform inference and learning in this model. <eos> third, we apply our proposed method on two large datasets ( 108 tokens, 105 words types ), and demonstrate that classes induced by our algorithm improve performance over brown clustering on the task of semisupervised supersense tagging and named entity recognition.
vector-space word representations have been very successful in recent years at improving performance across a variety of nlp tasks. <eos> however, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. <eos> as a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. <eos> this paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. <eos> we combine recursive neural networks ( rnns ), where each morpheme is a basic unit, with neural language models ( nlms ) to consider contextual information in learning morphologicallyaware word representations. <eos> our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.
most compositional-distributional models of meaning are based on ambiguous vector representations, where all the senses of a word are fused into the same vector. <eos> this paper provides evidence that the addition of a vector disambiguation step prior to the actual composition would be beneficial to the whole process, producing better composite representations. <eos> furthermore, we relate this issue with the current evaluation practice, showing that disambiguation-based tasks can not reliably assess the quality of composition. <eos> using a word sense disambiguation scheme based on the generic procedure of sch ? tze ( 1998 ), we first provide a proof of concept for the necessity of separating disambiguation from composition. <eos> then we demonstrate the benefits of an ? unambiguous ? <eos> system on a composition-only task.
determining the stance expressed by an author from a post written for a two-sided debate in an online debate forum is a relatively new problem in opinion mining. <eos> we extend a state-of-the-art learningbased approach to debate stance classification by ( 1 ) inducing lexico-syntactic patterns based on syntactic dependencies and semantic frames that aim to capture the meaning of a sentence and provide a generalized representation of it ; and ( 2 ) improving the classification of a test post via a novel way of exploiting the information in other test posts with the same stance. <eos> empirical results on four datasets demonstrate the effectiveness of our extensions.
large databases of facts are prevalent in many applications. <eos> such databases are accurate, but as they broaden their scope they become increasingly incomplete. <eos> in contrast to extending such a database, we present a system to query whether it contains an arbitrary fact. <eos> this work can be thought of as re-casting open domain information extraction : rather than growing a database of known facts, we smooth this data into a database in which any possible fact has membership with some confidence. <eos> we evaluate our system predicting held out facts, achieving 74.2 % accuracy and outperforming multiple baselines. <eos> we also evaluate the system as a commonsense filter for the reverb open ie system, and as a method for answer validation in a question answering task.
large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks. <eos> up till now, however, most of the evaluation has been done on monolithic corpora such as the penn treebank, the proposition bank. <eos> as a result, it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains. <eos> the completion of the ontonotes corpus, a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information, makes it possible to perform such an evaluation. <eos> this paper presents an analysis of the performance of publicly available, state-of-the-art tools on all layers and languages in the ontonotes v5.0 corpus. <eos> this should set the benchmark for future development of various nlp components in syntax and semantics, and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance.
coreference resolution systems can benefit greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources. <eos> however, since alignment itself is a challenging task and is often noisy, existing systems either align conservatively, resulting in very few links, or combine the attributes of multiple candidates, leading to a conflation of entities. <eos> our approach instead performs joint inference between within-document coreference and entity linking, maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference. <eos> further, we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity. <eos> these forms of global context enables our system to improve classifier-based coreference by 1.09 b3 f1 points, and improve over the previous state-of-art by 0.41 points, thus introducing a new state-of-art result on the ace 2004 data.
previous incremental parsers have used monotonic state transitions. <eos> however, transitions can be made to revise previous decisions quite naturally, based on further information. <eos> we show that a simple adjustment to the arc-eager transition system to relax its monotonicity constraints can improve accuracy, so long as the training data includes examples of mistakes for the nonmonotonic transitions to repair. <eos> we evaluate the change in the context of a stateof-the-art system, and obtain a statistically significant improvement ( p < 0.001 ) on the english evaluation and 5/10 of the conll languages.
this paper presents a collapsed variational bayesian inference algorithm for pcfgs that has the advantages of two dominant bayesian training algorithms for pcfgs, namely variational bayesian inference and markov chain monte carlo. <eos> in three kinds of experiments, we illustrate that our algorithm achieves close performance to the hastings sampling algorithm while using an order of magnitude less training time ; and outperforms the standard variational bayesian inference and the em algorithms with similar training time.
distributed word representations ( word embeddings ) have recently contributed to competitive performance in language modeling and several nlp tasks. <eos> in this work, we train word embeddings for more than 100 languages using their corresponding wikipedias. <eos> we quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. <eos> we find their performance to be competitive with near state-of-art methods in english, danish and swedish. <eos> moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. <eos> we will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.
in this work, we present an approach for multilingual portability of spoken language understanding systems. <eos> the goal of this approach is to avoid the effort of acquiring and labeling new corpora to learn models when changing the language. <eos> the work presented in this paper is focused on the learning of a specific translator for the task and the mechanism of transmitting the information among the modules by means of graphs. <eos> these graphs represent a set of hypotheses ( a language ) that is the input to the statistical semantic decoder that provides the meaning of the sentence. <eos> some experiments in a spanish task evaluated with input french utterances and text are presented. <eos> they show the good behavior of the system, mainly when speech input is considered.
the use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases. <eos> however, inherent ambiguities in the pivot language ( s ) can lead to inadequate paraphrases. <eos> we propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language, i.e., the language to be paraphrased. <eos> text in the input language is annotated with ? senses ? <eos> in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment. <eos> this approach shows 62 % relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent.
we propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. <eos> our approach is based on a novel combination of topic modeling and word alignment techniques. <eos> intuitively, our approach works by converting a comparable document-aligned corpus into a parallel topic-aligned corpus, then learning word alignments using co-occurrence statistics. <eos> this topicaligned corpus is similar in structure to the sentence-aligned corpus frequently used in statistical machine translation, enabling us to exploit advances in word alignment research. <eos> unlike many previous work, our framework does not require any languagespecific knowledge for initialization. <eos> furthermore, our framework attempts to handle polysemy by allowing multiple translation probability models for each word. <eos> on a large-scale wikipedia corpus, we demonstrate that our framework reliably extracts high-precision translation pairs on a wide variety of comparable data conditions.
in this paper, we address the problem of identifying relevant product aspects in a collection of online customer reviews. <eos> being able to detect such aspects represents an important subtask of aspect-based review mining systems, which aim at automatically generating structured summaries of customer opinions. <eos> we cast the task as a terminology extraction problem and examine the utility of varying term acquisition heuristics, filtering techniques, variant aggregation methods, and relevance measures. <eos> we evaluate the different approaches on two distinct datasets ( hotel and camera reviews ). <eos> for the best configuration, we find significant improvements over a state-of-the-art baseline method.
the acquisition of belief verbs lags behind the acquisition of desire verbs in children. <eos> some psycholinguistic theories attribute this lag to conceptual differences between the two classes, while others suggest that syntactic differences are responsible. <eos> through computational experiments, we show that a probabilistic verb learning model exhibits the pattern of acquisition, even though there is no difference in the model in the difficulty of the semantic or syntactic properties of belief vs. <eos> desire verbs. <eos> our results point to the distributional properties of various verb classes as a potentially important, and heretofore unexplored, factor in the observed developmental lag of belief verbs.
the conll-2013 shared task was devoted to grammatical error correction. <eos> in this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. <eos> we also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.
the conll-2013 shared task focuses on correcting grammatical errors in essays written by non-native learners of english. <eos> in this paper, we describe the university of illinois system that participated in the shared task. <eos> the system consists of five components and targets five types of common grammatical mistakes made by english as second language writers. <eos> we describe our underlying approach, which relates to our previous work, and describe the novel aspects of the system in more detail. <eos> out of 17 participating teams, our system is ranked first based on both the original annotation and on the revised annotation.
te of information systems and applications + department of computer science national tsing hua university hsinchu, taiwan, r.o.c. <eos> 30013 { maxis1718, teer1990, chiuhsunwen, joseph.yen, joanne.boisson, wujc86, jason.jschang } @ gmail.com abstract grammatical error correction has been an active research area in the field of natural language processing. <eos> this paper describes the grammatical error correction system developed at nthu in participation of the conll-2013 shared task. <eos> the system consists of four modules in a pipeline to correct errors related to determiners, prepositions, verb forms and noun number. <eos> although more types of errors are involved that than last year ? s shared task, leading to more complicated problem this year, our system still obtain higher f-score as compared to last year. <eos> we received an overall f-measure score of 0.325, which put our system in second place among 17 systems evaluated.
this paper describes the nara institute of science and technology ( naist ) error correction system in the conll 2013 shared task. <eos> we constructed three systems : a system based on the treelet language model for verb form and subjectverb agreement errors ; a classifier trained on both learner and native corpora for noun number errors ; a statistical machine translation ( smt ) -based model for preposition and determiner errors. <eos> as for subject-verb agreement errors, we show that the treelet language model-based approach can correct errors in which the target verb is distant from its subject. <eos> our system ranked fourth on the official run.
this paper describes the nlp2ct grammatical error detection and correction system for the conll 2013 shared task, with a focus on the errors of article or determiner ( artordet ), noun number ( nn ), preposition ( prep ), verb form ( vform ) and subject-verb agreement ( sva ). <eos> a hybrid model is adopted for this special task. <eos> the process starts with spellchecking as a preprocessing step to correct any possible erroneous word. <eos> we used a maximum entropy classifier together with manually rule-based filters to detect the grammatical errors in english. <eos> a language model based on the google n-gram corpus was employed to select the best correction candidate from a confusion matrix. <eos> we also explored a graphbased label propagation approach to overcome the sparsity problem in training the model. <eos> finally, a number of deterministic rules were used to increase the precision and recall. <eos> the proposed model was evaluated on the test set consisting of 50 essays and with about 500 words in each essay. <eos> our system achieves the 5 th and 3 rd f1 scores on official test set among all 17 participating teams based on goldstandard edits before and after revision, respectively.
we present an approach to grammatical error correction for the conll 2013 shared task based on a weighted tree-to-string transducer. <eos> rules for the transducer are extracted from the nucle training data. <eos> an n-gram language model is used to rerank k-best sentence lists generated by the transducer. <eos> our system obtains a precision, recall and f1 score of 0.27, 0.1333 and 0.1785, respectively, on the official test set. <eos> on the revised annotations, the f1 score increases to 0.2505. <eos> our system ranked 6th out of the participating teams on both the original and revised test set annotations.
this paper describes our use of phrasebased statistical machine translation ( pbsmt ) for the automatic correction of errors in learner text in our submission to the conll 2013 shared task on grammatical error correction. <eos> since the limited training data provided for the task was insufficient for training an effective smt system, we also explored alternative ways of generating pairs of incorrect and correct sentences automatically from other existing learner corpora. <eos> our approach does not yield particularly high performance but reveals many problems that require careful attention when building smt systems for error correction.
we introduce here a participating system of the conll-2013 shared task ? grammatical error correction ?. <eos> we focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. <eos> we carried out feature engineering and we found that ( among others ) the f-structure of an lfg parser can provide very informative features for the machine learning system.
this paper describes our system in the shared task of conll-2013. <eos> we illustrate that grammatical error detection and correction can be transformed into a multiclass classification task and implemented as a single-model system regardless of various error types with the aid of maximum entropy modeling. <eos> our system achieves the f1 score of 17.13 % on the standard test set.
we describe our grammar correction system for the conll-2013 shared task. <eos> our system corrects three of the five error types specified for the shared task noun-number, determiner and subject-verb agreement errors. <eos> for noun-number and determiner correction, we apply a classification approach using rich lexical and syntactic features. <eos> for subject-verb agreement correction, we propose a new rulebased system which utilizes dependency parse information and a set of conditional rules to ensure agreement of the verb group with its subject. <eos> our system obtained an f-score of 11.03 on the official test set using the m2 evaluation method ( the official evaluation method ).
this paper describes our submission for the conll 2013 shared task, which aims to to improve the detection and correction of the five most common grammatical error types in english text written by non-native speakers. <eos> our system concentrates only on two of them ; it employs machine learning classifiers for the artordet-, and a fully deterministic rule based workflow for the sva error type.
we describe the system developed for the conll-2013 shared task ? automatic english l2 grammar error correction. <eos> the system is based on the rule-based approach. <eos> it uses very few additional resources : a morphological analyzer and a list of 250 common uncountable nouns, along with the training data provided by the organizers. <eos> the system uses the syntactic information available in the training data : this information is represented as syntactic n-grams, i.e. <eos> n-grams extracted by following the paths in dependency trees. <eos> the system is simple and was developed in a short period of time ( 1 month ). <eos> since it does not employ any additional resources or any sophisticated machine learning methods, it does not achieve high scores ( specifically, it has low recall ) but could be considered as a baseline system for the task. <eos> on the other hand, it shows what can be obtained using a simple rule-based approach and presents a few situations where the rule-based approach can perform better than ml approach.
we describe the ? tilb ? <eos> team entry for the conll-2013 shared task. <eos> our system consists of five memory-based classifiers that generate correction suggestions for center positions in small text windows of two words to the left and to the right. <eos> trained on the google web 1t corpus, the first two classifiers determine the presence of a determiner or a preposition between all words in a text. <eos> the second pair of classifiers determine which is the most likely correction of an occurring determiner or preposition. <eos> the fifth classifier is a general word predictor which is used to suggest noun and verb form corrections. <eos> we report on the scores attained and errors corrected and missed. <eos> we point out a number of obvious improvements to boost the scores obtained by the system.
we report on the tor system that participated in the 2013 conll shared task on grammatical correction. <eos> the system was a provisional implementation of a beam search correction over a noisy channel model. <eos> although the results on the shared task test set were poor, the approach may still be promising, as there are many aspects of the current implementation that could be optimised. <eos> grammatical correction is inherently difficult both to perform and to evaluate. <eos> as such, possible improvements to the evaluation are also discussed.
this paper presents a hybrid model for the conll-2013 shared task which focuses on the problem of grammatical error correction. <eos> this year ? s task includes determiner, preposition, noun number, verb form, and subject-verb agreement errors which is more comprehensive than previous error correction tasks. <eos> we correct these five types of errors in different modules where either machine learning based or rule-based methods are applied. <eos> preprocessing and post-processing procedures are employed to keep idiomatic phrases from being corrected. <eos> we achieved precision of 35.65 %, recall of 16.56 %, f1 of 22.61 % in the official evaluation and precision of 41.75 %, recall of 20.29 %, f1 of 27.3 % in the revised version. <eos> some further comparisons employing different strategies are made in our experiments.
this paper describes an english grammatical error correction system for conll2013 shared task. <eos> error types covered by our system are article/determiner, preposition, and noun number agreement. <eos> this work is our first attempt on grammatical error correction research. <eos> in this work, we only focus on reimplementing the techniques presented before and optimizing the performance. <eos> as a result of the implementation, our system ? s final f1-score by m2 scorer is 0.1282 in our internal test set.
several discourse annotated corpora now exist for nlp. <eos> but they use different, not easily comparable annotation schemes : are the structures these schemes describe incompatible, incomparable, or do they share interpretations ? <eos> in this paper, we relate three types of discourse annotation used in corpora or discourse parsing : ( i ) rst, ( ii ) sdrt, and ( iii ) dependency tree structures. <eos> we offer a common language in which their structures can be defined and furnished a range of interpretations. <eos> we define translations between rst and dt preserving these interpretations, and introduce a similarity measure for discourse representations in these frameworks. <eos> this will enable researchers to exploit different types of discourse annotated data for automated tasks.
this work proposes a generative model to infer latent semantic structures on top of manual speech transcriptions in a spoken dialog reservation task. <eos> the proposed model is akin to a standard semantic role labeling system, except that it is unsupervised, it does not rely on any syntactic information and it exploits concepts derived from a domain-specific ontology. <eos> the semantic structure is obtained with unsupervised bayesian inference, using the metropolis-hastings sampling algorithm. <eos> it is evaluated both in terms of attachment accuracy and purity-collocation for clustering, and compared with strong baselines on the french media spoken-dialog corpus.
the identification of causal relations between verbal events is important for achieving natural language understanding. <eos> however, the problem has proven notoriously difficult since it is not clear which types of knowledge are necessary to solve this challenging problem close to human level performance. <eos> instead of employing a large set of features proved useful in other nlp tasks, we split the problem in smaller sub problems. <eos> since verbs play a very important role in causal relations, in this paper we harness, explore, and evaluate the predictive power of causal associations of verb-verb pairs. <eos> more specifically, we propose a set of knowledge-rich metrics to learn the likelihood of causal relations between verbs. <eos> employing these metrics, we automatically generate a knowledge base ( kbc ) which identifies three categories of verb pairs : strongly causal, ambiguous, and strongly non-causal. <eos> the knowledge base is evaluated empirically. <eos> the results show that our metrics perform significantly better than the state-of-the-art on the task of detecting causal verbal events.
an appealing methodology for natural language generation in dialogue systems is to train the system to match a target corpus. <eos> we show how users can provide such a corpus as a natural side effect of interacting with a prototype system, when the system uses mixed-initiative interaction and a reversible architecture to cover a domain familiar to users. <eos> we experiment with integrated problems of sentence planning and realization in a referential communication task. <eos> our model learns general and context-sensitive patterns to choose descriptive content, vocabulary, syntax and function words, and improves string match with user utterances to 85.8 % from a handcrafted baseline of 54.4 %.
research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. <eos> this has impacted the dialogue research community ? s ability to develop better theories, as well as good off-the-shelf tools for dialogue processing. <eos> happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. <eos> in particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. <eos> in this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic-independent way. <eos> our results show that our theoretically motivated features achieve 66 % accuracy, an improvement over a unigram baseline of an absolute 6 %.
in this paper we focus on modeling friendships between humans as a way of working towards technology that can initiate and sustain a lifelong relationship with users. <eos> we do this by predicting friendship status in a dyad using a set of automatically harvested verbal and nonverbal features from videos of the interaction of students in a peer tutoring study. <eos> we propose a new computational model used to model friendship status in our data, based on a group sparse model ( gsm ) with l2,1 norm which is designed to accommodate the sparse and noisy properties of the multi-channel features. <eos> our gsm model achieved the best overall performance compared to a non-sparse linear model ( nlm ) and a regular sparse linear model ( slm ), as well as outperforming human raters. <eos> dyadic features, such as number and length of conversational turns and mutual gaze, in addition to low level features such as f0 and gaze at task, were found to be good predictors of friendship status.
online debate forums provide a rich collection of differing opinions on various topics. <eos> in dual-sided debates, users present their opinions or judge other ? s opinions to support their stance. <eos> in this paper, we examine the use of users ? <eos> intentions and debate structure for stance classification of the debate posts. <eos> we propose a domain independent approach to capture users ? <eos> intent at sentence level using its dependency parse and sentiwordnet and to build the intention structure of the post to identify its stance. <eos> to aid the task of classification, we define the health of the debate structure and show that maximizing its value leads to better stance classification accuracies.
our aim is to acquire the attributes of concepts denoted by unknown words from users during dialogues. <eos> a word unknown to spoken dialogue systems can appear in user utterances, and systems should be capable of acquiring information on it from the conversation partner as a kind of selflearning process. <eos> as a first step, we propose a method for generating more specific questions than simple wh-questions to acquire the attributes, as such questions can narrow down the variation of the following user response and accordingly avoid possible speech recognition errors. <eos> specifically, we obtain an appropriately distributed confidence measure ( cm ) on the attributes to generate more specific questions. <eos> two basic cms are defined using ( 1 ) character and word distributions in the target database and ( 2 ) frequency of occurrence of restaurant attributes on web pages. <eos> these are integrated to complement each other and used as the final cm. <eos> we evaluated distributions of the cms by average errors from the reference. <eos> results showed that the integrated cm outperformed the two basic cms.
in situated dialogue, because humans and agents have mismatched capabilities of perceiving the shared physical world, referential grounding becomes difficult. <eos> humans and agents will need to make extra efforts by collaborating with each other to mediate a shared perceptual basis and to come to a mutual understanding of intended referents in the environment. <eos> in this paper, we have extended our previous graph-matching based approach to explicitly incorporate collaborative referring behaviors into the referential grounding algorithm. <eos> in addition, hypergraph-based representations have been used to account for group descriptions that are likely to occur in spatial communications. <eos> our empirical results have shown that incorporating the most prevalent pattern of collaboration with our hypergraph-based approach significantly improves reference resolution in situated dialogue by an absolute gain of over 18 %.
in this paper, we describe novel methods for topic segmentation based on patterns of discourse organization. <eos> using a corpus of news texts, our results show that it is possible to use discourse features ( based on rhetorical structure theory ) for topic segmentation and that we outperform some well-known methods.
this paper presents a practical methodology for the integration of reinforcement learning during the design of a spoken dialogue system ( sds ). <eos> it proposes a method that enables sds designers to know, in advance, the number of dialogues that their system will need in order to learn the value of each state-action couple. <eos> we ask the designer to provide a user model in a simple way. <eos> then, we run simulations with this model and we compute confidence intervals for the mean of the expected return of the state-action couples.
intelligent tutoring systems ( itss ) are now recognised as an interesting alternative for providing learning opportunities in various domains. <eos> the reinforcement learning ( rl ) approach has been shown reliable for finding efficient teaching strategies. <eos> however, similarly to other human-machine interaction systems such as spoken dialogue systems, itss suffer from a partial knowledge of the interlocutor ? s intentions. <eos> in the dialogue case, engineering work can infer a precise state of the user by taking into account the uncertainty provided by the spoken understanding language module. <eos> a model-free approach based on rl and echo state newtorks ( esns ), which retrieves similar information, is proposed here for tutoring.
we use hand-crafted simulated negotiators ( sns ) to train and evaluate dialogue policies for two-issue negotiation between two agents. <eos> these sns differ in their goals and in the use of strong and weak arguments to persuade their counterparts. <eos> they may also make irrational moves, i.e., moves not consistent with their goals, to generate a variety of negotiation patterns. <eos> different versions of these sns interact with each other to generate corpora for reinforcement learning ( rl ) of argumentation dialogue policies for each of the two agents. <eos> we evaluate the learned policies against hand-crafted sns similar to the ones used for training but with the modification that these sns no longer make irrational moves and thus are harder to beat. <eos> the learned policies generally do as well as, or better than the hand-crafted sns showing that rl can be successfully used for learning argumentation dialogue policies in twoissue negotiation scenarios.
in this work, we study the effectiveness of state-of-the-art, sophisticated supervised learning algorithms for dialogue act modeling across a comprehensive set of different spoken and written conversations including : emails, forums, meetings, and phone conversations. <eos> to this aim, we compare the results of svm-multiclass and two structured predictors namely svmhmm and crf algorithms. <eos> extensive empirical results, across different conversational modalities, demonstrate the effectiveness of our svm-hmm model for dialogue act recognition in conversations.
determining the quality of an ongoing interaction in the field of spoken dialogue systems is a hard task. <eos> while existing methods employing automatic estimation already achieve reasonable results, still there is a lot of room for improvement. <eos> hence, we aim at tackling the task by estimating the error of the applied statistical classification algorithms in a two-stage approach. <eos> correcting the hypotheses using the estimated model error increases performance by up to 4.1 % relative improvement in unweighted average recall.
scxml was proposed as one description language for dialog control in the w3c multimodal architecture but lacks the facilities required for grounding and reasoning. <eos> this prohibits the application of many dialog modeling techniques for multimodal applications following this w3c standard. <eos> by extending scxml with a prolog datamodel and scripting language, we enable those techniques to be employed again. <eos> thereby bridging the gap between respective dialog modeling research and a standardized architecture to access and coordinate modalities.
we address the problem of localized error detection in automatic speech recognition ( asr ) output to support the generation of targeted clarifications in spoken dialogue systems. <eos> localized error detection finds specific mis-recognized words in a user utterance. <eos> targeted clarifications, in contrast with generic ? please repeat/rephrase ? <eos> clarifications, target a specific mis-recognized word in an utterance ( stoyanchev et al, 2012a ) and require accurate detection of such words. <eos> we extend and modify work presented in ( stoyanchev et al., 2012b ) by experimenting with a new set of features for predicting the likelihood of a local error in an asr hypothesis on an unsifted version of the original dataset. <eos> we improve over baseline results, where only asrgenerated features are used, by constructing optimal feature sets for utterance and word mis-recognition prediction. <eos> the f-measure for identifying incorrect utterances improves by 2.2 % and by 3.9 % for identifiying incorrect words.
we model human responses to speech recognition errors from a corpus of human clarification strategies. <eos> we employ learning techniques to study 1 ) the decision to either stop and ask a clarification question or to continue the dialogue without clarification, and 2 ) the decision to ask a targeted clarification question or a more generic question. <eos> targeted clarification questions focus specifically on the part of an utterance that is misrecognized, in contrast with generic requests to ? please repeat ? <eos> or ? please rephrase ?. <eos> our goal is to generate targeted clarification strategies for handling errors in spoken dialogue systems, when appropriate. <eos> our experiments show that linguistic features, in particular the inferred part-ofspeech of a misrecognized word are predictive of human clarification decisions. <eos> a combination of linguistic features predicts a user ? s decision to continue or stop a dialogue with accuracy of 72.8 % over a majority baseline accuracy of 59.1 %. <eos> the same set of features predict the decision to ask a targeted question with accuracy of 74.6 % compared with the majority baseline of 71.8 %.1
this demo paper describes our artificial intelligent dialogue agent ( aida ), a dialogue management and orchestration platform under development at the institute for infocomm research. <eos> among other features, it integrates different human-computer interaction engines across multiple domains and communication styles such as command, question answering, task-oriented dialogue and chat-oriented dialogue. <eos> the platform accepts both speech and text as input modalities by either direct microphone/keyboard connections or by means of mobile device wireless connection. <eos> the output interface, which is supported by a talking avatar, integrates speech and text along with other visual aids.
we summarize the status of an ongoing project to develop and evaluate a companion for isolated older adults. <eos> four key scientific issues in the project are : embodiment, interaction paradigm, engagement and relationship. <eos> the system architecture is extensible and handles realtime behaviors. <eos> the system supports multiple activities, including discussing the weather, playing cards, telling stories, exercise coaching and video conferencing. <eos> a live, working demo system will be presented at the meeting.
we demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering ( qa ), and geographic information system ( gis ) technologies. <eos> in contrast to existing mobile applications which treat these problems independently, our android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. <eos> in this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users ( janarthanam et al, 2013 ). <eos> the new features include navigation based on visible landmarks, navigation adapted to the user ? s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. <eos> the system also uses social media to infer ? popularity ? <eos> of geographical entities.
the parlance system for interactive search processes dialogue at a microturn level, displaying dialogue phenomena that play a vital role in human spoken conversation. <eos> these dialogue phenomena include more natural turn-taking through rapid system responses, generation of backchannels, and user barge-ins. <eos> the parlance demonstration system differentiates from other incremental systems in that it is data-driven with an infrastructure that scales well.
while natural language as an interaction modality is increasingly being accepted by users, remaining technological challenges still hinder its widespread employment. <eos> tools that better support the design, development and improvement of these types of applications are required. <eos> this demo presents a prototyping framework for spoken dialog system ( sds ) design which combines existing language technology components for automatic speech recognition ( asr ), dialog management ( dm ), and text-to-speech synthesis ( tts ) with a multi-step component for natural language understanding ( nlu ).
the wizard of oz ( woz ) method has been used for a variety of purposes in early-stage development of dialogue systems and language technology applications, from data collection, to experimentation, prototyping and evaluation. <eos> however, software to support woz experimentation is often developed ad hoc for specific application scenarios. <eos> in this demo we present webwoz, a web-based woz prototyping platform that aims at supporting a variety of experimental settings and combinations of different language technology components. <eos> we argue that a generic and distributed platform such as webwoz can increase the usefulness of the woz method.
in this paper, we present a user study where a robot instructs a human on how to draw a route on a map, similar to a map task. <eos> this setup has allowed us to study user reactions to the robot ? s conversational behaviour in order to get a better understanding of how to generate utterances in incremental dialogue systems. <eos> we have analysed the participants ' subjective rating, task completion, verbal responses, gaze behaviour, drawing activity, and cognitive load. <eos> the results show that users utilise the robot ? s gaze in order to disambiguate referring expressions and manage the flow of the interaction. <eos> furthermore, we show that the user ? s behaviour is affected by how pauses are realised in the robot ? s speech.
in situated dialogue, speakers share time and space. <eos> we present a statistical model for understanding natural language that works incrementally ( i.e., in real, shared time ) and is grounded ( i.e., links to entities in the shared space ). <eos> we describe our model with an example, then establish that our model works well on nonsituated, telephony application-type utterances, show that it is effective in grounding language in a situated environment, and further show that it can make good use of embodied cues such as gaze and pointing in a fully multi-modal setting.
we describe the annotation of a multimodal corpus that includes pointing gestures and haptic actions ( force exchanges ). <eos> haptic actions are rarely analyzed as fullfledged components of dialogue, but our data shows haptic actions are used to advance the state of the interaction. <eos> we report our experiments on recognizing dialogue acts in both offline and online modes. <eos> our results show that multimodal features and the dialogue game aid in da classification.
we explore the presence of indicators of psychological distress in the linguistic behavior of subjects in a corpus of semistructured virtual human interviews. <eos> at the level of aggregate dialogue-level features, we identify several significant differences between subjects with depression and ptsd when compared to nondistressed subjects. <eos> at a more fine-grained level, we show that significant differences can also be found among features that represent subject behavior during specific moments in the dialogues. <eos> finally, we present statistical classification results that suggest the potential for automatic assessment of psychological distress in individual interactions with a virtual human dialogue system.
ha, christopher m. mitchell, kristy elizabeth boyer, and james c. lester department of computer science north carolina state university raleigh, nc 27695, usa { eha, cmmitch2, keboyer, lester } @ ncsu.edu abstract learning dialogue management models poses significant challenges. <eos> in a complex task-oriented domain in which information is ex-changed via parallel, interleaved dialogue and task streams, effective dialogue management models should be able to make dialogue moves based on both the dialogue and the task context. <eos> this paper presents a data-driven ap-proach to learning dialogue management mod-els that determine when to make dialogue moves to assist users ? <eos> task completion activi-ties, as well as the type of dialogue move that should be selected for a given user interaction context. <eos> combining features automatically ex-tracted from the dialogue and the task, we compare two alternate modeling approaches. <eos> the results of an evaluation indicate the learned models are effective in predicting both the timing and the type of system dialogue moves.
existing spoken dialogue systems are typically designed to operate in a static and well-defined domain, and are not well suited to tasks in which the concepts and values change dynamically. <eos> to handle dynamically changing domains, techniques will be needed to transfer and reuse existing dialogue policies and rapidly adapt them using a small number of dialogues in the new domain. <eos> as a first step in this direction, this paper addresses the problem of automatically extending a dialogue system to include a new previously unseen concept ( or slot ) which can be then used as a search constraint in an information query. <eos> the paper shows that in the context of gaussian process pomdp optimisation, a domain can be extended through a simple expansion of the kernel and then rapidly adapted. <eos> as well as being much quicker, adaptation rather than retraining from scratch is shown to avoid subjecting users to unacceptably poor performance during the learning stage.
this paper describes a new approach to automatic learning of strategies for social multi-user human-robot interaction. <eos> using the example of a robot bartender that tracks multiple customers, takes their orders, and serves drinks, we propose a model consisting of a social state recogniser ( ssr ) which processes audio-visual input and maintains a model of the social state, together with a social skills executor ( sse ) which takes social state updates from the ssr as input and generates robot responses as output. <eos> the sse is modelled as two connected markov decision processes ( mdps ) with action selection policies that are jointly optimised in interaction with a multi-user simulation environment ( muse ). <eos> the ssr and sse have been integrated in the robot bartender system and evaluated with human users in hand-coded and trained sse policy variants. <eos> the results indicate that the trained policy outperformed the hand-coded policy in terms of both subjective ( +18 % ) and objective ( +10.5 % ) task success.
due to the mobile internet revolution, people tend to browse the web while driving their car which puts the driver ? s safety at risk. <eos> therefore, an intuitive and nondistractive in-car speech interface to the web needs to be developed. <eos> before developing a new speech dialog system in a new domain developers have to examine what the user ? s preferred interaction style is in order to use such a system. <eos> this paper reports from a very recent driving simulation study and its preliminary results which are conducted in order to compare different speech dialog strategies. <eos> the use of command-based and conversational sds prototypes while driving is evaluated on usability and driving performance. <eos> different guis are designed in order to support the respective dialog strategy the most and to evaluate the effect of the gui on usability and driver distraction. <eos> the preliminary results show that the conversational speech dialog performs more efficient than the command-based dialog. <eos> however, the conversational dialog distracts more from driving than the command-based. <eos> furthermore, the results indicate that an sds supported by a gui is more efficient and better accepted by the user than without gui.
goal-oriented dialog agents are expected to recognize user-intentions from an utterance and execute appropriate tasks. <eos> typically, such systems use a semantic parser to solve this problem. <eos> however, semantic parsers could fail if user utterances contain out-of-grammar words/phrases or if the semantics of uttered phrases did not match the parser ? s expectations. <eos> in this work, we have explored a more robust method of task prediction. <eos> we define task prediction as a classification problem, rather than ? parsing ? <eos> and use semantic contexts to improve classification accuracy. <eos> our classifier uses semantic smoothing kernels that can encode information from knowledge bases such as wordnet, nell and freebase.com. <eos> our experiments on two spoken language corpora show that augmenting semantic information from these knowledge bases gives about 30 % absolute improvement in task prediction over a parserbased method. <eos> our approach thus helps make a dialog agent more robust to user input and helps reduce number of turns required to detected intended tasks.
we present virtual human dialogue models which primarily operate on the surface text level and can be extended to incorporate additional information state annotations such as topics or results from simpler models. <eos> we compare these models with previously proposed models as well as two human-level upper baselines. <eos> the models are evaluated by collecting appropriateness judgments from human judges for responses generated for a set of fixed dialogue contexts. <eos> our results show that the best performing models achieve close to human-level performance and require only surface text dialogue transcripts to train.
based on german production data from the ? kiel corpus of spontaneous speech ?, we conducted two perception experiments, using an innovative interactive task in which participants gave real oral responses to resynthesized question stimuli. <eos> differences in the time interval between stimulus question and response show that segmental reduction, intensity level, and the shape of the phrase-final rise all function as cues to turn-taking in conversation. <eos> thus, the phonetics of turntaking goes beyond the traditional triad of duration, voice quality, and f0 level.
a fundamental problem in manual based gesture semantics reconstruction is the specification of preferred semantic concepts for gesture trajectories. <eos> this issue is complicated by problems human raters have annotating fast-paced three dimensional trajectories. <eos> based on a detailed example of a gesticulated circular trajectory, we present a data-driven approach that covers parts of the semantic reconstruction by making use of motion capturing ( mocap ) technology. <eos> in our fa3me framework we use a complex event processing approach to analyse and annotate multi-modal events. <eos> this framework provides grounds for a detailed description of how to get at the semantic concept of circularity observed in the data.
in many environments ( e. g. sports commentary ), situations incrementally unfold over time and often the future appearance of a relevant event can be predicted, but not in all its details or precise timing. <eos> we have built a simulation framework that uses our incremental speech synthesis component to assemble in a timely manner complex commentary utterances. <eos> in our evaluation, the resulting output is preferred over that from a baseline system that uses a simpler commenting strategy. <eos> even in cases where the incremental system overcommits temporally and requires a filled pause to wait for the upcoming event, the system is preferred over the baseline.
in this paper, we propose a framework for conversational robots that facilitates fourparticipant groups. <eos> in three-participant conversations, the minimum unit for multiparty conversations, social imbalance, in which a participant is left behind in the current conversation, sometimes occurs. <eos> in such scenarios, a conversational robot has the potential to facilitate situations as the fourth participant. <eos> consequently, we present model procedures for obtaining conversational initiatives in incremental steps to engage such four-participant conversations. <eos> during the procedures, a facilitator must be aware of both the presence of dominant participants leading the current conversation and the status of any participant that is left behind. <eos> we model and optimize these situations and procedures as a partially observable markov decision process. <eos> the results of experiments conducted to evaluate the proposed procedures show evidence of their acceptability and feeling of groupness.
in this paper, we propose a novel approach to infer dialogue acts using the notion of tacit contracts. <eos> we describe the interpersonal linguistic features that our analysis grammar can identify in uttered texts and present an inference procedure that strictly separates the semantic and pragmatic steps of utterance understanding, thereby meeting a higher degree of modularity, a prerequisite for extending robot functionality. <eos> keywords : dialogue system ; dialogue act ; attitude ; stance
this study explores laughter distribution around topic changes in multiparty conversations. <eos> the distribution of shared and solo laughter around topic changes was examined in corpora containing two types of spoken interaction ; meetings and informal conversation. <eos> shared laughter was significantly more frequent in the 15 seconds leading up to topic change in the informal conversations. <eos> a sample of informal conversations was then analysed by hand to gain further insight into links between laughter and topic change.
we explore hedging in web forum conversations, which is interestingly different to hedging in academic articles, the main focus of recent automatic approaches to hedge detection. <eos> one of our main results is that forum posts using hedges are more likely to get high ratings of their usefulness. <eos> we also make a case for focusing annotation efforts on hedges that take the form of first-person epistemic phrases.
a challenge in dialogue act recognition is the mapping from noisy user inputs to dialogue acts. <eos> in this paper we describe an approach for re-ranking dialogue act hypotheses based on bayesian classifiers that incorporate dialogue history and automatic speech recognition ( asr ) n-best information. <eos> we report results based on the let ? s go dialogue corpora that show ( 1 ) that including asr n-best information results in improved dialogue act recognition performance ( +7 % accuracy ), and ( 2 ) that competitive results can be obtained from as early as the first system dialogue act, reducing the need to wait for subsequent system dialogue acts.
can speaker gaze and speaker arm movements be used as a practical information source for naturalistic conversational human ? computer interfaces ? <eos> to investigate this question, we recorded ( with eye tracking and motion capture ) a corpus of interactions with a ( wizarded ) system. <eos> in this paper, we describe the recording, analysis infrastructure that we built for such studies, and analysis we performed on these data. <eos> we find that with some initial calibration, a ? minimally invasive ?, stationary camera-based setting provides data of sufficient quality to support interaction.
even though open-domain conversational dialogue systems are required in many fields, their development is complicated because of the flexibility and variety of user utterances. <eos> to address this flexibility, previous research on conversational dialogue systems has selected system utterances from web articles based on surface cohesion and shallow semantic coherence ; however, the generated utterances sometimes contain irrelevant sentences with respect to the input user utterance. <eos> we propose a template-based approach that fills templates with the most salient words in a user utterance and with related words that are extracted using web-scale dependency structures gathered from twitter. <eos> our open-domain conversational dialogue system outperforms retrieval-based conventional systems in chat experiments.
er m. mitchell kristy elizabeth boyer james c. lester department of computer science north carolina state university raleigh, nc, usa { cmmitch2, keboyer, lester } @ ncsu.edu abstract learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance. <eos> in task-oriented dia-logue where the user can engage in task ac-tions in parallel with dialogue, unrestricted turn taking may be particularly important for dialogue success. <eos> this paper presents a novel markov decision process ( mdp ) representa-tion of dialogue with unrestricted turn taking and a parallel task stream in order to automat-ically learn effective turn-taking policies for a tutorial dialogue system from a corpus. <eos> it also presents and evaluates an approach to auto-matically selecting features for an mdp state representation of this dialogue. <eos> the results suggest that the mdp formulation and the feature selection framework hold promise for learning effective turn-taking policies in task-oriented dialogue systems.
natural language call routing remains a complex and challenging research area in machine intelligence and language understanding. <eos> this paper is in the area of classifying user utterances into different categories. <eos> the focus is on design of algorithm that combines supervised and unsupervised learning models in order to improve classification quality. <eos> we have shown that the proposed approach is able to outperform existing methods on a large dataset and do not require morphological and stop-word filtering. <eos> in this paper we present a new formula for term relevance estimation, which is a modification of fuzzy rules relevance estimation for fuzzy classifier. <eos> using this formula and only 300 frequent words for each class, we achieve an accuracy rate of 85.55 % on the database excluding the ? garbage ? <eos> class ( it includes utterances that can not be assigned to any useful class or that can be assigned to more than one class ). <eos> dividing the ? garbage ? <eos> class into the set of subclasses by agglomerative hierarchical clustering we achieve about 9 % improvement of accuracy rate on the whole database.
in this paper, we introduce our counseling dialog system. <eos> our system interacts with users by recognizing what the users say, predicting the context, and following the users ? <eos> feelings. <eos> for this interaction, our system follows three basic counseling techniques : paraphrasing, asking open questions, and reflecting feelings. <eos> to follow counseling techniques, we extracted 5w1h information and user emotions from user utterances, and we generated system utterances while using the counseling techniques. <eos> we used the conditional random field algorithm to extract 5w1h information, and constructed our counseling algorithm using a dialog strategy that was based on counseling techniques. <eos> a total of 16 adults tested our system and rated it with a higher score as an interactive communicator compared with the baseline system.
we present two dialogue systems for language learning which both restrict the dialog to a specific domain thereby promoting robustness and the learning of a given vocabulary. <eos> the systems vary in how much they constrain the learner ? s answer : one system places no other constrain on the learner than that provided by the restricted domain and the dialog context ; the other provides the learner with an exercise whose solution is the expected answer. <eos> the first system uses supervised learning for simulating a human tutor whilst the second one uses natural language generation techniques to produce grammar exercises which guide the learner toward the expected answer.
the demo shows wikipedia-based opendomain information access dialogues with a talking humanoid robot. <eos> the robot uses face-tracking, nodding and gesturing to support interaction management and the presentation of information to the partner.
we present a wizard of oz ( woz ) environment that was designed to build an artificial embodied intelligent tutoring system ( its ) that is capable of empathic conversations with school pupils aged between 10-13. <eos> we describe the components and the data that we plan to collect using the environment.
the demonstrator presents a test-bed for collecting data on human ? computer dialogue : a fully automated dialogue system that can perform map task with a user. <eos> in a first step, we have used the test-bed to collect human ? computer map task dialogue data, and have trained various data-driven models on it for detecting feedback response locations in the user ? s speech. <eos> one of the trained models has been tested in user interactions and was perceived better in comparison to a system using a random model. <eos> the demonstrator will exhibit three versions of the map task dialogue system ? each using a different trained data-driven model of response location detection.
we demonstrate a robotic agent in a 3d virtual environment that understands human navigational instructions. <eos> such an agent needs to select actions based on not only instructions but also situations. <eos> it is also expected to immediately react to the instructions. <eos> our agent incrementally understands spoken instructions and immediately controls a mobile robot based on the incremental understanding results and situation information such as the locations of obstacles and moving history. <eos> it can be used as an experimental system for collecting human-robot interactions in dynamically changing situations.
we present an online system that provides a complete web-based sandbox for creating, testing and publishing embodied conversation-al agents. <eos> the tool, called roundtable, em-powers many different types of authors and varying team sizes to create flexible interac-tions by automating many editing workflows while limiting complexity and hiding architec-tural concerns. <eos> finished characters can be pub-lished directly to web servers, enabling highly interactive applications.
we present a data-driven model for detecting suitable response locations in the user ? s speech. <eos> the model has been trained on human ? machine dialogue data and implemented and tested in a spoken dialogue system that can perform the map task with users. <eos> to our knowledge, this is the first example of a dialogue system that uses automatically extracted syntactic, prosodic and contextual features for online detection of response locations. <eos> a subjective evaluation of the dialogue system suggests that interactions with a system using our trained model were perceived significantly better than those with a system using a model that made decisions at random.
barge-in enables the user to provide input during system speech, facilitating a more natural and efficient interaction. <eos> standard methods generally focus on singlestage barge-in detection, applying the dialogue policy irrespective of the barge-in context. <eos> unfortunately, this approach performs poorly when used in challenging environments. <eos> we propose and evaluate a barge-in processing method that uses a prediction strategy to continuously decide whether to pause, continue, or resume the prompt. <eos> this model has greater task success and efficiency than the standard approach when evaluated in a public spoken dialogue system. <eos> index terms : spoken dialogue systems, barge-in
we present an analysis of several publicly available automatic speech recognizers ( asrs ) in terms of their suitability for use in different types of dialogue systems. <eos> we focus in particular on cloud based asrs that recently have become available to the community. <eos> we include features of asr systems and desiderata and requirements for different dialogue systems, taking into account the dialogue genre, type of user, and other features. <eos> we then present speech recognition results for six different dialogue systems. <eos> the most interesting result is that different asr systems perform best on the data sets. <eos> we also show that there is an improvement over a previous generation of recognizers on some of these data sets. <eos> we also investigate language understanding ( nlu ) on the asr output, and explore the relationship between asr and nlu performance.
for robust spoken conversational interaction, many dialog state tracking algorithms have been developed. <eos> few studies, however, have reported the strengths and weaknesses of each method. <eos> the dialog state tracking challenge ( dstc ) is designed to address this issue by comparing various methods on the same domain. <eos> in this paper, we present a set of techniques that build a robust dialog state tracker with high performance : wide-coverage and well-calibrated data selection, feature-rich discriminative model design, generalization improvement techniques and unsupervised prior adaptation. <eos> the dstc results show that the proposed method is superior to other systems on average on both the development and test datasets.
this paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domainindependent rules, using basic probability operations. <eos> the rules apply to observed system actions and partially observable user acts, without using any knowledge obtained from external resources ( i.e. <eos> without requiring training data ). <eos> the core insight is to maximise the amount of information directly gainable from an errorprone dialogue itself, so as to better lowerbound one ? s expectations on the performance of more advanced statistical techniques for the task. <eos> the proposed method is evaluated in the dialog state tracking challenge, where it achieves comparable performance in hypothesis accuracy to machine learning based systems. <eos> consequently, with respect to different scenarios for the belief tracking problem, the potential superiority and weakness of machine learning approaches in general are investigated.
statistical approaches to dialog state tracking synthesize information across multiple turns in the dialog, overcoming some speech recognition errors. <eos> when training a dialog state tracker, there is typically only a small corpus of well-matched dialog data available. <eos> however, often there is a large corpus of mis-matched but related data ? <eos> perhaps pertaining to different semantic concepts, or from a different dialog system. <eos> it would be desirable to use this related dialog data to supplement the small corpus of well-matched dialog data. <eos> this paper addresses this task as multi-domain learning, presenting 3 methods which synthesize data from different slots and different dialog systems. <eos> since deploying a new dialog state tracker often changes the resulting dialogs in ways that are difficult to predict, we study how well each method generalizes to unseen distributions of dialog data. <eos> our main result is the finding that a simple method for multi-domain learning substantially improves performance in highly mis-matched conditions.
many dialog state tracking algorithms have been limited to generative modeling due to the influence of the partially observable markov decision process framework. <eos> recent analyses, however, raised fundamental questions on the effectiveness of the generative formulation. <eos> in this paper, we present a structured discriminative model for dialog state tracking as an alternative. <eos> unlike generative models, the proposed method affords the incorporation of features without having to consider dependencies between observations. <eos> it also provides a flexible mechanism for imposing relational constraints. <eos> to verify the effectiveness of the proposed method, we applied it to the let ? s go domain ( raux et al, 2005 ). <eos> the results show that the proposed model is superior to the baseline and generative model-based systems in accuracy, discrimination, and robustness to mismatches between training and test datasets.
in this paper, we describe two dialogue state tracking models competing in the 2012 dialogue state tracking challenge ( dstc ). <eos> first, we detail a novel discriminative dialogue state tracker which directly estimates slot-level beliefs using deterministic state transition probability distribution. <eos> second, we present a generative model employing a simple dependency structure to achieve fast inference. <eos> the models are evaluated on the dstc data, and both significantly outperform the baseline dstc tracker.
this paper presents our approach to dialog state tracking for the dialog state tracking challenge task. <eos> in our approach we use discriminative general structured conditional random fields, instead of traditional generative directed graphic models, to incorporate arbitrary overlapping features. <eos> our approach outperforms the simple 1-best tracking approach.
we describe our experience with engineering the dialog state tracker for the first dialog state tracking challenge ( dstc ). <eos> dialog trackers are one of the essential components of dialog systems which are used to infer the true user goal from the speech processing results. <eos> we explain the main parts of our tracker : the observation model, the belief refinement model, and the belief transformation model. <eos> we also report experimental results on a number of approaches to the models, and compare the overall performance of our tracker to other submitted trackers. <eos> an extended version of this paper is available as a technical report ( kim et al, 2013 ).
while belief tracking is known to be important in allowing statistical dialog systems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques. <eos> the dialogue state tracking challenge has allowed for such an analysis, comparing multiple belief tracking approaches on a shared task. <eos> recent success in using deep learning for speech research motivates the deep neural network approach presented here. <eos> the model parameters can be learnt by directly maximising the likelihood of the training data. <eos> the paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particularly on a corpus of dialogs from a system not found in the training.
we present a number of semi-supervised parsing experiments on the irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences. <eos> we take two popular dependency parsers ? <eos> one graph-based and one transition-based ? <eos> and compare results for both. <eos> results show that using semisupervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy. <eos> we also try to use morphological information in a targeted way and fail to see any improvements.
we present the first statistical dependency parsing results for lithuanian, a morphologically rich language in the baltic branch of the indo-european family. <eos> using a greedy transition-based parser, we obtain a labeled attachment score of 74.7 with gold morphology and 68.1 with predicted morphology ( 77.8 and 72.8 unlabeled ). <eos> we investigate the usefulness of different features and find that rich morphological features improve parsing accuracy significantly, by 7.5 percentage points with gold features and 5.6 points with predicted features. <eos> as expected, case is the single most important morphological feature, but virtually all available features bring some improvement, especially under the gold condition.
we investigate statistical dependency parsing of two closely related languages, croatian and serbian. <eos> as these two morphologically complex languages of relaxed word order are generally under-resourced ? <eos> with the topic of dependency parsing still largely unaddressed, especially for serbian ? <eos> we make use of the two available dependency treebanks of croatian to produce state-of-the-art parsing models for both languages. <eos> we observe parsing accuracy on four test sets from two domains. <eos> we give insight into overall parser performance for croatian and serbian, impact of preprocessing for lemmas and morphosyntactic tags and influence of selected morphosyntactic features on parsing accuracy.
this paper describes cross-task flexible transition models ( ctf-tms ) and demonstrates their effectiveness for arabic natural language processing ( nlp ). <eos> nlp pipelines often suffer from error propagation, as errors committed in lower-level tasks cascade through the remainder of the processing pipeline. <eos> by allowing a flexible order of operations across and within multiple nlp tasks, a ctf-tm can mitigate both cross-task and within-task error propagation. <eos> our arabic ctf-tm models tokenization, affix detection, affix labeling, partof-speech tagging, and dependency parsing, achieving state-of-the-art results. <eos> we present the details of our general framework, our arabic ctf-tm, and the setup and results of our experiments.
this paper describes the ligm-alpage system for the spmrl 2013 shared task. <eos> we only participated to the french part of the dependency parsing track, focusing on the realistic setting where the system is informed neither with gold tagging and morphology nor ( more importantly ) with gold grouping of tokens into multi-word expressions ( mwes ). <eos> while the realistic scenario of predicting both mwes and syntax has already been investigated for constituency parsing, the spmrl 2013 shared task datasets offer the possibility to investigate it in the dependency framework. <eos> we obtain the best results for french, both for overall parsing and for mwe recognition, using a reparsing architecture that combines several parsers, with both pipeline architecture ( mwe recognition followed by parsing ), and joint architecture ( mwe recognition performed by the parser ).
the spmrl 2013 shared task was the opportunity to develop and test, with promising results, a simple beam-based shift-reduce dependency parser on top of the tabular logic programming system dyalog. <eos> the parser was also extended to handle ambiguous word lattices, with almost no loss w.r.t. <eos> disambiguated input, thanks to specific training, use of oracle segmentation, and large beams. <eos> we believe that this result is an interesting new one for shift-reduce parsing.
the inclusion of morphological features provides very useful information that helps to enhance the results when parsing morphologically rich languages. <eos> maltoptimizer is a tool, that given a data set, searches for the optimal parameters, parsing algorithm and optimal feature set achieving the best results that it can find for parsers trained with maltparser. <eos> in this paper, we present an extension of maltoptimizer that explores, one by one and in combination, the features that are geared towards morphology. <eos> from our experiments in the context of the shared task on parsing morphologically rich languages, we extract an in-depth study that shows which features are actually useful for transition-based parsing and we provide competitive results, in a fast and simple way.
this paper presents a dependency parsing system, presented as basque team at the spmrl ? 2013 shared task, based on the analysis of each morphological feature of the languages. <eos> once the specific relevance of each morphological feature is calculated, this system uses the most significant of them to create a series of analyzers using two freely available and state of the art dependency parsers, maltparser and mate. <eos> finally, the system will combine previously achieved parses using a voting approach.
we propose the use of the word categories and embeddings induced from raw text as auxiliary features in dependency parsing. <eos> to induce word features, we make use of contextual, morphologic and orthographic properties of the words. <eos> to exploit the contextual information, we make use of substitute words, the most likely substitutes for target words, generated by using a statistical language model. <eos> we generate morphologic and orthographic properties of word types in an unsupervised manner. <eos> we use a co-occurrence model with these properties to embed words onto a 25dimensional unit sphere. <eos> the ai-ku system shows improvements for some of the languages it is trained on for the first shared task of statistical parsing of morphologically rich languages.
we describe the submission from the columbia arabic & dialect modeling group ( cadim ) for the shared task at the fourth workshop on statistical parsing of morphologically rich languages ( spmrl ? 2013 ). <eos> we participate in the arabic dependency parsing task for predicted pos tags and features. <eos> our system is based on marton et al ( 2013 ).
in this paper we use statistical dependency parsing techniques to detect null or empty categories in the hindi sentences. <eos> we have currently worked on hindi dependency treebank which is released as part of colingmtpil 2012 workshop. <eos> earlier rule based approaches are employed to detect empty heads for hindi language but statistical learning for automatic prediction is not explored. <eos> in this approach we used a technique of introducing complex labels into the data to predict empty categories in sentences. <eos> we have also discussed about shortcomings and difficulties in this approach and evaluated the performance of this approach on different empty categories.
this paper investigates the impact of different morphological and lexical information on data-driven dependency parsing of persian, a morphologically rich language. <eos> we explore two state-of-the-art parsers, namely mstparser andmaltparser, on the recently released persian dependency treebank and establish some baselines for dependency parsing performance. <eos> three sets of issues are addressed in our experiments : effects of using gold and automatically derived features, finding the best features for the parser, and a suitable way to alleviate the data sparsity problem. <eos> the final accuracy is 87.91 % and 88.37 % labeled attachment scores for maltparser and mstparser, respectively.
we present an empirical study on constructing a japanese constituent parser, which can output function labels to deal with more detailed syntactic information. <eos> japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks, however, such expression is insufficient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure, which is required for practical applications such as syntactic reordering of machine translation. <eos> we describe a preliminary effort on constructing a japanese constituent parser by a penn treebank style treebank semi-automatically made from a dependency-based corpus. <eos> the evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers, and can output such function labels as the grammatical role of the argument and the type of adnominal phrases.
this paper revisits the work of ( malladi and mannem, 2013 ) which focused on building a statistical morphological analyzer ( sma ) for hindi and compares the performance of sma with other existing statistical analyzer, morfette. <eos> we shall evaluate sma in various experiment scenarios and look at how it performs for unseen words. <eos> the later part of the paper presents the effect of the predicted morph features on dependency parsing and extends the work to other morphologically rich languages : hindi and telugu, without any language-specific engineering.
this paper presents our preliminary conclusions as part of an ongoing effort to construct a new dependency representation framework for turkish. <eos> we aim for this new framework to accommodate the highly agglutinative morphology of turkish as well as to allow the annotation of unedited web data, and shape our decisions around these considerations. <eos> in this paper, we firstly describe a novel syntactic representation for morphosyntactic sub-word units ( namely inflectional groups ( igs ) in turkish ) which allows inter-ig relations to be discerned with perfect accuracy without having to hide lexical information. <eos> secondly, we investigate alternative annotation schemes for coordination structures and present a better scheme ( nearly 11 % increase in recall scores ) than the one in turkish treebank ( oflazer et al, 2003 ) for both parsing accuracies and compatibility for colloquial language.
this paper describes the ims-szeged-cis contribution to the spmrl 2013 shared task. <eos> we participate in both the constituency and dependency tracks, and achieve state-of-theart for all languages. <eos> for both tracks we make significant improvements through high quality preprocessing and ( re ) ranking on top of strong baselines. <eos> our system came out first for both tracks.
this paper reports on the first shared task on statistical parsing of morphologically rich languages ( mrls ). <eos> the task features data sets from nine languages, each available both in constituency and dependency annotation. <eos> we report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing mrls given different representation types. <eos> we present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios.
traditional information retrieval models assume keyword-based queries and use unstructured document representations. <eos> there is an abundance of event-centered texts ( e.g., breaking news ) and event-oriented information needs that often involve structure that can not be expressed using keywords. <eos> we present a novel retrieval model that uses a structured event-based representation. <eos> we structure queries and documents as graphs of event mentions and employ graph kernels to measure the query-document similarity. <eos> experimental results on two event-oriented test collections show significant improvements over state-ofthe-art keyword-based models.
we introduce an interactive visualization component for the jobimtext project. <eos> jobimtext is an open source platform for large-scale distributional semantics based on graph representations. <eos> first we describe the underlying technology for computing a distributional thesaurus on words using bipartite graphs of words and context features, and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking. <eos> then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization. <eos> the visualization can be used as a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering, and is provided as an open source tool.
wordnet, a widely used sense inventory for word sense disambiguation ( wsd ), is often too fine-grained for many natural language applications because of its narrow sense distinctions. <eos> we present a semi-supervised approach to learn similarity between wordnet synsets using a graph based recursive similarity definition. <eos> we seed our framework with sense similarities of all the word-sense pairs, learnt using supervision on humanlabelled sense clusterings. <eos> finally we discuss our method to derive coarse sense inventories at arbitrary granularities and show that the coarse-grained sense inventory obtained significantly boosts the disambiguation of nouns on standard test sets.
distance metric learning from high ( thousands or more ) dimensional data with hundreds or thousands of classes is intractable but in nlp and ir, high dimensionality is usually required to represent data points, such as in modeling semantic similarity. <eos> this paper presents algorithms to scale up learning of a mahalanobis distance metric from a large data graph in a high dimensional space. <eos> our novel contributions include random projection that reduces dimensionality and a new objective function that regularizes intra-class and inter-class distances to handle a large number of classes. <eos> we show that the new objective function is convex and can be efficiently optimized by a stochastic-batch subgradient descent method. <eos> we applied our algorithm to two different domains ; semantic similarity of documents collected from the web, and phenotype descriptions in genomic data. <eos> experiments show that our algorithm can handle the high-dimensional big data and outperform competing approximations in both domains.
in this work, we propose a graph-based approach to computing similarities between words in an unsupervised manner, and take advantage of heterogeneous feature types in the process. <eos> the approach is based on the creation of two separate graphs, one for words and one for features of different types ( alignmentbased, orthographic, etc. ). <eos> the graphs are connected through edges that link nodes in the feature graph to nodes in the word graph, the edge weights representing the importance of a particular feature for a particular word. <eos> high quality graphs are learned during training, and the proposed method outperforms experimental baselines.
after recasting the computation of a distributional thesaurus in a graph-based framework for term similarity, we introduce a new contextualization method that generates, for each term occurrence in a text, a ranked list of terms that are semantically similar and compatible with the given context. <eos> the framework is instantiated by the definition of term and context, which we derive from dependency parses in this work. <eos> evaluating our approach on a standard data set for lexical substitution, we show substantial improvements over a strong non-contextualized baseline across all parts of speech. <eos> in contrast to comparable approaches, our framework defines an unsupervised generative method for similarity in context and does not rely on the existence of lexical resources as a source for candidate expansions.
bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost. <eos> in bootstrapping, unlabeled instances can be harvested from the initial labeled ? seed ? <eos> set. <eos> the selected seed set affects accuracy, but how to select a good seed set is not yet clear. <eos> thus, an ? iterative seeding ? <eos> framework is proposed for bootstrapping to reduce its labeling cost. <eos> our framework iteratively selects the unlabeled instance that has the best ? goodness of seed ? <eos> and labels the unlabeled instance in the seed set. <eos> our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem. <eos> we propose a method called expected model rotation ( emr ) that works well on not well-separated data which frequently occur as realistic data. <eos> experimental results show that emr can select seed sets that provide significantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets.
review quality is determined by identifying the relevance of a review to a submission ( the article or paper the review was written for ). <eos> we identify relevance in terms of the semantic and syntactic similarities between two texts. <eos> we use a word order graph, whose vertices, edges and double edges help determine structure-based match across texts. <eos> we use wordnet to determine semantic relatedness. <eos> ours is a lexico-semantic approach, which predicts relevance with an accuracy of 66 % and f -measure of 0.67.
many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. <eos> medical patient records, industrial accident reports, lawsuit records and investigation reports. <eos> effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. <eos> in this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. <eos> for this purpose, we developed a graph-based text representation that makes the relations between textual units explicit. <eos> this representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. <eos> when applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes.
entity resolution is the task of identifying which records in a database refer to the same entity. <eos> a standard machine learning pipeline for the entity resolution problem consists of three major components : blocking, pairwise linkage, and clustering. <eos> the blocking step groups records by shared properties to determine which pairs of records should be examined by the pairwise linker as potential duplicates. <eos> next, the linkage step assigns a probability score to pairs of records inside each block. <eos> if a pair scores above a user-defined threshold, the records are presumed to represent the same entity. <eos> finally, the clustering step turns the input records into clusters of records ( or profiles ), where each cluster is uniquely associated with a single real-world entity. <eos> this paper describes the blocking and clustering strategies used to deploy a massive database of organization entities to power a major commercial people search engine. <eos> we demonstrate the viability of these algorithms for large data sets on a 50-node hadoop cluster.
this paper presents a system that performs skill extraction from text documents. <eos> it outputs a list of professional skills that are relevant to a given input text. <eos> we argue that the system can be practical for hiring and management of personnel in an organization. <eos> we make use of the texts and the hyperlink graph of wikipedia, as well as a list of professional skills obtained from the linkedin social network. <eos> the system is based on first computing similarities between an input document and the texts of wikipedia pages and then using a biased, hub-avoiding version of the spreading activation algorithm on the wikipedia graph in order to associate the input document with skills.
