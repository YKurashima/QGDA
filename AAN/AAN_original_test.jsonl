{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level , we instead use features to transfer the behavior of words at a type level .", "We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language .", "In a discriminative dependency parsing framework , our approach produces gains across a range of target languages , using two different lowresource training methodologies ( one weakly supervised and one indirectly supervised ) and two different dictionary sources ( one manually constructed and one automatically constructed ) ."]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["Our approach uses international phonetic alphabet ( IPA ) to learn the interlingual representation and thus allows us to use any word and its IPA representation as a training example .", "Thus , our approach requires only monolingual resources : a phoneme dictionary that lists words and their IPA representations.1 By adding a phoneme dictionary of a new language , we can readily build a transliteration system into any of the existing previous languages , without the expense of all-pairs data or computation .", "We also propose a regularization framework for learning the interlingual representation , which accounts for language specific phonemic variability , and thus it can find better mappings between languages .", "In this paper , we address the problem of building a multilingual transliteration system using an interlingual representation .", "Experimental results on the name transliteration task in five diverse languages show a maximum improvement of 29 % accuracy and an average improvement of 17 % accuracy compared to a state-of-the-art baseline system ."]}
{"orig_sents": ["3", "4", "5", "0", "1", "2", "6"], "shuf_sents": ["The seed distributions are propagated over a graph representing relations among words , and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions .", "We propose two types of the graphs : a co-occurrence graph , representing co-occurrence relations between words , and a similarity graph , representing context similarities between words .", "Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods .", "This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation .", "In previous work , it was established that performance drastically decreases when the coverage of a seed lexicon is small .", "We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations , in which each word is represented by a distribution of translated seeds .", "Further , the similarity graph achieved improved performance by clustering synonyms into the same translation ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9 % with only a small set of features .", "We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life .", "The focus of the study is to identify the lexical features that distinguish the two populations .", "We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Given the sources and scale of material used in these efforts , along with potential use cases of such analytic tools , discourse analysis should be reconsidered as a streaming challenge .", "Once in a streaming framework , and motivated by large data sets generated by social media services , we present novel results in approximate counting , showing its applicability to space efficient streaming classification .", "We show that under certain common formulations , the batchprocessing analytic framework can be decomposed into a sequential series of updates , using as an example the task of gender classification .", "Inferring attributes of discourse participants has been treated as a batch-processing task : data such as all tweets from a given author are gathered in bulk , processed , analyzed for a particular feature , then reported as a result of academic interest ."]}
{"orig_sents": ["4", "7", "6", "0", "5", "2", "9", "1", "8", "3"], "shuf_sents": ["Epinions trust/distrust ) .", "We also connect our analysis to social psychology theories of balance .", "We find that the polarity of the links between users can be predicted with high accuracy given the text they exchange .", "Inspired by that , we present a technique for identifying subgroups in discussions by partitioning singed networks representing them .", "A mixture of positive ( friendly ) and negative ( antagonistic ) relations exist among users in most social media applications .", "We study text exchanged between users in online communities .", "As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed ( e.g .", "However , many such applications do not allow users to explicitly express the polarity of their interactions .", "We show that the automatically predicted networks are consistent with those theories .", "This allows us to build a signed network representation of discussions ; where every edge has a sign : positive to denote a friendly relation , or negative to denote an antagonistic relation ."]}
{"orig_sents": ["6", "5", "4", "0", "3", "2", "1"], "shuf_sents": ["In this paper , we propose the first fully generative goal-driven simulator that is fully induced from data , without hand-crafting or goal annotation .", "We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs .", "We evaluate on two standard dialog resources , the Communicator and Let ? s Go datasets , and demonstrate that our model has substantially better fit to held out data than competing approaches .", "Our goals are latent , and take the form of topics in a topic model , clustering together semantically equivalent and phonetically confusable strings , implicitly modelling synonymy and speech recognition noise .", "Instead , they have been evaluated extrinsically by means of the dialog managers they are intended to train , leading to circularity of argument .", "At present , goal-driven simulators ( those that have a persistent notion of what they wish to achieve in the dialog ) require some task-specific engineering , making them impossible to evaluate intrinsically .", "User simulation is frequently used to train statistical dialog managers for task-oriented domains ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation ( IP ) strategies , allowing the system to generate and comprehend backchannels and barge-ins , by employing the recent psycholinguistic hypothesis of information density ( ID ) ( Jaeger , 2010 ) .", "Unfortunately , prior work has focused largely on deterministic incremental decision making , rendering system behaviour less flexible and adaptive than is desirable .", "Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems , such as backchannels or barge-ins , but that can enhance the responsiveness and naturalness of systems .", "Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23 % ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling , where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence .", "We extend this approach to allow each block of text to be a mixture of multiple classes .", "We show that this model performs well at predictive tasks on two conversation data sets , improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM .", "Under our model , the probability of a class in a text block is a log-linear function of the classes in the previous block .", "Additionally , we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models ."]}
{"orig_sents": ["4", "1", "0", "3", "2", "6", "5", "7"], "shuf_sents": ["Traditionally , there have been two distinct directions of EL research : one focusing on the effects of mention ? s context compatibility , assuming that ? the referent entity of a mention is reflected by its context ?", "Given many name mentions in a document , the goal of EL is to predict their referent entities in a knowledge base .", "In this paper , we propose a generative model ?", "; the other dealing with the effects of document ? s topic coherence , assuming that ? a mention ? s referent entity should be coherent with the document ? s main topics ? .", "Entity Linking ( EL ) has received considerable attention in recent years .", "By jointly modeling and exploiting the context compatibility , the topic coherence and the correlation between them , our model can accurately link all mentions in a document using both the local information ( including the words and the mentions in a document ) and the global knowledge ( including the topic knowledge , the entity context knowledge and the entity name knowledge ) .", "called entitytopic model , to effectively join the above two complementary directions together .", "Experimental results demonstrate the effectiveness of the proposed model ."]}
{"orig_sents": ["2", "4", "0", "3", "1"], "shuf_sents": ["This paper introduces a new task , called Open-Database Named-Entity Disambiguation ( Open-DB NED ) , in which a system must be able to resolve named entities to symbols in an arbitrary database , without requiring labeled data for each new database .", "In experiments on two domains , one with poor coverage by Wikipedia and the other with near-perfect coverage , our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25 % in accuracy .", "Existing techniques for disambiguating named entities in text mostly focus on Wikipedia as a target catalog of entities .", "We introduce two techniques for Open-DB NED , one based on distant supervision and the other based on domain adaptation .", "Yet for many types of entities , such as restaurants and cult movies , relational databases exist that contain far more extensive information than Wikipedia ."]}
{"orig_sents": ["3", "6", "5", "1", "4", "2", "0"], "shuf_sents": ["We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure .", "Given a set of basic features and an annotated document collection , our goal is to generate an initial set of rules with reasonable accuracy , that are interpretable and thus can be easily refined by a human developer .", "We also propose a simple notion of extractor complexity as a first step to quantify the interpretability of an extractor , and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules .", "Generic rule-based systems for Information Extraction ( IE ) have been shown to work reasonably well out-of-the-box , and achieve state-of-the-art accuracy with further domain customization .", "We present an efficient rule induction process , modeled on a fourstage manual rule development process and present initial promising results with our system .", "In this paper , we discuss an approach that facilitates the process of building customizable rules for Named-Entity Recognition ( NER ) tasks via rule induction , in the Annotation Query Language ( AQL ) .", "However , it is generally recognized that manually building and customizing rules is a complex and labor intensive process ."]}
{"orig_sents": ["5", "2", "3", "4", "1", "6", "0"], "shuf_sents": ["1", "Specifically , our co-selecting approach employs two feature subspace classifiers to collectively select most informative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement , and in the meanwhile , automatically label most informative majority-class samples , to reduce humanannotation efforts .", "In this paper , we focus on the imbalanced class distribution scenario for sentiment classification , wherein the number of positive samples is quite different from that of negative samples .", "This scenario posits new challenges to active learning .", "To address these challenges , we propose a novel active learning approach , named co-selecting , by taking both the imbalanced class distribution issue and uncertainty into account .", "Active learning is a promising way for sentiment classification to reduce the annotation cost .", "Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification ."]}
{"orig_sents": ["0", "2", "4", "3", "5", "1"], "shuf_sents": ["We propose the weakly supervised MultiExperts Model ( MEM ) for analyzing the semantic orientation of opinions expressed in natural language reviews .", "Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin .", "In contrast to most prior work , MEM predicts both opinion polarity and opinion strength at the level of individual sentences ; such fine-grained analysis helps to understand better why users like or dislike the entity under review .", "For this reason , MEM is weakly supervised : It starts with potentially noisy indicators obtained from coarse-grained training data ( i.e. , document-level ratings ) , a small set of diverse base predictors , and , if available , small amounts of fine-grained training data .", "A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength .", "We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning ."]}
{"orig_sents": ["2", "3", "0", "7", "1", "6", "5", "4", "8"], "shuf_sents": ["or ? long , startup ?", "changes along with different targets ( ? battery life ?", "This paper focuses on the task of collocation polarity disambiguation .", "The collocation refers to a binary tuple of a polarity word and a target ( such as ? long , battery life ?", "However , these contexts are limited , thus the resulting polarity is insufficient to be reliable .", "To disambiguate a collocation ? s polarity , previous work always turned to investigate the polarities of its surrounding contexts , and then assigned the majority polarity to the collocation .", "or ? startup ? ) .", ") , in which the sentiment orientation of the polarity word ( ? long ? )", "We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web , to help disambiguate a collocation ? s polarity.Without using any additional labeled data , experiments show that our method is effective ."]}
{"orig_sents": ["1", "2", "0", "3", "4", "5"], "shuf_sents": ["We present an important subtask for this overall goal , in which we align predicates across comparable texts , admitting partial argument structure correspondence .", "Generating coherent discourse is an important aspect in natural language generation .", "Our aim is to learn factors that constitute coherent discourse from data , with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level .", "The contribution of this work is two-fold : We first construct a large corpus resource of comparable texts , including an evaluation set with manual predicate alignments .", "Secondly , we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts .", "Our method significantly outperforms other alignment techniques when applied to this novel alignment task , by a margin of at least 6.5 percentage points in F1-score ."]}
{"orig_sents": ["2", "4", "0", "3", "5", "1"], "shuf_sents": ["Our algorithm is tested on the data from the metonymy resolution task ( Task 8 ) at SemEval 2007 .", "We show that such an unsupervised approach delivers promising results : it beats the supervised most frequent sense baseline and performs close to a supervised approach using only standard lexico-syntactic features .", "Computational approaches to metonymy resolution have focused almost exclusively on the local context , especially the constraints placed on a potentially metonymic word by its grammatical collocates .", "The results show that incorporation of the global context can improve over the use of the local context alone , depending on the types of metonymies addressed .", "We expand such approaches by taking into account the larger context .", "As a second contribution , we move towards unsupervised resolution of metonymies , made feasible by considering ontological relations as possible readings ."]}
{"orig_sents": ["2", "3", "4", "1", "0"], "shuf_sents": ["We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task .", "To this end , we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework .", "Learning inference relations between verbs is at the heart of many semantic applications .", "However , most prior work on learning such rules focused on a rather narrow set of information sources : mainly distributional similarity , and to a lesser extent manually constructed verb co-occurrence patterns .", "In this paper , we claim that it is imperative to utilize information from various textual scopes : verb co-occurrence within a sentence , verb cooccurrence within a document , as well as overall corpus statistics ."]}
{"orig_sents": ["2", "1", "4", "3", "0", "5"], "shuf_sents": ["As a pilot experimental evaluation , we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser .", "Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable , unlike EM methods , which can get stuck in local minima .", "Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs .", "We propose a simple yet powerful latent variable generative model for dependency parsing , and a spectral learning method to efficiently estimate it .", "In this paper , we present a novel extension of this class of spectral methods to learn dependency tree structures .", "Our approach gives us a moderate reduction in error of up to 4.6 % over the baseline re-ranker ."]}
{"orig_sents": ["0", "1", "4", "2", "3", "5"], "shuf_sents": ["Topic models traditionally rely on the bagof-words assumption .", "In data mining applications , this often results in end-users being presented with inscrutable lists of topical unigrams , single words inferred as representative of their topics .", "The model simultaneously infers the location , length , and topic of phrases within a corpus and relaxes the bagof-words assumption within phrases by using a hierarchy of Pitman-Yor processes .", "We use Markov chain Monte Carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters .", "In this article , we present a hierarchical generative probabilistic model of topical phrases .", "We show via an experiment on human subjects that our model finds substantially better , more interpretable topical phrases than do competing models ."]}
{"orig_sents": ["1", "3", "0", "2", "4"], "shuf_sents": ["Through both synthetic grammar induction and statistical machine translation experiments , we show that our model learns complex translational correspondences ?", "We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text .", "including discontiguous , many-to-many alignments ? and produces competitive translation results .", "The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules .", "Further , inference is efficient and we present results on significantly larger corpora than prior work ."]}
{"orig_sents": ["4", "2", "1", "0", "3"], "shuf_sents": ["The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting , soft and hard constraints .", "We present a method where such individual aspects are learned separately from data ( without any hand-engineering ) but optimized jointly using an integer linear programme .", "The summaries must be informative , succinct , grammatical , and obey stylistic writing conventions .", "Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries .", "Multi-document summarization involves many aspects of content selection and surface realization ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We find that with a stateof-the-art , comprehensive realization ranking model , dependency length minimization yields statistically significant improvements in BLEU scores and significantly reduces the number of heavy/light ordering errors .", "Through distributional analyses , we also show that with simpler ranking models , dependency length minimization can go overboard , too often sacrificing canonical word order to shorten dependencies , while richer models manage to better counterbalance the dependency length minimization preference against ( sometimes ) competing canonical word order preferences .", "In this paper , we investigate dependency length minimization in the context of discriminative realization ranking , focusing on its potential to eliminate egregious ordering errors as well as better match the distributional characteristics of sentence orderings in news text .", "Comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply , and other summarizing techniques , such as sentence compression , can be easily adapted as actions of the framework .", "The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method , in terms of ROUGE scores .", "We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning ( ASRL ) in this paper , which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary .", "The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points .", "Compared with the state of the art algorithm , our approach is highly scalable and produces better results , which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy .", "We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation ."]}
{"orig_sents": ["2", "3", "0", "1", "5", "4"], "shuf_sents": ["However , most of current Statistical Machine Translation ( SMT ) systems mainly depend on translation model and language model .", "They never consider and make full use of tense information .", "Tense is a small element to a sentence , however , error tense can raise odd grammars and result in misunderstanding .", "Recently , tense has drawn attention in many natural language processing applications .", "Experimental results on the NIST Chinese-English translation task show that our proposed tense models are very effective , contributing performance improvement by 0.62 BLUE points over a strong baseline .", "In this paper , we propose n-gram-based tense models for SMT and successfully integrate them into a state-of-the-art phrase-based SMT system via two additional features ."]}
{"orig_sents": ["3", "0", "2", "4", "1", "5"], "shuf_sents": ["We assume a small bi-text for the resourcepoor language to X pair , which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-rich and the resource-poor language ; we then adapt the former to get closer to the latter .", "3 BLEU points .", "Our experiments for Indonesian/Malay ? English translation show that using the large adapted resource-rich bitext yields 6.7 BLEU points of improvement over the unadapted one and 2.6 BLEU points over the original small bi-text .", "We propose a novel , language-independent approach for improving machine translation from a resource-poor language to X by adapting a large bi-text for a related resource-rich language and X ( the same target language ) .", "Moreover , combining the small bi-text with the adapted bi-text outperforms the corresponding combinations with the unadapted bi-text by 1.5 ?", "We also demonstrate applicability to other languages and domains ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency .", "We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results .", "We introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["None of these transitions has a negative impact on the computational complexity of the algorithm .", "In this paper , we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained , without sacrificing efficiency , by enriching the parsers with simple transitions that act on buffer nodes .", "First , we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre ? s arc-eager projective parser on a number of datasets from the CoNLL-X shared task .", "Although the experiments in this paper use the arc-eager parser , the approach is generic enough to be applicable to any stackbased dependency parser .", "Then , we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node ( allowing a limited form of non-projectivity ) ."]}
{"orig_sents": ["1", "0", "6", "2", "3", "5", "4"], "shuf_sents": ["On the other hand , transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant .", "State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize .", "The generalization is at the cost of asymptotic efficiency .", "To account for this , cube pruning for decoding is utilized ( Chiang , 2007 ) .", "Our parser achieves the state-of-art unlabeled accuracy of 93.06 % and labeled accuracy of 91.86 % on the standard test set for English , at a faster speed than a reimplementation of the third-order model of Koo et al2010 ) .", "For the first time , label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser .", "In this paper , we attempt to address this imbalance for graph-based parsing by generalizing the Eisner ( 1996 ) algorithm to handle arbitrary features over higherorder dependencies ."]}
{"orig_sents": ["4", "3", "0", "2", "1"], "shuf_sents": ["We then cast our task in the framework of supervised learning , where each known language serves as a training example , and predictions are made on unknown languages .", "Our model correctly predicts grapheme-phoneme pairs with over 88 % F1-measure .", "We induce an undirected graphical model that learns phonotactic regularities , thus relating textual patterns to plausible phonemic interpretations across the entire range of languages .", "First , we collect a data-set of 107 languages with known grapheme-phoneme relationships , along with a short text in each language .", "We consider the problem of inducing grapheme-to-phoneme mappings for unknown languages written in a Latin alphabet ."]}
{"orig_sents": ["3", "5", "7", "6", "2", "1", "4", "0"], "shuf_sents": ["We find that our method can effectively find name variants in a corpus of web strings used to refer to persons inWikipedia , improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance .", "Our variational EM learning algorithm alternately reestimates this phylogeny and the transducer parameters .", "strings in the collection .", "Many linguistic and textual processes involve transduction of strings .", "The final learned transducer can quickly link any test name into the final phylogeny , thereby locating variants of the test name .", "We show how to learn a stochastic transducer from an unorganized collection of strings ( rather than string pairs ) .", "Our generative model explains similarities among the strings by supposing that some strings in the collection were not generated ab initio , but were instead derived by transduction from other , ? similar ?", "The role of the transducer is to organize the collection ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["This result supports the uniform information density ( UID ) hypothesis and points a way to more realistic artificial speech generation .", "Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations , we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures , such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies , and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities .", "We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech .", "We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2 % improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6 .", "Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens , the reason is also often something undesirable , and if something desirable happens , the reason is also often something desirable .", "To the best of our knowledge , this is the first work that introduces sentiment analysis to non-factoid question answering .", "In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus ."]}
{"orig_sents": ["1", "5", "3", "0", "4", "2"], "shuf_sents": ["Our method is based on an integer linear program to solve several disambiguation tasks jointly : the segmentation of questions into phrases ; the mapping of phrases to semantic entities , classes , and relations ; and the construction of SPARQL triple patterns .", "The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF .", "We present experiments on both the question translation and the resulting query answering .", "To ease the task , this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources .", "Our solution harnesses the rich type system provided by knowledge bases in the web of linked data , to constrain our semantic-coherence objective function .", "Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users ."]}
{"orig_sents": ["2", "4", "0", "8", "6", "5", "1", "9", "7", "3"], "shuf_sents": ["For each aspect , the reviews and corresponding opinions on this aspect are stored .", "In order to generate appropriate answers from the review fragments , we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience , coherence , diversity , and parent-child relations among the aspects .", "This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews .", "Experimental results demonstrate the effectiveness of our approach .", "The hierarchy organizes product aspects as nodes following their parent-child relations .", "We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy .", "In particular , we first identify the ( explicit/implicit ) product aspects asked in the questions and their sub-aspects by referring to the hierarchy .", "The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products .", "We develop a new framework for opinion Questions Answering , which enables accurate question analysis and effective answer generation by making use the hierarchy .", "We conduct evaluations on 11 popular products in four domains ."]}
{"orig_sents": ["3", "6", "5", "1", "4", "2", "0", "7"], "shuf_sents": ["We propose efficient incremental training methods to put the local training into practice .", "Second , translations become inconsistent at the sentence level since tuning is performed globally on a document level .", "Unlike a global training method , such as MERT , in which a single weight is learned and used for all the input sentences , we perform training and testing in one step by learning a sentencewise weight for each input sentence .", "In statistical machine translation , minimum error rate training ( MERT ) is a standard method for tuning a single weight with regard to a given development data .", "In this paper , we propose a novel local training method to address these two problems .", "First , its performance is highly dependent on the choice of a development set , which may lead to an unstable performance for testing .", "However , due to the diversity and uneven distribution of source sentences , there are two problems suffered by this method .", "In NIST Chinese-to-English translation tasks , our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points , meanwhile its efficiency is comparable to that of the global method ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["On the Penn Chinese Treebank 5.0 , it achieves an F-measure of 98.43 % , significantly outperforms previous works although using a single classifier with only local features .", "In this paper we first describe the technology of automatic annotation transformation , which is based on the annotation adaptation algorithm ( Jiang et al2009 ) .", "It can automatically transform a human-annotated corpus from one annotation guideline to another .", "Experiments on Chinese word segmentation show that , the iterative training strategy together with predictself reestimation brings significant improvement over the simple annotation transformation baseline , and leads to classifiers with significantly higher accuracy and several times faster processing than annotation adaptation does .", "We then propose two optimization strategies , iterative training and predict-self reestimation , to further improve the accuracy of annotation guideline transformation ."]}
{"orig_sents": ["1", "2", "6", "3", "5", "0", "4"], "shuf_sents": ["This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision .", "? Grounded ?", "language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts .", "Their approach works well when each sentence potentially refers to one of a small set of possible meanings , such as in the sportscasting task .", "Experimental results on the navigation task demonstrates the effectiveness of our approach .", "However , it does not scale to problems with a large set of potential meanings for each sentence , such as the navigation instruction following task studied by Chen and Mooney ( 2011 ) .", "Bo ? rschinger et al2011 ) introduced an approach to grounded language learning based on unsupervised PCFG induction ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["As the first step in this line of research , we verify the effectiveness of our approach in a BTGbased phrasal system , and propose four FDTbased component models .", "We first describe how to generate different FDTs for each sentence pair in training corpus , and then present how to infer the optimal FDTs based on their derivation and alignment qualities .", "A forced derivation tree ( FDT ) of a sentence pair { f , e } denotes a derivation tree that can translate f into its accurate target translation e. In this paper , we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation ( SMT ) systems .", "Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks , and significant improvements are reported on both translation quality and alignment quality ."]}
{"orig_sents": ["1", "3", "5", "4", "2", "7", "0", "6"], "shuf_sents": ["We propose a novel approach to multi-instance multi-label learning for RE , which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables .", "Distant supervision for relation extraction ( RE ) ?", "For example , a sentence containing Balzac and France may express BornIn or Died , an unknown relation , or no relation at all .", "gathering training data by aligning a database of facts with text ?", "However , this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown .", "is an efficient approach to scale RE to thousands of different relations .", "Our model performs competitively on two difficult domains .", "Because of this , traditional supervised learning , which assumes that each example is explicitly mapped to a label , is not appropriate ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We report comparisons between several techniques for feature selection and various learning algorithms .", "Our best model , based on support vector machines ( SVM ) , significantly outperforms previous FFL formulas .", "This paper present a new readability formula for French as a foreign language ( FFL ) , which relies on 46 textual features representative of the lexical , syntactic , and semantic levels as well as some of the specificities of the FFL context .", "We also found that semantic features behave poorly in our case , in contrast with some previous readability studies on English as a first language ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["The 1-Inherit class of trees has exactly the same empirical coverage of natural language sentences as the class of mildly nonprojective trees , yet the optimal scoring tree can be found in an order of magnitude less time .", "Based on this property , two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees : unlike projective trees , a word can have descendants in more than one interval ; unlike spanning trees , these intervals can not be nested in arbitrary ways .", "We introduce gap inheritance , a new structural property on trees , which provides a way to quantify the degree to which intervals of descendants can be nested .", "Gap-minding trees ( the second class ) have the property that all edges into an interval of descendants come from the same node , and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals ."]}
{"orig_sents": ["1", "0", "4", "2", "3"], "shuf_sents": ["Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations .", "We introduce a novel coreference resolution system that models entities and events jointly .", "Our system handles nominal and verbal events as well as entities , and our joint formulation allows information from event coreference to help entity coreference , and vice versa .", "In a cross-document domain with comparable documents , joint coreference resolution performs significantly better ( over 3 CoNLL F1 points ) than two strong baselines that resolve entities and events separately .", "As clusters are built , information flows between entity and event clusters through features that model semantic role dependencies ."]}
{"orig_sents": ["3", "5", "0", "1", "2", "4", "6"], "shuf_sents": ["Chinese word segmentation followed by POS tagging and parsing , which suffers from error propagation and is unable to leverage information in later modules for earlier components .", "In our approach , we train the three individual models separately during training , and incorporate them together in a unified framework during decoding .", "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features .", "In this paper , we propose a novel decoding algorithm for discriminative joint Chinese word segmentation , part-of-speech ( POS ) tagging , and parsing .", "As far as we know , this is the first work on joint Chinese word segmentation , POS tagging and parsing .", "Previous work often used a pipeline method ?", "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system ."]}
{"orig_sents": ["4", "2", "0", "1", "3"], "shuf_sents": ["Compared with the traditional approaches which utilize the first pass translation hypotheses , cross-lingual data selection model avoids the problem of noisy proliferation .", "Furthermore , phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM , because it captures contextual information in modeling the selection of phrase as a whole .", "Given a source sentence in the translation task , this model directly estimates the probability that a sentence in the target LM training corpus is similar .", "Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance .", "In this paper , we propose a novel translation model ( TM ) based cross-lingual data selection model for language model ( LM ) adaptation in statistical machine translation ( SMT ) , from word models to phrase models ."]}
{"orig_sents": ["1", "0", "5", "2", "4", "6", "3"], "shuf_sents": ["However , stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses ?", "Open Information Extraction ( IE ) systems extract relational tuples from text , without requiring a pre-specified vocabulary , by identifying relation phrases and associated arguments in arbitrary sentences .", "This paper presents OLLIE , a substantially improved Open IE system that addresses both these limitations .", "OLLIE obtains 2.7 times the area under precision-yield curve ( AUC ) compared to REVERB and 1.9 times the AUC of WOEparse .", "First , OLLIE achieves high yield by extracting relations mediated by nouns , adjectives , and more .", "( 1 ) they extract only relations that are mediated by verbs , and ( 2 ) they ignore context , thus extracting tuples that are not asserted as factual .", "Second , a context-analysis step increases precision by including contextual information from the sentence in the extractions ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["In this paper , we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document .", "Experimental results show that with topic adaptation , our model significantly improves over existing approaches in terms of perplexity , and is able to uncover clear sequential structure on , for example , Herman Melville ? s book ? Moby Dick ? .", "Topic models are increasingly being used for text analysis tasks , often times replacing earlier semantic techniques such as latent semantic analysis .", "For this proposed model , a Gibbs sampler is developed for doing posterior inference ."]}
{"orig_sents": ["0", "2", "4", "3", "1"], "shuf_sents": ["In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods .", "The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method .", "We experiment with several possible combinations of representation and composition , exhibiting varying degrees of sophistication .", "We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests : ( 1 ) phrase similarity and ( 2 ) paraphrase detection .", "Some are shallow while others operate over syntactic structure , rely on parameter learning , or require access to very large corpora ."]}
{"orig_sents": ["6", "4", "5", "0", "3", "2", "1"], "shuf_sents": ["A relaxed , online maximum margin training algorithm is used for learning .", "In particular , our approach is much better at recognizing long and complicated phrases .", "The experimental results show that the use of chunk-level features can lead to significant performance improvement , and that our approach achieves state-of-the-art performance .", "Within this framework , we explored a variety of effective feature representations for Chinese phrase chunking .", "In this paper , we formulate phrase chunking as a joint segmentation and labeling task .", "We propose an efficient dynamic programming algorithm with pruning for decoding , which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks .", "Most existing systems solved the phrase chunking task with the sequence labeling approaches , in which the chunk candidates can not be treated as a whole during parsing process so that the chunk-level features can not be exploited in a natural way ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own ( HOO ) shared task data set .", "The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .", "These features include scores from discriminative classifiers for specific error categories , such as articles and prepositions .", "We present a novel beam-search decoder for grammatical error correction .", "Unlike all previous approaches , our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches ."]}
{"orig_sents": ["4", "3", "0", "2", "1", "5"], "shuf_sents": ["To this end , abstracts in evidence-based medicine can be labeled using a set of predefined medical categories , the socalled PICO criteria .", "Since both structural and sequential information are important for this classification task , we use kLog , a new language for statistical relational learning with kernels .", "This paper presents an approach to automatically annotate sentences in medical abstracts with these labels .", "This requires efficient access to such evidence .", "Evidence-based medicine is an approach whereby clinical decisions are supported by the best available findings gained from scientific research .", "Our results show a clear improvement with respect to state-of-the-art systems ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["In this paper , we explore the classification of emotions in songs , using the music and the lyrics representation of the songs .", "We show that textual and musical features can both be successfully used for emotion recognition in songs .", "We introduce a novel corpus of music and lyrics , consisting of 100 songs annotated for emotions .", "Moreover , through comparative experiments , we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers , with error rate reductions of up to 31 % ."]}
{"orig_sents": ["6", "3", "7", "9", "10", "0", "4", "2", "5", "8", "1"], "shuf_sents": ["We use a large corpus of English learners ?", "An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors .", "Our proposed feature measures the similarity of a given response with the most proficient group and is then estimates the learner ? s syntactic competence level .", "syntactic competence towards improving automated speech scoring systems .", "responses that are classified into four proficiency levels by human raters .", "Widely outperforming the state-of-the-art measures of syntactic complexity , our method attained a significant correlation with humanrated scores .", "This study presents a novel method that measures English language learners ?", "In contrast to most previous studies which focus on the length of production units such as the mean length of clauses , we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency .", "The correlation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower , 0.42 .", "We estimated the syntactic competence through the use of corpus-based NLP techniques .", "Assuming that the range and sophistication of grammatical expressions can be captured by the distribution of Part-ofSpeech ( POS ) tags , vector space models of POS tags were constructed ."]}
{"orig_sents": ["8", "3", "0", "6", "1", "5", "7", "4", "2"], "shuf_sents": ["The online scenario is particularly important for languages that routinely use transliteration-based text input methods , such as Chinese and Japanese , because the desired target characters can not be input at all unless they are in the list of candidates provided by an input method , and spelling errors prevent them from appearing in the list .", "'student ' in Chinese ; existing input methods fail to convert this misspelled input to the desired target Chinese characters .", "Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20 % ( from 8.9 % to 7.12 % ) over the previous state-of-the art based on a noisy channel model .", "Online spelling correction refers to the spelling correction as you type , as opposed to post-editing .", "A new method of automatically deriving parallel training data from user keystroke logs is also presented .", "In this paper , we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework .", "For example , a user might type suesheng by mistake to mean xuesheng ? ?", "At the phrase ( substring ) level , k most probable pinyin ( Romanized Chinese ) corrections are generated using a monotone decoder ; at the sentence level , input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels .", "This paper presents an integrated , end-to-end approach to online spelling correction for text input ."]}
{"orig_sents": ["2", "3", "1", "5", "0", "4", "6"], "shuf_sents": ["heighten anxiety ) .", "We show that Excitation is useful for extracting contradiction pairs ( e.g. , destroy cancer ?", "We propose a new semantic orientation , Excitation , and its automatic acquisition method .", "Excitation is a semantic property of predicates that classifies them into excitatory , inhibitory and neutral .", "Our experiments show that with automatically acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70 % precision from a 600 million page Web corpus .", "develop cancer ) and causality pairs ( e.g. , increase in crime ?", "Furthermore , by combining these extracted causality and contradiction pairs , we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus with reasonable precision ."]}
{"orig_sents": ["6", "5", "1", "0", "2", "4", "3"], "shuf_sents": ["We show how one can use monolingual corpora , which are far more numerous and larger than bilingual corpora , to obtain paraphrases that rival in quality those derived directly from bilingual corpora .", "While the former are regarded as a source of high-quality seed paraphrases , the latter are searched for paraphrases that match patterns learned from the seed paraphrases .", "In our experiments , the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases .", "Remaining noise can be further reduced by filtering seed paraphrases .", "Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are of reasonable quality .", "Unlike existing methods , ours uses both bilingual parallel and monolingual corpora .", "This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases : paraphrase patterns are first induced and then used to collect novel instances ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains .", "We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem .", "In particular , the paper analyses two existing methods : an optimisation problem and a ranking algorithm .", "This paper presents a comparative study of graph-based approaches for cross-domain sentiment classification ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper explores log-based query expansion ( QE ) models for Web search .", "Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries .", "Evaluations on a real world data set show that the lexicon models , integrated into a ranker-based QE system , not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods .", "These models are trained on pairs of user queries and titles of clicked documents ."]}
{"orig_sents": ["2", "4", "0", "5", "1", "3"], "shuf_sents": ["We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints .", "Overall , our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2 % of relative improvement in F1 .", "This paper addresses the task of constructing a timeline of events mentioned in a given text .", "The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system .", "To accomplish that , we present a novel representation of the temporal structure of a news article based on time intervals .", "Moreover , we present ways to leverage knowledge provided by event coreference to further improve the system performance ."]}
{"orig_sents": ["6", "4", "1", "5", "3", "0", "2"], "shuf_sents": ["( iii ) Sentence-internal punctuation boundaries help with longer-distance dependencies , since punctuation correlates with constituent edges .", "such as English determiners ?", "Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased , manually-tuned initializers .", "( ii ) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles , allowing us to model grammatical differences between complete and incomplete sentences .", "We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations : ( i ) Distributions of words that occur at sentence boundaries ?", "resemble constituent edges .", "We present a new family of models for unsupervised parsing , Dependency and Boundary models , that use cues at constituent boundaries to inform head-outward dependency tree generation ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["After presenting a new , simple baseline , we show that learned collocations used as features in a maxent model perform better still , but that the story is more mixed for the syntactic language model .", "The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem , typically using as features a mix of n-grams over characters and part of speech tags ( for small and fixed n ) and unigram function words .", "To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful , adaptor grammars have some promise .", "In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words , using both maxent and induced syntactic language model approaches to classification ."]}
{"orig_sents": ["5", "6", "0", "4", "3", "2", "1"], "shuf_sents": ["We are particularly interested in revealing and exploiting relationships between documents .", "Finally , the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges .", "Our method is highly scalable , running on a corpus of over 30 million words in about four minutes , more than 75 times faster than a dynamic topic model .", "To illustrate , we extract research threads from citation graphs and construct timelines from news articles .", "To this end , we focus on extracting diverse sets of threads ? singlylinked , coherent chains of important documents .", "We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections .", "As in clustering and topic modeling , our goal is to provide an organizing perspective into otherwise overwhelming amounts of information ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["A detailed quantified typology of subsentential paraphrases found in our corpus types is given .", "A corpus of 2,500 annotated sentences in English and French is described , and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition .", "This paper describes a study on the impact of the original signal ( text , speech , visual scene , event ) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition ."]}
{"orig_sents": ["0", "4", "2", "6", "1", "3", "5"], "shuf_sents": ["Graph-based dependency parsers suffer from the sheer number of higher order edges they need to ( a ) score and ( b ) consider during optimization .", "For second order grandparent models , our method considers , or scores , no more than 6 ? 13 % of the second order edges of the full model .", "This is achieved by iteratively parsing with a subset of higherorder edges , adding higher-order edges that may improve the score of the current solution , and adding higher-order edges that are implied by the current best first order edges .", "This yields up to an eightfold parsing speedup , while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation .", "Here we show that when working with LP relaxations , large fractions of these edges can be pruned before they are fully scored ? without any loss of optimality guarantees and , hence , accuracy .", "We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed .", "This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution ."]}
{"orig_sents": ["1", "2", "0", "4", "3"], "shuf_sents": ["With simplification , this method can be used in the traditional withindomain case , while still retaining the above features .", "We propose an adaptive ensemble method to adapt coreference resolution across domains .", "This method has three features : ( 1 ) it can optimize for any user-specified objective measure ; ( 2 ) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models ; ( 3 ) it can automatically adjust the active ensemble members during prediction .", "Empirically , we show the benefits of ( i ) on the six domains of the ACE 2005 data set in domain adaptation setting , and of ( ii ) on both the MUC-6 and the ACE 2005 data sets in within-domain setting .", "To the best of our knowledge , this work is the first to both ( i ) develop a domain adaptation algorithm for the coreference resolution problem and ( ii ) have the above features as an ensemble method ."]}
{"orig_sents": ["2", "1", "4", "5", "0", "3"], "shuf_sents": ["We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase .", "Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser : semantic supervision from a knowledge base , and syntactic supervision from dependencyparsed sentences .", "We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus , without any individually annotated sentences .", "On this task , the trained semantic parser achieves 80 % precision and 56 % recall , despite never having seen an annotated logical form .", "We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation .", "This semantic parser extracts instances of binary relations with state-of-theart accuracy , while simultaneously recovering much richer semantic structures , such as conjunctions of multiple relations with partially shared arguments ."]}
{"orig_sents": ["3", "0", "2", "1", "4"], "shuf_sents": ["Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages .", "We therefore propose syntactic reordering for cross-lingual language modeling , and present a first result that compares inversion transduction grammar ( ITG ) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system .", "The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages .", "This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary .", "Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance significantly , up to 3.4 % relative WER reduction in Cantonese transcription and 13.3 % relative bilingual evaluation understudy ( BLEU ) score improvement in Mandarin transcription compared with the system without reordering ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We examine the task of resolving complex cases of definite pronouns , specifically those for which traditional linguistic constraints on coreference ( e.g. , Binding Constraints , gender and number agreement ) as well as commonly-used resolution heuristics ( e.g. , string-matching facilities , syntactic salience ) are not useful .", "Being able to solve this task has broader implications in artificial intelligence : a restricted version of it , sometimes referred to as the Winograd Schema Challenge , has been suggested as a conceptually and practically appealing alternative to the Turing Test .", "We employ a knowledge-rich approach to this task , which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker .", "We treat the problem as a sequence labelling task , which allows us to incorporate sequence features without using gold standard information .", "We present results on two new corpora and an augmented version of a third , achieving a new state-of-the-art for systems using only realistic features .", "The present state-of-the-art system uses gold standard information from previous decisions in its features , which , when removed , results in a large drop in performance ."]}
{"orig_sents": ["4", "3", "5", "1", "0", "2"], "shuf_sents": ["Answers and ODP datasets , and assess the performance in terms of perplexity and clustering .", "We also prove that hLDA and hLLDA are special cases of SSHLDA.We conduct experiments on Yahoo !", "The experimental results show that predictive ability of SSHLDA is better than that of baselines , and SSHLDA can also achieve significant improvement over baselines for clustering on the FScore measure .", "Supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels , but can not explore new topics ; while unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space , but does not make use of any information from hierarchical labels .", "Supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics , such as hLLDA and hLDA .", "In this paper , we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical labels into the modeling process , called SemiSupervised Hierarchical Latent Dirichlet Allocation ( SSHLDA ) ."]}
{"orig_sents": ["1", "5", "0", "3", "2", "4"], "shuf_sents": ["We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task .", "Many NLP tasks make predictions that are inherently coupled to syntactic relations , but for many languages the resources required to provide such syntactic annotations are unavailable .", "At both training and test time we marginalize over this hidden structure , learning the optimal latent representations for the problem .", "Our method couples latent syntactic representations , constrained to form valid dependency graphs or constituency parses , with the prediction task via specialized factors in a Markov random field .", "Results show that this approach provides significant gains over a syntactically uninformed baseline , outperforming models that observe syntax on an English relation extraction task , and performing comparably to them in semantic role labeling .", "For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models , and what structures in the syntactic trees are most relevant to the current task ."]}
{"orig_sents": ["4", "2", "0", "1", "3"], "shuf_sents": ["Taking the MINGREEDY algorithm ( Ravi et al2010 ) as a starting point , we improve it with several intuitive heuristics .", "We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data .", "Here , we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries .", "Altogether , our augmentations produce improvements to performance over the original MIN-GREEDY algorithm for both English and Italian data .", "Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results , but the assumptions made about those dictionaries are often unrealistic : due to historical precedents , they assume access to information about labels in the raw and test sets ."]}
{"orig_sents": ["3", "2", "5", "1", "4", "0"], "shuf_sents": ["The experimental results show that the corpus level topic information provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three WePS datasets .", "This can alleviate the lack of clues where discriminative features can be reasonably weighted by taking their corpus level importance into account , not just relying on the current local context .", "In literature , the latter receives less attention and remains more challenging .", "In this paper , we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments .", "We therefore propose a topic-based model to exploit the person specific global importance and embed it into the person name similarity .", "We explore the feature space in this task and argue that collecting person specific evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature ? s importance in a given web page ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text .", "We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree .", "This is done by treating the parser ? s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy .", "Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods ."]}
{"orig_sents": ["5", "4", "3", "0", "2", "1"], "shuf_sents": ["One is targeted self-training with a simple evaluation function ; the other is based on training data selection from forced alignment of bilingual data .", "The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks .", "We also propose an auxiliary method for boosting alignment quality , by symmetrizing alignment matrices with respect to parse trees .", "In this paper , we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices .", "In the current state of the art these two components are mutually independent , thus causing problems like lack of rule generalization , and violation of syntactic correspondence in translation rules .", "The training of most syntactic SMT approaches involves two essential components , word alignment and monolingual parser ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Using the manually annotated English Chinese Translation Treebank , we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax .", "We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora .", "Furthermore , when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system , the transformed trees achieve a 0.9 BLEU improvement over baseline trees ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["Our system achieves a 36 % error reduction over a pipelined baseline .", "For example , our model learns that someone is unlikely to start a job at age two or to marry someone who hasn ? t been born yet .", "We present a distantly supervised system for extracting the temporal bounds of fluents ( relations which only hold during certain times , such as attends school ) .", "Instead , we model what makes timelines of fluents consistent by learning cross-fluent constraints , potentially spanning entities as well .", "Unlike previous pipelined approaches , our model does not assume independence between each fluent or even between named entities with known connections ( parent , spouse , employer , etc . ) ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Because the real world evolves over time , numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future .", "Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification .", "We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues .", "This study aims at resolving the intricacy in consistently compiling relations extracted from text , and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning ."]}
{"orig_sents": ["4", "0", "1", "2", "3"], "shuf_sents": ["However , NLP applications would gain from the ability to detect and type all entities mentioned in text , including the long tail of entities not prominent enough to have their own Wikipedia articles .", "In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked , this can further enable the detection and fine-grained typing of the unlinkable entities .", "Our proposed method for detecting unlinkable entities achieves 24 % greater accuracy than a Named Entity Recognition baseline , and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities .", "Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering .", "Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis .", "Our framework comprises a discourse segmenter , based on a binary classifier , and a discourse parser , which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field .", "We show on two corpora that our approach outperforms the state-of-the-art , often by a wide margin ."]}
{"orig_sents": ["4", "2", "1", "0", "3"], "shuf_sents": ["We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition , which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences .", "We propose a novel method for collecting paraphrases relying on the sequential event order in the discourse , using multiple sequence alignment with a semantic similarity measure .", "discourse structure as a useful information source .", "Our system beats an informed baseline by a margin of 50 % .", "Previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents ?"]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We propose a technique to generate nonprojective word orders in an efficient statistical linearization system .", "Our approach predicts liftings of edges in an unordered syntactic tree by means of a classifier , and uses a projective algorithm for tree linearization .", "We obtain statistically significant improvements on six typologically different languages : English , German , Dutch , Danish , Hungarian , and Czech ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80 % many-to-one accuracy on a 45-tag 1M word corpus .", "Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words .", "We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy .", "We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition ."]}
{"orig_sents": ["3", "2", "0", "4", "1"], "shuf_sents": ["We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models .", "Our experiments reveal that LDA and LSA each have different strengths ; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus .", "Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics .", "We apply two new automated semantic evaluations to three distinct latent topic models .", "We further compare the automated measures to other metrics for topic models , comparison to manually crafted semantic tests and document classification ."]}
{"orig_sents": ["0", "5", "4", "1", "2", "3"], "shuf_sents": ["Phrase-based machine translation models have shown to yield better translations than Word-based models , since phrase pairs encode the contextual information that is needed for a more accurate translation .", "This model is then applied to phrase table pruning .", "Tests show that considerable amounts of phrase pairs can be excluded , without much impact on the translation quality .", "In fact , we show that better translations can be obtained using our pruned models , due to the compression of the search space during decoding .", "In this work , we propose a relative entropy model for translation models , that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities .", "However , many phrase pairs do not encode any relevant context , which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other , and can be found on smaller phrase pairs , with little or no loss in translation accuracy ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["When trained on very large parallel corpora , the phrase table component of a machine translation system grows to consume vast computational resources .", "Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods .", "In this paper , we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation ."]}
{"orig_sents": ["3", "1", "0", "4", "2"], "shuf_sents": ["We also propose a novel pushdown automaton extension of the pFSM model for modeling word swapping and cross alignments that can not be captured by standard edit distance models .", "We first introduce a new regression model that uses a probabilistic finite state machine ( pFSM ) to compute weighted edit distance as predictions of translation quality .", "Our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations ( NIST OpenMT06,08 ; WMT0608 ) .", "Accurate and robust metrics for automatic evaluation are key to the development of statistical machine translation ( MT ) systems .", "Our models can easily incorporate a rich set of linguistic features , and automatically learn their weights , eliminating the need for ad-hoc parameter tuning ."]}
{"orig_sents": ["2", "4", "0", "5", "6", "7", "1", "3"], "shuf_sents": ["Is it true that for each task there is a gain which roughly implies significance ?", "notion of significance hold up in practical settings where future distributions are neither independent nor identically distributed , such as across domains ?", "We investigate two aspects of the empirical behavior of paired significance tests for NLP systems .", "We explore this question using a range of test set variations for constituency parsing .", "First , when one system appears to outperform another , how does significance level relate in practice to the magnitude of the gain , to the size of the test set , to the similarity of the systems , and so on ?", "We explore these issues across a range of NLP tasks using both large collections of past systems ?", "outputs and variants of single systems .", "Next , once significance levels are computed , how well does the standard i.i.d ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["To resolve these problems , this paper proposes two novel inference mechanisms to explore special characteristics in Chinese via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions .", "Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline .", "Current Chinese event extraction systems suffer much from two problems in trigger identification : unknown triggers and word segmentation errors to known triggers ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text , or only the existing knowledge base .", "We describe a distributed , Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base .", "Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction , but they do not exploit background knowledge of other relations in the knowledge base .", "We study how to extend a large knowledge base ( Freebase ) by reading relational information from a large Web text corpus ."]}
{"orig_sents": ["0", "1", "2", "3", "10", "9", "7", "8", "6", "4", "5"], "shuf_sents": ["Discovering significant types of relations from the web is challenging because of its open nature .", "Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance , but most of them rely on tagging arguments of predefined types .", "Recently , a new algorithm was proposed to jointly extract relations and their argument semantic classes , taking a set of relation instances extracted by an open IE algorithm as input .", "However , it can not handle polysemy of relation phrases and fails to group many similar ( ? synonymous ? )", "It is also very efficient .", "Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web .", "While maintaining approximately the same precision , the algorithm achieves significant improvement on recall compared to the previous method .", "The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction .", "Moreover , it explicitly disambiguates polysemous relation phrases and groups synonymous ones .", "In this paper , we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems .", "relation instances because of the sparseness of features ."]}
{"orig_sents": ["0", "3", "1", "2", "4", "5"], "shuf_sents": ["We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods .", "This leads to better parameter optimization .", "Another chief advantage of the framework is that arbitrary learning to rank methods can be applied .", "For the training of the reranker , we extract competing local subtrees , hence the training instances ( candidate subtree sets ) are very similar to those used during beamsearch parsing .", "We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker .", "The subtree ranking approach with a Maximum Entropy model significantly outperformed the other approaches ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together .", "We use this analysis to answer a range of questions about parser behaviour , including what linguistic constructions are difficult for stateof-the-art parsers , what types of errors are being resolved by rerankers , and what types are introduced when parsing out-of-domain text .", "Constituency parser performance is primarily interpreted through a single metric , F-score on WSJ section 23 , that conveys no linguistic information regarding the remaining errors ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level .", "As a linguistic means to achieve text coherence , lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning .", "Various ways to apply this feature to evaluate machinetranslated documents are presented , including one without reliance on reference translation .", "A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine .", "Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements ."]}
{"orig_sents": ["6", "1", "5", "4", "3", "2", "0"], "shuf_sents": ["We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications .", "We propose a system called FLAG to construct such graphs approximately from large data sets .", "We show our system ? s efficiency in both intrinsic and extrinsic experiments .", "These algorithms return the approximate nearest neighbors quickly .", "To find the approximate nearest neighbors , our algorithm pairs a new distributed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms ( variants of PLEB ) .", "To handle the large amount of data , our algorithm maintains approximate counts based on sketching algorithms .", "Many natural language processing problems involve constructing large nearest-neighbor graphs ."]}
{"orig_sents": ["1", "0", "3", "5", "6", "2", "4"], "shuf_sents": ["If a computer can reliably extract information from them , it will greatly benefit a variety of applications .", "Short listings such as classified ads or product listings abound on the web .", "Two key features in the system are a semantic parser that extracts objects and their attributes and a listing-focused clustering module that helps group together extracted tokens of same type .", "Short listings are , however , challenging to process due to their informal styles .", "Our evaluation shows that the semantic model learned by these two modules is effective across multiple domains .", "In this paper , we present an unsupervised information extraction system for short listings .", "Given a corpus of listings , the system builds a semantic model that represents typical objects and their attributes in the domain of the corpus , and then uses the model to extract information ."]}
{"orig_sents": ["2", "6", "4", "0", "5", "3", "1"], "shuf_sents": ["We describe 10 sketch methods , including existing and novel variants .", "Our experiments show that one sketch performs best for all the three tasks .", "Many NLP tasks rely on accurate statistics from large corpora .", "We evaluate several sketches on three important NLP problems .", "of frequency distributions .", "We compare and study the errors ( over-estimation and underestimation ) made by the sketches .", "Tracking complete statistics is memory intensive , so recent work has proposed using compact approximate ? sketches ?"]}
{"orig_sents": ["0", "2", "4", "1", "3"], "shuf_sents": ["Conditional random fields and other graphical models have achieved state of the art results in a variety of tasks such as coreference , relation extraction , data integration , and parsing .", "We demonstrate that our method converges more quickly than a traditional MCMC sampler for both marginal and MAP inference .", "Increasingly , practitioners are using models with more complex structure ? higher treewidth , larger fan-out , more features , and more data ? rendering even approximate inference methods such as MCMC inefficient .", "In an author coreference task with over 5 million mentions , we achieve a 13 times speedup over regular MCMC inference .", "In this paper we propose an alternative MCMC sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors ."]}
{"orig_sents": ["1", "0", "2", "5", "3", "4"], "shuf_sents": ["Typically , in structured prediction , an inference procedure is applied to each example independently of the others .", "This paper deals with the problem of predicting structures in the context of NLP .", "In this paper , we seek to optimize the time complexity of inference over entire datasets , rather than individual examples .", "We also identify several approximation schemes which can provide further speedup .", "We instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance .", "By considering the general inference representation provided by integer linear programs , we propose three exact inference theorems which allow us to re-use earlier solutions for certain instances , thereby completely avoiding possibly expensive calls to the inference procedure ."]}
{"orig_sents": ["4", "1", "5", "2", "3", "0"], "shuf_sents": ["Results show that the same approach can be used for exact optimization and sampling , while explicitly constructing only a fraction of the total implicit state-space .", "Motivated by adaptive rejection sampling and heuristic search , we propose a strategy based on sequentially refining a lower-order language model that is an upper bound on the true model we wish to decode and sample from .", "The ARPA format for language models is extended to enable an efficient use of the max-backoff quantities required to compute the upper bound .", "We evaluate our approach on two problems : a SMS-retrieval task and a POS tagging experiment using 5-gram models .", "We present a method for exact optimization and sampling from high order Hidden Markov Models ( HMMs ) , which are generally handled by approximation techniques .", "This allows us to build tractable variable-order HMMs ."]}
{"orig_sents": ["7", "5", "3", "2", "0", "1", "6", "4"], "shuf_sents": ["The PATTY taxonomy comprises 350,569 pattern synsets .", "Random-sampling-based evaluation shows a pattern accuracy of 84.7 % .", "It harnesses the rich type system and entity population of large knowledge bases .", "The PATTY system is based on efficient algorithms for frequent itemset mining and can process Web-scale corpora .", "The PATTY resource is freely available for interactive access and download .", "The patterns are semantically typed and organized into a subsumption taxonomy .", "PATTY has 8,162 subsumptions , with a random-sampling-based precision of 75 % .", "This paper presents PATTY : a large resource for textual patterns that denote binary relations between entities ."]}
{"orig_sents": ["1", "2", "0", "5", "3", "8", "6", "7", "4"], "shuf_sents": ["Our method works with linguisticallymotivated annotations , induced latent structure , lexicalization , or any mix of the three .", "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar .", "We present an approach where multiple annotations coexist , but in a factored manner that avoids this combinatorial explosion .", "First , by partitioning the factors , it speeds up parsing exponentially over the unfactored approach .", "Combining latent , lexicalized , and unlexicalized annotations , our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank .", "We use a structured expectation propagation algorithm that makes use of the factored structure in two ways .", "Using purely latent variable annotations , we can efficiently train and parse with up to 8 latent bits per symbol , achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na ?", "? ve approach .", "Second , it minimizes the redundancy of the factors during training , improving accuracy over an independent approach ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90 % .", "Our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns .", "We introduce a model of coherence which captures the intentional discourse structure in text .", "We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract , introduction and related work sections of these articles ."]}
{"orig_sents": ["8", "3", "2", "10", "9", "5", "7", "6", "1", "0", "4"], "shuf_sents": ["The compressed language model uses 26 % less RAM while equivalent search quality takes 27 % more CPU .", "In a GermanEnglish Moses system with target-side syntax , improved estimates yielded a 63 % reduction in CPU time ; for a Hiero-style version , the reduction is 21 % .", "Common practice uses lowerorder entries in an N -gram model to score the first few words of a fragment ; this violates assumptions made by common smoothing strategies , including Kneser-Ney .", "We contribute two changes that trade between accuracy of these estimates and memory , holding sentence-level scores constant .", "Source code is released as part of KenLM .", "Conversely , we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores , at the expense of less accurate estimates for sentence fragments .", "In order to interpret changes in search accuracy , we adjust the pop limit so that accuracy is unchanged and report the change in CPU time .", "These changes can be stacked , achieving better estimates with unchanged memory usage .", "Approximate search algorithms , such as cube pruning in syntactic machine translation , rely on the language model to estimate probabilities of sentence fragments .", "This improves search at the expense of memory .", "Instead , we use a unigram model to score the first word , a bigram for the second , etc ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Left-to-right decoding , which generates the target string in order , can improve decoding efficiency by simplifying the language model evaluation .", "Our method outperforms previously published tree-to-string decoders , including a competing left-to-right method .", "This paper presents a novel left to right decoding algorithm for tree-to-string translation , using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation .", "Decoding algorithms for syntax based machine translation suffer from high computational complexity , a consequence of intersecting a language model with a context free grammar ."]}
{"orig_sents": ["2", "3", "5", "1", "0", "4"], "shuf_sents": ["This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language .", "Our model assigns a vector and a matrix to every node in a parse tree : the vector captures the inherent meaning of the constituent , while the matrix captures how it changes the meaning of neighboring words or phrases .", "Single-word vector space models have been very successful at learning lexical information .", "However , they can not capture the compositional meaning of longer phrases , preventing them from a deeper understanding of language .", "The model obtains state of the art performance on three different experiments : predicting fine-grained sentiment distributions of adverb-adjective pairs ; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them .", "We introduce a recursive neural network ( RNN ) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length ."]}
{"orig_sents": ["5", "6", "7", "10", "1", "3", "4", "2", "0", "9", "8"], "shuf_sents": ["We evaluate this procedure with the Graduate Record Examination questions of ( Mohammed et al 2008 ) and find that the method improves on the results of that study .", "a word sense along with its synonyms and antonyms ?", "The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates , so as to induce a subspace with the desired property .", "is treated as a ? document , ?", "and the resulting document collection is subjected to LSA .", "Existing vector space models typically map synonyms and antonyms to similar word vectors , and thus fail to represent antonymy .", "We introduce a new vector space representation where antonyms lie on opposite sides of a sphere : in the word vector space , synonyms have cosine similarities close to one , while antonyms are close to minus one .", "We derive this representation with the aid of a thesaurus and latent semantic analysis ( LSA ) .", "Altogether , we improve on the best previous results by 11 points absolute in F measure .", "Further improvements result from refining the subspace representation with discriminative training , and augmenting the training data with general newspaper text .", "Each entry in the thesaurus ?"]}
{"orig_sents": ["1", "3", "2", "4", "0"], "shuf_sents": ["In addition to opening up a new empirical domain for research on distributional semantics , our observations concerning the attested vectors for the different types of adjectives , the nouns they modify , and the resulting noun phrases yield insights into modification that have been little evident in the formal semantics literature to date .", "Adjectival modification , particularly by expressions that have been treated as higherorder modifiers in the formal semantics tradition , raises interesting challenges for semantic composition in distributional semantic models .", "intersectively used color terms ( as in white towel , clearly first-order ) , subsectively used color terms ( white wine , which have been modeled as both first- and higher-order ) , and intensional adjectives ( former bassist , clearly higher-order ) ?", "We contrast three types of adjectival modifiers ?", "and test the ability of different composition strategies to model their behavior ."]}
{"orig_sents": ["5", "2", "0", "4", "1", "3"], "shuf_sents": ["expressions in free text to Wikipedia .", "To maximize the utility of the injected knowledge , we deploy a learningbased multi-sieve approach and develop novel entity-based features .", "To inject knowledge , we use a state-of-the-art system which cross-links ( or ? grounds ? )", "Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset .", "We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system .", "We explore the interplay of knowledge and structure in co-reference resolution ."]}
{"orig_sents": ["5", "3", "0", "4", "2", "1"], "shuf_sents": ["We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic .", "Compared with the best system from CoNLL2011 , which employs a rule-based method , our system shows competitive performance .", "Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset .", "Traditional merging methods adopt different strategies such as the bestfirst method and enforcing the transitivity constraint , but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end .", "Experimental results show that our joint learning system outperforms independent learning systems .", "Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features .", "We annotate and resolve a particular case of abstract anaphora , namely , thisissue anaphora .", "Our results show that ( a ) the model outperforms the strong adjacent-sentence baseline ; ( b ) general abstract-anaphora features , as distinguished from issue-specific features , play a crucial role in this-issue anaphora resolution , suggesting that our approach can be generalized for other NPs such as this problem and this debate ; and ( c ) it is possible to reduce the search space in order to improve performance .", "The model is not restricted to nominal or verbal antecedents ; rather , it is able to identify antecedents that are arbitrary spans of text ."]}
{"orig_sents": ["1", "6", "4", "5", "3", "7", "0", "2"], "shuf_sents": ["We explore strategies to learn the translation probabilities between words and the concepts using the Q & A archives and a popular entity catalog .", "Bridging the lexical gap between the user ? s question and the question-answer pairs in the Q & A archives has been a major challenge for Q & A retrieval .", "Experiments conducted on a large scale real data show that the proposed techniques are promising .", "This results in degraded retrieval performance .", "While useful , the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues .", "Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general .", "State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models .", "In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts ( entities ) ."]}
{"orig_sents": ["4", "2", "5", "1", "3", "0"], "shuf_sents": ["A comparison to stateof-the-art systems and a user study jointly demonstrate that our techniques are highly effective .", "The supervised approach directly learns semantic distances from users to propose meaningful task-specific taxonomies .", "However , given an arbitrary collection , pre-constructed taxonomies could not easily adapt to the specific topic/task present in the collection .", "The approach aims to produce globally optimized taxonomy structures by incorporating path consistency control and usergenerated task specification into the general learning framework .", "Taxonomies can serve as browsing tools for document collections .", "This paper explores techniques to quickly derive task-specific taxonomies supporting browsing in arbitrary document collections ."]}
{"orig_sents": ["1", "0", "2", "5", "3", "6", "4"], "shuf_sents": ["We introduce a setting where humans engage in classification with incrementally revealed features : the collegiate trivia circuit .", "Cost-sensitive classification , where the features used in machine learning tasks have a cost , has been explored as a means of balancing knowledge against the expense of incrementally obtaining new features .", "By providing the community with a web-based system to practice , we collected tens of thousands of implicit word-by-word ratings of how useful features are for eliciting correct answers .", "classification process , we improve the performance of a state-of-the art classifier .", "Our system learns when to answer a question , performing better than baselines and most human players .", "Observing humans ?", "We also use the dataset to evaluate a system to compete in the incremental classification task through a reduction of reinforcement learning to classification ."]}
{"orig_sents": ["3", "5", "1", "4", "0", "2"], "shuf_sents": ["When multi-domain learning is applied to these settings , ( 2 ) are multidomain methods improving because they capture domain-specific class biases ?", "( 1 ) Are multi-domain learning improvements the result of ensemble learning effects ?", "An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning , and it suggests some important open questions for improving beyond the current state of the art .", "We present a systematic analysis of existing multi-domain learning approaches with respect to two questions .", "Second , these algorithms are traditionally evaluated in a balanced class label setting , although in practice many multidomain settings have domain-specific class label biases .", "First , many multidomain learning algorithms resemble ensemble learning algorithms ."]}
{"orig_sents": ["1", "4", "0", "2", "3"], "shuf_sents": ["We argue that because the task is computationally intractable in general , it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features .", "Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains .", "Leveraging the Posterior Regularization framework , we develop an architecture for incorporating biases into representation learning .", "We investigate three types of biases , and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners , resulting in a relative reduction in error of more than 16 % for both tasks , with respect to existing state-of-the-art representation learning techniques .", "We present a novel , formal statement of the representation learning task ."]}
{"orig_sents": ["0", "4", "2", "5", "1", "3"], "shuf_sents": ["We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars .", "The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM .", "We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences .", "In our experiments of unsupervised dependency grammar learning , we show that unambiguity regularization is beneficial to learning , and in combination with annealing ( of the regularization strength ) and sparsity priors it leads to improvement over the current state of the art .", "The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid .", "The resulting family of algorithms includes the expectation-maximization algorithm ( EM ) and its variant , Viterbi EM , as well as a so-called softmax-EM algorithm ."]}
{"orig_sents": ["1", "0", "2", "3", "4", "5"], "shuf_sents": ["CRFs , however , do not readily model potentially useful segment-level information like syntactic constituent structure .", "Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields ( CRFs ) .", "Thus , we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level .", "We extend the original semi-CRF model ( Sarawagi and Cohen , 2004 ) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries .", "We evaluate performance on two opinion extraction tasks , and , in contrast to previous sequence labeling approaches to the task , explore the usefulness of segmentlevel syntactic parse features .", "Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks ."]}
{"orig_sents": ["1", "2", "3", "4", "6", "5", "0"], "shuf_sents": ["The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods .", "This paper proposes a novel approach to extract opinion targets based on wordbased translation model ( WTM ) .", "At first , we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words .", "Then , a graphbased algorithm is exploited to extract opinion targets , where candidate opinion relevance estimated from the mined associations , is incorporated with candidate importance to generate a global measure .", "By using WTM , our method can capture opinion relations more precisely , especially for long-span relations .", "By using graph-based algorithm , opinion targets are extracted in a global process , which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods , such as Double Propagation .", "In particular , compared with previous syntax-based methods , our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora ."]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["We also explore interactions in language use between menu prices and sentiment as expressed in user reviews .", "Our approach is to build predictive models of concrete external variables , such as restaurant menu prices .", "By focusing on prediction tasks and doing our analysis at scale , our methodology allows quantitative , objective measurements of the words and phrases used to describe food in restaurants .", "We investigate the use of language in food writing , specifically on restaurant menus and in customer reviews .", "We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities ."]}
{"orig_sents": ["6", "5", "0", "4", "3", "1", "2"], "shuf_sents": ["Until now , however , such conversion schemes have been created manually .", "Given the exponential size of the mapping space , we propose a novel method for optimizing over soft mappings , and use entropy regularization to drive those towards hard mappings .", "Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing.1", "We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping .", "Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages .", "This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers .", "We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags ."]}
{"orig_sents": ["6", "1", "3", "4", "2", "5", "0"], "shuf_sents": ["Meanwhile , our method also boosts the performance of POS tagging for pure Chinese texts .", "Therefore , it becomes an important and challenging topic to analyze Chinese-English mixed texts .", "In this paper , we present a method using dynamic features to tag POS of mixed texts .", "The underlying problem is how to tag part-of-speech ( POS ) for the English words involved .", "Due to the lack of specially annotated corpus , most of the English words are tagged as the oversimplified type , ? foreign words ? .", "Experiments show that our method achieves higher performance than traditional sequence labeling methods .", "In modern Chinese articles or conversations , it is very popular to involve a few English words , especially in emails and Internet literature ."]}
{"orig_sents": ["4", "5", "1", "0", "3", "2"], "shuf_sents": ["In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource , the Wiktionary .", "However , parallel text is not always available and techniques for using it require multiple complex algorithmic steps .", "We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank .", "Across eight languages for which we have labeled data to evaluate results , we achieve accuracy that significantly exceeds best unsupervised and parallel text methods .", "Despite significant recent work , purely unsupervised techniques for part-of-speech ( POS ) tagging have not achieved useful accuracies required by many language processing tasks .", "Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The results show that , thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods , we are able to achieve the state of the art in both monolingual and multilingual WSD settings .", "We present a multilingual joint approach to Word Sense Disambiguation ( WSD ) .", "Our method exploits BabelNet , a very large multilingual knowledge base , to perform graphbased WSD across different languages , and brings together empirical evidence from these languages using ensemble methods ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique .", "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation ( WSD ) .", "The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD .", "Our experiments , on new and gold-standard datasets , show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level ."]}
{"orig_sents": ["0", "2", "1", "3", "4"], "shuf_sents": ["A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment , despite ample evidence suggesting that language is grounded in perception and action .", "Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms ( i.e. , attributes native speakers consider important in describing the meaning of a word ) .", "In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data .", "The models differ in terms of the mechanisms by which they integrate the two modalities .", "Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two ."]}
{"orig_sents": ["1", "3", "2", "5", "0", "4"], "shuf_sents": ["To deal with the resulting global objective , we present an efficient and exact dual decomposition decoding algorithm .", "State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data .", "In this paper , we aim to compensate for the lack of available training data by exploiting similarities between test set sentences .", "When training data is out-of-domain or limited , accuracy degrades .", "In experiments , we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages .", "We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints ."]}
{"orig_sents": ["4", "3", "5", "1", "0", "6", "2"], "shuf_sents": ["By removing the intermediate word segmentation , the unified parser no longer needs separate notions for words and phrases .", "We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm .", "1", "We show how this assumption can fail badly , leading to many out-of-vocabulary words and incompatible annotations .", "Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation , thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese .", "Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable .", "Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words , phrases and sentences simultaneously ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees .", "Experimental evaluation on Chinese , Czech , English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system , which lead to improved state-of-theart results for all languages .", "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins ."]}
{"orig_sents": ["2", "4", "5", "3", "6", "1", "0"], "shuf_sents": ["The experiments on a large Twitter dataset shows our methods are very effective .", "To model smoothness of one state sequence , we propose a novel function which can capture the state context .", "Activities on social media increase at a dramatic rate .", "In this paper , we propose to identify event-related bursts via social media activities .", "When an external event happens , there is a surge in the degree of activities related to the event .", "These activities may be temporally correlated with one another , but they may also capture different aspects of an event and therefore exhibit different bursty patterns .", "We study how to correlate multiple types of activities to derive a global bursty pattern ."]}
{"orig_sents": ["4", "0", "1", "2", "3"], "shuf_sents": ["We propagate gender information through the videos and show that a user ? s gender can be predicted from her social environment with the accuracy above 90 % .", "We also show that the gender can be predicted from language alone ( 89 % ) .", "A surprising result of our study is that the latter predictions correlate more strongly with the gender predominant in the user ? s environment than with the sex of the person as reported in the profile .", "We also investigate how the two views ( linguistic and social ) can be combined and analyse how prediction accuracy changes over different age groups .", "We consider the task of predicting the gender of the YouTube1 users and contrast two information sources : the comments they leave and the social environment induced from the affiliation graph of users and videos ."]}
{"orig_sents": ["0", "1", "5", "3", "7", "6", "2", "4"], "shuf_sents": ["The question ? how predictable is English ? ?", "has long fascinated researchers .", "Our experimental results on a large-scale dataset show that both components help reduce typing effort .", "We are motivated by a novel application scenario : given the difficulty of typing on mobile devices , can we help reduce typing effort with message completion , especially in conversational settings ?", "We also perform an information-theoretic study in this setting and examine the entropy of user-generated content , especially in conversational scenarios , to better understand predictability of user generated English .", "While prior work has focused on formal English typically used in news articles , we turn to texts generated by users in online settings that are more informal in nature .", "Our approach models both the language used in responses and the specific context provided by the original message .", "We propose a method for automatic response completion ."]}
{"orig_sents": ["4", "8", "3", "7", "5", "6", "2", "1", "0"], "shuf_sents": ["The two grid constructions can also be combined to produce consistently strong results across all training sets .", "The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus .", "We evaluate these strategies on existing Wikipedia and Twitter corpora , as well as a new , larger Twitter corpus .", "Given training documents labeled with latitude/longitude coordinates , a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell ; then a location for a test document is chosen based on the most similar pseudo-document .", "The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text , often in the context of social media and online content .", "We define an alternative grid construction using k-d trees that more robustly adapts to data , especially with larger training sets .", "We also provide a better way of choosing the locations for pseudo-documents .", "Uniform grids are normally used , but they are sensitive to the dispersion of documents over the earth .", "One common approach for geolocating texts is rooted in information retrieval ."]}
{"orig_sents": ["4", "3", "1", "7", "5", "2", "6", "0"], "shuf_sents": ["It also attains a higher recall compared with the noisy channel model , and can therefore serve as a better filtering stage when combined with a ranker .", "The ranker , however , suffers from the fact the low recall of the first , suboptimal , search stage .", "The latent structural information is used to model the alignment of words in the spelling correction process .", "Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections , followed by discriminatively trained ranker applied to these candidates .", "Discriminative training in query spelling correction is difficult due to the complex internal structures of the data .", "In this model , we treat query spelling correction as a multiclass classification problem with structured input and output .", "Experiment results show that as a standalone speller , our model outperforms all the baseline systems .", "This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM ."]}
{"orig_sents": ["2", "4", "1", "0", "3"], "shuf_sents": ["In this paper , we present a comprehensive exploration of syntactic elements in writing styles , with particular emphasis on interpretable characterization of stylistic elements .", "Some very recent work has shown that PCFG models can detect distributional difference in syntactic styles , but without offering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship .", "Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements .", "We present analytic insights with respect to the authorship attribution task in two different domains .", "However , most previous research for computational stylometric analysis has relied on shallow lexico-syntactic patterns ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["Based on this intuition , we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph .", "The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small .", "Instead of using feature-based methods as conventional models , our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events , which are very effective for dating documents .", "Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps , the task of automatic dating of documents has been increasingly important ."]}
{"orig_sents": ["4", "1", "5", "0", "2", "3", "6"], "shuf_sents": ["We propose the use of several discourse analysis frameworks , including 1 ) Rhetorical Structure Theory ( RST ) , 2 ) PDTB-styled discourse relations , and 3 ) topical text segmentation .", "This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences .", "We explain how features derived from these frameworks can be effectively used with support vector machines ( SVM ) paired with convolution kernels .", "Experiments show that our proposal is effective in improving on the state-of-the-art significantly by as much as 16 % in terms of F1 , even if we only adopt less-than-perfect automatic discourse analyzers and parsers .", "In this paper we classify the temporal relations between pairs of events on an article-wide basis .", "To achieve this , we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features .", "Making use of more accurate discourse analysis can further boost gains to 35 % ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["The combination significantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting .", "In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data .", "Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text .", "A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting ."]}
{"orig_sents": ["1", "0", "4", "2", "3", "5"], "shuf_sents": ["Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning .", "Children learn various levels of linguistic structure concurrently , yet most existing models of language acquisition deal with only a single level of structure , implicitly assuming a sequential learning process .", "We test on child-directed utterances in English and Spanish and compare to single-task baselines .", "In the morphologically poorer language ( English ) , the model improves morphological segmentation , while in the morphologically richer language ( Spanish ) , it leads to better syntactic categorization .", "Here , we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks .", "These results provide further evidence that joint learning is useful , but also suggest that the benefits may be different for typologically different languages ."]}
{"orig_sents": ["0", "1", "5", "3", "4", "2"], "shuf_sents": ["We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation .", "We define the model as a Bayesian noisy channel ; we sample segmentations and word forms simultaneously from the posterior , using beam sampling to control the size of the search space .", "We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors , and relate these to developmental evidence .", "On data with variable pronunciations , the pipelined approach learns to treat syllables or morphemes as words .", "In contrast , our joint model , like infant learners , tends to learn multiword collocations .", "Compared to a pipelined approach in which segmentation is performed first , our model is qualitatively more similar to human learners ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks .", "We show how this framework can give us direct insight into the behavior of the system , allowing us to more easily diagnose sources of error .", "We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme ."]}
{"orig_sents": ["2", "3", "1", "4", "0"], "shuf_sents": ["We use the output of UNLOL to automatically normalize a large corpus of social media text , revealing a set of coherent orthographic styles that underlie online language variation .", "The weights of these features are trained in a maximumlikelihood framework , employing a novel sequential Monte Carlo training algorithm to overcome the large label space , which would be impractical for traditional dynamic programming solutions .", "We present a unified unsupervised statistical model for text normalization .", "The relationship between standard and non-standard tokens is characterized by a log-linear model , permitting arbitrary features .", "This model is implemented in a normalization system called UNLOL , which achieves the best known results on two normalization datasets , outperforming more complex systems ."]}
{"orig_sents": ["6", "4", "5", "1", "3", "0", "2"], "shuf_sents": ["We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages .", "replacing orthographically or lexically idiosyncratic forms with more standard variants ?", "To validate the utility of our approach , we evaluate extrinsically , showing that normalizing English tweets and then translating improves translation quality ( compared to translating unnormalized text ) using three standard web translation services as well as a phrase-based translation system trained on parallel microblog data .", "can improve performance .", "When confronted with such input , conventional text analysis tools often perform poorly .", "Normalization ?", "Compared to the edited genres that have played a central role in NLP research , microblog texts use a more informal register with nonstandard lexical items , abbreviations , and free orthographic variation ."]}
{"orig_sents": ["0", "4", "2", "1", "3"], "shuf_sents": ["In this paper , we address the problem of estimating question difficulty in community question answering services .", "Most importantly , our analysis shows that the text of question descriptions reflects the question difficulty .", "Our experimental results show that our model significantly outperforms a PageRank-based approach .", "This implies the possibility of predicting question difficulty from the text of question descriptions .", "We propose a competition-based model for estimating question difficulty by leveraging pairwise comparisons between questions and users ."]}
{"orig_sents": ["5", "0", "2", "1", "4", "3"], "shuf_sents": ["ideological positioning from their speeches .", "We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags ( filler distinguished only by its length in words ) .", "To accomplish this , we infer ideological cues from a corpus of political writings annotated with known ideologies .", "The results are validated against a set of preregistered , domain expertauthored hypotheses .", "We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign .", "We seek to measure political candidates ?"]}
{"orig_sents": ["2", "3", "5", "6", "4", "0", "1"], "shuf_sents": ["We tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module , which is also acquired via unsupervised learning and report improved quality of the generated responses .", "Finally , we report results with Maghrebi French hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages .", "We present a novel model , Freestyle , that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics , by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations .", "In this attack on the woefully under-explored natural language genre of music lyrics , we exploit a strictly unsupervised transduction grammar induction approach .", "To highlight some of the inherent challenges in adapting other algorithms to this novel task , we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based SMT system .", "Our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed , even though the domain of hip hop lyrics is particularly noisy and unstructured .", "We evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction , and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses , measured on fluency and rhyming criteria as judged by human evaluators ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["When reviewing scientific literature , it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles .", "Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach .", "In this context , this paper introduces topical influence , a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it .", "Given the text of the articles and their citation graph , we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["Our model can use arbitrary features for parsing sentences and adapt itself with out-ofdomain data .", "We introduce a novel method to jointly parse and detect disfluencies in spoken utterances .", "We show that our method , based on transition-based parsing , performs at a high level of accuracy for both the parsing and disfluency detection tasks .", "Additionally , our method is the fastest for the joint task , running in linear time ."]}
{"orig_sents": ["2", "5", "0", "4", "1", "3", "6", "7"], "shuf_sents": ["meaning representations while generating the overall semantics .", "We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations .", "We present a novel vector space model for semantic co-compositionality .", "We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality .", "This readily addresses some major challenges with current vector space models , notably the polysemy issue and the use of one representation per word type .", "Inspired by Generative Lexicon Theory ( Pustejovsky , 1995 ) , our goal is a compositional model where both predicate and argument are allowed to modify each others ?", "The model achieves the best result to date ( ?", "= 0.47 ) on the semantic similarity task of transitive verbs ( Grefenstette and Sadrzadeh , 2011 ) ."]}
{"orig_sents": ["2", "4", "1", "0", "3"], "shuf_sents": ["We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives .", "We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction .", "In this study , we use compositional distributional semantic methods to investigate restrictions in adjective ordering .", "Our results offer fresh insight into the semantic properties that determine adjective ordering , building a bridge between syntax and distributional semantics .", "Specifically , we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order ."]}
{"orig_sents": ["0", "2", "1", "5", "3", "4"], "shuf_sents": ["Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain .", "Specifically , we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains , and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation .", "In this paper , we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features , which are generalizable across domains while informative to the prediction task .", "We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset .", "The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods .", "We train this latent graphical model using a simple expectation-maximization ( EM ) algorithm ."]}
{"orig_sents": ["2", "4", "3", "0", "1"], "shuf_sents": ["By fixing this , we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well .", "In fact , the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also .", "Two recent measures incorporate the notion of statistical significance in basic PMI formulation .", "Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable , they have been applied slightly inappropriately .", "In some tasks , we find that the new measures perform worse than the PMI ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models .", "Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions .", "In this paper we present a minimallysupervised approach to the multi-domain acquisition of wide-coverage glossaries ."]}
{"orig_sents": ["2", "4", "0", "1", "3", "5", "6"], "shuf_sents": ["requiring no language-specific knowledge ?", "to the conventional manual approach for creating pronunciation dictionaries .", "The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer ( ASR ) .", "We present a hierarchical Bayesian model , which jointly discovers the phonetic inventory and the Letter-to-Sound ( L2S ) mapping rules in a language using only transcribed data .", "In this paper , we propose an unsupervised alternative ?", "When tested on a corpus of spontaneous queries , the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart , in which the latent phonetic inventory and L2S mappings are learned separately .", "Furthermore , the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using conventional supervised procedures ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs .", "The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration ( noise ) parts in phrase pairs .", "It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We develop a new decoding algorithm that combines the speed of beam search with the optimal certificate property of Lagrangian relaxation , and apply it to phrase- and syntax-based translation decoding .", "The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation .", "The new method is efficient , utilizes standard MT algorithms , and returns an exact solution on the majority of translation examples in our test data .", "Beam search is a fast and empirically effective method for translation decoding , but it lacks formal guarantees about search error ."]}
{"orig_sents": ["2", "0", "1", "5", "3", "4", "6", "7"], "shuf_sents": ["In this paper , we propose an efficient method for implementing ngram models based on doublearray structures .", "First , we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency .", "Ngram language models tend to increase in size with inflating the corpus size , and consume considerable resources .", "Embedding probabilities into unused spaces in double-array structures reduces the model size .", "Moreover , tuning the word IDs in the language model makes the model smaller and faster .", "Next , we propose two optimization methods for improving the efficiency of data representation in the double-array structures .", "We also show that our method can be used for building large language models using the division method .", "Lastly , we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd , where n is the length of the training corpus and d is the number of observed features .", "We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization .", "We present a model that grows logarithmically in d , making it possible to efficiently leverage longer contexts ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["In our experiments , our approach outperforms previous interactive translation systems , and achieves estimated effort reductions of as much as 48 % relative over a traditional post-edition system .", "Here , we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models , and integrates error-correction all in a unified statistical framework .", "Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output .", "Alternatively , an interactive framework that integrates the human knowledge into the translation process has been presented in previous works ."]}
{"orig_sents": ["1", "4", "5", "2", "3", "0"], "shuf_sents": ["Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method .", "Traditional synchronous grammar induction estimates parameters by maximizing likelihood , which only has a loose relation to translation quality .", "This further facilitates the incorporation of various non-local features that are defined on the target side .", "We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system .", "Alternatively , we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation , which directly optimizes translation quality measured by BLEU .", "In the max-margin estimation of parameters , we only need to calculate Viterbi translations ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Here , we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types .", "Using this tool , we first compare the error distributions across a large set of systems , then analyze common errors across the top ten systems , empirically characterizing the major unsolved challenges of the coreference resolution task .", "Coreference resolution metrics quantify errors but do not analyze them ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["In particular , a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively , and two effective methods ( refining syntactic parser and refining learning example generation ) are employed to exploit zero pronouns for Chinese coreference resolution .", "Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution .", "This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution .", "Coreference resolution plays a critical role in discourse analysis ."]}
{"orig_sents": ["3", "6", "1", "2", "4", "5", "0"], "shuf_sents": ["Experiments show consistent improvements across a number of datasets and experimental conditions , including over 11 % reduction in MUC coreference error and nearly 21 % reduction in F1 NEL error on ACE 2004 newswire data .", "This paper demonstrates that these two tasks are complementary .", "We introduce NECO , a new model for named entity linking and coreference resolution , which solves both problems jointly , reducing the errors made on each .", "Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge .", "NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves .", "Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase , while coreference provides better context modeling by propagating named-entity links within mention clusters .", "Errors in named-entity linking ( NEL ) , on the other hand , are often caused by superficial modeling of entity context ."]}
{"orig_sents": ["4", "3", "1", "0", "2"], "shuf_sents": ["We propose an approach that uses automatically extracted antecedents of CSNs as training data to interpret ASNs .", "We tackle this challenge by exploiting cataphoric shell nouns ( CSNs ) whose construction makes them particularly easy to interpret ( e.g. , the fact that X ) .", "We achieve precisions in the range of 0.35 ( baseline = 0.21 ) to 0.72 ( baseline = 0.44 ) , depending upon the shell noun .", "One obstacle in developing methods for automatically interpreting ASNs is the lack of annotated data .", "Interpreting anaphoric shell nouns ( ASNs ) such as this issue and this fact is essential to understanding virtually any substantial natural language text ."]}
{"orig_sents": ["1", "3", "4", "5", "0", "2", "6"], "shuf_sents": ["We update the representation values with a semi-supervised approach .", "Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation .", "Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961 .", "However , the ability of these models is restricted by the availability of annotated data and the design of features .", "We propose a scalable semi-supervised feature engineering approach .", "In contrast to previous works using pre-defined taskspecific features with fixed values , we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus .", "The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1st-order models .", "We present an approximated conditional random field using coarse-to-fine decoding and early updating .", "Training higher-order conditional random fields is prohibitive for huge tag sets ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies .", "We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models .", "We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages ."]}
{"orig_sents": ["1", "2", "3", "4", "0"], "shuf_sents": ["We obtain 74 % accuracy in identifying triliteral Hebrew roots , while performing morphological segmentation with an F1-score of 78.1 .", "This paper contributes an approach for expressing non-concatenative morphological phenomena , such as stem derivation in Semitic languages , in terms of a mildly context-sensitive grammar formalism .", "This offers a convenient level of modelling abstraction while remaining computationally tractable .", "The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons , including ones with discontiguous strings as elements , from unannotated data .", "Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Our method uses data collected from chat negotiations of the game The Settlers of Catan and exploits the conversation to construct dynamically a partial model of each player ? s preferences .", "We compare our method against four baselines and show that tracking how preferences evolve through the dialogue and reasoning about equilibrium moves are both crucial to success .", "This paper describes a method that predicts which trades players execute during a winlose game .", "This in turn yields equilibrium trading moves via principles from game theory ."]}
{"orig_sents": ["3", "2", "0", "4", "1", "5"], "shuf_sents": ["Our approach is to model likelihood between events by drawing on several of these lines of previous work .", "We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts , and evaluate our results by collecting human judgments of contingency .", "Researchers in NLP have tackled modeling such expectations from a range of perspectives , including treating it as the inference of the CONTINGENT discourse relation , or as a type of common-sense causal reasoning .", "Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events , and the expectations about what is likely to happen next that results from such reasoning .", "We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another .", "Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64 % over a baseline of 50 % , as compared to an average accuracy of 75.15 % without web search ."]}
{"orig_sents": ["0", "2", "5", "3", "4", "6", "1"], "shuf_sents": ["In situated dialogue , humans and agents have mismatched capabilities of perceiving the shared environment .", "This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue .", "Their representations of the shared world are misaligned .", "To address this issue , we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment .", "Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9 % .", "Thus referring expression generation ( REG ) will need to take this discrepancy into consideration .", "However , while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment ( e.g. , 84 % ) , they perform rather poorly when the agent has imperfect perception of the environment ( e.g. , 45 % ) ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The class labels are more numerous and more diverse than those produced by current extraction methods .", "from Web search queries .", "Also extracted are representative sets of instances ( singapore , united kingdom ) for the class labels .", "This paper introduces a method for extracting fine-grained class labels ( ? countries with double taxation agreements with india ? )"]}
{"orig_sents": ["4", "3", "5", "2", "1", "0"], "shuf_sents": ["Evaluation results on the ACE 2007 English Relation Detection and Categorization ( RDC ) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules .", "Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed specifically for the relation extraction task .", "into clusters corresponding to underlying semantic relation types ( e.g. , BornIn , Located ) .", "Our model partitions tuples representing an observed syntactic relationship between two named entities ( e.g. , ? X was born in Y ?", "In this paper we present an unsupervised approach to relational information extraction .", "and ? X is from Y ? )"]}
{"orig_sents": ["3", "8", "1", "0", "6", "4", "7", "2", "5"], "shuf_sents": ["First , we train a local predictor g0 with learning to rank as base learner , to generate initial ranking list of candidates .", "We propose a fast collective disambiguation approach based on stacking .", "Experiments show its effectiveness over various algorithms on several public datasets .", "Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base .", "A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem .", "By learning a rich semantic relatedness measure between entity categories and context document , performance is further improved .", "Second , top k candidates of related instances are searched for constructing expressive global coherence features .", "The proposed method is fast and easy to implement .", "Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes ."]}
{"orig_sents": ["11", "4", "3", "0", "12", "1", "6", "5", "2", "10", "7", "9", "8"], "shuf_sents": ["Unfortunately , Wikipedia features only one-ninth the number of entities as Freebase , and these are a highly biased sample of well-connected , frequently mentioned ? head ?", "To bring hope to ? tail ?", "We present TMI , a bipartite graphical model for joint type-mention inference .", "Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation .", "Entity annotators need to be trained on sample mention snippets .", "The two tasks are synergistic : knowing the types of unfamiliar entities helps disambiguate mentions , and words in mention contexts help assign types to entities .", "entities , we broaden our goal to a second task : assigning types to entities in Freebase but not Wikipedia .", "In experiments involving 780,000 people in Wikipedia , 2.3 million people in Freebase , 700 million Web pages , and over 20 professional editors , TMI shows considerable annotation accuracy improvement ( e.g. , 70 % ) compared to baselines ( e.g. , 46 % ) , especially for ? tail ?", "We also compare with Google ? s recent annotations of the same corpus with Freebase entities , and report considerable improvements within the people domain .", "and emerging entities .", "TMI attempts no schema integration or entity resolution , but exploits the above-mentioned synergy .", "Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase .", "entities ."]}
{"orig_sents": ["1", "5", "2", "3", "0", "4"], "shuf_sents": ["This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets , and sheds some light on the issue .", "A large number of Open Relation Extraction approaches have been proposed recently , covering a wide range of NLP machinery , from ? shallow ?", "( e.g. , semantic role labeling ? SRL ) .", "A natural question then is what is the tradeoff between NLP depth ( and associated computational cost ) versus effectiveness .", "The paper also describes a novel method , EXEMPLAR , which adapts ideas from SRL to less costly NLP machinery , resulting in substantial gains both in efficiency and effectiveness , over binary and n-ary relation extraction tasks .", "( e.g. , part-of-speech tagging ) to ? deep ?"]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["This paper proposes a framework for automatically engineering features for two important tasks of question answering : answer sentence selection and answer extraction .", "The results show that our models greatly improve on the state of the art , e.g. , up to 22 % on F1 ( relative improvement ) for answer extraction , while using no additional resources and no manual feature engineering .", "We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction .", "We represent question and answer sentence pairs with linguistic structures enriched by semantic information , where the latter is produced by automatic classifiers , e.g. , question classifier and Named Entity Recognizer .", "Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees ."]}
{"orig_sents": ["4", "3", "1", "7", "0", "2", "6", "5"], "shuf_sents": ["This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs .", "Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations .", "Wikipedia is used to extract domains in each language , to link domains between languages and to create generic translation dictionaries .", "Their manual construction requires strong expertise in both languages involved and is a costly process .", "Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems .", "The newly introduced method compares favorably to existing methods in all configurations tested .", "The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs : FrenchEnglish and Romanian-English .", "We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia ."]}
{"orig_sents": ["2", "1", "7", "5", "6", "3", "4", "0"], "shuf_sents": ["Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model , our summarization system can yield significant performance gain as compared to the state-of-the-art .", "However , such word-based joint optimization is computationally expensive .", "Joint compression and summarization has been used recently to generate high quality summaries .", "Using this corpus , we train a supervised sentence compression model using a set of word- , syntax- , and documentlevel features .", "During summarization , we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences .", "pipeline approach for compressive summarization , but propose to perform summary guided compression , rather than generic sentence-based compression .", "To create an annotated corpus , the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences .", "In this paper we adopt the ? sentence compression + sentence selection ?"]}
{"orig_sents": ["1", "5", "3", "4", "0", "2"], "shuf_sents": ["As the edges link anchors that may span multiple translation units at decoding time , our AG model effectively encodes global contextual information that is previously absent .", "Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine of translation units .", "We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a largescale Chinese-to-English translation task .", "( AG ) model where we use a graph structure to model global contextual information that is crucial for reordering .", "The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units , which we refer to as anchors .", "We present the ? Anchor Graph ?"]}
{"orig_sents": ["4", "2", "3", "0", "6", "5", "1"], "shuf_sents": ["We present extensive experiments on 22 language pairs , including preordering into English from 7 other languages .", "Many of these gains are also significant in human evaluations .", "Unlike existing preordering models , we train feature-rich discriminative classifiers that directly predict the target-side word order .", "Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .", "We present a simple and novel classifier-based preordering approach .", "For languages from different families the improvements often exceed 2 BLEU .", "We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task ."]}
{"orig_sents": ["3", "0", "4", "1", "2"], "shuf_sents": ["For language pairs with few bilingual data , a possible solution in pivot-based SMT using another language as a `` bridge '' to generate source-target translation .", "To alleviate the problem , we utilize Markov random walks to connect possible translation phrases between source and target language .", "Experimental results on European Parliament data , spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system .", "This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation ( SMT ) .", "However , one of the weaknesses is that some useful sourcetarget translations can not be generated if the corresponding source phrase and target phrase connect to different pivot phrases ."]}
{"orig_sents": ["0", "3", "5", "4", "2", "1"], "shuf_sents": ["This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques .", "Experiments on two Chinese-to-English translation datasets show significant improvements , 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based ( IHMM ) system , and 4.75 and 3.53 points over the best single machine translation systems .", "The solutions from this framework , termed Pareto optimal solutions , are then combined to construct confusion networks .", "In this area , most of techniques usually utilize confusion networks ( CN ) as their central data structure to compact an exponential number of an potential hypotheses , and because better hypothesis alignment may benefit constructing better quality confusion networks , it is natural to add more useful information to improve alignment results .", "In the multi-objective optimization framework , each information source is viewed as an independent objective , and a new goal of improving all objectives can be searched by mature algorithms .", "However , these information may be heterogeneous , so the widely-used Viterbi algorithm for searching the best alignment may not apply here ."]}
{"orig_sents": ["6", "4", "5", "1", "0", "3", "2"], "shuf_sents": ["All three schemes significantly improve results of any single system on four testsets .", "The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features .", "We also provide a detailed experimental and qualitative analysis of the results .", "We find that specification ? a more constrained scheme that almost entirely uses forest-to-string rules , but optionally uses hiero rules for shorter spans ? comes out as the strongest , yielding improvement up to 0.9 ( Ter-Bleu ) /2 points .", "We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder .", "We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models ? specification , generalization , and interchange .", "Machine translation benefits from system combination ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system .", "In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations , which often exacerbate the data sparsity problem and cause other problems , our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models : 1 ) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans ; 2 ) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language .", "The features produced by both models are used as soft constraints to guide the translation process .", "Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["The recursive autoencoders are capable of generating vector space representations for variable-sized phrases , which enable predicting orders to exploit syntactic and semantic information from a neural language modeling ? s perspective .", "Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points .", "Unlike previous work that only uses boundary words , we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively .", "While inversion transduction grammar ( ITG ) is well suited for modeling ordering shifts between languages , how to make applying the two reordering rules ( i.e. , straight and inverted ) dependent on actual blocks being merged remains a challenge ."]}
{"orig_sents": ["5", "1", "4", "3", "7", "0", "6", "2"], "shuf_sents": ["A model trained on the same data achieves state-of-the-art performance on the related task of fluency edit classification .", "Edit category classification assigns categories such as spelling error correction , paraphrase or vandalism to edits in a document .", "Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles .", "In a supervised machine learning experiment , we achieve a micro-averaged F1 score of .62 on a corpus of edits from the English Wikipedia .", "Our features are based on differences between two versions of a document including meta data , textual and language properties and markup .", "In this paper , we analyze a novel set of features for the task of automatic edit category classification .", "We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles .", "In this corpus , each edit has been multi-labeled according to a 21-category taxonomy ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets ( RTE and paraphrase ) , while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment .", "Additional experiments highlight the potential benefit of our alignment model to RTE , paraphrase identification and question answering , where even a naive application of our model ? s alignment score approaches the state of the art .", "We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Experiments on ACE and Ontonotes data show that L3M and its constrained version , CL3M , are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature .", "We show that L3M admits efficient inference and can be augmented with knowledge-based constraints ; we also present a fast stochastic gradient based learning .", "In this paper , we describe the Latent Left Linking model ( L3M ) , a novel , principled , and linguistically motivated latent structured prediction approach to coreference resolution .", "Coreference resolution is a well known clustering task in Natural Language Processing ."]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["In this paper , we show that the classical method of centering , the transformation that shifts the origin of the space to the data centroid , provides an effective way to reduce hubs .", "We show analytically why hubs emerge and why they are suppressed by centering , under a simple probabilistic model of data .", "The performance of nearest neighbor methods is degraded by the presence of hubs , i.e. , objects in the dataset that are similar to many other objects .", "Our experimental results show that ( weighted ) centering is effective for natural language data ; it improves the performance of the k-nearest neighbor classifiers considerably in word sense disambiguation and document classification tasks .", "To further reduce hubs , we also move the origin more aggressively towards hubs , through weighted centering ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We derive a spectral method for unsupervised learning of Weighted Context Free Grammars .", "The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix .", "We frame WCFG induction as finding a Hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs , achieving performance comparable to a human-curated set .", "We address the problem of identifying multiword expressions in a language , focusing on English phrasal verbs .", "Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages ."]}
{"orig_sents": ["0", "2", "4", "3", "1"], "shuf_sents": ["This study explores the feasibility of performing Chinese word segmentation ( CWS ) and POS tagging by deep learning .", "We also describe a perceptron-style algorithm for training the neural networks , as an alternative to maximum-likelihood method , to speed up the training process and make the learning algorithm easier to be implemented .", "We try to avoid task-specific feature engineering , and use deep layers of neural networks to discover relevant features to the tasks .", "Our networks achieved close to state-of-theart performance with minimal computational cost .", "We leverage large-scale unlabeled data to improve internal representation of Chinese characters , and use these improved representations to enhance supervised word segmentation and POS tagging models ."]}
{"orig_sents": ["2", "6", "4", "3", "5", "0", "1"], "shuf_sents": ["tasks and train our model on two heterogeneous corpora simultaneously .", "Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information , and achieves significant improvements over the state-of-the-art methods .", "Chinese word segmentation and part-ofspeech tagging ( S & T ) are fundamental steps for more advanced Chinese language processing tasks .", "We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora , Penn Chinese Treebank ( CTB ) and PKU ? s People ? s Daily ( PPD ) .", "In this paper , we propose a unified model for Chinese S & T with heterogeneous annotation corpora .", "Then we regard the Chinese S & T with heterogeneous corpora as two ? related ?", "Recently , it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S & T ."]}
{"orig_sents": ["6", "1", "7", "3", "10", "8", "2", "9", "5", "4", "0"], "shuf_sents": ["This also makes drift of meaning quite measurable so that FA graphs provide a quantitative measure of the semantic coherence of small groups of words .", "The techniques used in these papers are simple and the main results are found by understanding the structure of cycles in the directed graph ( where words point to definitions ) .", "construction of dictionaries .", "These are responses by subjects to a cue word , which are then summarized by a directed , free association graph .", "The FA graph is tighter than the DD graph , because of the large number of triangles .", "In NLP , semantic groups or clusters are interesting for various applications such as word sense disambiguation .", "Studies of the graph of dictionary definitions ( DD ) ( Picard et al , 2009 ; Levary et al , 2012 ) have revealed strong semantic coherence of local topological structures .", "Based on our earlier work ( Levary et al , 2012 ) , we study a different class of word definitions , namely those of the Free Association ( FA ) dataset ( Nelson et al , 2004 ) .", "This difference can be explained by the very nature of free association as compared to the more ? logical ?", "It thus sheds some ( quantitative ) light on the psychology of free association .", "We find that the structure of this network is quite different from both the Wordnet and the dictionary networks ."]}
{"orig_sents": ["2", "4", "0", "1", "3"], "shuf_sents": ["The clusters can be used as language-independent semantic relations , by mapping clustered expressions in different languages onto the same relation .", "Our approach needs no parallel text for training , but outperforms a baseline that uses machine translation on a cross-lingual question answering task .", "Creating a language-independent meaning representation would benefit many crosslingual NLP tasks .", "We also show how to use the semantics to improve the accuracy of machine translation , by using it in a simple reranker .", "We introduce the first unsupervised approach to this problem , learning clusters of semantically equivalent English and French relations between referring expressions , based on their named-entity arguments in large monolingual corpora ."]}
{"orig_sents": ["3", "4", "1", "2", "0"], "shuf_sents": ["We plan to release this resource to the NLP community .", "In the second stage , we enlarge the first stage classifier ? s training data with new contradiction pairs obtained by combining the output of the first stage ? s classifier and that of an entailment classifier .", "We acquired this way 750,000 typed Japanese contradiction pattern pairs with an estimated precision of 80 % .", "In this paper we propose a two-stage method to acquire contradiction relations between typed lexico-syntactic patterns such as Xdrug prevents Ydisease and Ydisease caused by Xdrug .", "In the first stage , we train an SVM classifier to detect contradiction pattern pairs in a large web archive by exploiting the excitation polarity ( Hashimoto et al , 2012 ) of the patterns ."]}
{"orig_sents": ["3", "4", "2", "5", "1", "6", "0"], "shuf_sents": ["We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition .", "We have developed a sarcasm recognizer to identify this type of sarcasm in tweets .", "or ? enjoy ? , followed by an expression that describes an undesirable activity or state ( e.g. , ? taking exams ?", "A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation .", "For example , many sarcastic tweets include a positive sentiment , such as ? love ?", "or ? being ignored ? ) .", "We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets ."]}
{"orig_sents": ["5", "6", "1", "8", "2", "3", "0", "7", "4"], "shuf_sents": ["To achieve the learning process , we propose a collective factor graph ( CoFG ) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors .", "Therefore , it is always a challenge for people to find desired information from them .", "Here , using social networks is motivated by the intuition that , people with similar academic , business or social connections ( e.g .", "co-major , co-university , and cocorporation ) tend to have similar experience and summaries .", "*", "Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications , such as talent recommendation and contextual advertising .", "However , personal profiles usually lack organization confronted with the large amount of available information .", "Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach .", "In this paper , we address the task of personal profile summarization by leveraging both personal profile textual information and social networks ."]}
{"orig_sents": ["0", "2", "7", "6", "4", "1", "5", "3"], "shuf_sents": ["Recently , much research focuses on event storyline generation , which aims to produce a concise , global and temporal event summary from a collection of articles .", "Combining these local/global aspects with summarization requirements together , we utilize an optimization method to generate the component summaries along the timeline .", "Generally , each event contains multiple sub-events and the storyline should be composed by the component summaries of all the sub-events .", "Evaluation and comparison results indicate the effectiveness of our proposed method .", "To distinguish different types of sub-events , we propose a mixture-event-aspect model which models different sub-events into local and global aspects .", "We develop experimental systems on 6 distinctively different datasets .", "interests but seldom considered in previous work .", "However , different sub-events have different part-whole relationship with the major event , which is important to correspond to users ?"]}
{"orig_sents": ["6", "1", "2", "5", "0", "7", "4", "3"], "shuf_sents": ["In this paper , we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries .", "In various summarization tasks , the summary length is manually defined .", "However , how to find the proper summary length is quite a problem ; and keeping all summaries restricted to the same length is not always a good choice .", "Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination .", "by a Bayesian framework which selects sentences to form a good summary .", "It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information .", "Document summarization is an important task in the area of natural language processing , which aims to extract the most important information from a single document or a cluster of documents .", "Assuming that an original document can be reconstructed from its summary , we describe the ? reconstruction ?"]}
{"orig_sents": ["2", "4", "6", "0", "1", "5", "3"], "shuf_sents": ["Summary creation is also guided by the distribution of CoreSC categories found in the full articles , in order to adequately represent the article content .", "Finally , we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task .", "We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles .", "The questions were answered with a precision of 75 % , where the upper bound for human summaries ( abstracts ) was 95 % .", "Full papers are first automatically annotated using the CoreSC scheme , which captures 11 contentbased concepts such as Hypothesis , Result , Conclusion etc at the sentence level .", "Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66 % of complex content-related questions designed on the basis of paper abstracts .", "A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary , making a distinction between dependent and independent categories ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We prove the correctness of our algorithm rigorously .", "We present the first provably optimal polynomial time dynamic programming ( DP ) algorithm for best-first shift-reduce parsing , which applies the DP idea of Huang and Sagae ( 2010 ) to the best-first parser of Sagae and Lavie ( 2006 ) in a non-trivial way , reducing the complexity of the latter from exponential to polynomial .", "Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser , and makes exact search under such a model tractable for the first time ."]}
{"orig_sents": ["2", "4", "1", "5", "3", "0"], "shuf_sents": ["The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset .", "In this work , we compare representative models : a window-based model , a topic model , a distributional memory and a commonsense knowledge database , ConceptNet , in two visual recognition scenarios : human action recognition and object prediction .", "The problem of learning language models from large text corpora has been widely studied within the computational linguistic community .", "We determine the usefulness of different language models in aiding the two visual recognition tasks .", "However , little is known about the performance of these language models when applied to the computer vision domain .", "We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images ."]}
{"orig_sents": ["2", "1", "4", "0", "6", "5", "3"], "shuf_sents": ["a large , real-world digital library of scientific articles in computational linguistics .", "DefMiner achieves 85 % F1 on a Wikipedia benchmark corpus , significantly improving the previous state-of-the-art by 8 % .", "This paper presents DefMiner , a supervised sequence labeling system that identifies scientific terms and their accompanying definitions .", "Obtaining a list of popular defined terms in a corpus of computational linguistics papers , we find that concepts can often be categorized into one of three categories : resources , methodologies and evaluation metrics .", "We exploit DefMiner to process the ACL Anthology Reference Corpus ( ARC ) ?", "We highlight several interesting observations : more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms .", "The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles ."]}
{"orig_sents": ["0", "3", "4", "2", "1", "5"], "shuf_sents": ["State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors .", "Furthermore , because the joint learning model considers interacting phenomena during training , it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss .", "We show that it is possible to identify interactions well enough to facilitate a joint approach and , consequently , that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce .", "Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words .", "In this paper , we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning .", "Overall , our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction ."]}
{"orig_sents": ["3", "0", "5", "4", "1", "2"], "shuf_sents": ["eye movements in reading .", "Increasing the inductive bias also makes learning across readers possible .", "In fact we observe next-to-no performance drop when evaluating models trained on gaze records of multiple readers on new readers .", "Nilsson and Nivre ( 2009 ) introduced a treebased model of persons ?", "While a tree-based model seems plausible for eye movements , we show that competitive results can be obtained with a linear CRF model .", "The individual variation between readers reportedly made application across readers impossible ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode .", "This paper explores to what extent lemmatisation , lexical resources , distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon , the resulting lack of easily identifiable surface markers and their relative rarity .", "We substantially improve bridging recognition without impairing performance on other IS classes .", "We develop linguistically motivated discourse structure , lexico-semantic and genericity detection features and integrate these into a cascaded minority preference algorithm that models bridging recognition as a subtask of learning finegrained information status ( IS ) ."]}
{"orig_sents": ["0", "2", "3", "1", "5", "4"], "shuf_sents": ["We present an approach to time normalization ( e.g .", "Time expressions are then parsed using an extended CYK+ algorithm , and converted to a normalized form by applying the operators recursively .", "the day before yesterday ? 2013-04-12 ) based on a synchronous context free grammar .", "Synchronous rules map the source language to formally defined operators for manipulating times ( FINDENCLOSED , STARTATENDOF , etc . ) .", "Our model outperforms HeidelTime , the best time normalization system in TempEval 2013 , on four different time normalization corpora .", "For evaluation , a small set of synchronous rules for English time expressions were developed ."]}
{"orig_sents": ["4", "2", "0", "3", "5", "1"], "shuf_sents": ["We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia : while rule-based IE dominates the commercial world , it is widely regarded as dead-end technology by the academia .", "We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice .", "analytics over unstructured text has led to renewed interest in information extraction ( IE ) .", "We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE , as well as academia ? s perception that rulebased IE is devoid of research challenges .", "The rise of ? Big Data ?", "We make a case for the importance of rule-based IE to industry practitioners ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["We present extensive experimental results validating this finding .", "Path Ranking Algorithm ( PRA ) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph .", "Automatically constructed Knowledge Bases ( KBs ) are often incomplete and there is a genuine need to improve their coverage .", "For the first time , we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task .", "The resources presented in this paper are publicly available ."]}
{"orig_sents": ["1", "3", "0", "2", "4"], "shuf_sents": ["In this short paper , we propose a novel method to model rules as observed generation output of a compact hidden model , which leads to better generalization capability .", "Most of the machine translation systems rely on a large set of translation rules .", "We present a preliminary generative model to test this idea .", "These rules are treated as discrete and independent events .", "Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking .", "Neural network language models , or continuous-space language models ( CSLMs ) , have been shown to improve the performance of statistical machine translation ( SMT ) when they are used for reranking n-best translations .", "However , CSLMs have not been used in the first pass decoding of SMT , because using CSLMs in decoding takes a lot of time .", "In contrast , we propose a method for converting CSLMs into back-off n-gram language models ( BNLMs ) so that we can use converted CSLMs in decoding ."]}
{"orig_sents": ["1", "3", "0", "4", "2"], "shuf_sents": ["Instead , we present a new MIRA method , which employs an exact corpus-level BLEU to compute the model loss .", "MIRA based tuning methods have been widely used in statistical machine translation ( SMT ) system with a large number of features .", "Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations .", "Since the corpus-level BLEU is not decomposable , these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses .", "Our method is simpler in implementation ."]}
{"orig_sents": ["4", "3", "0", "1", "2", "5"], "shuf_sents": ["For our experiments with multilingual online discussions , we first tag the language of individual words using language models and dictionaries .", "Secondly , we incorporate context to improve the performance .", "We achieve an accuracy of 98 % .", "Analyses of large scale multilingual data require automatic language identification at the word level .", "Multilingual speakers switch between languages in online and spoken communication .", "Besides word level accuracy , we use two new metrics to evaluate this task ."]}
{"orig_sents": ["6", "5", "0", "1", "2", "3", "4"], "shuf_sents": ["However few work has focused on short text such as microblog post .", "Microblog posts are short and noisy .", "Previous method can extract few features from the post context .", "In this paper we propose to use extra posts for the microblog entity linking task .", "Experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3 % and 7.5 % respectively .", "Entity linking in long text has been well studied in previous works .", "Linking name mentions in microblog posts to a knowledge base , namely microblog entity linking , is useful for text mining tasks on microblog ."]}
{"orig_sents": ["2", "1", "4", "0", "3"], "shuf_sents": ["We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL .", "However , when there are multiple metadata attributes available , it is not always straightforward to select a single best attribute for domain partition , and it is possible that combining more than one metadata attributes ( including continuous attributes ) can lead to better MDL performance .", "Multi-Domain learning ( MDL ) assumes that the domain labels in the dataset are known .", "Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods .", "In this work , we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems .", "We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts .", "We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems , including the famously uncracked Zodiac 340 .", "For particularly difficult homophonic ciphers , we find that big gains in accuracy are to be had by running upwards of 100K random restarts , which we accomplish efficiently using a GPU-based parallel implementation ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction .", "An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress .", "We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word ? s stem and suffix .", "We frame this as a ranking problem , where the objective is to rank the pronunciation with the correct stress above those with incorrect stress ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams .", "By employing pruning techniques and a distributed framework , we make the computation for very large corpora feasible on comparably small computational resources .", "Evaluating against lexical resources using two measures , we show that our approach produces higher quality DTs than previous approaches , and is thus preferable in terms of speed and quality for large corpora .", "We introduce a new highly scalable approach for computing Distributional Thesauri ( DTs ) ."]}
{"orig_sents": ["2", "5", "1", "4", "3", "0"], "shuf_sents": ["Finally , we combine latent features with fine-grained n-gram overlap features , yielding performance that is 3 % more accurate than the prior state-of-the-art .", "We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification .", "Matrix and tensor factorization have been applied to a number of semantic relatedness tasks , including paraphrase identification .", "Next , we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy .", "First , we design a new discriminative term-weighting metric called TF-KLD , which outperforms TF-IDF .", "The key idea is that similarity in the latent space implies semantic relatedness ."]}
{"orig_sents": ["4", "3", "1", "0", "2"], "shuf_sents": ["In this paper , we empirically evaluate the performance of different corpora in sentiment similarity measurement , which is the fundamental task for word polarity classification .", "Nowadays , Twitter has aggregated huge amount of data that are full of people ? s sentiments .", "Experiment results show that the Twitter data can achieve a much better performance than the Google , Web1T and Wikipedia based methods .", "However , no work is done for comparing different corpora in the polarity classification task .", "Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word ? s sentiment polarity ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better .", "We think , based on the explicit sentences , several Support Vector Machine ( SVM ) classifiers can be established to do this task .", "Nevertheless , we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods .", "Implicit feature detection , also known as implicit feature identification , is an essential aspect of feature-specific opinion mining but previous works have often ignored it ."]}
{"orig_sents": ["3", "5", "2", "1", "0", "4"], "shuf_sents": ["( 2012 ) to hypergraphs and apply it to the cube-pruning parser of Zhang and McDonald ( 2012 ) .", "In this paper , we generalize the violation-fixing perceptron of Huang et al .", "However , perceptron training with inexact search is less studied for bottom-up parsing and , more generally , inference over hypergraphs .", "Online learning algorithms like the perceptron are widely used for structured prediction tasks .", "This results in the highest reported scores on WSJ evaluation set ( UAS 93.50 % and LAS 92.41 % respectively ) without the aid of additional resources .", "For sequential search problems , like left-to-right tagging and parsing , beam search has been successfully combined with perceptron variants that accommodate search errors ( Collins and Roark , 2004 ; Huang et al. , 2012 ) ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["We present a classification model that predicts the presence or omission of a lexical connective between two clauses , based upon linguistic features of the clauses and the type of discourse relation holding between them .", "We also present results of an experiment that provides insight into the nature and difficulty of the task .", "The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6 % .", "Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text ."]}
{"orig_sents": ["0", "3", "6", "1", "2", "5", "4"], "shuf_sents": ["In Japanese , zero references often occur and many of them are categorized into zero exophora , in which a referent is not mentioned in the document .", "To deal with zero exophora , our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns .", "In addition , we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns .", "However , previous studies have focused on only zero endophora , in which a referent explicitly appears .", "The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora .", "We represent their particular behavior in a discourse as a feature vector of a machine learning model .", "We present a zero reference resolution model considering zero exophora and author/reader of a document ."]}
{"orig_sents": ["5", "1", "4", "3", "0", "2"], "shuf_sents": ["This dataset consists of both naturally formed conversations , manually labeled data , and a large repository of candidate responses .", "In this paper we propose a retrieval-based automatic response model for short-text conversation , to exploit the vast amount of short conversation instances available on social media .", "Our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset .", "This dataset provides rich collection of instances for the research on finding natural and relevant short responses to a given short text , and useful for both training and testing of conversation models .", "For this purpose we introduce a dataset of short-text conversation based on the real-world instances from Sina Weibo ( a popular Chinese microblog service ) , which will be soon released to public .", "Natural language conversation is widely regarded as a highly difficult problem , which is usually attacked with either rule-based or learning-based models ."]}
{"orig_sents": ["3", "4", "0", "5", "1", "2", "6"], "shuf_sents": ["These explanations can help readers get easily comprehensible information of the discussed products and aspects .", "In this work , we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level .", "We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints .", "Explanatory sentences are employed to clarify reasons , details , facts , and so on .", "High quality online product reviews usually include not only positive or negative opinions , but also a variety of explanations of why these opinions were given .", "Moreover , explanatory relations can also benefit sentiment analysis applications .", "Experimental results demonstrate the effectiveness of the proposed method ."]}
{"orig_sents": ["3", "1", "0", "5", "4", "2"], "shuf_sents": ["It helps the reader to contextualize the information contained in a single article , by navigating backward or forward in the thread from this article .", "An event thread is basically a succession of events belonging to the same story .", "We also share interesting comments concerning our manual annotation procedure for building a training and testing set1 .", "We present an approach for building multidocument event threads from a large corpus of newswire articles .", "In order to build these event threads , we use a cascade of classifiers and other modules , taking advantage of the redundancy of information in the newswire corpus .", "A specific effort is also made on the detection of reactions to a particular event ."]}
{"orig_sents": ["4", "3", "1", "5", "2", "0"], "shuf_sents": ["*", "In addition , we have explored the way of selecting compatible features for different part-of-speech cues .", "Compared with the state of the art scope detection systems , our system achieves substantial improvement .", "This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information .", "Scope detection is a key task in information extraction .", "Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes ."]}
{"orig_sents": ["1", "0", "6", "5", "3", "2", "4", "7"], "shuf_sents": ["However , text use changes with time , which can affect many applications .", "Temporal variations of text are usually ignored in NLP applications .", "We use Gaussian Processes , a state-ofthe-art bayesian non-parametric model , with a novel periodic kernel .", "We use this for regression in order to forecast the volume of a hashtag based on past data .", "We demonstrate this in a text classification setting , assigning the tweet hashtag based on the rest of its text .", "Focusing on hashtag frequency in Twitter , we first automatically identify the periodic patterns .", "In this paper we model periodic distributions of words over time .", "This method shows significant improvements over competitive baselines ."]}
{"orig_sents": ["3", "1", "2", "0", "4"], "shuf_sents": ["We propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora , one of which is a contribution of this work .", "However , simply focusing on direct quotations ignores around half of all reported speech , which is in the form of indirect or mixed speech .", "This work presents the first large-scale experiments in indirect and mixed quotation extraction and attribution .", "Direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy .", "We further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution ."]}
{"orig_sents": ["6", "3", "5", "4", "1", "2", "0"], "shuf_sents": ["We propose a new representation for Web search queries based on identifying the concepts in queries and show that we can significantly improve query reformulation performance using features of query concepts .", "However , this work has been limited to bag-of-words models where the main signals being used are word overlap , character level edit distance and word level edit distance .", "In this work , we show that relying solely on surface level text similarity results in many false positives where queries with different intents yet similar topics are mistakenly predicted as query reformulations .", "This process is referred to as ? Query Reformulation ? .", "Some research has studied the problem of predicting whether the current query is a reformulation of the previous query or not .", "Previous research has mainly focused on proposing query reformulations in the form of suggested queries for users .", "Web search users frequently modify their queries in hope of receiving better results ."]}
{"orig_sents": ["1", "4", "5", "3", "0", "2"], "shuf_sents": ["Our extensive experiments and analysis demonstrate that our method significantly improves passage retrieval , compared to using textual features alone .", "Passage retrieval is a crucial first step of automatic Question Answering ( QA ) .", "As an additional contribution , we make available to the research community the code and the search behavior data used in this study , with the hope of encouraging further research in this area .", "Specifically , we exploit detailed examination data , such as mouse cursor movements and scrolling , to infer the parts of the document the searcher found interesting , and then incorporate this signal into passage retrieval for QA .", "While existing passage retrieval algorithms are effective at selecting document passages most similar to the question , or those that contain the expected answer types , they do not take into account which parts of the document the searchers actually found useful .", "We propose , to the best of our knowledge , the first successful attempt to incorporate searcher examination data into passage retrieval for question answering ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["This paper presents the Kazakh Language Corpus ( KLC ) , which is one of the first attempts made within a local research community to assemble a Kazakh corpus .", "KLC is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres : literary , publicistic , official , scientific and informal .", "KLC has a web-based corpus management system that helps to navigate the data and retrieve necessary information .", "Along with its primary part KLC comprises such parts as : ( i ) annotated sub-corpus , containing segmented documents encoded in the eXtensible Markup Language ( XML ) that marks complete morphological , syntactic , and structural characteristics of texts ; ( ii ) as well as a sub-corpus with the annotated speech data .", "KLC is also open for contributors , who are willing to make suggestions , donate texts and help with annotation of existing materials ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The method is demonstrated on Egyptian Arabic and German , two morphologically rich languages .", "The method consists of a core languageindependent algorithm , which can be optimized for specific languages .", "Our best method for Egyptian Arabic provides an error reduction of 55.6 % over a simple baseline ; our best method for German achieves a 66.7 % error reduction .", "We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora ."]}
{"orig_sents": ["1", "4", "3", "5", "0", "2"], "shuf_sents": ["We show competitive accuracy compared to the traditional channel model features .", "We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words .", "Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU , and by 1.1 BLEU on average across several test sets .", "We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically .", "The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models .", "Our joint model builds on a well known recurrent neural network language model ( Mikolov , 2012 ) augmented by a layer of additional inputs from the source language ."]}
{"orig_sents": ["3", "5", "0", "1", "4", "2"], "shuf_sents": ["In this paper , we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning ( MTL ) , with in-domain models tailored for each specific domain and a general-domain model shared by different domains .", "The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better .", "Furthermore , it also outperforms the individual adaptation of each specific domain .", "Domain adaptation for SMT usually adapts models to an individual specific domain .", "Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline .", "However , it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We present a novel translation model , which simultaneously exploits the constituency and dependency trees on the source side , to combine the advantages of two types of trees .", "Our rules hold the property of long distance reorderings and the compatibility with phrases .", "Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string ( +2.45 BLEU on average ) and dependencyto-string ( +0.91 BLEU on average ) models , which only employ single type of trees , and significantly outperforms the state-of-theart hierarchical phrase-based model ( +1.12 BLEU on average ) , on three Chinese-English NIST test sets .", "We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules , and the target side as strings ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["When using a machine translation ( MT ) model trained on OLD-domain parallel data to translate NEW-domain text , one major challenge is the large number of out-of-vocabulary ( OOV ) and new-translation-sense words .", "We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs .", "Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus , our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs , while minimizing the divergence from the OLD-domain distribution .", "Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines ."]}
{"orig_sents": ["5", "3", "4", "2", "0", "1"], "shuf_sents": ["In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in ( Watanabe et al , 2006b ) .", "Unlike that algorithm , using experiments over multiple language pairs we show two new results : our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero , four times faster ; and by introducing new distortion and reordering features for LR decoding , it maintains the same translation quality ( as in BLEU scores ) obtained phrase-based and CKY Hiero with the same translation model .", "It requires a single language model ( LM ) history for each target hypothesis rather than two LM histories per hypothesis as in CKY .", "It generates the target sentence by extending the hypotheses only on the right edge .", "LR decoding has complexity O ( n2b ) for input of n words and beam size b , compared toO ( n3 ) for the CKY algorithm .", "Left-to-right ( LR ) decoding ( Watanabe et al , 2006b ) is a promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We present a simple procedure that can be used with any statistical machine translation ( MT ) system .", "We explore three ways of using diverse translations : ( 1 ) system combination , ( 2 ) discriminative reranking with rich features , and ( 3 ) a novel post-editing scenario in which multiple translations are presented to users .", "This paper addresses the problem of producing a diverse set of plausible translations .", "We find that diversity can improve performance on these tasks , especially for sentences that are difficult for MT ."]}
{"orig_sents": ["3", "2", "1", "0", "4"], "shuf_sents": ["Extensive phrase-based translation experiments on both Chinese-to-English and Spanish-to-English tasks show substantial gains in BLEU by up to +2.3/+2.0 on dev/test over MERT , thanks to 20M+ sparse features .", "We instead present a very simple yet theoretically motivated approach by extending the recent framework of ? violation-fixing perceptron ? , using forced decoding to compute the target derivations .", "Most recent efforts along this line are not scalable ( training on the small dev set with features from top ? 100 most frequent words ) and overly complicated .", "While large-scale discriminative training has triumphed in many NLP problems , its definite success on machine translation has been largely elusive .", "This is the first successful effort of large-scale online discriminative training for MT ."]}
{"orig_sents": ["4", "3", "6", "2", "1", "0", "7", "5"], "shuf_sents": ["This is possible because we transform the document space to a similarity space and learning is performed in this new space .", "The classifier can be applied to documents of the involved userids .", "Instead , it uses documents from other userids for classifier building .", "Since multiple userids may belong to the same author , it is hard to directly apply supervised learning to solve the problem .", "This paper studies the problem of identifying users who use multiple userids to post in social media .", "The experimental results using a large number of userids and their reviews show that the proposed method is highly effective .", "This paper proposes a new method , which still uses supervised learning but does not require training documents from the involved userids .", "Our evaluation is done in the online review domain ."]}
{"orig_sents": ["2", "3", "0", "1", "4"], "shuf_sents": ["We find that the gender inference problem in quite diverse languages can be addressed using existing machinery .", "Further , accuracy gains can be made by taking language-specific features into account .", "While much work has considered the problem of latent attribute inference for users of social media such as Twitter , little has been done on non-English-based content and users .", "Here , we conduct the first assessment of latent attribute inference in languages beyond English , focusing on gender inference .", "We identify languages with complex orthography , such as Japanese , as difficult for existing methods , suggesting a valuable direction for future research ."]}
{"orig_sents": ["0", "6", "3", "4", "1", "5", "2"], "shuf_sents": ["Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views .", "The clusters are directly interpretable and improve on our evaluation tasks .", "We find that the three- , four- , and five-dimensional models significantly outperform models using only one or two modalities , and that nontextual modalities each provide separate , disjoint knowledge that can not be forced into a shared , latent structure .", "( 1 ) We outperform text-only models in two different evaluations , and demonstrate that low-level visual features are directly compatible with the existing model .", "( 2 ) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images .", "( 3 ) We provide two novel ways to extend the bimodal models to support three or more modalities .", "In this work , we improve a two-dimensional multimodal version of Latent Dirichlet Allocation ( Andrews et al , 2009 ) in various ways ."]}
{"orig_sents": ["1", "4", "3", "2", "0"], "shuf_sents": ["Although we experiment only with binarization and function labels in this study , there is much scope for applying this approach to other grammar extraction strategies .", "It has recently been shown that different NLP models can be effectively combined using dual decomposition .", "this represents an absolute improvement of 0.7 and an error reduction rate of 7 % over a strong PCFG-LA product-model baseline .", "We experiment with the different models which result from alternative methods of extracting a grammar from a treebank ( retaining or discarding function labels , left binarization versus right binarization ) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 ?", "In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way ."]}
{"orig_sents": ["0", "2", "1", "3", "6", "4", "5"], "shuf_sents": ["NLP models have many and sparse features , and regularization is key for balancing model overfitting versus underfitting .", "We reinterpret this noising as an explicit regularizer , and approximate it with a second-order formula that can be used during training without actually generating fake data .", "A recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data .", "We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs .", "The regularizer is a sum over inputs , so we can estimate it more accurately via a semi-supervised or transductive extension .", "Applied to text classification and NER , our method provides a > 1 % absolute performance gain over use of standard L2 regularization .", "We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["In terms of modeling assumption , we found it is effective to assign a topic to only some parts of a document .", "Our blocked sampler can efficiently search for higher probability space even with higher order n-grams .", "One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation .", "We advance the previous study of the Bayesian topic language model by Wallach ( 2006 ) in two directions : one , investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics , and two , developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["In this paper we report an empirical study on semi-supervised Chinese word segmentation using co-training .", "Our experimental results show that co-training captures 20 % and 31 % of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively .", "These two segmenters are initially trained with a small amount of segmented data , and then iteratively improve each other using the large amount of unlabelled data .", "We utilize two segmenters : 1 ) a word-based segmenter leveraging a word-level language model , and 2 ) a character-based segmenter using characterlevel features within a CRF-based sequence labeler ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["A precise syntacto-semantic analysis of English requires a large detailed lexicon with the possibility of treating multiple tokens as a single meaning-bearing unit , a word-with-spaces .", "Our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy .", "We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics , a process we call ubertagging .", "However parsing with such a lexicon , as included in the English Resource Grammar , can be very slow ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We then apply the acquired knowledge to a case alternation task and prove its usefulness .", "We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese .", "By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus , our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases ."]}
{"orig_sents": ["6", "5", "8", "0", "7", "3", "2", "1", "9", "4"], "shuf_sents": ["This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery .", "A set of novel features is proposed for the ranking model .", "Then , we apply a statistical ranking model to select correct hypernyms .", "First , we extract candidate hypernyms from the above sources .", "Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset .", "Most previous methods are based on lexical patterns but perform badly on opendomain data .", "Hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other .", "Given an entity name , we try to discover its hypernyms by leveraging knowledge from multiple sources , i.e. , search engine results , encyclopedias , and morphology of the entity name .", "Other work extracts hypernym relations from encyclopedias but has limited coverage .", "We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Experimental results show that incorporating the semantic structure of sentences is beneficial .", "When training data is unavailable , scores obtained from the logic prover in an unsupervised manner outperform supervised methods .", "This paper presents a novel approach to determine textual similarity .", "A layered methodology to transform text into logic forms is proposed , and semantic features are derived from a logic prover ."]}
{"orig_sents": ["1", "2", "5", "0", "4", "3"], "shuf_sents": ["not as much ?", "Why do certain combinations of words such as ? disadvantageous peace ?", "or ? metal to the petal ?", "We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words , then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity .", "We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original , interesting , and at times even artistic .", "appeal to our minds as interesting expressions with a sense of creativity , while other phrases such as ? quiet teenager ? , or ? geometrical base ?"]}
{"orig_sents": ["4", "5", "3", "6", "2", "1", "0"], "shuf_sents": ["We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered .", "prior polarity for sentiment analysis .", "Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets , our learning approach consistently outperforms the single metrics , providing a new state-ofthe-art approach in computing words ?", "In the literature , various approaches based on SentiWordNet have been proposed .", "Assigning a positive or negative score to a word out of context ( i.e .", "a word ? s prior polarity ) is a challenging task for sentiment analysis .", "In this paper , we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["This paper describes a simulation study with queries spoken by non-native speakers that suggests that indicates that finding relevant content is often possible within a half minute , and that combining features based on automatically recognized words with features designed for automated prediction of query difficulty can serve as a useful basis for predicting when that useful content has been found .", "Building search engines that can respond to spoken queries with spoken content requires that the system not just be able to find useful responses , but also that it know when it has heard enough about what the user wants to be able to do so ."]}
{"orig_sents": ["7", "2", "5", "3", "0", "4", "6", "1"], "shuf_sents": ["Specifically , we first investigate two requisite properties of an ideal storyline .", "Experiments on real-world datasets show that our method is quite efficient and highly competitive , which can bring about quicker , clearer and deeper comprehension to readers .", "Especially for a complex news event , it is difficult to understand its general idea within a single coherent picture .", "In this paper , we propose a novel solution to tackle the challenging problem of storylines extraction and reconstruction .", "Then a unified algorithm is devised to extract all effective storylines by optimizing these properties at the same time .", "A complex event often contains branches , intertwining narratives and side news which are all called storylines .", "Finally , we reconstruct all extracted lines and generate the high-quality story map .", "The rapid development of Web2.0 leads to significant information redundancy ."]}
{"orig_sents": ["4", "5", "1", "3", "0", "2"], "shuf_sents": ["We describe two template-based description generation models that operate over visual dependency representations .", "In this paper , we introduce visual dependency representations to capture the relationships between the objects in an image , and hypothesize that this representation can improve image description .", "In an image description task , we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements .", "We test this hypothesis using a new data set of region-annotated images , associated with visual dependency representations and gold-standard descriptions .", "Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them .", "Previous approaches have represented images as unstructured bags of regions , which makes it difficult to accurately predict meaningful relationships between regions ."]}
{"orig_sents": ["6", "1", "5", "4", "0", "2", "3"], "shuf_sents": ["The meta features are used together with base features in our final parser .", "base features ) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data .", "Our studies indicate that our proposed approach is very effective in processing unseen data and features .", "Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data .", "meta features ) with the help of a large amount of automatically parsed data .", "In this paper , we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features ( i.e .", "In current dependency parsing models , conventional features ( i.e ."]}
{"orig_sents": ["6", "1", "5", "0", "2", "3", "4"], "shuf_sents": ["Experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination .", "In this paper , we propose a segmentation criterion combining both lexical cohesion and disruption , enabling a trade-off between the two .", "Gains were observed in all conditions , with segments of either regular or varying length and abrupt or smooth topic shifts .", "Long segments benefit more than short segments .", "However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences .", "We provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation .", "Topic segmentation classically relies on one of two criteria , either finding areas with coherent vocabulary use or detecting discontinuities ."]}
{"orig_sents": ["2", "0", "6", "1", "3", "4", "7", "5"], "shuf_sents": ["To do this , we collected customer reviews from Starbucks , Dunkin ?", "We study the brand related language use through these reviews , with focuses on the brand satisfaction and gender factors .", "We propose a Laplacian structured sparsity model to study computational branding analytics .", "In particular , we perform three tasks : automatic brand identification from raw text , joint brand-satisfaction prediction , and joint brandgender-satisfaction prediction .", "This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model .", "In addition , qualitative analysis of our model reveals important features of the language uses associated with the specific brands .", "Donuts , and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA .", "Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms ."]}
{"orig_sents": ["2", "4", "8", "1", "7", "0", "6", "5", "3"], "shuf_sents": ["The major novelty of our work is that we automatically learn commercial intents revealed from microblogs .", "For ecommerce companies , it is very important to make rapid and correct response to these hot trends in order to improve product sales .", "Hot trends are likely to bring new business opportunities .", "We perform extensive experiments and the results showed that our methods are very effective .", "For example , ? Air Pollution ?", "In order to solve this problem , we further propose a graph based method , which jointly models relevance and associativity .", "We carefully construct a data collection for this task and present quite a few insightful findings .", "In this paper , we take the initiative to study the task of how to identify trend related products .", "might lead to a significant increase of the sales of related products , e.g. , mouth mask ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["that add value in prediction of clinical assessments .", "Using Pennebaker ? s Linguistic Inquiry and Word Count ( LIWC ) lexicon to provide baseline features , we show that straightforward topic modeling using Latent Dirichlet Allocation ( LDA ) yields interpretable , psychologically relevant ? themes ?", "We investigate the value-add of topic modeling in text analysis for depression , and for neuroticism as a strongly associated personality measure ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The model makes an initial prediction based on the meaning of the utterance , and revises it continuously based on the user ? s behavior .", "The combined model outperforms its components in predicting reference resolution and when to give feedback .", "We present a statistical model for predicting how the user of an interactive , situated NLP system resolved a referring expression ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["To our knowledge , this is the first work to report results obtained by an end-toend Chinese zero pronoun resolver .", "We extend Zhao and Ng 's ( 2007 ) Chinese anaphoric zero pronoun resolver by ( 1 ) using a richer set of features and ( 2 ) exploiting the coreference links between zero pronouns during resolution .", "Results on OntoNotes show that our approach significantly outperforms two state-of-the-art anaphoric zero pronoun resolvers ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge .", "Our model is based on scoring functions that operate by learning low-dimensional embeddings of words , entities and relationships from a knowledge base .", "We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data ( 4M entities , 23k relationships ) to improve over methods that rely on text features alone ."]}
{"orig_sents": ["4", "5", "0", "3", "2", "1"], "shuf_sents": ["We also propose to average parameters in training .", "The proposed model marks scores competitive with state-of-the-art RNN-based models .", "We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity .", "Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model .", "In this paper , we present a recursive neural network ( RNN ) model that works on a syntactic tree .", "Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["Our results show that with word class models , the baseline can be improved by up to 1.4 % BLEU and 1.0 % TER on the French ? German task and 0.3 % BLEU and 1.1 % TER on the German ? English task .", "Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing .", "We present a very simple and easy to implement method for using these word classes to improve translation quality .", "We show its efficacy on a small German ? English and a larger French ? German translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models .", "It can be applied across different machine translation paradigms and with arbitrary types of models ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We apply it to postordering of phrase-based machine translation ( PBMT ) for Japanese-to-English patent tasks .", "Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system .", "Our model uses rich syntax parsing features for word reordering and runs in linear time .", "This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We develop a new model that combines the neural probabilistic language model of Bengio et al , rectified linear units , and noise-contrastive estimation , and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder .", "Our large-scale , large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu .", "We explore the application of neural language models to machine translation ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We introduce bilingual word embeddings : semantic embeddings associated across two languages in the context of neural language models .", "The new embeddings significantly out-perform baselines in word semantic similarity .", "A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task .", "We propose a method to learn bilingual embeddings from a large unlabeled corpus , while utilizing MT word alignments to constrain translational equivalence ."]}
{"orig_sents": ["5", "0", "6", "3", "1", "2", "4"], "shuf_sents": ["Methods used in this approach rank parts of a document based on the similarity to a presumably related document .", "Automatically constructed anchor texts are manually evaluated in terms of relatedness to linked documents and compared to baseline consisting of originally inserted anchor texts .", "Additionally we use crowdsourcing for evaluation of original anchors and automatically constructed anchors .", "A number of different methods from information retrieval and natural language processing are adapted for this task .", "Results show that our best adapted methods rival the precision of the baseline method .", "In this paper we present a novel approach to automatic creation of anchor texts for hyperlinks in a document pointing to similar documents .", "Ranks are then used to automatically construct the best anchor text for a link inside original document to the compared document ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["Although a variety of language models have been applied to this task in previous work , none of the existing approaches incorporate syntactic information .", "Sentence completion is a challenging semantic modeling task in which models must choose the most appropriate word from a given set to complete a sentence .", "In this paper we propose to tackle this task using a pair of simple language models in which the probability of a sentence is estimated as the probability of the lexicalisation of a given syntactic dependency tree .", "We apply our approach to the Microsoft Research Sentence Completion Challenge and show that it improves on n-gram language models by 8.7 percentage points , achieving the highest accuracy reported to date apart from neural language models that are more complex and expensive to train ."]}
{"orig_sents": ["2", "4", "0", "1", "3"], "shuf_sents": ["Using vector representations , such an approach captures both distributional semantic similarities among words as well as the structural relations between them ( encoded as the structure of the parse tree ) .", "We show an efficient formulation to compute this kernel using simple matrix operations .", "In this paper , we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces .", "We present our results on three diverse NLP tasks , showing state-of-the-art results .", "Our proposed approach subsumes a general framework for word-similarity , and in particular , provides a flexible way to incorporate distributed representations ."]}
{"orig_sents": ["5", "1", "0", "4", "3", "2"], "shuf_sents": ["We train an idiom classifier on a newly gathered corpus of over 60,000 Wiktionary multi-word definitions , incorporating features that model whether phrase meanings are constructed compositionally .", "In this paper , we study the problem of automatically identifying idiomatic dictionary entries with such resources .", "In a set of Wiktionary definition example sentences , the more complete set of idioms boosts detection recall by over 28 percentage points .", "These gains also translate to idiom detection in sentences , by simply using known word sense disambiguation algorithms to match phrases to their definitions .", "Experiments demonstrate that the learned classifier can provide high quality idiom labels , more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65 % .", "Online resources , such as Wiktionary , provide an accurate but incomplete source of idiomatic phrases ."]}
{"orig_sents": ["4", "1", "3", "2", "0"], "shuf_sents": ["The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings .", "We compute the compositionality of a phrase through substituting the constituent words with their ? neighbours ?", "Several methods of obtaining neighbours are presented .", "in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases .", "We present a novel unsupervised approach to detecting the compositionality of multi-word expressions ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data .", "The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense .", "We introduce an extended naive Bayes model for word sense induction ( WSI ) and apply it to a WSI task ."]}
{"orig_sents": ["3", "2", "1", "4", "0"], "shuf_sents": ["This paper describes a recently-launched attempt to address this issue with a series of human judgment tasks , posed to subjects in the form of games .", "For example , the verb spray ( of the Spray class ) , involves the predicates MOTION , NOT , and LOCATION , where the event can be decomposed into an AGENT causing a THEME that was originally not in a particular location to now be in that location .", "The current semantic predicates can be thought of semantic primitives , into which the concepts denoted by a verb can be decomposed .", "This research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in VerbNet , a lexicon of about 6300 English verbs .", "Although VerbNet ? s predicates are theoretically well-motivated , systematic empirical data is scarce ."]}
{"orig_sents": ["0", "5", "1", "3", "4", "2"], "shuf_sents": ["This paper offers an approach for governments to harness the information contained in social media in order to make public inspections and disclosure more efficient .", "which are done for restaurants throughout the United States and in most of the world and are a frequently cited example of public inspections and disclosure .", "Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers .", "We present the first empirical study that shows the viability of statistical models that learn the mapping between textual signals in restaurant reviews and the hygiene inspection records from the Department of Public Health .", "The learned model achieves over 82 % accuracy in discriminating severe offenders from places with no violation , and provides insights into salient cues in reviews that are indicative of the restaurant ? s sanitary conditions .", "As a case study , we turn to restaurant hygiene inspections ?"]}
{"orig_sents": ["6", "1", "2", "4", "5", "3", "0"], "shuf_sents": ["The proposed method strongly outperforms existing techniques in systematic experiments on a blog corpus .", "texts not written by the authors to which they are attributed ?", "has important historical , forensic and commercial applications .", "The crucial point is that document similarity not be measured in any of the standard ways but rather be based on the output of a recently introduced algorithm for authorship verification .", "We introduce an unsupervised technique for identifying pseudepigrapha .", "The idea is to identify textual outliers in a corpus based on the pairwise similarities of all documents in the corpus .", "The identification of pseudepigraphic texts ?"]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing .", "Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features , while computing fewer than 30 % of the feature templates .", "We test our method on 7 languages .", "We propose a faster framework of dynamic feature selection , where features are added sequentially as needed , edges are pruned early , and decisions are made online for each sentence .", "We model this as a sequential decision-making problem and solve it by imitation learning techniques ."]}
{"orig_sents": ["1", "4", "3", "2", "0", "5"], "shuf_sents": ["We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews .", "Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language .", "Specifically , we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model , while minimizing the prediction log-losses of labeled documents .", "In this paper , we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words .", "An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks .", "Our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach ."]}
{"orig_sents": ["4", "5", "0", "3", "6", "1", "2"], "shuf_sents": [".", "In this paper we present experiments getting experts to provide regular expressions , as well as crowdsourced annotation tasks from which regular expressions can be derived .", "Somewhat surprisingly , it turns out that these crowdsourced feature combinations outperform automatic feature combination methods , as well as expert features , by a very large margin and reduce error by 24-41 % over n-gram representations .", ".", "Often the bottleneck in document classification is finding good representations that zoom in on the most important aspects of the documents .", "Most research uses n-gram representations , but relevant features often occur discontinuously , e.g. , not .", "good in sentiment analysis ."]}
{"orig_sents": ["6", "2", "3", "1", "5", "0", "4"], "shuf_sents": ["An evaluation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality .", "We also extend an existing unsupervised compression method with a learning module .", "We address this problem and present a method to automatically build a compression corpus with hundreds of thousands of instances on which deletion-based algorithms can be trained .", "In our corpus , the syntactic trees of the compressions are subtrees of their uncompressed counterparts , and hence supervised systems which require a structural alignment between the input and output can be successfully trained .", "Also , the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting , significantly outperforming a strong baseline .", "The new system uses structured prediction to learn from lexical , syntactic and other features .", "A major challenge in supervised sentence compression is making use of rich feature representations because of very scarce parallel data ."]}
{"orig_sents": ["4", "3", "5", "1", "0", "6", "2", "7"], "shuf_sents": ["Our approach first relaxes the length constraint using Lagrangian relaxation .", "We propose an efficient decoding algorithm for fast compressive summarization using graph cuts .", "Since finding the tightest lower bound suffers from local optimality , we use convex relaxation for initialization .", "In contrast , joint compression and summarization can use smaller units such as words and phrases , resulting in summaries containing more information .", "Extractive summarization typically uses sentences as summarization units .", "The goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint .", "Then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem , which can be solved efficiently using graph max-flow/min-cut .", "Experimental results on TAC2008 dataset demonstrate our method achieves competitive ROUGE score and has good readability , while is much faster than the integer linear programming ( ILP ) method ."]}
{"orig_sents": ["2", "1", "3", "0", "5", "4"], "shuf_sents": ["Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data .", "Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent .", "In a language generation system , a content planner selects which elements must be included in the output text and the ordering between them .", "In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering .", "Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches .", "We develop two approaches : the first one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records ; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document ."]}
{"orig_sents": ["0", "4", "6", "3", "5", "1", "2", "7"], "shuf_sents": ["Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem , a Maximum Coverage Problem or a Budgeted Median Problem .", "First , we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree , which allows us to take a treetrimming approach to summarization .", "Second , we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem , then solve it with integer linear programming ( ILP ) .", "This paper proposes a single document summarization method based on the trimming of a discourse tree .", "These methods successfully improved summarization quality , but they did not consider the rhetorical relations between the textual units of a source document .", "This is a two-fold process .", "Thus , summaries generated by these methods may lack logical coherence .", "Evaluation results showed that our method improved ROUGE scores ."]}
{"orig_sents": ["3", "1", "5", "9", "10", "8", "4", "7", "2", "6", "0"], "shuf_sents": ["From a system aspect , CET substantially boosts the performance of two information retrieval models ( i.e. , vector space model and query likelihood language model ) .", "Currently , most of UGC is listed as a whole or in pre-defined categories .", "We further evaluate the performance of CET on UGC organization in both user and system aspects .", "Social media like forums and microblogs have accumulated a huge amount of user generated content ( UGC ) containing human knowledge .", "With Yahoo !", "This ? list-based ?", "From a user aspect , our user study demonstrates that , with CET-based structure , users perform significantly better in knowledge learning than using traditional list-based approach .", "Answers as a test case , we conduct experiments and the results show the effectiveness of our framework in constructing CET .", "By using a large-scale entity repository , we design a three-step framework to organize UGC in a novel hierarchical structure called ? cluster entity tree ( CET ) ? .", "approach is simple , but hinders users from browsing and learning knowledge of certain topics effectively .", "To address this problem , we propose a hierarchical entity-based approach for structuralizing UGC in social media ."]}
{"orig_sents": ["1", "0", "2", "3", "5", "6", "4"], "shuf_sents": ["Instead of relying on annotated logical forms , which is especially expensive to obtain at large scale , we learn from question-answer pairs .", "In this paper , we train a semantic parser that scales up to Freebase .", "The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question .", "We tackle this problem in two ways : First , we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus .", "Additionally , we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline .", "Second , we use a bridging operation to generate additional predicates based on neighboring predicates .", "On the dataset of Cai and Yates ( 2013 ) , despite not having annotated logical forms , our system outperforms their state-of-the-art parser ."]}
{"orig_sents": ["7", "4", "5", "0", "3", "6", "1", "2"], "shuf_sents": ["and ? number of people living in ?", "The parser is learned from question-answer pairs , uses a probabilistic CCG to build linguistically motivated logicalform meaning representations , and includes an ontology matching model that adapts the output logical forms for each target ontology .", "Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets , including a nine point accuracy improvement on a recent Freebase QA corpus .", "can not be directly represented in Freebase , whose ontology instead encodes facts about gender , parenthood , and population .", "In such settings , the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology .", "For example , even simple phrases such as ? daughter ?", "In this paper , we introduce a new semantic parsing approach that learns to resolve such ontological mismatches .", "We consider the challenge of learning semantic parsers that scale to large , open-domain problems , such as question answering with Freebase ."]}
{"orig_sents": ["4", "2", "0", "1", "3"], "shuf_sents": ["Using a small amount of annotated data , we train an information extraction ( IE ) system to identify veterinary patient attributes .", "We then apply the IE system to a large collection of unannotated texts to produce a lexicon of veterinary patient attribute terms .", "We create a text classifier that incorporates automatically generated attribute lists for veterinary patients to tackle this problem .", "Our experimental results show that using the learned attribute lists to encode patient information in the text classifier yields improved performance on this task .", "The goal of our research is to distinguish veterinary message board posts that describe a case involving a specific patient from posts that ask a general question ."]}
{"orig_sents": ["5", "0", "1", "2", "3", "4"], "shuf_sents": ["In this paper , we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation : 1 ) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis , 2 ) and a probability cohesion model that further takes chain word translation probabilities into account .", "We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers .", "We then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models .", "We verify the effectiveness of the two models using a hierarchical phrase-based translation system .", "Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices .", "Lexical chains provide a representation of the lexical cohesion structure of a text ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["In this paper we introduce a convex relaxation of IBM Model 2 , and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates .", "The IBM translation models have been hugely influential in statistical machine translation ; they are the basis of the alignment models used in modern translation systems .", "Excluding IBM Model 1 , the IBM translation models , and practically all variants proposed in the literature , have relied on the optimization of likelihood functions or similar functions that are non-convex , and hence have multiple local optima .", "Our approach gives the same level of alignment accuracy as IBM Model 2 ."]}
{"orig_sents": ["3", "4", "0", "1", "2"], "shuf_sents": ["We examine finitestate and SMT-based methods for these related tasks , and demonstrate that the tasks have different characteristics ?", "finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not .", "We also show that we can increase accuracy by modeling syllable stress .", "Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa .", "Translation models can be used to derive character-level paraphrases on either side of this transduction , allowing for the automatic derivation of alternative pronunciations or spellings ."]}
{"orig_sents": ["2", "4", "0", "1", "3"], "shuf_sents": ["We propose disambiguation algorithms for a number of tensor-based models , which we then test on a variety of tasks .", "The results show that disambiguation can provide better compositional representation even for the case of tensor-based models .", "Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambiguation step .", "Furthermore , we confirm previous findings regarding the positive effect of disambiguation on vector mixture models , and we compare the effectiveness of the two approaches .", "The purpose of this paper is to generalise these ideas to tensor-based models , where relational words such as verbs and adjectives are represented by linear maps ( higher order tensors ) acting on a number of arguments ( vectors ) ."]}
{"orig_sents": ["0", "2", "4", "3", "5", "1"], "shuf_sents": ["We present Multi-Relational Latent Semantic Analysis ( MRLSA ) which generalizes Latent Semantic Analysis ( LSA ) .", "We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources , MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations , antonymy and is-a .", "MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor .", "Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix .", "Similar to LSA , a lowrank approximation of the tensor is derived using a tensor decomposition .", "The degree of two words having a specific relation can then be measured through simple linear algebraic operations ."]}
{"orig_sents": ["3", "2", "1", "0", "4"], "shuf_sents": ["Results reveal that , contrary to conclusions from prior work , the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons .", "We test the quality of the induced bilingual vector spaces , and analyze the influence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction ( BLE ) for two language pairs .", "The paper systematically introduces and describes all key elements of the bootstrapping procedure : ( 1 ) starting point or seed lexicon , ( 2 ) the confidence estimation and selection of new dimensions of the space , and ( 3 ) convergence .", "We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion .", "We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets ."]}
{"orig_sents": ["4", "3", "0", "1", "2"], "shuf_sents": ["Here , we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales ( e.g. , okay < good < excellent ) .", "We evaluate the scales on the indirect answers to yes/no questions corpus ( de Marneffe et al , 2010 ) .", "We obtain 72.8 % accuracy , which outperforms previous results ( ? 60 % ) on this corpus and highlights the quality of the scales extracted , providing further support that the continuous space word representations are meaningful .", "Mikolov et al ( 2013 ) show that these representations do capture syntactic and semantic regularities .", "Continuous space word representations extracted from neural network language models have been used effectively for natural language processing , but until recently it was not clear whether the spatial relationships of such representations were interpretable ."]}
{"orig_sents": ["4", "7", "2", "3", "0", "8", "5", "6", "1"], "shuf_sents": ["To address them , we introduce the Recursive Neural Tensor Network .", "Lastly , it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases .", "To remedy this , we introduce a Sentiment Treebank .", "It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality .", "Semantic word spaces have been very useful but can not express the meaning of longer phrases in a principled way .", "It pushes the state of the art in single sentence positive/negative classification from 80 % up to 85.4 % .", "The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7 % , an improvement of 9.7 % over bag of features baselines .", "Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .", "When trained on the new treebank , this model outperforms all previous methods on several metrics ."]}
{"orig_sents": ["0", "5", "1", "2", "4", "3"], "shuf_sents": ["We propose a novel approach to sentiment analysis for a low resource setting .", "This representation allows us to model sentiment detection as a sequence tagging problem , jointly discovering people and organizations along with whether there is sentiment directed towards them .", "We compare performance in both Spanish and English on microblog data , using only a sentiment lexicon as an external resource .", "Our models in English , trained on a much smaller dataset , are not yet statistically significant against their baselines .", "By leveraging linguisticallyinformed features within conditional random fields ( CRFs ) trained to minimize empirical risk , our best models in Spanish significantly outperform a strong baseline , and reach around 90 % accuracy on the combined task of named entity recognition and sentiment prediction .", "The intuition behind this work is that sentiment expressed towards an entity , targeted sentiment , may be viewed as a span of sentiment expressed across the entity ."]}
{"orig_sents": ["0", "6", "3", "1", "2", "4", "5"], "shuf_sents": ["Aspect extraction is one of the key tasks in sentiment analysis .", "To tackle the issue , some knowledge-based topic models have been proposed , which allow the user to input some prior domain knowledge to generate coherent aspects .", "However , existing knowledge-based topic models have several major shortcomings , e.g. , little work has been done to incorporate the can not -link type of knowledge or to automatically adjust the number of topics based on domain knowledge .", "However , such models without any domain knowledge often produce aspects that are not interpretable in applications .", "This paper proposes a more advanced topic model , called MC-LDA ( LDA with m-set and c-set ) , to address these problems , which is based on an Extended generalized P ? lya urn ( E-GPU ) model ( which is also proposed in this paper ) .", "Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly .", "In recent years , statistical models have been used for the task ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["In experiments , we observe BLEU gains of 1.2 to 1.8 across three different test sets .", "We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500 % .", "We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data ."]}
{"orig_sents": ["1", "2", "5", "3", "4", "0", "6"], "shuf_sents": ["Our approach relies on morphological analysis of the target language , but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer .", "Translation into morphologically rich languages is an important but recalcitrant problem in MT .", "We present a simple and effective approach that deals with the problem in two phases .", "Then , this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as ? synthetic ?", "phrases .", "First , a discriminative model is learned to predict inflections of target words from rich source-side annotations .", "We report significant improvements in translation quality when translating from English to Russian , Hebrew and Swahili ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["Our training and test data are made publicly available .", "We propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences .", "We present an approach to learning bilingual n-gram correspondences from relevance rankings of English documents for Japanese queries .", "We show in an experimental evaluation on patent prior art search that our approach , and in particular a consensus-based combination of boosting and translation-based approaches , yields substantial improvements in CLIR performance .", "We show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval ( CLIR ) ."]}
{"orig_sents": ["4", "1", "3", "5", "2", "0"], "shuf_sents": ["Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations .", "The models have a generation and a conditioning aspect .", "Secondly , we show that they are remarkably sensitive to the word order , syntax , and meaning of the source sentence despite lacking alignments .", "The generation of the translation is modelled with a target Recurrent Language Model , whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model .", "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words , phrases and sentences and do not rely on alignments or phrasal translation units .", "Through various experiments , we show first that our models obtain a perplexity with respect to gold translations that is > 43 % lower than that of stateof-the-art alignment-based translation models ."]}
{"orig_sents": ["4", "0", "6", "3", "5", "7", "2", "1", "8"], "shuf_sents": ["Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering ( QA ) ?", "Then , we present a method for extracting relations between the events , which exploits these structural properties by performing joint inference over the set of extracted relations .", "We represent processes by graphs whose edges describe a set of temporal , causal and co-reference event-event relations , and characterize the structural properties of these graphs ( e.g. , the graphs are connected ) .", "and ? Why ? ?", "Biological processes are complex phenomena involving a series of events that are related to one another through various relationships .", "questions .", "specifically ? How ? ?", "In this paper , we present the task of process extraction , in which events within a process and the relations between the events are automatically extracted from text .", "On a novel dataset containing 148 descriptions of biological processes ( released with this paper ) , we show significant improvement comparing to baselines that disregard process structure ."]}
{"orig_sents": ["5", "6", "4", "1", "7", "2", "0", "3"], "shuf_sents": ["In a human evaluation , our schemas outperform Chambers ? s schemas by wide margins on several evaluation criteria .", "This often leads to subject-verb-object triples that are not meaningful in the real-world .", "Our approach uses cooccurrence statistics of semantically typed relational triples , which we call Rel-grams ( relational n-grams ) .", "Both Rel-grams and event schemas are freely available to the research community .", "It is due in part to their pair-wise representation that treats subjectverb independently from verb-object .", "Chambers and Jurafsky ( 2009 ) demonstrated that event schemas can be automatically induced from text corpora .", "However , our analysis of their schemas identifies several weaknesses , e.g. , some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor .", "We present a novel approach to inducing open-domain event schemas that overcomes these limitations ."]}
{"orig_sents": ["1", "3", "0", "4", "2"], "shuf_sents": ["In this paper , we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics , but then computes latent relations between these .", "Cross-lingual topic modelling has applications in machine translation , word sense disambiguation and terminology alignment .", "We show that on a crosslingual mate retrieval task , our model significantly outperforms LDA , LSI , and ESA , as well as a baseline that translates every word in a document into the target language .", "Multilingual extensions of approaches based on latent ( LSI ) , generative ( LDA , PLSI ) as well as explicit ( ESA ) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages .", "Thus , the method combines the benefits of both explicit and latent topic modelling approaches ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["Various linguistic and statistical features are utilized to facilitate the learning algorithms .", "Previous approaches for automated essay scoring ( AES ) learn a rating model by minimizing either the classification , regression , or pairwise classification loss , depending on the learning algorithm used .", "In this paper , we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters .", "Experiments on the publicly available English essay dataset , Automated Student Assessment Prize ( ASAP ) , show that our proposed approach outperforms the state-of-the-art algorithms , and achieves performance comparable to professional human raters , which suggests the effectiveness of our proposed method for automated essay scoring .", "To this end , we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model , where the agreement between the human and machine raters is directly incorporated into the loss function ."]}
{"orig_sents": ["4", "2", "1", "3", "0"], "shuf_sents": ["Closer analyses lead to several new insights into characteristics of the writing style in successful literature , including findings that are contrary to the conventional wisdom with respect to good writing style and readability .", "Based on novels over several different genres , we probe the predictive power of statistical stylometry in discriminating successful literary works , and identify characteristic stylistic elements that are more prominent in successful writings .", "We examine the quantitative connection , if any , between writing style and successful literature .", "Our study reports for the first time that statistical stylometry can be surprisingly effective in discriminating highly successful literature from less successful counterpart , achieving accuracy up to 84 % .", "Predicting the success of literary works is a curious question among publishers and aspiring writers alike ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model .", "Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts ( and their interactions ) .", "We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance ."]}
{"orig_sents": ["5", "2", "3", "0", "1", "4"], "shuf_sents": ["We encode the heuristics in a probabilistic graphical model to create the NEWSSPIKE algorithm for mining news streams .", "We present experiments demonstrating that NEWSSPIKE significantly outperforms several competitive baselines .", "Unfortunately , these methods have several drawbacks , such as confusing synonyms with antonyms and causes with effects .", "This paper introduces three Temporal Correspondence Heuristics , that characterize regularities in parallel news streams , and shows how they may be used to generate high precision paraphrases for event relations .", "In order to spur further research , we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE .", "The distributional hypothesis , which states that words that occur in similar contexts tend to have similar meanings , has inspired several Web mining algorithms for paraphrasing semantically equivalent phrases ."]}
{"orig_sents": ["1", "3", "4", "0", "5", "2"], "shuf_sents": ["In this paper we introduce a novel approach to Wikification by incorporating , along with statistical methods , richer relational analysis of the text .", "Wikification , commonly referred to as Disambiguation to Wikipedia ( D2W ) , is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages .", "Our results show significant improvements in both Wikification and the TAC Entity Linking task .", "Previous approaches to D2W focused on the use of local and global statistics over the given text , Wikipedia articles and its link structures , to evaluate context compatibility among a list of probable candidates .", "However , these methods fail ( often , embarrassingly ) , when some level of text understanding is needed to support Wikification .", "We provide an extensible , efficient and modular Integer Linear Programming ( ILP ) formulation of Wikification that incorporates the entity-relation inference problem , and show that the ability to identify relations in text helps both candidate generation and ranking Wikipedia titles considerably ."]}
{"orig_sents": ["3", "7", "4", "6", "1", "0", "5", "2"], "shuf_sents": ["It also provides an interesting contrast with a recent HMM-based model .", "Our generative model is conceptually simpler than the pipelined approach and requires far less training data .", "Our generative model matches the pipeline ? s performance , and outperforms the HMM by 7 F1 points ( 20 % ) .", "Event schema induction is the task of learning high-level representations of complex events ( e.g. , a bombing ) and their entity roles ( e.g. , perpetrator and victim ) from unlabeled text .", "Recent research suggests event schemas can be learned from raw text .", "We evaluate on a common dataset for template schema extraction .", "Inspired by a pipelined learner based on named entity coreference , this paper presents the first generative model for schema induction that integrates coreference chains into learning .", "Event schemas have important connections to early NLP research on frames and scripts , as well as modern applications like template extraction ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We show that soft constraints give statistically significant performance improvements when compared to hard constraints .", "IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference .", "This paper introduces IQPs ( Integer Quadratic Programs ) as a way to model joint inference for the task of concept recognition in clinical domain ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Different demographics , e.g. , gender or age , can demonstrate substantial variation in their language use , particularly in informal contexts such as social media .", "We show that gender differences in subjective language can effectively be used to improve sentiment analysis , and in particular , polarity classification for Spanish and Russian .", "In this paper we focus on learning gender differences in the use of subjective language in English , Spanish , and Russian Twitter data , and explore cross-cultural differences in emoticon and hashtag use for male and female users .", "Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5 % and 1 % for Russian , 2 % and 0.5 % for Spanish , and 2.5 % and 5 % for English for polarity and subjectivity classification ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["A very valuable piece of information in newspaper articles is the tonality of extracted statements .", "To this end , we will compare several state-of-the-art approaches for Opinion Mining in newspaper articles in this paper .", "In the evaluation , we use two different corpora consisting of news articles , by which we show that the new approach achieves better results than the four state-of-the-art methods .", "Furthermore , we will introduce a new technique to extract entropy-based word connections which identifies the word combinations which create a tonality .", "For the analysis of tonality of newspaper articles either a big human effort is needed , when it is carried out by media analysts , or an automated approach which has to be as accurate as possible for a Media Response Analysis ( MRA ) ."]}
{"orig_sents": ["1", "5", "2", "0", "3", "6", "4"], "shuf_sents": ["We propose an unsupervised label propagation algorithm to address the problem .", "Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style .", "Such fine-grained word-level task has not been well investigated in microblogs yet .", "The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets .", "Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms .", "In this paper , we study the problem of extracting opinion targets of Chinese microblog messages .", "Topics in microblogs are identified by hashtags or using clustering algorithms ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents an approach for detecting promotional content in Wikipedia .", "By incorporating stylometric features , including features based on n-gram and PCFG language models , we demonstrate improved accuracy at identifying promotional articles , compared to using only lexical information and metafeatures ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We evaluate the resulting representation ? s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions .", "We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words .", "We explore Debatepedia , a communityauthored encyclopedia of sociopolitical debates , as evidence for inferring a lowdimensional , human-interpretable representation in the domain of issues and positions ."]}
{"orig_sents": ["6", "5", "4", "2", "1", "0", "7", "3"], "shuf_sents": ["We further propose a duration-based regularization component to find bursty events .", "We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events .", "In this paper , we try to model topics , events and users on Twitter in a unified way .", "Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics .", "Moreover , people ? s posting behaviors on events are often closely tied to their personal interests .", "On the one hand , people tweets about their daily lives , and on the other hand , when major events happen , people also follow and tweet about them .", "With the rapid growth of social media , Twitter has become one of the most widely adopted platforms for people to post short and instant message .", "We also propose to use event-topic affinity vectors to model the association between events and topics ."]}
{"orig_sents": ["2", "3", "6", "0", "1", "4", "5"], "shuf_sents": ["We introduce the concept of an author ? s unique ? signature ? , and show that such signatures are typical of many authors when writing very short texts .", "We also present a new authorship attribution feature ( ? flexible patterns ? )", "Work on authorship attribution has traditionally focused on long texts .", "In this work , we tackle the question of whether the author of a very short text can be successfully identified .", "and demonstrate a significant improvement over our baselines .", "Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task .", "We use Twitter as an experimental testbed ."]}
{"orig_sents": ["2", "4", "5", "1", "3", "7", "8", "0", "6"], "shuf_sents": ["We address the interesting question how well training an outof-the-box SRL model works for English data .", ": the entity evaluated positively , the entity evaluated negatively , and the aspect under which the comparison is made .", "This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling ( SRL ) system on product reviews for the new task of detecting comparisons .", "In user-generated product reviews , the ? predicate ?", "An ( opinionated ) comparison consists of a comparative ? predicate ?", "and up to three ? arguments ?", "We observe that even without any feature engineering or other major adaptions to our task , the system outperforms a reasonable heuristic baseline in all steps ( predicate identification , argument identification and argument classification ) and in three different datasets .", "and ? arguments ?", "are expressed in highly heterogeneous ways ; but since the elements are textually annotated in existing datasets , SRL is technically applicable ."]}
{"orig_sents": ["4", "0", "2", "3", "1"], "shuf_sents": ["Graphics Processing Units ( GPUs ) have previously been used to accelerate CKY chart evaluation , but gains over CPU parsers were modest .", "The techniques we introduce include grammar compilation , recursive symbol blocking , and cache-sharing .", "In this paper , we describe a collection of new techniques that enable chart evaluation at close to the GPU ? s practical maximum speed ( a Teraflop ) , or around a half-trillion rule evaluations per second .", "Net parser performance on a 4-GPU system is over 1 thousand length30 sentences/second ( 1 trillion rules/sec ) , and 400 general sentences/second for the Berkeley Parser Grammar .", "Constituency parsing with rich grammars remains a computational challenge ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features .", "In this work , we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases , as obtained from their compositionally-derived distributional semantic representations , can resolve syntactic ambiguities .", "We exploit this idea to choose the correct parsing of NPs ( e.g. , ( live fish ) transporter rather than live ( fish transporter ) ) ."]}
{"orig_sents": ["2", "0", "1", "4", "3", "5"], "shuf_sents": ["Despite an extremely large space of possible expressions , we demonstrate effective learning of a globally normalized log-linear distribution .", "This learning is enabled by a new , multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms .", "We present a new approach to referring expression generation , casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world .", "Experiments show the approach is able to learn accurate models , which generate over 87 % of the expressions people used .", "We train and evaluate the approach on a new corpus of references to sets of visual objects .", "Additionally , on the previously studied special case of single object reference , we show a 35 % relative error reduction over previous state of the art ."]}
{"orig_sents": ["3", "2", "0", "4", "1"], "shuf_sents": ["Given a dataset of target words , their sentential contexts and the potential substitutions for the target words , the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness .", "On two datasets widely used for lexical substitution , our best models significantly advance the state-of-the-art .", "In this paper , we tackle this task as a supervised ranking problem .", "The problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task .", "As a key contribution , we customize and evaluate several learning-to-rank models to the lexical substitution task , including classification-based and regression-based approaches ."]}
{"orig_sents": ["4", "1", "5", "0", "3", "2"], "shuf_sents": ["We introduce a semi-supervised manifold ranking algorithm for this task , which relies on a small set of labeled individual reviews for training .", "fake reviews written to sound authentic and deliberately mislead readers .", "Experiments on a novel dataset of hotel reviews show that the proposed method outperforms state-of-art learning baselines .", "Then , in the absence of gold standard labels ( at an offering level ) , we introduce a novel evaluation procedure that ranks artificial instances of real offerings , where each artificial offering contains a known number of injected deceptive reviews .", "Recent work has developed supervised methods for detecting deceptive opinion spam ?", "And whereas past work has focused on identifying individual fake reviews , this paper aims to identify offerings ( e.g. , hotels ) that contain fake reviews ."]}
{"orig_sents": ["0", "3", "1", "5", "2", "4", "6"], "shuf_sents": ["Recommendation systems ( RS ) take advantage of products and users information in order to propose items to consumers .", "In contrast , we propose a new domain-independent semantic RS .", "The system includes a new similarity measure keeping up both the accuracy of rating predictions and coverage .", "Collaborative , content-based and a few hybrid RS have been developed in the past .", "We propose an innovative way to apply a fast adaptation scheme at a semantic level , providing recommendations and arguments in phase with the very recent past .", "By providing textually well-argued recommendations , we aim to give more responsibility to the end user in his decision .", "We have performed several experiments on films data , providing textually well-argued recommendations ."]}
{"orig_sents": ["3", "4", "0", "1", "5", "2", "7", "6"], "shuf_sents": ["Second , it is commonly used on a noisy , non-convex loss function that becomes more difficult to optimize as the number of parameters increases .", "To address these issues , we study the addition of a regularization term to the MERT objective function .", "and present methods for efficiently integrating them during search .", "Minimum Error Rate Training ( MERT ) remains one of the preferred methods for tuning linear parameters in machine translation systems , yet it faces significant issues .", "First , MERT is an unregularized learner and is therefore prone to overfitting .", "Since standard regularizers such as `2 are inapplicable to MERT due to the scale invariance of its objective function , we turn to two regularizers ? `0 and a modification of `2 ?", "Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO , a learner often used with large feature sets .", "To improve search in large parameter spaces , we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT ? s exact line searches ."]}
{"orig_sents": ["1", "2", "0", "5", "4", "3"], "shuf_sents": ["These image-based models should be complementary to text-based ones , providing a more cognitively plausible view of meaning grounded in visual perception .", "Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora .", "Recently , the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections .", "Despite some unsatisfactory , but explained outcomes ( in particular , failure to detect differential association of models with brain areas ) , the results show , on the one hand , that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain , and , on the other , that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility .", "Our results indicate that , indeed , there is a significant correlation between image-based and brain-based semantic similarities , and that image-based models complement text-based ones , so that the best correlations are achieved when the two modalities are combined .", "In this study , we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal ."]}
{"orig_sents": ["1", "3", "5", "0", "4", "2"], "shuf_sents": ["without crafted heuristics .", "Classical coreference systems encode various syntactic , discourse , and semantic phenomena explicitly , using heterogenous features computed from hand-crafted heuristics .", "Nonetheless , our final system1 outperforms the Stanford system ( Lee et al ( 2011 ) , the winner of the CoNLL 2011 shared task ) by 3.5 % absolute on the CoNLL metric and outperforms the IMS system ( Bjo ? rkelund and Farkas ( 2012 ) , the best publicly available English coreference system ) by 1.9 % absolute .", "In contrast , we present a state-of-the-art coreference system that captures such phenomena implicitly , with a small number of homogeneous feature templates examining shallow properties of mentions .", "These features are successful on syntax and discourse ; however , they do not model semantic compatibility well , nor do we see gains from experiments with shallow semantic features from the literature , suggesting that this approach to semantics is an ? uphill battle . ?", "Surprisingly , our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena , allowing us to win ? easy victories ?"]}
{"orig_sents": ["5", "2", "3", "1", "0", "8", "7", "4", "6"], "shuf_sents": ["Experiments on grammar induction show that pursuing different transforms ( e.g. , discarding parts of a learned model or ignoring portions of training data ) results in improvements .", "Our building blocks are operators of two types : ( i ) transform , which suggests new places to search , via non-random restarts from already-found local optima ; and ( ii ) join , which merges candidate solutions to find better optima .", "But accuracy tends to suffer with current techniques , which often explore either too narrowly or too broadly : hill-climbers can get stuck in local optima , whereas samplers may be inefficient .", "We propose to arrange individual local optimizers into organized networks .", "Our complete system achieves 48.6 % accuracy ( directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data ) ?", "Many statistical learning problems in NLP call for local model search methods .", "more than 5 % higher than the previous state-of-the-art .", "Using these tools , we designed several modular dependency grammar induction networks of increasing complexity .", "Groups of locally-optimal solutions can be further perturbed jointly , by constructing mixtures ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["To this end , we use automatically word aligned bitext between the source and target language pair , and learn a discriminative conditional random field model on the target side .", "Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information .", "We show improvements over strong baselines for two tasks : part-of-speech tagging and namedentity segmentation .", "We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization ."]}
{"orig_sents": ["1", "3", "0", "4", "2"], "shuf_sents": ["We convert the clusters into graphs , add smoothing/syntacticinformation-carrier vertices , and compute the similarity between phrases with a random walk-based measure , the commute time .", "We describe a novel method that extracts paraphrases from a bitext , for both the source and target languages .", "The co-occurrence count distribution belongs to the power-law family .", "In order to reduce the search space , we decompose the phrase-table into sub-phrase-tables and construct separate clusters for source and target phrases .", "The resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial cooccurrence counts with a novel technique ."]}
{"orig_sents": ["3", "5", "6", "2", "0", "4", "1"], "shuf_sents": ["This intuition is encoded as a distance-dependent CRP with a distance between two syntactic signatures indicating how likely they are to correspond to a single semantic role .", "Both models achieve state-of-the-art results when evaluated on PropBank , with the coupled model consistently outperforming the factored counterpart in all experimental set-ups .", "In a more refined hierarchical model , we inject the intuition that the clusterings are similar across different predicates , even though they are not necessarily identical .", "We introduce two Bayesian models for unsupervised semantic role labeling ( SRL ) task .", "These distances are automatically induced within the model and shared across predicates .", "The models treat SRL as clustering of syntactic signatures of arguments with clusters corresponding to semantic roles .", "The first model induces these clusterings independently for each predicate , exploiting the Chinese Restaurant Process ( CRP ) as a prior ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["We introduce two ways to detect entailment using distributional semantic representations of phrases .", "Our first experiment shows that the entailment relation between adjective-noun constructions and their head nouns ( big cat |= cat ) , once represented as semantic vector pairs , generalizes to lexical entailment among nouns ( dog |= animal ) .", "Moreover , nominal and quantifier phrase entailment appears to be cued by different distributional correlates , as predicted by the type-based view of entailment in formal semantics .", "Our second experiment shows that a classifier fed semantic vector pairs can similarly generalize the entailment relation among quantifier phrases ( many dogs|=some dogs ) to entailment involving unseen quantifiers ( all cats|=several cats ) ."]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["However , the syntactic contexts which are modelled are usually severely limited , a fact which is reflected in the lexical-level WSD-like evaluation methods used .", "In this paper , we broaden the scope of these models to build sentence-level representations , and argue that phrase representations are best evaluated in terms of the inference decisions that they support , invariant to the particular syntactic constructions used to guide composition .", "A major focus of current work in distributional models of semantics is to construct phrase representations compositionally from word representations .", "We find that the models outperform a simple lemma overlap baseline slightly , demonstrating that distributional approaches can already be useful for tasks requiring deeper inference .", "We propose two evaluation methods in relation classification and QA which reflect these goals , and apply several recent compositional distributional models to the tasks ."]}
{"orig_sents": ["4", "0", "2", "1", "3"], "shuf_sents": ["Converting outputs from one framework to another is less than optimal as it easily introduces noise into the process .", "This extends a previously proposed framework for cross-theory evaluation and allows us to compare a wider class of parsers .", "Here we present a principled protocol for evaluating parsing results across frameworks based on function trees , tree generalization and edit distance metrics .", "We demonstrate the usefulness and language independence of our procedure by evaluating constituency and dependency parsers on English and Swedish .", "A serious bottleneck of comparative parser evaluation is the fact that different parsers subscribe to different formal frameworks and theoretical assumptions ."]}
{"orig_sents": ["4", "3", "0", "2", "1"], "shuf_sents": ["We show that the results achieved by state-of-the-art data-driven parsers on Hungarian and English ( which is at the other end of the configurational-nonconfigurational spectrum ) are quite similar to each other in terms of attachment scores .", "This analysis highlights that addressing the language-specific phenomena is required for a further remarkable error reduction .", "We reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages .", "Here , we introduce results on dependency parsing of Hungarian that employ a 80K , multi-domain , fully manually annotated corpus , the Szeged Dependency Treebank .", "Hungarian is a stereotype of morphologically rich and non-configurational languages ."]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["We apply this approach to obtain undirected variants of the planar and 2-planar parsers and of Covington ? s non-projective parser .", "We introduce a new approach to transitionbased dependency parsing in which the parser does not directly construct a dependency structure , but rather an undirected graph , which is then converted into a directed dependency tree in a post-processing step .", "Undirected parsers can be obtained by simplifying existing transition-based parsers satisfying certain conditions .", "We perform experiments on several datasets from the CoNLL-X shared task , showing that these variants outperform the original directed algorithms in most of the cases .", "This alleviates error propagation , since undirected parsers do not need to observe the single-head constraint ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy .", "In this paper , we describe a model that takes into account complete structures as they become available to rescore the elements of a beam , combining the advantages of transition-based and graph-based approaches .", "We apply the new transition-based parser on typologically different languages such as English , Chinese , Czech , and German and report competitive labeled and unlabeled attachment scores .", "Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available ."]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["key-word matching .", "In this paper we describe an algorithm that addresses this problem , but rather than looking at it on a term matching/term reformulation level , we focus on the syntactic differences between questions and relevant text passages .", "In Information Retrieval ( IR ) in general and Question Answering ( QA ) in particular , queries and relevant textual content often significantly differ in their properties and are therefore difficult to relate with traditional IR methods , e.g .", "We evaluate our algorithm in a QA setting , and show that it outperforms a baseline that uses only dependency information contained in the questions by 300 % and that it also improves performance of a state of the art QA system significantly .", "To this end we propose a novel algorithm that analyzes dependency structures of queries and known relevant text passages and acquires transformational patterns that can be used to retrieve relevant textual content ."]}
{"orig_sents": ["1", "8", "7", "0", "5", "6", "2", "4", "3"], "shuf_sents": ["In a nutshell , answers are extracted from clicked web-snippets originating from any class of web-site , including Knowledge Bases ( KBs ) .", "In this paper , we examined click patterns produced by users of Yahoo !", "These testing queries were also submitted by search engine users , and their answer candidates were taken from their respective returned web-snippets .", "In particular , our results underline the importance of non-KB training data .", "This corpus helped both techniques to finish with an accuracy higher than 70 % , and to predict over 85 % of the answers clicked by users .", "On the other hand , nonanswers are acquired from redundant pieces of text across web-snippets .", "The effectiveness of this corpus was assessed via training two state-of-the-art models , wherewith answers to unseen queries were distinguished .", "Regularities across these click patterns are then utilized for constructing a large and heterogeneous training corpus for answer ranking .", "search engine when prompting definition questions ."]}
{"orig_sents": ["6", "3", "0", "4", "2", "5", "1"], "shuf_sents": ["We propose two ways to achieve the adaptation effect and both of them are aimed at tuning parameter weights on a set of parallel queries .", "We show that these improvements are due both to the integration of the adaptation and syntax-features for the query translation task .", "We also extend the second approach by using syntax-based features .", "In the scenario that we focus on access to the document collection itself is not available and changes to the IR model are not possible .", "The first approach is via a standard tuning procedure optimizing for BLEU score and the second one is via a reranking approach optimizing for MAP score .", "Our experiments show improvements of 1-2.5 in terms of MAP score over the retrieval with the non-adapted translation .", "This work proposes to adapt an existing general SMT model for the task of translating queries that are subsequently going to be used to retrieve information from a target language collection ."]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["The quality of this search space can thus be evaluated by computing the best achievable hypothesis in the lattice , the so-called oracle hypothesis .", "For common SMT metrics , this problem is however NP-hard and can only be solved using heuristics .", "The search space of Phrase-Based Statistical Machine Translation ( PBSMT ) systems can be represented under the form of a directed acyclic graph ( lattice ) .", "These new decoders are positively evaluated and compared with several alternatives from the literature for three language pairs , using lattices produced by two PBSMT systems .", "In this work , we present two new methods for efficiently computing BLEU oracles on lattices : the first one is based on a linear approximation of the corpus BLEU score and is solved using the FST formalism ; the second one relies on integer linear programming formulation and is solved directly and using the Lagrangian relaxation framework ."]}
{"orig_sents": ["7", "3", "6", "1", "2", "0", "5", "4"], "shuf_sents": ["In this paper , we examine an idealization where a phrase-table is given .", "We report translation results for an end-to-end translation system using these monolingual features alone .", "Our method only requires monolingual corpora in source and target languages , a small bilingual dictionary , and a small bitext for tuning feature weights .", "We extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for MT-scale phrasetables .", "We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features .", "We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80 % + of the loss can be recovered with monolingually estimated features alone .", "We propose a novel algorithm to estimate reordering probabilities from monolingual data .", "We estimate the parameters of a phrasebased statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Our pivot translations outperform the baselines by a large margin .", "In this paper we investigate the use of character-level translation models to support the translation from and to underresourced languages and textual domains via closely related pivot languages .", "Our experiments show that these low-level models can be successful even with tiny amounts of training data .", "We test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task ."]}
{"orig_sents": ["4", "1", "3", "6", "0", "5", "2"], "shuf_sents": ["Experimental results not only report significant improvements over random sentence selection but also an improvement over a system trained with the whole available data .", "However , it is not clear whether all the training data actually help or not .", "Afterwards , we show that a much larger room for improvement exists , although this is done under non-realistic conditions .", "A system trained on a subset of such huge bilingual corpora might outperform the use of all the bilingual data .", "Nowadays , there are large amounts of data available to train statistical machine translation systems .", "Surprisingly , the improvements are obtained with just a small fraction of the data that accounts for less than 0.5 % of the sentences .", "This paper studies such issues by analysing two training data selection techniques : one based on approximating the probability of an indomain corpus ; and another based on infrequent n-gram occurrence ."]}
{"orig_sents": ["3", "5", "6", "1", "0", "4", "2"], "shuf_sents": ["favor recall over precision ?", "We train a sequence model and show that a simple modification to the online learner ? a loss function encouraging it to ? arrogantly ?", "We then adapt our model with self-training on unlabeled target-domain data ; enforcing the same recall-oriented bias in the selftraining stage yields marginal gains.1", "We consider the problem of NER in Arabic Wikipedia , a semisupervised domain adaptation setting for which we have no labeled training data in the target domain .", "substantially improves recall and F1 .", "To facilitate evaluation , we obtain annotations for articles in four topical groups , allowing annotators to identify domain-specific entity types in addition to standard categories .", "Standard supervised learning on newswire text leads to poor target-domain recall ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["We analyse the effect of using several tree representations .", "Two aspects make the task more difficult with respect to previous NER tasks : i ) named entities annotated used in this work have a tree structure , thus the task can not be tackled as a sequence labelling task ; ii ) the data used are more noisy than data used for previous NER tasks .", "In this paper we deal with Named Entity Recognition ( NER ) on transcriptions of French broadcast data .", "We approach the task in two steps , involving Conditional Random Fields and Probabilistic Context-Free Grammars , integrated in a single parsing algorithm .", "Our system outperforms the best system of the evaluation campaign by a significant margin ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Previous research has mainly focused on temporal links for events , and we extend that work to include fluents as well , presenting a common methodology for linking both events and relations to timestamps within the same sentence .", "Our best systems achieve F1-scores of 0.76 on events and 0.72 on fluents .", "Our approach combines tree kernels with classical feature-based learning to exploit context and achieves competitive F1-scores on event-time linking , and comparable F1scores for fluents .", "We present work on linking events and fluents ( i.e. , relations that hold for certain periods of time ) to temporal information in text , which is an important enabler for many applications such as timelines and reasoning ."]}
{"orig_sents": ["3", "4", "2", "6", "0", "1", "5"], "shuf_sents": ["Our model uses these seeds to improve both topicword distributions ( by biasing topics to produce appropriate seed words ) and to improve document-topic distributions ( by biasing documents to select topics related to the seed words they contain ) .", "Extrinsic evaluation on a document clustering task reveals a significant improvement when using seed information , even over other models that use seed information na ?", "We propose a simple and effective way to guide topic models to learn topics of specific interest to a user .", "Topic models have great potential for helping users understand document corpora .", "This potential is stymied by their purely unsupervised nature , which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks ( Chang et al 2009 ) .", "? vely .", "We achieve this by providing sets of seed words that a user believes are representative of the underlying topics in a corpus ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The new model , called DUALSUM , results in the second or third position in terms of the ROUGE metrics when tuned for previous TAC competitions and tested on TAC-2011 , being statistically indistinguishable from the winning system .", "Update summarization is a new challenge in multi-document summarization focusing on summarizing a set of recent documents relatively to another set of earlier documents .", "A manual evaluation of the generated summaries shows state-of-the art results for DUALSUM with respect to focus , coherence and overall responsiveness .", "We present an unsupervised probabilistic approach to model novelty in a document collection and apply it to the generation of update summaries ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["The learning method applies to all submodular summarization methods , and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets .", "In this paper , we present a supervised learning approach to training submodular scoring functions for extractive multidocument summarization .", "By taking a structured prediction approach , we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure .", "Compared to state-of-theart functions that were tuned manually , our method significantly improves performance and enables high-fidelity models with number of parameters well beyond what could reasonably be tuned by hand ."]}
{"orig_sents": ["5", "6", "2", "1", "3", "0", "4"], "shuf_sents": ["In addition , it models such aspects of child acquisition as ? fast mapping , ?", "We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian expectation maximization .", "The learner then has to infer the meanings and syntactic properties of the words in the input along with a parsing model .", "When tested on utterances from the CHILDES corpus , our learner outperforms a state-of-the-art semantic parser .", "while also countering previous criticisms of statistical syntactic learners .", "This paper presents an incremental probabilistic learner that models the acquistion of syntax and semantics from a corpus of child-directed utterances paired with possible representations of their meanings .", "These meaning representations approximate the contextual input available to the child ; they do not specify the meanings of individual words or syntactic derivations ."]}
{"orig_sents": ["5", "6", "0", "2", "1", "3", "4"], "shuf_sents": ["An effective approach to translate text documents is to follow an interactive-predictive paradigm in which both the system is guided by the user and the user is assisted by the system to generate error-free translations .", "Is in this scenario where the use of active learning techniques is compelling .", "Unfortunately , when processing such unbounded data streams even this approach requires an overwhelming amount of manpower .", "In this work , we propose different active learning techniques for interactive machine translation .", "Results show that for a given translation quality the use of active learning allows us to greatly reduce the human effort required to translate the sentences in the stream .", "Translation needs have greatly increased during the last years .", "In many situations , text to be translated constitutes an unbounded stream of data that grows continually with time ."]}
{"orig_sents": ["4", "1", "2", "0", "5", "7", "3", "6"], "shuf_sents": ["We reconfirm that this is indeed the case , but emphasize the importance of using also texts translated in the ? wrong ?", "However , much research in Translation Studies indicates that the direction of translation matters , as translated language ( translationese ) has many unique properties .", "Specifically , phrase tables constructed from parallel corpora translated in the same direction as the translation task perform better than ones constructed from corpora translated in the opposite direction .", "We define entropybased measures that estimate the correspondence of target-language phrases to translationese , thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation .", "Translation models used for statistical machine translation are compiled from parallel corpora ; such corpora are manually translated , but the direction of translation is usually unknown , and is consequently ignored .", "direction .", "We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent , statistically significant improvement in the quality of the translation .", "We take advantage of information pertaining to the direction of translation in constructing phrase tables , by adapting the translation model to the special properties of translationese ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["In this paper we investigate the relevance of aspectual type for the problem of temporal information processing , i.e .", "We then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way .", "The improved performance of the resulting models shows that ( i ) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that ( ii ) temporal information processing can profit from information about aspectual type .", "For a large list of verbs , we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with specific aspectual types .", "the problems of the recent TempEval challenges ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["In this paper , we define a new type of summary for sentiment analysis : a singlesentence summary that consists of a supporting sentence that conveys the overall sentiment of a review as well as a convincing reason for this sentiment .", "We present a system for extracting supporting sentences from online product reviews , based on a simple and unsupervised method .", "We design a novel comparative evaluation method for summarization , using a crowdsourcing service .", "The evaluation shows that our sentence extraction method performs better than a baseline of taking the sentence with the strongest sentiment ."]}
{"orig_sents": ["3", "1", "5", "2", "4", "0"], "shuf_sents": ["The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set , and achieves performance levels comparable to supervised training with 700 manually annotated documents .", "Due to the domain-specificity of this task , event extraction systems must be retrained with new annotated data for each domain .", "We aim to rapidly train a state-of-the-art event extraction system using a small set of ? seed nouns ?", "Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents .", "for each event role , a collection of relevant ( in-domain ) and irrelevant ( outof-domain ) texts , and a semantic dictionary .", "In this paper , we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["In this paper , we describe a new approach to semi-supervised adaptive learning of event extraction from text .", "Given a set of exam-ples and an un-annotated text corpus , the BEAR system ( Bootstrapping Events And Relations ) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text , such as events involving multiple entities and their roles .", "A series of evaluations using the ACE data and event set show a signifi-cant performance improvement over our baseline system .", "For example , given a series of descriptions of bombing and shooting inci-dents ( e.g. , in newswire ) the system will learn to extract , with a high degree of accu-racy , other attack-type events mentioned elsewhere in text , irrespective of the form of description ."]}
{"orig_sents": ["2", "0", "3", "1", "4"], "shuf_sents": ["In this paper we begin to address this problem by building a novel , color-emotion-concept association lexicon via crowdsourcing .", "We investigate the relation between color and concept , and color and emotion , reinforcing results from previous studies , as well as discovering new associations .", "Existing concept-color-emotion lexicons limit themselves to small sets of basic emotions and colors , which can not capture the rich pallet of color terms that humans use in communication .", "This lexicon , which we call CLEX , has over 2,300 color terms , over 3,000 affect terms and almost 2,000 concepts .", "We also investigate cross-cultural differences in color-emotion associations between US and India-based annotators ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["We associate multiple ranks with the set of permutations originating from the same source document , as opposed to the original pairwise rankings .", "We extend the original entity-based coherence model ( Barzilay and Lapata , 2008 ) by learning from more fine-grained coherence preferences in training data .", "With no additional manual annotations required , our extended model is able to outperform the original model on two tasks : sentence ordering and summary coherence rating .", "We also study the effect of the permutations used in training , and the effect of the coreference component used in entity extraction ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We show that generalization causes significant improvements and that the impact of improvement depends on the type of classifier and on how much training and test data differ from each other .", "In this paper , we compare three different generalization methods for in-domain and cross-domain opinion holder extraction being simple unsupervised word clustering , an induction method inspired by distant supervision and the usage of lexical resources .", "We also address the less common case of opinion holders being realized in patient position and suggest approaches including a novel ( linguisticallyinformed ) extraction method how to detect those opinion holders without labeled training data as standard datasets contain too few instances of this type .", "The generalization methods are incorporated into diverse classifiers ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Our work aims to answer key questions about how best to ( 1 ) identify representative event chains from a source text , ( 2 ) gather statistics from the event chains , and ( 3 ) choose ranking functions for predicting new script events .", "We design , evaluate and compare different methods for constructing models for script event prediction : given a partial chain of events in a script , predict other events that are likely to belong to the script .", "In this paper , we extend current state-of-theart research on unsupervised acquisition of scripts , that is , stereotypical and frequently observed sequences of events .", "We make several contributions , introducing skip-grams for collecting event statistics , designing improved methods for ranking event predictions , defining a more reliable evaluation metric for measuring predictiveness , and providing a systematic analysis of the various event prediction models ."]}
{"orig_sents": ["2", "1", "0", "3", "4"], "shuf_sents": ["The approach is based on supervised machine learning using language model probabilities , string similarity measured over different representations of user edits , comparison of part-of-speech tags and named entities , and a set of adaptive features extracted from large amounts of unlabeled user edits .", "In this paper we introduce a scalable approach for automatically distinguishing between factual and fluency edits in document revision histories .", "Document revision histories are a useful and abundant source of data for natural language processing , but selecting relevant data for the task at hand is not trivial .", "Applied to contiguous edit segments , our method achieves statistically significant improvements over a simple yet effective edit-distance baseline .", "It reaches high classification accuracy ( 88 % ) and is shown to generalize to additional sets of unseen data ."]}
{"orig_sents": ["0", "4", "1", "2", "6", "3", "9", "7", "8", "5"], "shuf_sents": ["Online community is an important source for latest news and information .", "In this paper , we develop a recommendation system for online forums .", "There are a lot of differences between online forums and formal media .", "Content topics in the same forum are more focused than sources like news websites .", "Accurate prediction of a user ? s interest can help provide better user experience .", "Our experimental results demonstrate that our proposed hybrid approach works well in all three types of forums .", "For example , content generated by users in online forums contains more noise compared to formal documents .", "In our recommendation system , we propose to ( a ) use latent topics to interpolate with content-based recommendation ; ( b ) model latent user groups to utilize information from other users .", "We have collected three types of forum data sets .", "Some of these differences present challenges to traditional word-based user profiling and recommendation systems , but some also provide opportunities for better recommendation performance ."]}
{"orig_sents": ["4", "0", "3", "1", "5", "2"], "shuf_sents": ["From a corpus labeled with grammatical dependencies , PONG learns the distribution of word relations for each POS N-gram .", "We derive the probability that one word has a given grammatical relation to the other .", "PONG achieves higher average precision on 16 relations than a state-of-the-art baseline in a pseudo-disambiguation task , but lower coverage and recall .", "From the much larger but unlabeled Google N-grams corpus , PONG learns the distribution of POS N-grams for a given pair of words .", "We present the PONG method to compute selectional preferences using part-of-speech ( POS ) N-grams .", "PONG estimates this probability by combining both distributions , whether or not either word occurs in the labeled corpus ."]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["Using surprisal as a linking function , a significant correlation between unlexicalized surprisal and RT has been reported ( e.g. , Demberg and Keller , 2008 ) , but success using lexicalized models has been limited .", "In this study , phrase structure grammars and recurrent neural networks estimated both lexicalized and unlexicalized surprisal for words of independent sentences from narrative sources .", "Probabilistic accounts of language processing can be psychologically tested by comparing word-reading times ( RT ) to the conditional word probabilities estimated by language models .", "The results show that lexicalized surprisal according to both models is a significant predictor of RT , outperforming its unlexicalized counterparts .", "These same sentences were used as stimuli in a self-paced reading experiment to obtain RTs ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["We present a learning algorithm that , like other spectral methods , is efficient and nonsusceptible to local minima .", "Furthermore , we also present an inside-outside algorithm for the parsing model that runs in cubic time , hence maintaining the standard parsing costs for context-free grammars .", "In this paper we study spectral learning methods for non-deterministic split headautomata grammars , a powerful hiddenstate formalism for dependency parsing .", "We show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions ."]}
{"orig_sents": ["2", "6", "7", "1", "4", "0", "3", "5"], "shuf_sents": ["On the other four smaller benchmark corpora , it performs either better or almost as good as the existing approaches .", "In this paper , we propose a novel hybrid kernel that combines ( automatically collected ) dependency patterns , trigger words , negative cues , walk features and regular expression patterns along with tree kernel and shallow linguistic kernel .", "Kernel based methods dominate the current trend for various relation extraction tasks including protein-protein interaction ( PPI ) extraction .", "Moreover , empirical results show that the proposed hybrid kernel attains considerably higher precision than the existing approaches , which indicates its capability of learning more accurate models .", "The proposed kernel outperforms the exiting state-of-the-art approaches on the BioInfer corpus , the largest PPI benchmark corpus available .", "This also demonstrates that the different types of information that we use are able to complement each other for relation extraction .", "PPI information is critical in understanding biological processes .", "Despite considerable efforts , previously reported PPI extraction results show that none of the approaches already known in the literature is consistently better than other approaches when evaluated on different benchmark PPI corpora ."]}
{"orig_sents": ["4", "2", "3", "0", "1"], "shuf_sents": ["We evaluate the performance of the proposed method on the Genia corpus and the Wall Street Journal portion of the Penn Treebank .", "Results show it increases the percentage of sentences in which coordination structures are detected correctly , compared with each of the two algorithms alone .", "We propose a method for disambiguating coordination structures .", "In this method , dual decomposition is used as a framework to take advantage of both HPSG parsing and coordinate structure analysis with alignment-based local features .", "Coordination disambiguation remains a difficult sub-problem in parsing despite the frequency and importance of coordination structures ."]}
{"orig_sents": ["5", "3", "4", "2", "0", "1"], "shuf_sents": ["Extensive experiments comparing different settings of the hybrid LM are reported on publicly available benchmarks based on TED talks , from Arabic to English and from English to French .", "The proposed models show to better exploit in-domain data than conventional word-based LMs for the target language modeling component of a phrase-based statistical machine translation system .", "Hybrid LMs are used to complement word-based LMs with statistics about the language style of the talks .", "Modeling the style of this genre can be very challenging given the shortage of available in-domain training data .", "We investigate the use of a hybrid LM , where infrequent words are mapped into classes .", "In this paper , we address statistical machine translation of public conference talks ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin .", "In this paper , we extend the work on using latent cross-language topic models for identifying word translations across comparable corpora .", "We present a novel precisionoriented algorithm that relies on per-topic word distributions obtained by the bilingual LDA ( BiLDA ) latent topic model .", "In addition , we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations .", "The algorithm aims at harvesting only the most probable word translations across languages in a greedy fashion , without any prior knowledge about the language pair , relying on a symmetrization process and the one-to-one constraint ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Previous work on treebank parsing with discontinuous constituents using Linear Context-Free Rewriting systems ( LCFRS ) has been limited to sentences of up to 30 words , for reasons of computational complexity .", "Instead , we introduce a technique which removes this length restriction , while maintaining a respectable accuracy .", "There have been some results on binarizing an LCFRS in a manner that minimizes parsing complexity , but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible .", "The resulting parser has been applied to a discontinuous treebank with favorable results ."]}
{"orig_sents": ["1", "5", "4", "2", "0", "3"], "shuf_sents": ["However , we can build predictive performance functions that account for up to 50 % of the variance in learning gain by combining features based on standard evaluation scores and on the confusion matrix entries .", "It is not always clear how the differences in intrinsic evaluation metrics for a parser or classifier will affect the performance of the system that uses it .", "We show that standard intrinsic metrics such as F-score alone do not predict the outcomes well .", "We argue that building such predictive models can help us better evaluate performance of NLP components that can not be distinguished based on F-score alone , and illustrate our approach by comparing the current interpretation component in the system to a new classifier trained on the evaluation data .", "Following the PARADISE methodology , we use multiple linear regression to build predictive models of learning gain , an important objective outcome metric in tutorial dialogue .", "We investigate the relationship between the intrinsic evaluation scores of an interpretation component in a tutorial dialogue system and the learning outcomes in an experiment with human users ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We describe a set of experiments using automatically labelled data to train supervised classifiers for multi-class emotion detection in Twitter messages with no manual intervention .", "By cross-validating between models trained on different labellings for the same six basic emotion classes , and testing on manually labelled data , we conclude that the method is suitable for some emotions ( happiness , sadness and anger ) but less able to distinguish others ; and that different labelling conventions are more suitable for some emotions than others ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Unlike most previous work , which has used a small number of grammatical categories , we work with 680 morpho-syntactic tags .", "We present experiments with part-ofspeech tagging for Bulgarian , a Slavic language with rich inflectional and derivational morphology .", "We combine a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus , achieving accuracy of 97.98 % , which is a significant improvement over the state-of-the-art for Bulgarian ."]}
{"orig_sents": ["5", "6", "1", "2", "3", "0", "4"], "shuf_sents": ["The method refrains from encoding features specific to a particular domain or annotation , to ensure immediate applicability to new , previously unseen annotations .", "rather than ? Locations ?", ") , but also the correct level of generality ( e.g. , ? Composers ?", "rather than ? People ? , or ? Jazz Composers ? ) .", "Over a gold standard of semantic annotations and concepts that best capture their arguments , the method substantially outperforms three baselines , on average , computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts .", "Whether automatically extracted or human generated , open-domain factual knowledge is often available in the form of semantic annotations ( e.g. , composed-by ) that take one or more specific instances ( e.g. , rhapsody in blue , george gershwin ) as their arguments .", "This paper introduces a method for converting flat sets of instance-level annotations into hierarchically organized , concept-level annotations , which capture not only the broad semantics of the desired arguments ( e.g. , ? People ?"]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We present a model of semantic processing of spoken language that ( a ) is robust against ill-formed input , such as can be expected from automatic speech recognisers , ( b ) respects both syntactic and pragmatic constraints in the computation of most likely interpretations , ( c ) uses a principled , expressive semantic representation formalism ( RMRS ) with a well-defined model theory , and ( d ) works continuously ( producing meaning representations on a wordby-word basis , rather than only for full utterances ) and incrementally ( computing only the additional contribution by the new word , rather than re-computing for the whole utterance-so-far ) .", "We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component ( around 10 % absolute , over a syntax-only baseline ) ."]}
{"orig_sents": ["0"], "shuf_sents": ["In this paper we extend our work described in ( Dinu et al 2011 ) by adding more conjugational rules to the labelling system introduced there , in an attempt to capture the entire dataset of Romanian verbs extracted from ( Barbu , 2007 ) , and we employ machine learning techniques to predict a verb ? s correct label ( which says what conjugational pattern it follows ) when only the infinitive form is given ."]}
{"orig_sents": ["5", "1", "4", "0", "2", "3"], "shuf_sents": ["In particular , the precision of statistical methods has been largely over-estimated , while the precision of knowledge-based approaches has been under-estimated .", "For that purpose , we extract naturally occurring errors and their contexts from the Wikipedia revision history .", "Additionally , we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations .", "Finally , we show that statistical and knowledgebased methods can be combined for increased performance .", "We show that such natural errors are better suited for evaluation than the previously used artificially created errors .", "We evaluate measures of contextual fitness on the task of detecting real-word spelling errors ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We also explore adapting multiple ( 4 ? 10 ) data sets with no a priori distinction between in-domain and out-of-domain data except for an in-domain development set .", "While techniques for domain adaptation of monolingual data can be borrowed for parallel data , we explore conceptual differences between translation model and language model domain adaptation and their effect on performance , such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately .", "We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation ( SMT ) ."]}
{"orig_sents": ["4", "0", "2", "3", "1"], "shuf_sents": ["Subcat-LMF is able to represent SCFs at a very fine-grained level .", "The SubcatLMF DTD , the conversion tools and the standardized versions of VerbNet and IMSlex subset are publicly available.1", "We utilized SubcatLMF to standardize lexicons with largescale SCF information : the English VerbNet and two German lexicons , i.e. , a subset of IMSlex and GermaNet verbs .", "To evaluate our LMF-model , we performed a crosslingual comparison of SCF coverage and overlap for the standardized versions of the English and German lexicons .", "This paper describes Subcat-LMF , an ISOLMF compliant lexicon representation format featuring a uniform representation of subcategorization frames ( SCFs ) for the two languages English and German ."]}
{"orig_sents": ["1", "4", "0", "3", "7", "5", "6", "2"], "shuf_sents": ["In this paper , we address the influence of text type and domain differences on text prediction quality .", "Text prediction is the task of suggesting text while the user is typing .", "The second-best result was obtained by leaveone-out experiments on the test questions , even though this training corpus was much smaller ( 2,672 words ) than the other corpora ( 1.5 Million words ) .", "By training and testing our text prediction algorithm on four different text types ( Wikipedia , Twitter , transcriptions of conversational speech and FAQ ) with equal corpus sizes , we found that there is a clear effect of text type on text prediction quality : training and testing on the same text type gave percentages of saved keystrokes between 27 and 34 % ; training on a different text type caused the scores to drop to percentages between 16 and 28 % .", "Its main aim is to reduce the number of keystrokes that are needed to type a text .", "We found that both text type and topic domain play a role in text prediction quality .", "The best performing training corpus was a set of medical pages from Wikipedia .", "In our case study , we compared a number of training corpora for a specific data set for which training data is sparse : questions about neurological issues ."]}
{"orig_sents": ["4", "0", "5", "6", "2", "3", "1"], "shuf_sents": ["A single document that is relevant but overlooked during a patent search can turn into an expensive proposition .", "For the task of finding all patents of a company , our approach improves recall from 96.7 % ( when using a state-of-the-art patent search engine ) to 99.5 % , while precision is compromised by only 3.7 % .", "We introduce technology in order to improve retrieval effectiveness despite the presence of typographical ambiguities .", "In this regard , we ( 1 ) quantify spelling errors in terms of edit distance and phonological dissimilarity and ( 2 ) render error detection as a learning problem that combines word dissimilarities with patent meta-features .", "The search in patent databases is a risky business compared to the search in other domains .", "While recent research engages in specialized models and algorithms to improve the effectiveness of patent retrieval , we bring another aspect into focus : the detection and exploitation of patent inconsistencies .", "In particular , we analyze spelling errors in the assignee field of patents granted by the United States Patent & Trademark Office ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["All resources in UBY can be accessed with an easy to use publicly available API .", "For FrameNet , VerbNet and all collaboratively constructed resources , this is done for the first time .", "We present UBY , a large-scale lexicalsemantic resource combining a wide range of information from expert-constructed and collaboratively constructed resources for English and German .", "Our LMF model captures lexical information at a fine-grained level by employing a large number of Data Categories from ISOCat and is designed to be directly extensible by new languages and resources .", "It currently contains nine resources in two languages : English WordNet , Wiktionary , Wikipedia , FrameNet and VerbNet , German Wikipedia , Wiktionary and GermaNet , and multilingual OmegaWiki modeled according to the LMF standard ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task .", "We go on to establish state-of-the-art results over two WSI datasets , and apply the proposed model to a novel sense detection task .", "We apply topic modelling to automatically induce word senses of a target word , and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses , as well as token occurrences of those senses .", "We start by exploring the utility of standard topic models for word sense induction ( WSI ) , with a pre-determined number of topics ( =senses ) ."]}
{"orig_sents": ["3", "8", "0", "7", "4", "2", "6", "1", "5"], "shuf_sents": ["Topic Identification first identifies tweets that are relevant to a desired topic ( e.g. , a politician or event ) , and Sentiment Analysis extracts each tweet ? s attitude toward the topic .", "Our results better correlate with Gallup ? s Presidential Job Approval polls than previous work .", "We show that distant supervision leads to improved performance in the Topic Identification task as well in the downstream Sentiment Analysis stage .", "Microblogging websites such as Twitter offer a wealth of insight into a population ? s current mood .", "Here , we present an approach that instead uses distant supervision to train a classifier on the tweets returned by the search .", "Finally , we discover a surprising baseline that outperforms previous work without a Topic Identification stage .", "We then use a system that incorporates distant supervision into both stages to analyze the sentiment toward President Obama expressed in a dataset of tweets .", "Many techniques for Topic Identification simply involve selecting tweets using a keyword search .", "Automated approaches to identify general sentiment toward a particular topic often perform two steps : Topic Identification and Sentiment Analysis ."]}
{"orig_sents": ["1", "3", "0", "2", "4"], "shuf_sents": ["We demonstrate that concept drift is an important consideration .", "Open issue trackers are a type of social media that has received relatively little attention from the text-mining community .", "We show the effectiveness of online learning algorithms by evaluating them on several bug report datasets collected from open issue trackers associated with large open-source projects .", "We investigate the problems inherent in learning to triage bug reports from time-varying data .", "We make this collection of data publicly available ."]}
{"orig_sents": ["5", "0", "4", "3", "2", "1"], "shuf_sents": ["address in dialogue is not distinguished overtly in modern English , e.g .", "It assigns T/V at sentence level with up to 68 % accuracy , relying mainly on lexical features ; ( c ) , there is a marked asymmetry between lexical features for formal speech ( which are conventionalized and therefore general ) and informal speech ( which are text-specific ) .", "Our main findings are : ( a ) human raters can label monolingual English utterances as T or V fairly well , given sufficient context ; ( b ) , a bilingual corpus can be exploited to induce a supervised classifier for T/V without human annotation .", "Our study investigates the status of the T/V distinction in English literary texts .", "by pronoun choice like in many other languages such as French ( ? tu ? / ? vous ? ) .", "Informal and formal ( ? T/V ? )"]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["Our kernel compares the characters of different novels to one another by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them .", "Better representations of plot structure could greatly improve computational methods for summarizing and generating stories .", "We present a kernel for comparing novelistic plots at a higher level , in terms of the cast of characters they depict and the social relationships between them .", "Given a corpus of 19thcentury novels as training data , our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates , demonstrating its ability to robustly represent important aspects of plot structure .", "Current representations lack abstraction , focusing too closely on events ."]}
{"orig_sents": ["1", "4", "9", "3", "2", "0", "8", "5", "7", "6"], "shuf_sents": ["The number of forms needed in average is a measure of predictability of an inflection system .", "Morphological lexica are often implemented on top of morphological paradigms , corresponding to different ways of building the full inflection table of a word .", "If the result is uncertain , more forms are given for discrimination .", "It is a metaparadigm , which inspects the base form and tries to infer which low-level paradigm applies .", "Computationally precise lexica may use hundreds of paradigms , and it can be hard for a lexicographer to choose among them .", "This paper evaluates the smart paradigms implemented in the open-source GF Resource Grammar Library .", "The main result is that predictability does not decrease when the complexity of morphology grows , which means that smart paradigms provide an efficient tool for the manual construction and/or automatically bootstrapping of lexica .", "Predictability and complexity are estimated for four different languages : English , French , Swedish , and Finnish .", "The overall complexity of the system also has to take into account the code size of the paradigms definition itself .", "To automate this task , this paper introduces the notion of a smart paradigm ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["We propose a novel method for learning morphological paradigms that are structured within a hierarchy .", "The hierarchical structuring of paradigms groups morphologically similar words close to each other in a tree structure .", "Our evaluation using ( Kurimo et al. , 2011a ; Kurimo et al 2011b ) dataset shows that our method performs competitively when compared with current state-ofart systems .", "This allows detecting morphological similarities easily leading to improved morphological segmentation ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["We model both inflection and word-formation for the task of translating into German .", "The current state-of-the-art in statistical machine translation ( SMT ) suffers from issues of sparsity and inadequate modeling power when translating into morphologically rich languages .", "We show that improved modeling of inflection and wordformation leads to improved SMT .", "We translate from English words to an underspecified German representation and then use linearchain CRFs to predict the fully specified German representation ."]}
{"orig_sents": ["3", "0", "4", "6", "2", "1", "5", "7"], "shuf_sents": ["In addition , Arabic morphosyntactic agreement interacts with the lexical semantic feature of rationality , which has no morphological realization .", "Our results show that the MLE technique is preferred for words seen in the training data , while the Yamcha technique is optimal for unseen words , which are our real target .", "We study a number of orthographic , morphological and syntactic learning features .", "Arabic morphology is complex , partly because of its richness , and partly because of common irregular word forms , such as broken plurals ( which resemble singular nouns ) , and nouns with irregular gender ( feminine nouns that look masculine and vice versa ) .", "In this paper , we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number , and rationality in Arabic .", "Furthermore , we show that for unseen words , morphological features help beyond orthographic features and that syntactic features help even more .", "We compare two techniques , using simple maximum likelihood ( MLE ) with back-off and a support vector machine based sequence tagger ( Yamcha ) .", "A combination of the two techniques improves overall performance even further ."]}
{"orig_sents": ["5", "3", "0", "2", "1", "4"], "shuf_sents": ["In addition , it is presupposed that a single semantic role is assigned to each syntactic argument .", "We propose a new framework for semantic role annotation which solves these problems by extending the theory of lexical conceptual structure ( LCS ) .", "This is not necessarily true when we consider internal structures of verb semantics .", "Their semantic roles are not strictly defined ; therefore , their meanings and semantic characteristics are unclear .", "By comparing our framework with that of existing resources , including VerbNet and FrameNet , we demonstrate that our extended LCS framework can give a formal definition of semantic role labels , and that multiple roles of arguments can be represented strictly and naturally .", "Widely accepted resources for semantic parsing , such as PropBank and FrameNet , are not perfect as a semantic role labeling framework ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Like the distillation algorithm of Danescu-Niculescu-Mizil et al ( 2009 ) , the initialization of our method depends on the correlation between DEOs and negative polarity items ( NPIs ) .", "Our method is also amenable to a bootstrapping method that co-learns DEOs and NPIs , and achieves the best results in identifying DEOs in two corpora .", "However , our method trusts the initialization more and aggressively separates likely DEOs from spurious distractors and other words , unlike distillation , which we show to be equivalent to one iteration of EM prior re-estimation .", "We propose an unsupervised , iterative method for detecting downward-entailing operators ( DEOs ) , which are important for deducing entailment relations between sentences ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["While the identification of explicit and zero subjects has attracted the attention of researchers in the past , the automatic identification of impersonal constructions in Spanish has not been addressed yet and this work is the first such study .", "The evaluation results show that our system performs better in detecting explicit subjects than alternative systems .", "In this paper we present a corpus to underpin research on the automatic detection of these linguistic phenomena in Spanish and a novel machine learning-based methodology for their computational treatment .", "In pro-drop languages , the detection of explicit subjects , zero subjects and nonreferential impersonal constructions is crucial for anaphora and co-reference resolution .", "This study also provides an analysis of the features , discusses performance across two different genres and offers error analysis ."]}
{"orig_sents": ["0", "4", "3", "1", "5", "2"], "shuf_sents": ["The task of paraphrase acquisition from related sentences can be tackled by a variety of techniques making use of various types of knowledge .", "paraphrase vs. not paraphrase ) , allowing any paraphrase acquisition technique to be easily integrated into the combination system .", "Relative improvements in F-measure close to 18 % are obtained on both languages over the best performing techniques .", "We implement this as a bi-class classification problem ( i.e .", "In this work , we make the hypothesis that their performance can be increased if candidate paraphrases can be validated using information that characterizes paraphrases independently of the set of techniques that proposed them .", "We report experiments on two languages , English and French , with 5 individual techniques on parallel monolingual parallel corpora obtained via multiple translation , and a large set of classification features including surface to contextual similarity measures ."]}
{"orig_sents": ["5", "4", "2", "1", "0", "3"], "shuf_sents": ["This is a difficult problem , as German verbal elements can appear in different positions within a clause ( in contrast with English verbal elements , whose positions do not vary as much ) .", "The reordering rules place English verbal elements in the positions within the clause they will have in the German translation .", "We use a sequence of hand-crafted reordering rules applied to English parse trees .", "We obtain a significant improvement in translation performance .", "We reorder English as a preprocessing step for English-to-German SMT .", "When translating English to German , existing reordering models often can not model the long-range reorderings needed to generate German translations with verbs in the correct position ."]}
{"orig_sents": ["6", "5", "2", "1", "4", "0", "3"], "shuf_sents": ["We take the Zhang and Clark system as the baseline , and incorporate an N-gram model by applying online large-margin training .", "By using CCG and learning guided search , Zhang and Clark reported the highest scores on this task .", "There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words ( Wan et al 2009 ; Zhang and Clark , 2011 ) .", "Our system significantly improved on the baseline by 3.7 BLEU points .", "One limitation of their system is the absence of an N-gram language model , which has been used by text generation systems to improve fluency .", "Word ordering is a computationally difficult problem , which can be constrained to some extent for particular applications , for example by using synchronous grammars for statistical machine translation .", "A fundamental problem in text generation is word ordering ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Results show that the generation system outperforms state-of-the-art systems , automatically generating some of the most natural image descriptions to date .", "This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections .", "By leveraging syntactically informed word co-occurrence statistics , the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We show that female users spend significantly less time looking away from the road when using our system compared to a baseline system .", "We present a system for the real-time generation of car navigation instructions with landmarks .", "Our system relies exclusively on freely available map data from OpenStreetMap , organizes its output to fit into the available time until the next driving maneuver , and reacts in real time to driving errors ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Surprisingly , in both settings , the sentence-external features perform poorly compared to the sentenceinternal ones , and do not improve over a baseline model capturing the syntactic functions of the constituents .", "In a more controlled setting , we develop a constituent ordering classifier that is trained on a German treebank with gold coreference annotation .", "We compare the impact of sentenceinternal vs. sentence-external features on word order prediction in two generation settings : starting out from a discriminative surface realisation ranking model for an LFG grammar of German , we enrich the feature set with lexical chain features from the discourse context which can be robustly detected and reflect rough grammatical correlates of notions from theoretical approaches to discourse coherence ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper , we propose an annotation schema for the discourse analysis of Wikipedia Talk pages aimed at the coordination efforts for article improvement .", "We apply the annotation schema to a corpus of 100 Talk pages from the Simple English Wikipedia and make the resulting dataset freely available for download1 .", "Furthermore , we perform automatic dialog act classification on Wikipedia discussions and achieve an average F1-score of 0.82 with our classification pipeline ."]}
{"orig_sents": ["3", "0", "2", "1", "4"], "shuf_sents": ["Models of stylistic shift that focus on specific features are limited in terms of the contexts to which they can be applied if the goal of the analysis is to model socially motivated speech style accommodation .", "This greatly expands the applicability of the model across contexts .", "In this paper , we present an unsupervised Dynamic Bayesian Model that allows us to model stylistic style accommodation in a way that is agnostic to which specific speech style features will shift in a way that resembles socially motivated stylistic variation .", "Speech style accommodation refers to shifts in style that are used to achieve strategic goals within interactions .", "Our hypothesis is that stylistic shifts that occur as a result of social processes are likely to display some consistency over time , and if we leverage this insight in our model , we will achieve a model that better captures inherent structure within speech ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We investigate the use of rich knowledge sources for this task in combination with a rule-based approach and a learning-based approach .", "While information status ( IS ) plays a crucial role in discourse processing , there have only been a handful of attempts to automatically determine the IS of discourse entities .", "We examine a related but more challenging task , fine-grained IS determination , which involves classifying a discourse entity as one of 16 IS subtypes .", "In experiments with a set of Switchboard dialogues , the learning-based approach achieves an accuracy of 78.7 % , outperforming the rulebased approach by 21.3 % ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["In general , the result of the composition is an extended top-down tree transducer that is no longer linear or nondeleting , but in a number of cases these properties can easily be recovered by a post-processing step .", "A composition procedure for linear and nondeleting extended top-down tree transducers is presented .", "It is demonstrated that the new procedure is more widely applicable than the existing methods ."]}
{"orig_sents": ["1", "3", "2", "4", "5", "0"], "shuf_sents": ["A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs .", "Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents .", "We view different patent classes and different patent text sections such as title , abstract , and claims , as separate translation tasks , and investigate the influence of such tasks on machine translation performance .", "In this paper we analyze patents along the orthogonal dimensions of topic and textual structure .", "We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning .", "We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters ."]}
{"orig_sents": ["1", "3", "0", "2", "4"], "shuf_sents": ["This hypothesis is supported by a novel kind of computational experiments that reconstruct and compare attested variations of the German definite article paradigm .", "German case syncretism is often assumed to be the accidental by-product of historical development .", "The experiments show how the intricate interaction between those variations and the rest of the German ? linguistic landscape ?", "This paper contradicts this claim and argues that the evolution of German case is driven by the need to optimize the cognitive effort and memory required for processing and interpretation .", "may direct language change ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["RG can also be used as feedback for lexicographers , and as a supporting component of automatic semantic classifiers , especially when dealing with a very fine-grained set of semantic categories .", "IAA correlates with the granularity of word senses and they both correlate with the amount of information they give as well as with its reliability .", "Low interannotator agreement ( IAA ) is a well-known issue in manual semantic tagging ( sense tagging ) .", "We compare different approaches to semantic tagging in WordNet , FrameNet , PropBank and OntoNotes with a small tagged data sample based on the Corpus Pattern Analysis to present the reliable information gain ( RG ) , a measure used to optimize the semantic granularity of a sense inventory with respect to its reliability indicated by the IAA in the given data set ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["This paper demonstrates a novel distributed architecture to facilitate the acquisition of Language Resources .", "The factory is designed as a platform where functionalities are deployed as web services , which can be combined in complex acquisition chains using workflows .", "We build a factory that automates the stages involved in the acquisition , production , updating and maintenance of these resources .", "We show a case study , which acquires a Translation Memory for a given pair of languages and a domain using web services for crawling , sentence alignment and conversion to TMX ."]}
{"orig_sents": ["2", "5", "0", "1", "3", "4"], "shuf_sents": ["We propose the demonstration of an end-to-end tool integrated in the HAL open archive for enabling efficient translation for scientific texts .", "This tool can give translation suggestions adapted to the scientific domain , improving by more than 10 points the BLEU score of a generic system .", "French researchers are required to frequently translate into French the description of their work published in English .", "It also provides a post-edition service which captures user post-editing data that can be used to incrementally improve the translations engines .", "Thus it is helpful for users which need to translate or to access scientific texts .", "At the same time , the need for French people to access articles in English , or to international researchers to access theses or papers in French , is incorrectly resolved via the use of generic translation tools ."]}
{"orig_sents": ["4", "1", "0", "2", "3"], "shuf_sents": ["To help scholars identify and correct potential writing problems , we introduce SWAN ( Scientific Writing AssistaNt ) tool .", "Writing reader-friendly text , however , is challenging due to difficulty in recognizing problems in one ? s own writing .", "SWAN is a rule-based system that gives feedback based on various quality metrics based on years of experience from scientific writing classes including 960 scientists of various backgrounds : life sciences , engineering sciences and economics .", "According to our first experiences , users have perceived SWAN as helpful in identifying problematic sections in text and increasing overall clarity of manuscripts .", "Difficulty of reading scholarly papers is significantly reduced by reader-friendly writing principles ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from Arabic , Czech , Danish , Farsi , French , German , Italian , Polish , Portuguese , Spanish and Turkish into English .", "The Moses-based system was optimised for the news domain and differs from other available systems in four ways : ( 1 ) News items are automatically categorised on the source side , before translation ; ( 2 ) Named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language , making use of a separate entity repository ; ( 3 ) News titles are translated with a separate translation system which is optimised for the specific style of news titles ; ( 4 ) The system was optimised for speed in order to cope with the large volume of daily news articles ."]}
{"orig_sents": ["3", "2", "1", "0", "4"], "shuf_sents": ["The first one extracts relevant noun phrases for their use as a heading , the second one automatically constructs headings by selecting words appearing in the text , and , finally , the third one uses nominalization in order to propose informative and catchy titles .", "So , our application relies on three very different automatic titling methods .", "The aim of such application is to attribute a title for a given text .", "This paper deals with an application of automatic titling .", "Experiments based on 1048 titles have shown that our methods provide relevant titles ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Besides facilitating the exploration of Portuguese lexical knowledge bases , Folheador is connected to services that access Portuguese corpora , which provide authentic examples of the semantic relations in context .", "This paper presents Folheador , an online service for browsing through Portuguese semantic relations , acquired from different sources ."]}
{"orig_sents": ["3", "1", "0", "4", "5", "2"], "shuf_sents": ["These high accuracy tasks require a human transcriber .", "In some scenarios , high quality transcriptions are needed and , therefore , fully automatic systems are not suitable for them .", "This system automatically recognizes speech while allowing the user to interactively modify the transcription .", "Current automatic speech transcription systems can achieve a high accuracy although they still make mistakes .", "However , we consider that automatic techniques could improve the transcriber ? s efficiency .", "With this idea we present an interactive speech recognition system integrated with a word processor in order to assists users when transcribing speech ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper presents the first demonstration of a statistical spoken dialogue system that uses automatic belief compression to reason over complex user goal sets .", "Reasoning over the power set of possible user goals allows complex sets of user goals to be represented , which leads to more natural dialogues .", "A modified form of Value Directed Compression ( VDC ) is applied to the POMDP belief states producing a near-lossless compression which reduces the number of bases required to represent the belief distribution .", "The use of the power set results in a massive expansion in the number of belief states maintained by the Partially Observable Markov Decision Process ( POMDP ) spoken dialogue manager ."]}
{"orig_sents": ["1", "0", "5", "6", "3", "2", "4"], "shuf_sents": ["Previous experiments have proven that bilingual lexicons can be created by applying word alignment on parallel corpora .", "The aim of our software presentation is to demonstrate that corpus-driven bilingual dictionaries generated fully by automatic means are suitable for human use .", "In particular , the scarce availability of parallel texts for medium density languages imposes limitations on the size of the resulting dictionary .", "However , the proposed technique have to face some difficulties , as well .", "Our objective is to design and implement a dictionary building workflow and a query system that is apt to exploit the additional benefits of the method and overcome the disadvantages of it .", "Such an approach , especially the corpus-driven nature of it , yields several advantages over more traditional approaches .", "Most importantly , automatically attained translation probabilities are able to guarantee that the most frequently used translations come first within an entry ."]}
{"orig_sents": ["1", "5", "2", "4", "3", "0"], "shuf_sents": ["During the demo session , we will run MaltOptimizer on different data sets ( user-supplied if possible ) and show how the user can interact with the system and track the improvement in parsing accuracy .", "Data-driven systems for natural language processing have the advantage that they can easily be ported to any language or domain for which appropriate training data can be found .", "We present MaltOptimizer , a tool developed to facilitate optimization of parsers developed using MaltParser , a data-driven dependency parser generator .", "Experiments show that MaltOptimizer can improve parsing accuracy by up to 9 percent absolute ( labeled attachment score ) compared to default settings .", "MaltOptimizer performs an analysis of the training data and guides the user through a three-phase optimization process , but it can also be used to perform completely automatic optimization .", "However , many data-driven systems require careful tuning in order to achieve optimal performance , which may require specialized knowledge of the system ."]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["Unfortunately , most current models of deep language processing incorporate assumptions from generative grammar that are at odds with the cognitive movement in linguistics .", "This demonstration shows how Fluid Construction Grammar ( FCG ) , a fully operational and bidirectional unification-based grammar formalism , caters for this increasing demand .", "Cognitive linguistics has reached a stage of maturity where many researchers are looking for an explicit formal grounding of their work .", "This demonstration highlights the main differences between FCG and related formalisms .", "FCG features many of the tools that were pioneered in computational linguistics in the 70s-90s , but combines them in an innovative way ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The user can dynamically set a probability threshold over the geolocation predictions , and also the time interval to present data for .", "The system operates by querying the data stream with a user-specified set of keywords , filtering out non-English messages , and probabilistically geolocating each message .", "This paper describes a system designed to support event detection over Twitter ."]}
{"orig_sents": ["3", "4", "5", "0", "1", "2"], "shuf_sents": ["Furthermore , the results of these services are not comparable due to different formats .", "This prevents the comparison of the performance of these services as well as their possible combination .", "We address this problem by proposing NERD , a framework which unifies 10 popular named entity extractors available on the web , and the NERD ontology which provides a rich set of axioms aligning the taxonomies of these tools .", "Named Entity Extraction is a mature task in the NLP field that has yielded numerous services gaining popularity in the Semantic Web community for extracting knowledge from web documents .", "These services are generally organized as pipelines , using dedicated APIs and different taxonomy for extracting , classifying and disambiguating named entities .", "Integrating one of these services in a particular application requires to implement an appropriate driver ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The direction of time for the episode starting point : backwards or forward ( with respect to certain moment orienting the episode ) is recognised with precision 74.4 % .", "The Patient history section is automatically split into episodes ( clauses between two temporal markers ) ; then drugs , diagnoses and conditions are recognised within the episodes with accuracy higher than 90 % .", "The temporal markers , which refer to absolute or relative moments of time , are identified with precision 87 % and recall 68 % .", "This demo presents Information Extraction from discharge letters in Bulgarian language ."]}
{"orig_sents": ["3", "4", "2", "1", "0"], "shuf_sents": ["The interactive interface allows the users to retrieve news report supporting the relations of interest .", "The network of actors and their relations can be mined for insights about the structure of the narration , including the identification of the key players , of the network of political support of each of them , a representation of the similarity of their political positions , and other information concerning their role in the media narration of events .", "( one example of a triplet of this kind is ? Romney Criticised Obama ? ) .", "We present a web tool that allows users to explore news stories concerning the 2012 US Presidential Elections via an interactive interface .", "The tool is based on concepts of ? narrative analysis ? , where the key actors of a narration are identified , along with their relations , in what are sometimes called ? semantic triplets ?"]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["The main innovation of LangLog is the implementation of two highly customizable components that cluster and classify the queries in the log .", "We push the standard Search Log Analysis forward taking into account the semantics of the queries .", "This article describes GALATEAS LangLog , a system performing Search Log Analysis .", "LangLog illustrates how NLP technologies can be a powerful support tool for market research even when the source of information is a collection of queries each one consisting of few words ."]}
{"orig_sents": ["1", "4", "2", "3", "0"], "shuf_sents": ["The demo will illustrate the different features of the platform , including navigation , visualization and editing .", "Data-driven approaches in computational semantics are not common because there are only few semantically annotated resources available .", "We have created a wiki-like Web-based platform on which a crowd of expert annotators ( i.e .", "linguists ) can log in and adjust linguistic analyses in real time , at various levels of analysis , such as boundaries ( tokens , sentences ) and tags ( part of speech , lexical categories ) .", "We are building a large corpus of public-domain English texts and annotate them semi-automatically with syntactic structures ( derivations in Combinatory Categorial Grammar ) and semantic representations ( Discourse Representation Structures ) , including events , thematic roles , named entities , anaphora , scope , and rhetorical structure ."]}
{"orig_sents": ["1", "3", "5", "4", "6", "0", "2"], "shuf_sents": ["The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems .", "We propose a set of open-source software modules to perform structured Perceptron Training , Prediction and Evaluation within the Hadoop framework .", "The execution of the modules applied to specific NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs .", "Apache Hadoop is a freely available environment for running distributed applications on a computer cluster .", "Thanks to distributed computing , the proposed software reduces substantially execution times while handling huge data-sets .", "The software is designed within the Map-Reduce paradigm .", "The distributed Perceptron training algorithm preserves convergence properties , thus guaranties same accuracy performances as the serial Perceptron ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task , showing a 15 % decrease in total annotation time .", "BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques .", "BRAT is available under an opensource license from : http : //brat.nlplab.org", "We introduce the brat rapid annotation tool ( BRAT ) , an intuitive web-based tool for text annotation supported by Natural Language Processing ( NLP ) technology ."]}
{"orig_sents": ["0", "4", "6", "3", "2", "5", "1"], "shuf_sents": ["Machine Translation is a well ? established field , yet the majority of current systems translate sentences in isolation , losing valuable contextual information from previously translated sentences in the discourse .", "This paper attempts to explain why and to provide insight into the factors affecting performance .", "This work assesses the extent to which source-language annotation of coreferring pronouns can improve English ? Czech Statistical Machine Translation ( SMT ) .", "Disregarding a pronoun ? s antecedent in translation can lead to inappropriate coreferring forms in the target text , seriously degrading a reader ? s ability to understand it .", "One important type of contextual information concerns who or what a coreferring pronoun corefers to ( i.e. , its antecedent ) .", "As with previous attempts that use this method , the results show little improvement .", "Languages differ significantly in how they achieve coreference , and awareness of antecedents is important in choosing the correct pronoun ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["This article introduces the first approach to this task , which exploits text features that can be considered stable genre predictors across languages .", "My experiments show this method to perform equally well or better than full text translation combined with monolingual classification , while requiring fewer resources .", "Classifying text genres across languages can bring the benefits of genre classification to the target language without the costs of manual annotation ."]}
{"orig_sents": ["0", "2", "4", "3", "1"], "shuf_sents": ["Adaptive Dialogue Systems are rapidly becoming part of our everyday lives .", "This is the first work , to the best of our knowledge , to evaluate online RL algorithms on the dialogue problem and in a dynamic environment .", "As they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment .", "In this work we compare several standard and state of the art online RL algorithms that are used to train the dialogue manager in a dynamic environment , aiming to aid researchers / developers choose the appropriate RL algorithm for their system .", "Research in this field is currently focused on how to achieve adaptation , and particularly on applying Reinforcement Learning ( RL ) techniques , so a comparative study of the related methods , such as this , is necessary ."]}
{"orig_sents": ["3", "0", "2", "8", "1", "9", "7", "4", "5", "6"], "shuf_sents": ["Up to our knowledge , considerable amount of work has not yet been done in describing Myanmar script using formal language theory .", "We make our CFG in conformity with the properties of LL ( 1 ) grammar so that we can apply conventional parsing technique called predictive top-down parsing to identify Myanmar syllables .", "This paper presents manually constructed context free grammar ( CFG ) with ? 111 ?", "Myanmar language and script are unique and complex .", "We make LL ( 1 ) grammar in which ? 1 ?", "does not mean exactly one character of lookahead for parsing because of the above mentioned contracted forms .", "We use five basic sub syllabic elements to construct CFG and found that all possible syllable combinations in Myanmar Orthography can be parsed correctly using the proposed grammar .", "We also discuss the preprocessing step called contraction for vowels and consonant conjuncts .", "productions to describe the Myanmar Syllable Structure .", "We present Myanmar syllable structure according to orthographic rules ."]}
{"orig_sents": ["0", "5", "1", "3", "4", "2"], "shuf_sents": ["There are lexical , syntactic , semantic and discourse variations amongst the languages used in various biomedical subdomains .", "We report here on the semantic variations that occur in the sublanguages of two biomedical subdomains , i.e .", "More specifically , our classifier can distinguish between documents belonging to each subdomain with an accuracy of 91.1 % F-score .", "cell biology and pharmacology , at the level of named entity information .", "By building a classifier using ratios of named entities as features , we show that named entity information can discriminate between documents from each subdomain .", "It is important to recognise such differences and understand that biomedical tools that work well on some subdomains may not work as well on others ."]}
{"orig_sents": ["1", "4", "8", "9", "2", "3", "7", "0", "5", "6"], "shuf_sents": ["The YALI algorithm has slightly lower accuracy but classifies around 17 times faster and its training is more than 4000 times faster .", "Language identification of written text has been studied for several decades .", "The objective of this paper is to investigate the sources of such degradation .", "In order to isolate the impact of individual factors , 5 different algorithms and 3 different number of languages are used .", "Despite this fact , most of the research is focused on a few most spoken languages , whereas the minor ones are ignored .", "Three different data sets with various number of languages and sample sizes were prepared to overcome the lack of standardized data sets .", "These data sets are now publicly available .", "The Support Vector Machine algorithm achieved an accuracy of 98 % for 90 languages and the YALI algorithm based on a scoring function had an accuracy of 95.4 % .", "The identification of a larger number of languages brings new difficulties that do not occur for a few languages .", "These difficulties are causing decreased accuracy ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Uni- , bi- and trigrams were used on four 19th century French short stories by Maupassant .", "To cluster textual sequence types ( discourse types/modes ) in French texts , K-means algorithm with high-dimensional embeddings and fuzzy clustering algorithm were applied on clauses whose POS ( part-ofspeech ) n-gram profiles were previously extracted .", "Preliminary results show that highdimensional embeddings improve the quality of clustering , contrasting the use of biand trigrams whose performance is disappointing , possibly because of feature space sparsity .", "For high-dimensional embeddings , power transformations on the chisquared distances between clauses were explored ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["In an empirical evaluation , the model outperforms the Kneser-Ney model in terms of perplexity , and achieves preliminary improvements in English-German translation .", "I argue that the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem , and demonstrate the approach by proposing a model for German compounds .", "In this work I address the challenge of augmenting n-gram language models according to prior linguistic intuitions ."]}
{"orig_sents": ["2", "4", "5", "0", "6", "3", "1"], "shuf_sents": ["The goal of this work is to investigate how lexeme selection affects the quality of obtained sentiment estimations .", "The experiments were carried out for Polish .", "This paper is focused on one aspect of SOPMI , an unsupervised approach to sentiment vocabulary acquisition proposed by Turney ( Turney and Littman , 2003 ) .", "The work can be also interpreted as sensitivity analysis on SO-PMI with regard to paradigm word selection .", "The method , originally applied and evaluated for English , is often used in bootstrapping sentiment lexicons for European languages where no such resources typically exist .", "In general , SO-PMI values are computed from word co-occurrence frequencies in the neighbourhoods of two small sets of paradigm words .", "This has been achieved by comparing ad hoc random lexeme selection with two alternative heuristics , based on clustering and SVD decomposition of a word co-occurrence matrix , demonstrating superiority of the latter methods ."]}
{"orig_sents": ["1", "4", "2", "5", "0", "7", "6", "3"], "shuf_sents": ["language .", "Null subjects are non overtly expressed subject pronouns found in pro-drop languages such as Italian and Spanish .", "Next , we evaluate null subjects ?", "A second evaluation of the improved Its-2 system shows an average increase of 15.46 % in correct pro-drop translations for Italian-French and 12.80 % for Spanish-French .", "In this study we quantify and compare the occurrence of this phenomenon in these two languages .", "translation into French , a ? non prodrop ?", "Then we add a rule-based preprocessor and a statistical post-editor to the Its-2 translation pipeline .", "We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation : Its-2 , a rule-based system developed at LATL , and a statistical system built using the Moses toolkit ."]}
{"orig_sents": ["1", "4", "2", "0", "3", "5"], "shuf_sents": ["I selected this book because it contains multiple , interweaving narratives within its sprawling 1,000-plus pages .", "Many works ( of both fiction and non-fiction ) span multiple , intersecting narratives , each of which constitutes a story in its own right .", "The motivating example I use is David Foster Wallace ? s fictional text Infinite Jest .", "I propose and evaluate a novel unsupervised approach to MND that is motivated by the theory of narratology .", "In this work I introduce the task of multiple narrative disentanglement ( MND ) , in which the aim is to tease these narratives apart by assigning passages from a text to the sub-narratives to which they belong .", "This method achieves strong empirical results , successfully disentangling the threads in Infinite Jest and significantly outperforming baseline strategies in doing so ."]}
{"orig_sents": ["2", "4", "3", "0", "1"], "shuf_sents": ["We further determine that entrainment is more important to the perception of female-male social behavior than it is for same-gender pairs , and it is more important to the smoothness and flow of male-male dialogue than it is for female-female or mixedgender pairs .", "Finally , we find that entrainment is more pronounced when intensity or speaking rate is especially high or low .", "In conversation , speakers have been shown to entrain , or become more similar to each other , in various ways .", "We find that male-female pairs entrain on all features , while male-male pairs entrain only on particular acoustic features ( intensity mean , intensity maximum and syllables per second ) .", "We measure entrainment on eight acoustic features extracted from the speech of subjects playing a cooperative computer game and associate the degree of entrainment with a number of manually-labeled social variables acquired using Amazon Mechanical Turk , as well as objective measures of dialogue success ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We propose an automated approach to detecting high-level organizational elements in argumentative discourse that combines a rule-based system and a probabilistic sequence model in a principled manner .", "Argumentative discourse contains not only language expressing claims and evidence , but also language used to organize these claims and pieces of evidence .", "Differentiating between the two may be useful for many applications , such as those that focus on the content ( e.g. , relation extraction ) of arguments and those that focus on the structure of arguments ( e.g. , automated essay scoring ) .", "We present quantitative results on a dataset of human-annotated persuasive essays , and qualitative analyses of performance on essays and on political debates ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost .", "We first show that their model can be approximated using structured belief propagation , with a gain in alignment quality stemming from the use of marginals in decoding .", "With this new constraint , we achieve a relative error reduction of 40 % in F5 and a 5.5x speed-up .", "We then consider a more flexible , non-ITG matching constraint which is less efficient for exact inference but more efficient for BP .", "For example , the model of DeNero and Klein ( 2010 ) uses an ITG constraint and beam-based Viterbi decoding for tractability , but is still slow ."]}
{"orig_sents": ["4", "1", "2", "0", "3"], "shuf_sents": ["In order to handle a large set of translation units , these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture .", "For lack of sufficient training data , most models only consider a small amount of context .", "As a partial remedy , we explore here several continuous space translation models , where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations .", "In small scale and large scale English to French experiments , we show that the resulting models can effectively be trained and used on top of a n-gram translation system , delivering significant improvements in performance .", "The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models ."]}
{"orig_sents": ["3", "1", "0", "2", "4"], "shuf_sents": ["The dialectal sentences are selected from a large corpus of Arabic web text , and translated using Amazon ? s Mechanical Turk .", "We use crowdsourcing to cheaply and quickly build LevantineEnglish and Egyptian-English parallel corpora , consisting of 1.1M words and 380k words , respectively .", "We use this data to build Dialectal Arabic MT systems , and find that small amounts of dialectal data have a dramatic impact on translation quality .", "Arabic Dialects present many challenges for machine translation , not least of which is the lack of data resources .", "When translating Egyptian and Levantine test sets , our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus ."]}
{"orig_sents": ["0", "4", "5", "2", "3", "1"], "shuf_sents": ["Standard entity clustering systems commonly rely on mention ( string ) matching , syntactic features , and linguistic resources like English WordNet .", "On an Arabic-English corpus that contains seven different text genres , our best model yields a 24.3 % F1 gain over the baseline .", "Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures .", "Crucially , we do not assume a pre-existing entity list ( knowledge base ) , so entity characteristics are unknown .", "When co-referent text mentions appear in different languages , these techniques can not be easily applied .", "Consequently , we develop new methods for clustering text mentions across documents and languages simultaneously , producing cross-lingual entity clusters ."]}
{"orig_sents": ["0", "4", "1", "2", "5", "3"], "shuf_sents": ["This paper addresses the extraction of event records from documents that describe multiple events .", "To exploit the inherent connections between field extraction and event identification , we propose to model them jointly .", "Our model is novel in that it integrates information from separate sequential models , using global potentials that encourage the extracted event records to have desired properties .", "We experiment with two data sets that consist of newspaper articles describing multiple terrorism events , and show that our model substantially outperforms traditional pipeline models .", "Specifically , we aim to identify the fields of information contained in a document and aggregate together those fields that describe the same event .", "While the model contains high-order potentials , efficient approximate inference can be performed with dualdecomposition ."]}
{"orig_sents": ["4", "1", "8", "0", "6", "3", "2", "7", "5"], "shuf_sents": ["Citing sentences that cite multiple papers are common in scientific writing .", "Citing sentences have been studied and used in many applications .", "In this paper , we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference .", "For instance , when a citing sentence is used in a summary of a scientific paper , only the fragments of the sentence that are relevant to the summarized paper should be included in the summary .", "A citing sentence is one that appears in a scientific article and cites previous work .", "Our experiments show that segment classification achieves the best results .", "This observation should be taken into consideration when using citing sentences in applications .", "Our methods are : word classification , sequence labeling , and segment classification .", "For example , they have been used in scientific paper summarization , automatic survey generation , paraphrase identification , and citation function classification ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["Intrinsic evaluation of our model ( i.e. , with respect to a gold standard ) yields results on par with prior work .", "Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise , when compared with only adapting to one affective state or ignoring affect entirely .", "We present a model for detecting user disengagement during spoken dialogue interactions .", "Correlation analyses show crucially that our automatic disengagement labels correlate with system performance in the same way as the gold standard ( manual ) labels , while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance .", "However , since our goal is immediate implementation in a system that already detects and adapts to user uncertainty , we go further than prior work and present an extrinsic evaluation of our model ( i.e. , with respect to the real-world task ) ."]}
{"orig_sents": ["1", "3", "0", "5", "4", "2"], "shuf_sents": ["In this paper , we explore features representing the accuracy of the content of a spoken response .", "Most previous research on automated speech scoring has focused on restricted , predictable speech .", "The correlations decrease somewhat due to recognition errors when evaluated on the output of an automatic speech recognition system ; however , the additional use of word confidence scores can achieve correlations at a similar level as for human transcriptions .", "For automated scoring of unrestricted spontaneous speech , speech proficiency has been evaluated primarily on aspects of pronunciation , fluency , vocabulary and language usage but not on aspects of content and topicality .", "All of the features exhibit moderately high correlations with human proficiency scores on human speech transcriptions .", "Content features are generated using three similarity measures , including a lexical matching method ( Vector Space Model ) and two semantic similarity measures ( Latent Semantic Analysis and Pointwise Mutual Information ) ."]}
{"orig_sents": ["0", "6", "5", "3", "1", "7", "2", "4"], "shuf_sents": ["This study aims to infer the social nature of conversations from their content automatically .", "This classification task provides a convenient tool to probe the nature of telephone conversations .", "For classifying different types of social relationships such as family vs other , we investigated features related to language use ( entropy ) , hand-crafted dictionary ( LIWC ) and topics learned using unsupervised latent Dirichlet models ( LDA ) .", "As a first step , we learned a binary classifier to filter out business related conversation , achieving an accuracy of about 85 % .", "Our results show that the posteriors over topics from LDA provide consistently higher accuracy ( 60-81 % ) compared to LIWC or language use features in distinguishing different types of conversations .", "For this purpose , we collected a comprehensive and naturalistic corpus comprising of all the incoming and outgoing telephone calls from 10 subjects over the duration of a year .", "To place this work in context , our motivation stems from the need to understand how social disengagement affects cognitive decline or depression among older adults .", "We evaluated the utility of openings and closing in differentiating personal calls , and find that empirical results on a large corpus do not support the hypotheses by Schegloff and Sacks that personal conversations are marked by unique closing structures ."]}
{"orig_sents": ["4", "1", "0", "3", "2", "5"], "shuf_sents": ["Some NLP phenomena , however , suggest CRFs with more complex topologies .", "It is well known how to train CRFs with certain topologies that admit exact inference , such as linear-chain CRFs .", "Stoyanov et al ( 2011 ) recently argued for training parameters to minimize the task-specific loss of whatever approximate inference and decoding methods will be used at test time .", "Should such models be used , considering that they make exact inference intractable ?", "Conditional Random Fields ( CRFs ) are a popular formalism for structured prediction in NLP .", "We apply their method to three NLP problems , showing that ( i ) using more complex CRFs leads to improved performance , and that ( ii ) minimumrisk training learns more accurate models ."]}
{"orig_sents": ["0", "4", "6", "7", "1", "2", "3", "5"], "shuf_sents": ["Most existing theory of structured prediction assumes exact inference , which is often intractable in many practical problems .", "This framework subsumes and justifies the popular heuristic ? early-update ?", "for perceptron with beam search ( Collins and Roark , 2004 ) .", "We also propose several new update methods within this framework , among which the ? max-violation ?", "This leads to the routine use of approximate inference such as beam search but there is not much theory behind it .", "method dramatically reduces training time ( by 3 fold as compared to earlyupdate ) on state-of-the-art part-of-speech tagging and incremental parsing systems .", "Based on the structured perceptron , we propose a general framework of ? violation-fixing ?", "perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We propose a new segmentation evaluation metric , called segmentation similarity ( S ) , that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance , essentially using edit distance as a penalty function and scaling penalties by segmentation size .", "We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance .", "We show that S is configurable enough to suit a wide variety of segmentation evaluations , and is an improvement upon the state of the art .", "We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation ."]}
{"orig_sents": ["3", "4", "2", "1", "0"], "shuf_sents": ["We conclude with a list of recommended practices for future research combining language and vision processing techniques .", "These results illuminate incorrect assumptions and improper practices regarding preprocessing , evaluation metrics , and the collection of gold image annotations .", "Some of our baselines match or exceed the best published scores for those datasets .", "We examine evaluation methods for systems that automatically annotate images using cooccurring text .", "We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval , computer vision , and extractive summarization ."]}
{"orig_sents": ["3", "0", "4", "1", "2"], "shuf_sents": ["We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus .", "Finally , we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics .", "We release both the new dataset and the error analysis annotations for use by the community .", "We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years .", "In addition , we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Derived from the Stanford dependency types , it consists of over 32K characters drawn from a collection of poems written in the 8th century CE .", "We report on the design of new dependency relations , discuss aspects of the annotation process and evaluation , and illustrate its use in a study of parallelism in Classical Chinese poetry .", "As interest grows in the use of linguistically annotated corpora in research and teaching of foreign languages and literature , treebanks of various historical texts have been developed .", "We introduce the first large-scale dependency treebank for Classical Chinese literature ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["The task is feasible but non-trivial , which is demonstrated by creating and comparing three alternative baseline systems .", "We believe that this corpus will be of interest to the researchers working in textual entailment and will stimulate new developments both in natural language processing in tutorial dialogue systems and textual entailment , contradiction detection and other techniques of interest for a variety of computational linguistics tasks .", "We propose a new shared task on grading student answers with the goal of enabling welltargeted and flexible feedback in a tutorial dialogue setting .", "We provide an annotated corpus designed for the purpose , a precise specification for a prediction task and an associated evaluation methodology ."]}
{"orig_sents": ["2", "3", "1", "4", "6", "0", "5"], "shuf_sents": ["We propose to account for this in a simple modification of the windowDiff metric .", "The results suggest that , while the overall agreement is relatively low , the annotators show high agreement on a subset of topical breaks ?", "In a large-scale study of how people find topical shifts in written text , 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel .", "We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns .", "places where most prominent topic shifts occur .", "We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks , and emphasize the more insightful nature of the latter analysis .", "We recommend taking into account the prominence of topical shifts when evaluating topical segmentation , effectively penalizing more severely the errors on more important breaks ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["We review well-known algorithms , arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation .", "We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others .", "This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning , specifically the framework of empirical risk minimization .", "Most notably , our structured ramp loss minimization algorithm , RAMPION , is less sensitive to initialization and random seeds than standard approaches .", "Instead , most have implicit connections to particular forms of ramp loss ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["This involves implicitly intersecting up to 100 automata .", "The algorithm is based on dual decomposition : the automata attempt to agree on a string by communicating about features of the string .", "We propose an algorithm to find the best path through an intersection of arbitrarily many weighted automata , without actually performing the intersection .", "We demonstrate the algorithm on the Steiner consensus string problem , both on synthetic data and on consensus decoding for speech recognition ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["We present an online learning algorithm for statistical machine translation ( SMT ) based on stochastic gradient descent ( SGD ) .", "We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm .", "Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results .", "Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs .", "Under the online setting of rank learning , a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that can not be linearly decomposed into a sentence-wise loss , such as BLEU ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation .", "Every sensible tree transformation computed by an arbitrary weighted extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer .", "A tree transformation is sensible if the size of each output tree is uniformly bounded by a linear function in the size of the corresponding input tree ."]}
{"orig_sents": ["2", "1", "0", "5", "4", "3"], "shuf_sents": ["e-mails titling , text generation , summarization , and so forth .", "Automatic titling is an essential task for several applications : ? No Subject ?", "The important mass of textual documents is in perpetual growth and requires strong applications to automatically process information .", "The evaluation of the approach , described in the paper , indicates that titles stemming from this method are informative and/or catchy .", "In particular , morphological and semantic processing are employed to obtain a nominalized form which has to respect titles characteristics ( in particular , relevance and catchiness ) .", "This study presents an original approach consisting in titling journalistic articles by nominalizing ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["For both tasks , we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work .", "We present a system for comma error correction in English that achieves an average of 89 % precision and 25 % recall on two corpora of unedited student essays .", "While the field of grammatical error detection has progressed over the past few years , one area of particular difficulty for both native and non-native learners of English , comma placement , has been largely ignored .", "This system also achieves state-of-theart performance in the sister task of restoring commas in well-formed text ."]}
{"orig_sents": ["3", "5", "7", "1", "2", "4", "6", "0"], "shuf_sents": ["ambiguities ; and quantifying the impact of constructions like pro-drop .", "72.73 ( P & K ) and 67.09 ( C & C ) F -score on ? ? ? ?", "6 .", "We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese with the new Chinese CCGbank , bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent .", "We explore the challenges of Chinese ? ? ?", "We train two state-of-the-art English ? ? ?", "parsing through three novel ideas : developing corpus variants rather than treating the corpus as fixed ; controlling noun/verb and other ? ? ?", "parsers : the parser of Petrov and Klein ( P & K ) , and the Clark and Curran ( C & C ) parser , uncovering a surprising performance gap between them not observed in English ?"]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["The resulting system significantly outperforms previous work in such automatic conversion .", "We investigate the problem of automatically converting from a dependency representation to a phrase structure representation , a key aspect of understanding the relationship between these two representations for NLP work .", "A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing .", "We also achieve comparable results to a system using a phrase-structure parser for the conversion .", "We implement a new approach to this problem , based on a small number of supertags , along with an encoding of some of the underlying principles of the Penn Treebank guidelines ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser .", "We find increases in accuracy of up to 5.3 % absolute .", "While some of this results from the feature set capturing information unrelated to morphology , there is still significant improvement , up to 4.6 % absolute , due to the agreement model .", "Compared to the built-in morphological feature set , ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks ."]}
{"orig_sents": ["0", "2", "1", "4", "3"], "shuf_sents": ["We present an approach to automatically recover hidden attributes of scientific articles , such as whether the author is a native English speaker , whether the author is a male or a female , and whether the paper was published in a conference or workshop proceedings .", "The classifiers perform well in this challenging domain , identifying non-native writing with 95 % accuracy ( over a baseline of 67 % ) .", "We train classifiers to predict these attributes in computational linguistics papers .", "We give a detailed analysis of which words and syntax most predict a particular attribute , and we show a strong correlation between our predictions and a paper ? s number of citations .", "We show the benefits of using syntactic features in stylometry ; syntax leads to significant improvements over bag-of-words models on all three tasks , achieving 10 % to 25 % relative error reduction ."]}
{"orig_sents": ["5", "1", "4", "3", "0", "2"], "shuf_sents": ["Our system achieves state-of-the-art results on the first story detection task , beating both the best supervised and unsupervised systems .", "A major problem in this task is the high degree of lexical variation in documents which makes it very difficult to detect stories that talk about the same event but expressed using different words .", "To test our approach on large data , we construct a corpus of events for Twitter , consisting of 50 million documents , and show that paraphrasing is also beneficial in this domain .", "We show a novel way of integrating paraphrases with locality sensitive hashing ( LSH ) in order to obtain an efficient FSD system that can scale to very large datasets .", "We suggest using paraphrases to alleviate this problem , making this the first work to use paraphrases for FSD .", "First story detection ( FSD ) involves identifying first stories about events from a continuous stream of documents ."]}
{"orig_sents": ["4", "3", "5", "2", "6", "1", "0"], "shuf_sents": ["The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users .", "We present a prototype writing assistant , TransAhead , that applies the method to computer-assisted translation and language learning .", "The method involves learning syntax-based phraseology and translation equivalents .", "In our approach , predictions are offered aimed at alleviating users ?", "We introduce a method for learning to predict text completion given a source text and partial translation .", "burden on lexical and grammar choices , and improving productivity .", "At run-time , the source and its translation prefix are sliced into ngrams to generate and rank completion candidates , which are then displayed to users ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["We define a set of linguistically motivated feature templates for a log-linear classification model , train this classifier on sentence pairs extracted from the Cambridge Learner Corpus , and achieve 89 % accuracy improving upon a 33 % baseline .", "Furthermore , we incorporate our classifier into a novel application that takes as input a set of corrected essays that have been sentence aligned with their originals and outputs the individual corrections classified by error type .", "We report the F-Score of our implementation on this task .", "We present a classifier that discriminates between types of corrections made by teachers of English in student essays ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We introduce a new segmentation evaluation measure , WinPR , which resolves some of the limitations of WindowDiff .", "WinPR distinguishes between false positive and false negative errors ; produces more intuitive measures , such as precision , recall , and F-measure ; is insensitive to window size , which allows us to customize near miss sensitivity ; and is based on counting errors not windows , but still provides partial reward for near misses ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Our methods can be used with any G2P algorithm that outputs posterior probabilities of phoneme sequences for a given word .", "Motivated by the fact that the pronunciation of a name may be influenced by its language of origin , we present methods to improve pronunciation prediction of proper names using word origin information .", "We train grapheme-to-phoneme ( G2P ) models on language-specific data sets and interpolate the outputs .", "We perform experiments on US surnames , a data set where word origin variation occurs naturally ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We evaluate the performance of an morphological analyser for Inuktitut across a mediumsized corpus , where it produces a useful analysis for two out of every three types .", "Our observations show that the richer approaches provide little as compared to simply finding the head , which is more in line with the particularities of the task .", "We then compare its segmentation to that of simpler approaches to morphology , and use these as a pre-processing step to a word alignment task ."]}
{"orig_sents": ["2", "5", "1", "4", "0", "3"], "shuf_sents": ["In this paper , we perform experiments on automatically and manually generated transcripts .", "weights are computed from the topical similarity between the utterances , evaluated using probabilistic latent semantic analysis ( PLSA ) , and from word overlap .", "This paper proposes an improved approach to extractive summarization of spoken multi-party interaction , in which integrated random walk is performed on a graph constructed on topical/ lexical relations .", "For automatic transcripts , our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances .", "We model intra-speaker topics by partially sharing the topics from the same speaker in the graph .", "Each utterance is represented as a node of the graph , and the edges ?"]}
{"orig_sents": ["5", "2", "3", "4", "1", "0"], "shuf_sents": ["This pilot result is a step towards improving ASR more generally by using EEG to distinguish mental states .", "We analyze how its performance depends on EEG classification accuracy .", "We use a previously published method ( Mostow et al. , 2011 ) to train the EEG classifier .", "We use its probabilistic output to control weighted interpolation of separate language models for easy and difficult reading .", "The EEG-adapted ASR achieves higher accuracy than two baselines .", "We report on a pilot experiment to improve the performance of an automatic speech recognizer ( ASR ) by using a single-channel EEG signal to classify the speaker ? s mental state as reading easy or hard text ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Even though the model is generic and we use the same architecture and features for all languages , the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus , ranging from 3 % to 11 % .", "We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages .", "We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["Sequential transduction tasks , such as grapheme-to-phoneme conversion and machine transliteration , are usually addressed by inducing models from sets of input-output pairs .", "We describe several experiments that involve a variety of supplemental data and two state-of-the-art transduction systems , yielding error rate reductions ranging from 12 % to 43 % .", "We further apply our approach to system combination , with error rate reductions between 4 % and 9 % .", "We apply a unified reranking approach to both grapheme-to-phoneme conversion and machine transliteration demonstrating substantial accuracy improvements by utilizing heterogeneous transliterations and transcriptions of the input word .", "Supplemental representations offer valuable additional information , but incorporating that information is not straightforward ."]}
{"orig_sents": ["2", "3", "1", "4", "0"], "shuf_sents": ["The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction .", "Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step .", "In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations .", "The model is essentially an infinite HMM that infers the number of states from data .", "We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided ."]}
{"orig_sents": ["3", "2", "4", "0", "1"], "shuf_sents": ["In this paper , we introduce a novel method using forced decoding to confirm the validity of this constraint , and we demonstrate that it can be exploited in order to improve machine translation quality .", "Three ways of incorporating such a preference into a hierarchical phrase-based MT model are proposed , and the approach where all three are combined yields the greatest improvements for both Arabic-English and ChineseEnglish translation experiments .", "and it has been argued that a related ? one translation per discourse ?", "It has long been observed that monolingual text exhibits a tendency toward ? one sense per discourse , ?", "constraint is operative in bilingual contexts as well ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We perform empirical comparisons of eight different tuning strategies , including MERT , in a variety of settings .", "There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach .", "Among other results , we find that a simple and efficient batch version of MIRA performs at least as well as training online , and consistently outperforms other options .", "We analyze a number of these algorithms in terms of their sentencelevel loss functions , which motivates several new approaches , including a Structured SVM ."]}
{"orig_sents": ["1", "6", "0", "5", "2", "3", "4"], "shuf_sents": ["We investigate the problem in a novel real-time Session Initiation Protocol ( SIP ) based S2S framework .", "In a conventional telephone conversation between two speakers of the same language , the interaction is real-time and the speakers process the information stream incrementally .", "We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog .", "We present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation .", "Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach .", "The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition .", "In this work , we address the problem of incremental speech-to-speech translation ( S2S ) that enables cross-lingual communication between two remote participants over a telephone ."]}
{"orig_sents": ["4", "2", "5", "0", "1", "3"], "shuf_sents": ["In this way , we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework .", "We achieve an accuracy of 72 % on an adapted TempEval-2 task ?", "While most approaches to the task have used regular expressions and similar linear pattern interpretation rules , the possibility of phrasal embedding and modification in time expressions motivates our use of a compositional grammar of time expressions .", "comparable to state of the art systems .", "We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference .", "This grammar is used to construct a latent parse which evaluates to the time the phrase would represent , as a logical parse might evaluate to a concrete entity ."]}
{"orig_sents": ["4", "2", "0", "3", "1"], "shuf_sents": ["In this paper , novel ideas to reveal positive implicit meaning using focus of negation are presented .", "New annotation and features to detect fine-grained focus are discussed and results reported .", "Regardless of the semantic representation one adopts , pinpointing the positive concepts within a negated statement is needed in order to encode the statement ? s meaning .", "The concept of granularity of focus is introduced and justified .", "Negated statements often carry positive implicit meaning ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["This paper presents a novel approach for inducing lexical taxonomies automatically from text .", "Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm .", "We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms .", "Essentially , the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph .", "We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness ."]}
{"orig_sents": ["2", "0", "5", "1", "4", "3"], "shuf_sents": ["While previous work has focused primarily on English , we extend these results to other languages along two dimensions .", "Second , and more interestingly , we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction .", "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure .", "When applying the same method to direct transfer of named-entity recognizers , we observe relative improvements of up to 26 % .", "Specifically , we show that by augmenting direct-transfer systems with cross-lingual cluster features , the relative error of delexicalized dependency parsers , trained on English treebanks and transferred to foreign languages , can be reduced by up to 13 % .", "First , we show that these results hold true for a number of languages across families ."]}
{"orig_sents": ["0", "2", "5", "6", "4", "1", "3"], "shuf_sents": ["We introduce lightly supervised learning for dependency parsing .", "We show on dependency parsing tasks in 14 languages that with only 1 % of fully labeled data , and light-feedback on the remaining 99 % of the training data , our algorithm achieves , on average , only 5 % lower performance than when training with fully annotated training set .", "In this paradigm , the algorithm is initiated with a parser , such as one that was built based on a very limited amount of fully annotated training data .", "We also evaluate the algorithm in different feedback settings and show its robustness to noise .", "There is no direct information about the correctness of any edge .", "Then , the algorithm iterates over unlabeled sentences and asks only for a single bit of feedback , rather than a full parse tree .", "Specifically , given an example the algorithm outputs two possible parse trees and receives only a single bit indicating which of the two alternatives has more correct edges ."]}
{"orig_sents": ["1", "4", "2", "3", "0"], "shuf_sents": ["Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages .", "Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .", "Our first- , second- , and third-order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .", "We observe speed-ups of up to two orders of magnitude compared to exhaustive search .", "We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades ."]}
{"orig_sents": ["4", "1", "3", "2", "0"], "shuf_sents": ["Experiments show that this new method outperforms random sampling in terms of both annotation effort and peak performance .", "( i ) It uses bootstrapped neighborhood pooling , which ensures a class-balanced pool even though gold labels are not available .", "( iii ) It is based on a query-by-committee selection strategy in contrast to earlier uncertainty sampling work .", "( ii ) It employs neighborhood selection , a selection strategy that ensures coverage of both positive and negative links for selected markables .", "We present an active learning method for coreference resolution that is novel in three respects ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Recent exploratory efforts in discourse-level language modeling have relied heavily on calculating Pointwise Mutual Information ( PMI ) , which involves significant computation when done over large collections .", "This is demonstrated in the context of inducing Shankian script-like structures over news articles .", "We show the method of Conditional Random Sampling , thus far an underutilized technique , to be a space-efficient means of representing the sufficient statistics in discourse that underly recent PMI-based work .", "Prior work has required aggressive pruning or independence assumptions to compute scores on large collections ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We present an email corpus with utterances annotated for ODP and present a supervised learning system to predict it .", "We obtain a best cross validation F-measure of 65.8 using gold dialog act features and 55.6 without using them .", "We analyze overt displays of power ( ODPs ) in written dialogs ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["This paper describes our ongoing work on resolving third person pronouns and deictic words in a multi-modal corpus .", "The usage of hapticostensive actions in a co-reference model is a novel contribution of our work .", "We show that about two thirds of these referring expressions have antecedents that are introduced by pointing gestures or by haptic-ostensive actions ( actions that involve manipulating an object ) .", "After describing our annotation scheme , we discuss the co-reference models we learn from multi-modal features ."]}
{"orig_sents": ["3", "5", "1", "0", "2", "4"], "shuf_sents": ["and ? average rule length . ?", "These traits are simple properties of the MT output such as ? average output length ?", "Our method is designed to select hypotheses which vary in trait value but do not significantly degrade in BLEU score .", "In the area of machine translation ( MT ) system combination , previous work on generating input hypotheses has focused on varying a core aspect of the MT system , such as the decoding algorithm or alignment algorithm .", "These hypotheses can be combined using standard system combination techniques to produce a 1.21.5 BLEU gain on the Arabic-English NIST MT06/MT08 translation task .", "In this paper , we propose a new method for generating diverse hypotheses from a single MT system using traits ."]}
{"orig_sents": ["4", "0", "2", "1", "3", "5"], "shuf_sents": ["However , Shallow-n grammars require parameters which can not be directly optimized using minimum error-rate tuning by the decoder .", "We introduce two rules : a BITG-style reordering glue rule and a simpler monotonic concatenation rule .", "This paper introduces some novel improvements to the translation model for Shallow-n grammars .", "We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights .", "Shallow-n grammars ( de Gispert et al , 2010 ) were introduced to reduce over-generation in the Hiero translation model ( Chiang , 2005 ) resulting in much faster decoding and restricting reordering to a desired level for specific language pairs .", "We show this formulation of Shallow-n hierarchical phrasebased translation is comparable in translation quality to full Hiero-style decoding ( without shallow rules ) while at the same time being considerably faster ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We do this with existing machinery , making use of an existing word alignment model for this task .", "Isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction .", "We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline .", "We present a novel method to detect parallel fragments within noisy parallel corpora ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We propose a tuning method for statistical machine translation , based on the pairwise ranking approach .", "Hopkins and May ( 2011 ) presented a method that uses a binary classifier .", "In this work , we use linear regression and show that our approach is as effective as using a binary classifier and converges faster ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Automating this process has been a challenge addressed before in the computational linguistics literature , with most studies attempting to predict the particular grade level of a text .", "However , guided reading levels developed by educators operate at a more fine-grained level , with multiple levels corresponding to each grade .", "We find that ranking performs much better than classification at the fine-grained leveling task , and that features derived from the visual layout of a book are just as predictive as standard text features of level ; including both sets of features , we find that we can predict the reading level up to 83 % of the time on a small corpus of children ? s books .", "Determining the reading level of children ? s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms , namely TextTiling and C99 .", "Additionally , we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information .", "This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms .", "We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Much information can be gained from comparable texts , and we present an algorithm which , given any bodies of text in multiple languages , uses existing named entity recognition software and topic detection algorithm to generate pairs of comparable texts without requiring a parallel corpus training phase .", "We evaluate the system ? s performance firstly on data from the online newspaper domain , and secondly on Wikipedia cross-language links .", "Parallel corpora have applications in many areas of Natural Language Processing , but are very expensive to produce ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We also analyze user annotation behaviors to find that certain labeling actions have an impact on classifier accuracy , drawing attention to the important role these behavioral factors play in interactive learning systems .", "We attempt to replicate previous results using multiple ? average ?", "Internet users instead of a few domain experts as annotators .", "This paper describes a user study where humans interactively train automatic text classifiers ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We test our M2 scorer on the Helping Our Own ( HOO ) shared task data and show that our method results in more accurate evaluation for grammatical error correction .", "The core of our method , which we call MaxMatch ( M2 ) , is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation .", "We present a novel method for evaluating grammatical error correction .", "This optimal edit sequence is subsequently scored using F1 measure ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We describe and evaluate several methods for estimating the confidence in the per-edge correctness of a predicted dependency parse .", "We show empirically that the confidence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efficiently .", "We evaluate our methods on parsing text in 14 languages ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We then present concave models for dependency grammar induction and validate them experimentally .", "We find our concave models to be effective initializers for the dependency model of Klein and Manning ( 2004 ) and show that we can encode linguistic knowledge in them for improved performance .", "We begin with the most well-known example , IBM Model 1 for word alignment ( Brown et al , 1993 ) and analyze its properties , discussing why other models for unsupervised learning are so seldom concave .", "We investigate models for unsupervised learning with concave log-likelihood functions ."]}
{"orig_sents": ["4", "0", "5", "3", "1", "2"], "shuf_sents": ["The implementation extends the English Resource Grammar ( ERG , Flickinger ( 2000 ) ) with HPSG types and rules that capture the form of the linguistic signal , the form of the gestural signal and their relative timing to constrain the meaning of the multimodal action .", "We capture this by identical speech and gesture token edges .", "Further , the semantic contribution of gestures is encoded by lexical rules transforming a speech phrase into a multimodal entity of conjoined spoken and gestural semantics .", "Using the current machinery , the main challenge for the grammar engineer is the nonlinear input : the modalities can overlap temporally .", "This paper reports on an implementation of a multimodal grammar of speech and co-speech gesture within the LKB/PET grammar engineering environment .", "The grammar yields a single parse tree that integrates the spoken and gestural modality thereby drawing on standard semantic composition techniques to derive the multimodal meaning representation ."]}
{"orig_sents": ["1", "0", "4", "3", "2"], "shuf_sents": ["Some prior research finds no gain over and above what is obtained with ngram features ? arguably the most widely used features in text classification .", "Are word-level affect lexicons useful in detecting emotions at sentence level ?", "On the other hand , affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain .", "We further show that while ngram features tend to be accurate , they are often unsuitable for use in new domains .", "Here , we experiment with two very different emotion lexicons and show that even in supervised settings , an affect lexicon can provide significant gains ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Public debate functions as a forum for both expressing and forming opinions , an important aspect of public life .", "We present results for automatically classifying posts in online debate as to the position , or STANCE that the speaker takes on an issue , such as Pro or Con .", "We show that representing the dialogic structure of the debates in terms of agreement relations between speakers , greatly improves performance for stance classification , over models that operate on post content and parentpost context alone ."]}
{"orig_sents": ["0", "5", "4", "3", "2", "1"], "shuf_sents": ["Sentiment analysis of citations in scientific papers and articles is a new and interesting problem which can open up many exciting new applications in bibliographic search and bibliometrics .", "We then explore the effect of context windows of different lengths on the performance of a stateof-the-art citation sentiment detection system when using this context-enhanced gold standard definition .", "We believe that this gold standard is closer to the truth than annotation that looks only at the citation sentence itself .", "We present a new citation sentiment corpus which has been annotated to take the dominant sentiment in the entire citation context into account .", "In this paper , we address the problem of context-enhanced citation sentiment detection .", "Current work on citation sentiment detection focuses on only the citation sentence ."]}
{"orig_sents": ["4", "1", "5", "3", "0", "2"], "shuf_sents": ["The feature design process leverages aggregate statistics over the entire social network to balance sparsity and informativeness .", "Predicting whether a message will elicit a user response opens the possibility of maximizing the virality , reach and effectiveness of messages and ad campaigns on these networks .", "We use real-world tweets to train models and empirically show that they are capable of generating accurate predictions for a large number of tweets .", "The approach uses features derived from various sources , such as the language used in the tweet , the user ? s social network and history .", "Microblogging networks serve as vehicles for reaching and influencing users .", "We propose a discriminative model for predicting the likelihood of a response or a retweet on the Twitter network ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Although first names and nicknames in the United States have been well documented , there has been almost no quantitative analysis on the usage and association of these names amongst themselves .", "To the best of our knowledge , this is the largest collection of its kind , making it a natural resource for tasks such as coreference resolution , record linkage , named entity recognition , people and expert search , information extraction , demographic and sociological studies , etc .", "The collection will be made freely available .", "In this paper we introduce the Intelius Nickname Collection , a quantitative compilation of millions of namenickname associations based on information gathered from billions of public records ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We show that the models are essentially equivalent if syntactic information is ignored , and that the substantial performance differences previously reported disappear to a large extent when these simplified variants are evaluated under identical conditions .", "We clarify the connections between these models , simplify their formulation and evaluate them in a unified setting .", "Furthermore , our reformulation allows for the design of a straightforward and fast implementation .", "This paper compares a number of recently proposed models for computing context sensitive word similarity ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Our model consists of a committee of vector space models built on a text corpus , Web search results and thesauruses , and measures the semantic word relatedness using the averaged cosine similarity scores .", "Despite its simplicity , our system correlates with human judgements better or similarly compared to existing methods on several benchmark datasets , including WordSim353 .", "Noticing that different information sources often provide complementary coverage of word sense and meaning , we propose a simple and yet effective strategy for measuring lexical semantics ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["Given a parallel corpus , if two distinct words in language A , a1 and a2 , are aligned to the same word b1 in language B , then this might signal that b1 is polysemous , or it might signal a1 and a2 are synonyms .", "Both assumptions with successful work have been put forward in the literature .", "We investigate these assumptions , along with other questions of word sense , by looking at sampled parallel sentences containing tokens of the same type in English , asking how often they mean the same thing when they are : 1. aligned to the same foreign type ; and 2. aligned to different foreign types .", "Results for French-English and Chinese-English parallel corpora show similar behavior : Synonymy is only very weakly the more prevalent scenario , where both cases regularly occur ."]}
{"orig_sents": ["3", "4", "0", "2", "1"], "shuf_sents": ["In contrast , we confront this challenge head on using the MapReduce framework .", "Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline ( 2.39 BLEU points in the best case ) .", "On a modest cluster , our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia .", "It is well known that the output quality of statistical machine translation ( SMT ) systems increases with more training data .", "To obtain more parallel text for translation modeling , researchers have turned to the web to mine parallel sentences , but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Our method clusters sentences based on their timestamps and temporal similarity .", "Each resulting cluster is assigned an importance score which can then be used as a weight in traditional sentence ranking techniques .", "Temporal importance weighting offers consistent improvements over baseline systems .", "In this paper , we investigate the use of temporal information for improving extractive summarization of historical articles ."]}
{"orig_sents": ["0", "4", "2", "1", "3"], "shuf_sents": ["Natural Language Generation ( NLG ) systems often use a pipeline architecture for sequential decision making .", "Our focus will be on a comparison of Bayesian Networks and HMMs in terms of user satisfaction and naturalness .", "We present a joint learning framework based on Hierarchical Reinforcement Learning ( HRL ) which uses graphical models for surface realisation .", "While the former perform best in isolation , the latter present a scalable alternative within joint systems .", "Recent studies however have shown that treating NLG decisions jointly rather than in isolation can improve the overall performance of systems ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["In recent years we start seeing deployments of referring expression generators moving away from limited domains with custom-made ontologies .", "Our experiments on a fully annotated anaphora resolution training set and a larger , volunteersubmitted news corpus show that existing algorithms are efficient enough to deal with large scale ontologies but need to be extended to deal with undefined values and some measure for information salience .", "In this work , we explore the feasibility of using large scale noisy ontologies ( folksonomies ) for open domain referring expression generation , an important task for summarization by re-generation .", "Generating referring expressions has received considerable attention in Natural Language Generation ."]}
{"orig_sents": ["1", "3", "4", "0", "2"], "shuf_sents": ["Rather than retrieving individual microblog messages in response to an event query , we propose retrieving a ranked list of historical event summaries by distilling high quality event representations using a novel temporal query expansion technique .", "Microblog streams often contain a considerable amount of information about local , regional , national , and global events .", "The results of an exploratory study carried out over a large archive of Twitter messages demonstrates both the value of the microblog event retrieval task and the effectiveness of our proposed search methodologies .", "Most existing microblog search capabilities are focused on recent happenings and do not provide the ability to search and explore past events .", "This paper proposes the problem of structured retrieval of historical event information over microblog archives ."]}
{"orig_sents": ["1", "2", "3", "4", "5", "0"], "shuf_sents": ["Since this is an introductory paper , we present baseline results on these tasks using off-the-shelf NLP solutions , and encourage the NLP community to contribute better models in the future .", "We introduce the social study of bullying to the NLP community .", "Bullying , in both physical and cyber worlds ( the latter known as cyberbullying ) , has been recognized as a serious national health issue among adolescents .", "However , previous social studies of bullying are handicapped by data scarcity , while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes .", "Our main contribution is to present evidence that social media , with appropriate natural language processing techniques , can be a valuable and abundant data source for the study of bullying in both worlds .", "We identify several key problems in using such data sources and formulate them as NLP tasks , including text classification , role labeling , sentiment analysis , and topic modeling ."]}
{"orig_sents": ["5", "1", "2", "4", "3", "0"], "shuf_sents": ["We demonstrate that our supervised model performs better than baselines that ignore syntactic features and constraints .", "This is because of a lack of both ( 1 ) annotated data at the word level and ( 2 ) algorithms that can leverage syntactic information in a principled way .", "We address the first need by annotating articles from the information technology business press via crowdsourcing to provide training and testing data .", "We show that a factor graph derived from this data structure acquires these relationships with a small number of word-level features .", "To address the second need , we propose a suffix-tree data structure to represent syntactic relationships between opinion targets and words in a sentence that are opinion-bearing .", "Existing work in fine-grained sentiment analysis focuses on sentences and phrases but ignores the contribution of individual words and their grammatical connections ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["To achieve compactness , we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markov networks constructed from labeled and unlabeled data .", "We present novel methods to construct compact natural language lexicons within a graphbased semi-supervised learning framework , an attractive platform suited for propagating soft labels onto new natural language types from seed data .", "Compared to standard graph-based learning methods , for two lexicon expansion problems , our approach produces significantly smaller lexicons and obtains better predictive performance .", "Sparse measures are desirable for high-dimensional multi-class learning problems such as the induction of labels on natural language types , which typically associate with only a few labels ."]}
{"orig_sents": ["3", "2", "4", "1", "0"], "shuf_sents": ["Furthermore , experiments on POS tagging , information extraction , and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn ? t available earlier , exhibiting the benefits of the UEM framework .", "UEM is as efficient and easy to implement as standard EM .", "UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM , constrained versions of EM such as ConstraintDriven Learning ( Chang et al , 2007 ) and Posterior Regularization ( Ganchev et al , 2010 ) , along with a range of new EM algorithms .", "We present a general framework containing a graded spectrum of Expectation Maximization ( EM ) algorithms called Unified Expectation Maximization ( UEM . )", "For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature ( Ganchev et al , 2008 ; Koo et al , 2010 ; Rush and Collins , 2011 ) ."]}
{"orig_sents": ["4", "2", "3", "1", "0"], "shuf_sents": ["Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches .", "A key quality of our approach is that feature engineering can be done separately on the input and output spaces ; the relationship between inputs and outputs is learned automatically .", "We propose a novel family of reranking algorithms based on learning separate low-dimensional embeddings of the task ? s input and output spaces .", "This embedding is learned in such a way that prediction becomes a low-dimensional nearest-neighbor search , which can be done computationally efficiently .", "The accuracy of many natural language processing tasks can be improved by a reranking step , which involves selecting a single output from a list of candidate outputs generated by a baseline system ."]}
{"orig_sents": ["1", "5", "4", "3", "0", "2"], "shuf_sents": ["As part of this investigation , we collected a dataset of autocorrection mistakes from true text message users and experimented with a rich set of features in our self-assessment task .", "Text input aids such as automatic correction systems play an increasingly important role in facilitating fast text entry and efficient communication between text message users .", "Our experimental results indicate that there are salient cues from the text message discourse that allow systems to assess their own behaviors with high precision .", "To address this , this paper presents a novel task of self-assessment of autocorrection performance based on interactions between text message users .", "To improve its autocorrection performance , it is important for the system to have the capability to assess its own performance and learn from its mistakes .", "Although these tools are beneficial when they work correctly , they can cause significant communication problems when they fail ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Experimental results on two target languages demonstrate the promise of our approach .", "However , the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difficult to deploy coreference technologies across a large number of natural languages .", "To build a coreference resolver for a new language , the typical approach is to first coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques .", "To alleviate this corpus annotation bottleneck , we examine a translation-based projection approach to multilingual coreference resolution ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["We also train MaxEnt models with expectation constraints , using posterior regularization , and find that posterior regularization performs comparably to or slightly better than co-training .", "By extracting semantic and temporal features of medical concepts found in clinical text , we create conditionally independent data views ; co-training MaxEnt classifiers on this data works almost as well as supervised learning for the task of pairwise coreference resolution of medical concepts .", "We investigate the task of medical concept coreference resolution in clinical text using two semi-supervised methods , co-training and multi-view learning with posterior regularization .", "We describe the process of semantic and temporal feature extraction and demonstrate our methods on a corpus of case reports from the New England Journal of Medicine and a corpus of patient narratives obtained from The Ohio State University Wexner Medical Center ."]}
{"orig_sents": ["5", "3", "6", "2", "4", "0", "1"], "shuf_sents": ["Our data shows that good gaps are of variable length and span all semantic roles , i.e. , nouns as well as verbs , and that a majority of good questions do not focus on named entities .", "Our resulting system can generate fill-in-the-blank ( cloze ) questions from generic source materials .", "Our work focuses on the complementary aspect of determining what part of a sentence we should be asking about in the first place ; we call this ? gap selection . ?", "Our goal is to provide such learners with the well-known benefits of testing by automatically generating quiz questions for online text .", "We address this problem by asking human judges about the quality of questions generated from a Wikipedia-based corpus , and then training a model to effectively replicate these judgments .", "Not all learning takes place in an educational setting : more and more self-motivated learners are turning to on-line text to learn about new topics .", "Prior work on question generation has focused on the grammaticality of generated questions and generating effective multiple-choice distractors for individual question targets , both key parts of this problem ."]}
{"orig_sents": ["4", "5", "1", "2", "0", "3", "6"], "shuf_sents": ["Rather than breaking up the generation process into a sequence of local decisions , we define a probabilistic context-free grammar that globally describes the inherent structure of the input ( a corpus of database records and text describing some of them ) .", "and surface realization ( ? how to say ? )", "in an unsupervised domain-independent fashion .", "We represent our grammar compactly as a weighted hypergraph and recast generation as the task of finding the best derivation tree for a given input .", "Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input .", "We present a joint model that captures content selection ( ? what to say ? )", "Experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain specific constraints , explicit feature engineering or labeled data ."]}
{"orig_sents": ["2", "4", "3", "0", "1"], "shuf_sents": ["We find that using text alone , we are able to achieve high accuracies at this task , and that incorporating features derived from computer vision algorithms improves performance .", "Finally , we show that we can reliably mine visual nouns and adjectives from large corpora and that we can use these effectively in the classification task .", "When people describe a scene , they often include information that is not visually apparent ; sometimes based on background knowledge , sometimes to tell a story .", "To do so , we first concretely define what it means to be visual , annotate visual text and then develop algorithms to automatically classify noun phrases as visual or non-visual .", "We aim to separate visual text ? descriptions of what is being seen ? from non-visual text in natural images and their descriptions ."]}
{"orig_sents": ["3", "1", "0", "4", "2"], "shuf_sents": ["In addition to describing our approach , we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation .", "Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the softK-Means algorithm .", "Finally , we describe a method for annotating clusters with usage examples .", "We propose an unsupervised method for clustering the translations of a word , such that the translations in each cluster share a common semantic sense .", "By comparing our induced clusters to reference clusters generated from WordNet , we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora ."]}
{"orig_sents": ["5", "0", "1", "4", "3", "2"], "shuf_sents": ["Much less attention has been given to the underlying structure of the topics themselves .", "As a result , most topic models generate topics independently from a single underlying distribution and require millions of parameters , in the form of multinomial distributions over the vocabulary .", "The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters .", "Our model learns these component distributions and the structure of how to combine subsets of them into topics .", "In this paper , we introduce the Shared Components Topic Model ( SCTM ) , in which each topic is a normalized product of a smaller number of underlying component distributions .", "With a few exceptions , extensions to latent Dirichlet alocation ( LDA ) have focused on the distribution over topics for each document ."]}
{"orig_sents": ["4", "0", "3", "2", "1"], "shuf_sents": ["In this paper , we focus on one of the most precarious and least understood stages in a bill ? s life : its consideration , behind closed doors , by a Congressional committee .", "These models give significant reductions in prediction error and highlight the importance of bill substance in explanations of policy-making and agenda-setting .", "We augment the model with information from the contents of bills , comparing different hypotheses about how a committee decides a bill ? s fate .", "We construct predictive models of whether a bill will survive committee , starting with a strong , novel baseline that uses features of the bill ? s sponsor and the committee it is referred to .", "A U.S. Congressional bill is a textual artifact that must pass through a series of hurdles to become a law ."]}
{"orig_sents": ["3", "1", "2", "4", "6", "0", "8", "7", "9", "5"], "shuf_sents": ["We report on experiments to find experts in a university domain using two different methods to extract a ranked list of candidates : a database-driven method and a data-driven method .", "Many different methods have been devised to address this need .", "Applying these methods to identify the experts within an organisation has attracted a lot of attention .", "Effective knowledge management is a key factor in the development and success of any organisation .", "We look at one such problem that arises within universities on a daily basis but has attracted little attention in the literature , namely the problem of a searcher who is trying to identify a potential PhD supervisor , or , from the perspective of the university ? s research office , to allocate a PhD application to a suitable supervisor .", "As a baseline , we use a system that ranks candidates simply based on frequency of occurrence within the top documents .", "We reduce this problem to identifying a ranked list of experts for a given query ( representing a research area ) .", "all members of academic staff ) while the second method is based on automatic Named-Entity Recognition ( NER ) .", "The first one is based on a fixed list of experts ( e.g .", "We use a graded weighting based on proximity between query and candidate name to rank the list of candidates ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We introduce the automatic annotation of noun phrases in parsed sentences with tags from a fine-grained semantic animacy hierarchy .", "Only the first two of these three feature sets have a substantial impact on performance , but the resulting model is able to fairly accurately classify new data from that corpus , and shows promise for binary animacy classification and for use on automatically parsed text .", "We train a discriminative classifier on an annotated corpus of spoken English , with features capturing each noun phrase ? s constituent words , its internal structure , and its syntactic relations with other key words in the sentence .", "This information is of interest within lexical semantics and has potential value as a feature in several NLP tasks ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The results indicate that combining such semantic approaches with current methods could result in more accurate and robust AO systems .", "Nevertheless , Adjective Ordering ( AO ) systems do not generally exploit semantic features .", "This paper describes a system that orders adjectives with significantly abovechance accuracy ( 73.0 % ) solely on the basis of semantic features pertaining to the cognitive-semantic dimension of subjectivity .", "The preferred order of pre-nominal adjectives in English is determined primarily by semantics ."]}
{"orig_sents": ["6", "2", "1", "5", "4", "0", "3"], "shuf_sents": ["The experiment is performed on 2031 sentences of diabetes case history .", "exist in any dictionary .", "The main challenge is to automatically recognise phrases and paraphrases for which no ? canonical forms ?", "The F-measure varies between 60 and 96 % in the separate processing phases .", "A combined machine-learning and rule-based approach is applied .", "The focus is on extracting blood sugar level and body weight change which are some of the dominant factors when diagnosing diabetes .", "This paper discusses a method for identifying diabetes symptoms and conditions in free text electronic health records in Bulgarian ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The main result is that the choice of evaluation metric used to optimize these problems ( with one exception among popular metrics ) has little effect on the solution to the optimization .", "This paper seeks to quantitatively evaluate the degree to which a number of popular metrics provide overlapping information to parser designers .", "Two routine tasks are considered : optimizing a machine learning regularization parameter and selecting an optimal machine learning feature set ."]}
{"orig_sents": ["2", "5", "0", "4", "6", "1", "3"], "shuf_sents": ["Other work is based on the Brown corpus , or more recently , Wikipedia .", "To address these problems , this paper proposes a domain-specific semantic relatedness measure based on part of Wikipedia that analyzes course descriptions to suggest whether a course can be transferred from one institution to another .", "Semantic relatedness , or its inverse , semantic distance , measures the degree of closeness between two pieces of text determined by their meaning .", "We show that our results perform well when compared to previous work .", "Wikipediabased measures , however , typically do not take into account the rapid growth of that resource , which exponentially increases the time to prepare and query the knowledge base .", "Related work typically measures semantics based on a sparse knowledge base such as WordNet1 or CYC that requires intensive manual efforts to build and maintain .", "Furthermore , the generalized knowledge domain may be difficult to adapt to a specific domain ."]}
{"orig_sents": ["0", "2", "5", "3", "6", "1", "4"], "shuf_sents": ["ology-based Approaches to Representing Speech Transcripts for Automated Speech Scoring Miao Chen School of Information Studies Syracuse University Syracuse , NY 13244 , USA mchen14 @ Syr.edu Abstract This paper presents a thesis proposal on ap-proaches to automatically scoring non-native speech from second language tests .", "One baseline and two ontolo-gy-based representations are compared in ex-periments .", "Current speech scoring systems assess speech by pri-marily using acoustic features such as fluency and pronunciation ; however content features are barely involved .", "For content features , a central question is how speech content can be represented in appro-priate means to facilitate automated speech scoring .", "Preliminary results show that ontology-based representation slightly im-proves performance of one content feature for automated scoring over the baseline system .", "Motivated by this limita-tion , the study aims to investigate the use of content features in speech scoring systems .", "The study proposes using ontology-based representation to perform concept level representation on speech transcripts , and fur-thermore the content features computed from ontology-based representation may facilitate speech scoring ."]}
{"orig_sents": ["2", "5", "3", "0", "4", "1"], "shuf_sents": ["In this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classifiers .", "We suggest methods for overcoming these limitations , which will form part of our final thesis work .", "Statistical natural language processing ( NLP ) builds models of language based on statistical features extracted from the input text .", "Recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results .", "We also hypothesise that modelling long-range dependencies in the input and ( separately ) in the output layers would further improve performance .", "We investigate deep learning methods for unsupervised feature learning for NLP tasks ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["In this thesis , we propose and evaluate novel text quality metrics that utilize the unique properties of different genres .", "We focus on three genres : academic publications , news articles about science , and machine generated text , in particular the output from automatic text summarization systems .", "But these metrics have not been defined with special relevance to any particular genre but rather proposed as general indicators of writing quality .", "To date , researchers have proposed different ways to compute the readability and coherence of a text using a variety of lexical , syntax , entity and discourse properties ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["A state-of-theart Open IE system consists of natural language processing tools to identify entities and extract sentences that relate such entities , followed by using text clustering to identify the relations among co-occurring entity pairs .", "In particular , we study how the current weighting scheme used for Open IE affects the clustering results and propose a term weighting scheme that significantly improves on the state-of-theart in the task of relation extraction both when used in conjunction with the standard tf ?", "idf scheme , and also when used as a pruning filter .", "We study1 the problem of extracting all possible relations among named entities from unstructured text , a task known as Open Information Extraction ( Open IE ) ."]}
{"orig_sents": ["5", "1", "7", "0", "3", "2", "4", "6"], "shuf_sents": ["To achieve this goal , we will extend the related work surveyed hereinafter , adding different types of humor and characteristics to distinguish between them , including stylistic , syntactic , semantic and pragmatic ones .", "The task of classifying humorous tweets according to the type of humor has not been confronted so far , as far as we know .", "These tend to be remarkably different from the type specific ones recognized in related works .", "We will keep in mind the complex nature of the task at hand , which emanates from the informal language applied in tweets and variety of humor types and styles .", "We will use semi-supervised classifiers on a dataset of humorous tweets driven from different Twitter humor groups or funny tweet sites .", "Much has been written about humor and even sarcasm automatic recognition on Twitter .", "Using a Mechanical Turk we will create a gold standard in which each tweet will be tagged by several annotators , in order to achieve an agreement between them , although the nature of the humor might allow one tweet to be classified under more than one class and topic of humor .", "This research is aimed at applying classification and other NLP algorithms to the challenging task of automatically identifying the type and topic of humorous messages on Twitter ."]}
{"orig_sents": ["0", "3", "4", "1", "2"], "shuf_sents": ["Source code re-use has become an important problem in academia .", "DeSoCoRe compares two source codes at the level of methods or functions even when written in different programming languages .", "The system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re-used .", "The amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re-use .", "We present the DeSoCoRe tool based on techniques of Natural Language Processing ( NLP ) applied to detect source code re-use ."]}
{"orig_sents": ["0"], "shuf_sents": ["In this paper , we present XOpin , a graphical user interface that have been developed to provide a smart access to the results of a feature-based opinion detection system , build on top of a parser ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We present a tool for exploring large , community-created com-ments threads in an efficient manner .", "Comment threads contain fascinating and use-ful insights into public reactions , but are chal-lenging to read and understand without computational assistance ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["As a novel task for topic model application , we also discuss possible approaches to the task of selecting the best product categories to illustrate the hidden topics discovered for our product domain .", "As an initial step in creating this experience , we constructed a prototype of an online shopping interface which combines product ontology information with topic model results to allow users to explore items from the food and kitchen domain .", "At present , online shopping is typically a search-oriented activity where a user gains access to products which best match their query .", "the online store and enjoy browsing a variety of items .", "Instead , we propose a surf-oriented online shopping paradigm , which links associated products allowing users to ? wander around ?"]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["We demonstrate a conversational humanoid robot that allows users to follow their own dialogue structures .", "Our system uses a hierarchy of reinforcement learning dialogue agents , which support transitions across sub-dialogues in order to relax the strictness of hierarchical control and therefore support flexible interactions .", "The novel features in our system are ( a ) the flexibility given to users to navigate flexibly in the interaction ; and ( b ) a framework for investigating adaptive and flexible dialogues .", "Whilst language input and dialogue control is autonomous or wizarded , language output is provided by the robot combining verbal and non-verbal contributions .", "We demonstrate our system with the Nao robot playing two versions of a Quiz game ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We describe MSR SPLAT , a toolkit for language analysis that allows easy access to the linguistic analysis tools produced by the NLP group at Microsoft Research .", "The toolkit is accessible as a web service , which can be used from a broad set of programming languages .", "As we expand the tools we develop for our own research , the set of tools available in MSR SPLAT will be extended .", "The tools include both traditional linguistic analysis tools such as part-of-speech taggers , constituency and dependency parsers , and more recent developments such as sentiment detection and linguistically valid morphology ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["In addition , we demonstrate that our system can perform temporal reasoning by comparing normalized temporal expressions with respect to several temporal relations .", "Our system makes use of an existing state-of-the-art temporal extraction system , on top of which we add several important novel contributions .", "This paper presents a demonstration of a temporal reasoning system that addresses three fundamental tasks related to temporal expressions in text : extraction , normalization to time intervals and comparison .", "Experimental study shows that the system achieves excellent performance on all the tasks we address ."]}
{"orig_sents": ["4", "7", "5", "0", "2", "8", "3", "1", "6"], "shuf_sents": ["Attitude predictions are used to construct a signed network representation of the discussion thread .", "The system can be used in different applications such as : word polarity identification , identifying attitudinal sentences and their signs , signed social network extraction from text , subgroup detect in discussion .", "In this network , each discussant is represented by a node .", "The sign ( positive or negative ) of the edge is set based on the polarity of the attitude identified in the text associated with the edge .", "This demonstration presents AttitudeMiner , a system for mining attitude from online discussions .", "The goal of this analysis is to identify the polarity of the attitude the discussants carry towards one another .", "The system is publicly available for download and has an online demonstration at http : //clair.eecs.umich.edu/AttitudeMiner/ .", "AttitudeMiner uses linguistic techniques to analyze the text exchanged between participants of online discussion threads at different levels of granularity : the word level , the sentence level , the post level , and the thread level .", "An edge connects two discussants if they exchanged posts ."]}
{"orig_sents": ["2", "0", "1", "5", "4", "3"], "shuf_sents": ["Both techniques have pros and cons .", "While the N-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation , the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding , in terms of better search performance and superior selection of translation units .", "N-gram-based models co-exist with their phrase-based counterparts as an alternative SMT framework .", "Our system outperforms state-of-the-art phrase-based systems ( Moses and Phrasal ) and N-gram-based systems by a significant margin on German , French and Spanish to English translation tasks .", "Our experiments show that using this combination not only improves the search accuracy of the N-gram model but that it also improves the BLEU scores .", "In this paper we combine N-grambased modeling with phrase-based decoding , and obtain the benefits of both approaches ."]}
{"orig_sents": ["2", "1", "4", "5", "0", "3", "6"], "shuf_sents": ["The resulting models are evaluated in an intrinsic task of lexical selection for MT as well as a full MT system , through n-best reranking .", "As a result , they rely on large phrase pairs and target language models to recover contextual effects in translation .", "Standard phrase-based translation models do not explicitly model context dependence between translation units .", "These experiments demonstrate that additional contextual modeling does indeed benefit a phrase-based system and that the direction of conditioning is important .", "In this work , we explore n-gram models over Minimal Translation Units ( MTUs ) to explicitly capture contextual dependencies across phrase boundaries in the channel model .", "As there is no single best direction in which contextual information should flow , we explore multiple decomposition structures as well as dynamic bidirectional decomposition .", "Integrating multiple conditioning orders provides consistent benefit , and the most important directions differ by language pair ."]}
{"orig_sents": ["1", "4", "3", "2", "5", "0"], "shuf_sents": ["We show that sparse decoder features outperform maximum entropy handily , indicating that there are major advantages to optimizing reordering features directly for BLEU with the decoder in the loop .", "There have been many recent investigations into methods to tune SMT systems using large numbers of sparse features .", "Using a hierarchical reordering model as our baseline , we show that simple features coupling phrase orientation to frequent words or wordclusters can improve translation quality , with boosts of up to 1.2 BLEU points in ChineseEnglish and 1.8 in Arabic-English .", "We use sparse features to address reordering , which is often considered a weak point of phrase-based translation .", "However , there have not been nearly so many examples of helpful sparse features , especially for phrasebased systems .", "We compare this solution to a more traditional maximum entropy approach , where a probability model with similar features is trained on wordaligned bitext ."]}
{"orig_sents": ["3", "5", "1", "0", "2", "4"], "shuf_sents": ["We evaluated our model on Turkish-English parallel data .", "The model jointly induces word and morpheme alignments using an EM algorithm .", "We obtained significant improvement of BLEU scores over IBM Model 4 .", "Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words .", "Our results indicate that utilizing information from morphology improves the quality of word alignments .", "We present a two-level alignment model that distinguishes between words and morphemes , in which we embed an IBM Model 1 inside an HMM based word alignment model ."]}
{"orig_sents": ["4", "0", "1", "3", "2"], "shuf_sents": ["We propose a multi-faceted event recognition approach , which identifies documents about an event using event phrases as well as defining characteristics of the event .", "Our research focuses on civil unrest events and learns civil unrest expressions as well as phrases corresponding to potential agents and reasons for civil unrest .", "We use the bootstrapped dictionaries to identify civil unrest documents and show that multi-faceted event recognition can yield high accuracy .", "We present a bootstrapping algorithm that automatically acquires event phrases , agent terms , and purpose ( reason ) phrases from unannotated texts .", "Identifying documents that describe a specific type of event is challenging due to the high complexity and variety of event descriptions ."]}
{"orig_sents": ["4", "1", "5", "2", "3", "0"], "shuf_sents": ["The Chinese model retrained on this new combined dataset outperforms the strong baseline by over 3 % F1 score .", "We propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple Integer Linear Program , which encourages entity tags to agree via bilingual constraints .", "In particular , Chinese performance improves by over 5 % absolute F1 score .", "We can then annotate a large amount of bilingual text ( 80k sentence pairs ) using our method , and add it as uptraining data to the original monolingual NER training corpus .", "Different languages contain complementary cues about entities , which can be used to improve Named Entity Recognition ( NER ) systems .", "Bilingual NER experiments on the large OntoNotes 4.0 Chinese-English corpus show that the proposed method can improve strong baselines for both Chinese and English ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["We extend their framework and develop a minimally supervised method applicable to multiple languages .", "We propose a minimally supervised method for multilingual paraphrase extraction from definition sentences on the Web .", "Hashimoto et al ( 2011 ) extracted paraphrases from Japanese definition sentences on the Web , assuming that definition sentences defining the same concept tend to contain paraphrases .", "Our experiments show that our method is comparable to Hashimoto et al ? s for Japanese and outperforms previous unsupervised methods for English , Japanese , and Chinese , and that our method extracts 10,000 paraphrases with 92 % precision for English , 82.5 % precision for Japanese , and 82 % precision for Chinese .", "However , their method requires manually annotated data and is language dependent ."]}
{"orig_sents": ["2", "5", "1", "6", "7", "4", "3", "0"], "shuf_sents": ["By doing so our approach outperforms stateof-the-art distant supervision .", "The need for existing datasets can be avoided by using a universal schema : the union of all involved schemas ( surface form predicates as in OpenIE , and relations in the schemas of preexisting databases ) .", "Traditional relation extraction predicts relations within some fixed and finite target schema .", "More importantly , by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase , we are able to reason about unstructured and structured data in mutually-supporting ways .", "We show that such latent models achieve substantially higher accuracy than a traditional classification approach .", "Machine learning approaches to this task require either manual annotation or , in the case of distant supervision , existing structured sources of the same schema .", "This schema has an almost unlimited set of relations ( due to surface forms ) , and supports integration with existing structured data ( through the relation types of existing databases ) .", "To populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations ."]}
{"orig_sents": ["4", "2", "1", "3", "0"], "shuf_sents": ["We show that this extension can be used to extract a larger set of relevant features .", "We evaluate different relevancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features .", "This method uses multiple corpora to allow for the removal of data set specific patterns , and addresses both feature relevancy and redundancy .", "Our feature set is a broad class of syntactic patterns , and to better capture the signal we extend the Bayesian Tree Substitution Grammar induction algorithm to a supervised mixture of latent grammars .", "We develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author ."]}
{"orig_sents": ["4", "1", "0", "3", "2"], "shuf_sents": ["A theory of grounding ( Phillips , 2010 ) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs .", "This begs the question of what causes certain constructions to be more or less frequent .", "Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing .", "This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times .", "The frequency of words and syntactic constructions has been observed to have a substantial effect on language processing ."]}
{"orig_sents": ["3", "4", "1", "0", "2", "5"], "shuf_sents": ["The method consists of two main steps : ( 1 ) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses , ( 2 ) it provides ranked lists of similar words according to the similarity of their semantic word response vectors .", "Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word .", "We evaluate our approach in the task of bilingual lexicon extraction ( BLE ) for a variety of language pairs .", "We propose a new approach to identifying semantically similar words across languages .", "The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words ( which includes both source and target language words ) as their top semantic word responses .", "We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics ."]}
{"orig_sents": ["5", "4", "2", "1", "6", "0", "3"], "shuf_sents": ["We also demonstrate a discrepancy in the performance of our model and human infants on an artificial-language task in which stress cues and transition-probability information are pitted against one another .", "We show that this model improves segmentation accuracy over purely segmental input representations , and recovers the dominant stress pattern of the data .", "Our new model treats identification of word boundaries and prevalent stress patterns in the language as a joint inference task .", "We argue that this discrepancy indicates a bound on rationality in the mechanisms of human segmentation .", "We extend the generative model of Goldwater et al ( 2006 ) to segment using syllable stress as well as phonemic form .", "Humans identify word boundaries in continuous speech by combining multiple cues ; existing state-of-the-art models , though , look at a single cue .", "Additionally , our model retains high performance even without single-word utterances ."]}
{"orig_sents": ["1", "3", "2", "0", "4", "5"], "shuf_sents": ["We see significant improvements for all eight treebanks when training on the full training sets .", "We consider the problem of training a statistical parser in the situation when there are multiple treebanks available , and these treebanks are annotated according to different linguistic conventions .", "To evaluate and analyze the adaptation methods , we train parsers on treebank pairs in four languages : German , Swedish , Italian , and English .", "To address this problem , we present two simple adaptation methods : the first method is based on the idea of using a shared feature representation when parsing multiple treebanks , and the second method on guided parsing where the output of one parser provides features for a second one .", "However , the clearest benefits are seen when we consider smaller training sets .", "Our experiments were carried out with unlabeled dependency parsers , but the methods can easily be generalized to other featurebased parsers ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["For this paper , we attempt to create true low-resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages Kinyarwanda and Malagasy .", "Given these severely limited amounts of either type supervision ( tag dictionaries ) or token supervision ( labeled sentences ) , we are able to dramatically improve the learning of a hidden Markov model through our method of automatically generalizing the annotations , reducing noise , and inducing word-tag frequency information .", "Most work on weakly-supervised learning for part-of-speech taggers has been based on unrealistic assumptions about the amount and quality of training data ."]}
{"orig_sents": ["3", "5", "2", "1", "0", "4"], "shuf_sents": ["We describe a number of key steps used to obtain this level of performance ; these should be relevant to other work on the application of spectral learning algorithms .", "We show that the algorithm provides models with the same accuracy as EM , but is an order of magnitude more efficient .", "This paper describes experiments using the spectral algorithm .", "Latent-variable PCFGs ( L-PCFGs ) are a highly successful model for natural language parsing .", "We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM .", "Recent work ( Cohen et al , 2012 ) has introduced a spectral algorithm for parameter estimation of L-PCFGs , which ? unlike the EM algorithm ? is guaranteed to give consistent parameter estimates ( it has PAC-style guarantees of sample complexity ) ."]}
{"orig_sents": ["6", "3", "5", "4", "2", "1", "0"], "shuf_sents": ["We show that the proposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic .", "The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves .", "Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms .", "using LDA , are now widely used in Computational Linguistics .", "We introduce an alternative approach in which topics are represented using images .", "Topics are normally represented as a set of keywords , often the n terms in a topic with the highest marginal probabilities .", "Topics generated automatically , e.g ."]}
{"orig_sents": ["8", "3", "6", "4", "7", "0", "1", "2", "5"], "shuf_sents": ["and route of administration ( smoking , oral , etc . )", "Since a purely unsupervised topic model is unlikely to discover these specific factors of interest , we develop a novel method of incorporating prior knowledge by leveraging user generated tags as priors in our model .", "We demonstrate that this model can be used as an exploratory tool for learning about these drugs from the Web by applying it to the task of extractive summarization .", "We consider such models for clinical research of new recreational drugs and trends , an important application for mining current information for healthcare workers .", "f-LDA variant to jointly model combinations of drug ( marijuana , salvia , etc .", "In addition to providing useful output for this important public health task , our prior-enriched model provides a framework for the application of fLDA to other tasks .", "We use a ? three-dimensional ?", ") , aspect ( effects , chemistry , etc . )", "Multi-dimensional latent text models , such as factorial LDA ( f-LDA ) , capture multiple factors of corpora , creating structured output for researchers to better understand the contents of a corpus ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We motivate our approach by applying over conversational data , and show that our framework improves performance significantly over baseline algorithms .", "We build an entailment graph over phrases that are extracted from the sentences , and use the entailment relations to identify and select the most relevant phrases .", "We then aggregate those selected phrases by means of phrase generalization and merging .", "We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We present a new hierarchical Bayesian model for unsupervised topic segmentation .", "Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi ? s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets .", "We develop an MCMC inference algorithm to split/merge segment ( s ) .", "This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents ."]}
{"orig_sents": ["1", "6", "2", "0", "3", "4", "5"], "shuf_sents": ["search and multiple sequence alignment ( MSA ) .", "The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates .", "In this paper , we describe an improved method for combining partial captions into a final output based on weighted A ?", "In contrast to prior work , our method allows the tradeoff between accuracy and speed to be tuned , and provides formal error bounds .", "Our method outperforms the current state-of-the-art on Word Error Rate ( WER ) ( 29.6 % ) , BLEU Score ( 41.4 % ) , and F-measure ( 36.9 % ) .", "The end goal is for these captions to be used by people , and so we also compare how these metrics correlate with the judgments of 50 study participants , which may assist others looking to make further progress on this problem .", "Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists , each of whom types part of what they hear ."]}
{"orig_sents": ["2", "3", "6", "5", "0", "1", "7", "4"], "shuf_sents": ["We evaluate the approach under several scenarios , including both supervised and unsupervised training , the latter achieved by training on the output of a baseline automatic word-alignment model .", "We also adapt the ASR models to the domain , and evaluate the impact of error rate on performance .", "Automatically assessing the fidelity of a retelling to the original narrative ?", "a task of growing clinical importance ?", "A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance , yielding strong improvements over using either approach alone .", "We present a word tagging approach using conditional random fields ( CRFs ) that allows a diversity of features to be considered during inference , including some capturing acoustic confusions encoded in word confusion networks .", "is challenging , given extensive paraphrasing during retelling along with cascading automatic speech recognition ( ASR ) errors .", "We find strong robustness to ASR errors , even using just the 1-best system output ."]}
{"orig_sents": ["8", "7", "1", "2", "9", "0", "5", "4", "6", "3"], "shuf_sents": ["Further gains ( up to a 4 % reduction in equal error rate ) are obtained when in-domain and out-of-domain models are interpolated .", "In-domain data , however , is expensive to collect for each new domain .", "In this study we focus on lexical models and investigate how well out-of-domain data ( either outside the domain , or from single-user scenarios ) can fill in for matched in-domain data .", "? Work done while first author was an intern with Microsoft .", "We find that the first 1.5 seconds of an utterance contain most of the lexical information for AD , and analyze which lexical items convey this .", "Finally , we examine which parts of an utterance are most useful .", "Overall , we conclude that the H-H-C scenario can be approximated by combining data from H-C and H-H scenarios only .", "Recent work on AD ( Shriberg et al , 2012 ) showed good results using prosodic and lexical features trained on in-domain data .", "Addressee detection ( AD ) is an important problem for dialog systems in human-humancomputer scenarios ( contexts involving multiple people and a system ) because systemdirected speech must be distinguished from human-directed speech .", "We find that human-addressed speech can be modeled using out-of-domain conversational speech transcripts , and that human-computer utterances can be modeled using single-user data : the resulting AD system outperforms a system trained only on matched in-domain data ."]}
{"orig_sents": ["4", "6", "0", "5", "3", "2", "1"], "shuf_sents": ["In order to improve ASR performance for our diverse data set , adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful .", "It was also found to be important to synchronize various pipeline components in order to minimize latency .", "Among other results , our experiments demonstrate that a good segmentation is useful , and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation .", "We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments .", "The study presented in this work is a first effort at real-time speech translation of TED talks , a compendium of public talks with different speakers addressing a variety of topics .", "In order to improve machine translation ( MT ) performance , techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use .", "We address the goal of achieving a system that balances translation accuracy and latency ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["As a consequence , parsers trained on such treebanks usually make mistakes when selecting the arguments of predicative lexemes .", "In this paper , we propose an original way to correct subcategorization errors by combining subparses of a sentence S that appear in the list of the n-best parses of S. The subcategorization information comes from three different resources , the first one is extracted from a treebank , the second one is computed on a large corpora and the third one is an existing syntactic lexicon .", "Experiments on the French Treebank showed a 15.24 % reduction of erroneous subcategorization frames ( SF ) selections for verbs as well as a relative decrease of the error rate of 4 % Labeled Accuracy Score on the state of the art parser on this treebank .", "Treebanks are not large enough to adequately model subcategorization frames of predicative lexemes , which is an important source of lexico-syntactic constraints for parsing ."]}
{"orig_sents": ["6", "3", "1", "4", "0", "2", "5"], "shuf_sents": ["Not only does this two-phase learning approach prevent overfitting , the second pass optimizes corpus-level BLEU of the Viterbi translation of the decoder .", "First , the contribution of individual sparse features is estimated using large amounts of parallel data .", "We demonstrate significant improvements using sparse rule indicator features in three different translation tasks .", "To ensure their reliable estimation and to prevent overfitting , we use a two-phase learning algorithm .", "Second , a small development corpus is used to determine the relative contributions of the sparse features and standard dense features .", "To our knowledge , this is the first large-scale discriminative training algorithm capable of showing improvements over the MERT baseline with only rule indicator features in addition to the standard MERT features .", "We introduce a new large-scale discriminative learning algorithm for machine translation that is capable of learning parameters in models with extremely sparse features ."]}
{"orig_sents": ["1", "0", "2", "5", "4", "3"], "shuf_sents": ["Existing methods , mostly based on statistical information in corpora , do not actually measure informativeness of a term with regard to its semantic context .", "Measuring term informativeness is a fundamental NLP task .", "This paper proposes a new lightweight feature-free approach to encode term informativeness in context by leveraging web knowledge .", "The performance is state-of-theart or close to it for each application , demonstrating its effectiveness and generality .", "We apply our method to three applications : core term extraction from snippets ( text segment ) , scientific keywords extraction ( paper ) , and back-of-the-book index generation ( book ) .", "Given a term and its context , we model contextaware term informativeness based on semantic similarity between the context and the term ? s most featured context in a knowledge base , Wikipedia ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The extraction systems are trained on auto-annotated summaries ( containing the induced concepts ) and evaluated on humanannotated documents .", "We evaluate the two approaches in two different information extraction settings : monolingual and cross-lingual information extraction .", "Extraction results are promising , being close in performance to those achieved when the system is trained on human-annotated summaries .", "We here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages ."]}
{"orig_sents": ["4", "3", "7", "5", "1", "0", "6", "2"], "shuf_sents": ["It concerns a text based method , ( the Levenshtein Distance ) , and an acoustic approach using extracted mean pitch values .", "This study explores two methods for measuring the linguistic distance of six South African languages .", "Cluster analysis resulting from the distance matrices from both methods correlates closely with human perceptual distances and existing literature about the six languages .", "In order to predict or improve system performance , a thorough investigation into these variations , similarities and dissimilarities , is required .", "Language variations are generally known to have a severe impact on the performance of Human Language Technology Systems .", "However , not much work has been done on language distance measures , and even less work has been done involving South African languages .", "The Levenshtein distance uses parallel word transcriptions from all six languages with as little as 144 words , whereas the pitch method is text-independent and compares mean language pitch differences .", "Distance measures have been used in several applications of speech processing to analyze different varying speech attributes ."]}
{"orig_sents": ["4", "2", "5", "3", "1", "0"], "shuf_sents": ["English test sets while reducing the occurrence of sparsity and ambiguity problems common to large label sets .", "We show that the smaller label set provides improved translation scores by 1.14 BLEU on two Chinese ?", "We induce bilingual labels into the SAMT grammar , use them for category coarsening , then project back to monolingual labeling as in standard SAMT .", "grammar with the same expressive power and format as the original , but many fewer nonterminal labels .", "We present a new variant of the SyntaxAugmented Machine Translation ( SAMT ) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars .", "The result is a ? collapsed ?"]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["This paper presents an N-best reranking method based on keyphrase extraction .", "Compression candidates generated by a word graph-based MSC approach are reranked according to the number and relevance of keyphrases they contain .", "Both manual and automatic evaluations were performed using a dataset made of clusters of newswire sentences .", "Multi-Sentence Compression ( MSC ) is the task of generating a short single sentence summary from a cluster of related sentences .", "Results show that the proposed method significantly improves the informativity of the generated compressions ."]}
{"orig_sents": ["0", "4", "1", "2", "3"], "shuf_sents": ["This paper describes the annotation process and linguistic properties of the Persian syntactic dependency treebank .", "One of the unique features of this treebank is that there are almost 4800 distinct verb lemmas in its sentences making it a valuable resource for educational goals .", "The treebank is constructed with a bootstrapping approach by means of available tagging and parsing tools and manually correcting the annotations .", "The data is splitted into standard train , development and test set in the CoNLL dependency format and is freely available to researchers .", "The treebank consists of approximately 30,000 sentences annotated with syntactic roles in addition to morpho-syntactic features ."]}
{"orig_sents": ["3", "2", "1", "5", "4", "0"], "shuf_sents": ["Our experiments , involving Urdu-English , show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points , and a publicly available state-of-the-art MT system , Hiero , by 3 BLEU points .", "However , for efficiently solving the TSP , the model is restricted to pairwise features which examine only a pair of words and their neighborhood .", "This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem ( TSP ) .", "Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order .", "In addition to using a more informative set of source side features , we also capture target side features indirectly by using the translation score assigned to a reordering .", "In this work , we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies ."]}
{"orig_sents": ["5", "1", "2", "4", "3", "0"], "shuf_sents": ["This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing .", "in an indexed parallel corpus using suffix arrays .", "However , this can be slow because on-demand extraction of phrase tables is computationally expensive .", "Compared to a highly-optimized , state-of-the-art serial CPU-based implementation , our techniques achieve at least an order of magnitude improvement in terms of throughput .", "We address this problem by developing novel algorithms for general purpose graphics processing units ( GPUs ) , which enable suffix array queries for phrase lookup and phrase extraction to be massively parallelized .", "Translation models in statistical machine translation can be scaled to large corpora and arbitrarily-long phrases by looking up translations of source phrases ? on the fly ?"]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In this paper , we propose to scale up discriminative training of ( He and Deng , 2012 ) to train features with 150 million parameters , which is one order of magnitude higher than previously published effort , and to apply discriminative training to redistribute probability mass that is lost due to model pruning .", "Until recently , the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters .", "The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["In this work , rather than optimising for this indirect measure , we directly optimise for BLEU on the tuning set and show improvements in average performance over two data sets and 8 language pairs .", "One way of doing this is to build separate translation models from each data set and linearly interpolate them , and to date the main method for optimising the interpolation weights is to minimise the model perplexity on a heldout set .", "In Statistical Machine Translation we often have to combine different sources of parallel training data to build a good system ."]}
{"orig_sents": ["5", "6", "0", "2", "3", "1", "4"], "shuf_sents": ["We present ELISSA , a machine translation ( MT ) system for DA to MSA .", "A manual error analysis of ELISSA ? s output shows that it produces correct MSA translations over 93 % of the time .", "ELISSA employs a rule-based approach that relies on morphological analysis , transfer rules and dictionaries in addition to language models to produce MSA paraphrases of DA sentences .", "ELISSA can be employed as a general preprocessor for DA when using MSA NLP tools .", "Using ELISSA to produce MSA versions of DA sentences as part of an MSA-pivoting DA-to-English MT solution , improves BLEU scores on multiple blind test sets between 0.6 % and 1.4 % .", "Modern Standard Arabic ( MSA ) has a wealth of natural language processing ( NLP ) tools and resources .", "In comparison , resources for dialectal Arabic ( DA ) , the unstandardized spoken varieties of Arabic , are still lacking ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["In addition , the paper presents a quantitative analysis of the lexical diversity of social media text , and its relationship to other corpora .", "The rise of social media has brought computational linguistics in ever-closer contact with bad language : text that defies our expectations about vocabulary , spelling , and syntax .", "This paper surveys the landscape of bad language , and offers a critical review of the NLP community ? s response , which has largely followed two paths : normalization and domain adaptation .", "Each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["We show that , unlike previous methods , minibatch learning ( in serial mode ) actually improves the converged accuracy for both perceptron and MIRA learning , and when combined with simple parallelization , minibatch leads to very significant speedups ( up to 9x on 12 processors ) on stateof-the-art parsing and tagging systems .", "Online learning algorithms such as perceptron and MIRA have become popular for many NLP tasks thanks to their simpler architecture and faster convergence over batch learning methods .", "However , while batch learning such as CRF is easily parallelizable , online learning is much harder to parallelize : previous efforts often witness a decrease in the converged accuracy , and the speedup is typically very small ( ? 3 ) even with many ( 10+ ) processors .", "We instead present a much simpler architecture based on ? mini-batches ? , which is trivially parallelizable ."]}
{"orig_sents": ["2", "4", "5", "1", "3", "6", "0"], "shuf_sents": ["and annotated data .", "Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre .", "We consider the problem of part-of-speech tagging for informal , online conversational text .", "Additionally , we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines .", "We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy .", "With these features , our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks ; Twitter tagging is improved from 90 % to 93 % accuracy ( more than 3 % absolute ) .", "Tagging software , annotation guidelines , and large-scale word clusters are available at : http : //www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the ? CMU Twitter Part-of-Speech Tagger ?"]}
{"orig_sents": ["2", "0", "4", "1", "5", "3"], "shuf_sents": ["The method first creates augmented versions of dependency graphs by applying a series of modifications designed to directly capture higherorder lexical path dependencies .", "As bilexical dependencies are sparse , a novel directed distributional word similarity measure is used to smooth edge score estimates .", "We describe a new self-learning framework for parser lexicalisation that requires only a plain-text corpus of in-domain text .", "The approach achieves significant improvements on WSJ and biomedical text over the unlexicalised baseline parser , which is originally trained on a subset of the Brown corpus .", "Scores are assigned to each edge in the graph using statistics from an automatically parsed background corpus .", "Edge scores are then combined into graph scores and used for reranking the topn analyses found by the unlexicalised parser ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["Experiments with two tasks show that the learned latent factor representation can give good performance on a relation polarity prediction task and improve the performance of a subgroup detection task .", "Advances in sentiment analysis have enabled extraction of user relations implied in online textual exchanges such as forum posts .", "As user interactions can be sparse in online discussions , we propose to apply collaborative filtering through probabilistic matrix factorization to generalize and improve the opinion matrices extracted from forum posts .", "However , recent studies in this direction only consider direct relation extraction from text ."]}
{"orig_sents": ["3", "0", "4", "1", "2", "5"], "shuf_sents": ["This work strives to reduce the noise introduced into aggregated features from disparate and generic training data in order to allow for contextual features that more closely model the entities in the target data .", "To this end , models are trained for an existing NER system using the top documents from the training set that are similar to the target document in order to demonstrate that this technique can be applied to improve any pre-built NER system .", "Initial results show an improvement over the University of Illinois NE tagger with a weighted average F1 score of 91.67 compared to the Illinois tagger ? s score of 91.32 .", "Feature and context aggregation play a large role in current NER systems , allowing significant opportunities for research into optimizing these features to cater to different domains .", "The proposed approach trains models based on only a part of the training set that is more similar to the target domain .", "This research serves as a proof-of-concept for future planned work to cluster the training documents to produce a number of more focused models from a given training set , thereby reducing noise and extracting a more representative feature set ."]}
{"orig_sents": ["1", "4", "2", "0", "5", "6", "3", "7"], "shuf_sents": ["or ? country western , ?", "Language can describe our visual world at many levels , including not only what is literally there but also the sentiment that it invokes .", "Sentimental properties , including labels such as ? youthful ?", "In a series of experiments , we demonstrate that such learned models can be used for a range of tasks , including predicting sentimental words and using them to rank and build avatars .", "In this paper , we study visual language , both literal and sentimental , that describes the overall appearance and style of virtual characters .", "must be inferred from descriptions of the more literal properties , such as facial features and clothing selection .", "We present a new dataset , collected to describe Xbox avatars , as well as models for learning the relationships between these avatars and their literal and sentimental descriptions .", "Together , these results demonstrate that sentimental language provides a concise ( though noisy ) means of specifying low-level visual properties ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In this paper , we retarget an existing state-of-the-art MSA morphological tagger to Egyptian Arabic ( ARZ ) .", "Our evaluation demonstrates that our ARZ morphology tagger outperforms its MSA variant on ARZ input in terms of accuracy in part-of-speech tagging , diacritization , lemmatization and tokenization ; and in terms of utility for ARZ-toEnglish statistical machine translation .", "The many differences between Dialectal Arabic and Modern Standard Arabic ( MSA ) pose a challenge to the majority of Arabic natural language processing tools , which are designed for MSA ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination .", "We present a novel , structured language model - Supertagged Dependency Language Model to model the syntactic dependencies between words .", "The goal is to identify ungrammatical hypotheses from a set of candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features .", "We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Moreover , we demonstrate a reduction of translation error and an improvement in the performance of an English-to-Arabic machine translation system .", "We construct a classification-based framework to automate this decision , evaluate our classifier both in the limited news and the diverse Wikipedia domains , and achieve promising accuracy .", "We report the results of our work on automating the transliteration decision of named entities for English to Arabic machine translation ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We compare the results to those obtained using the PR Algorithm .", "This paper describes an approach to improve summaries for a collection of Twitter posts created using the Phrase Reinforcement ( PR ) Algorithm ( Sharifi et al , 2010a ) .", "We parse these summaries using a dependency parser and use the dependencies to eliminate some of the excess text and build better-formed summaries .", "The PR algorithm often generates summaries with excess text and noisy speech ."]}
{"orig_sents": ["4", "0", "1", "2", "3", "5"], "shuf_sents": ["The model allows for arbituary features extracted from a phrase pair to be incorporated as evidence .", "The parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an N-best list based expected BLEU as the objective function .", "The model is easy to be incoporated into a standard phrase-based statistical machine translation system , requiring no code change in the runtime engine .", "Evaluation is performed on two Europarl translation tasks , GermanEnglish and French-English .", "This paper presents a general , statistical framework for modeling phrase translation via Markov random fields .", "Results show that incoporating the Markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system , leading to a gain of 0.8-1.3 BLEU points ."]}
{"orig_sents": ["5", "4", "0", "1", "2", "3"], "shuf_sents": ["We can also identify the unspecified lemmas in the treebank with an accuracy over 97 % .", "Furthermore , we demonstrate that using our automatic annotations improves the performance of a state-of-the-art Arabic morphological tagger .", "Our approach combines a variety of techniques from corpus-based statistical models to linguistic rules that target specific phenomena .", "These results suggest that the cost of treebanking can be reduced by designing underspecified treebanks that can be subsequently enriched automatically .", "We show that we can map from a tagset of size six to one with 485 tags at an accuracy rate of 94 % -95 % .", "In this paper , we study the problem of automatic enrichment of a morphologically underspecified treebank for Arabic , a morphologically rich language ."]}
{"orig_sents": ["0", "3", "4", "1", "5", "2"], "shuf_sents": ["Social media texts are written in an informal style , which hinders other natural language processing ( NLP ) applications such as machine translation .", "In this paper , to further improve other downstream NLP applications , we argue that other normalization operations should also be performed , e.g. , missing word recovery and punctuation correction .", "Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks , for both Chinese and English .", "Text normalization is thus important for processing of social media text .", "Previous work mostly focused on normalizing words by replacing an informal word with its formal form .", "A novel beam-search decoder is proposed to effectively integrate various normalization operations ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["In this paper we present a modification of the LDA-frames algorithm allowing the number of frames and roles to be determined automatically , based on the character and size of training data .", "The most limiting property of the algorithm is such that the number of frames and roles must be predefined .", "LDA-frames is an unsupervised approach for identifying semantic frames from semantically unlabeled text corpora , and seems to be a useful competitor for manually created databases of selectional preferences ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We test our algorithm on two treebanks , and get significant improvements in parsing speed .", "We provide an approximation algorithm for PCFG parsing , which asymptotically improves time complexity with respect to the input grammar size , and prove upper bounds on the approximation quality ."]}
{"orig_sents": ["4", "5", "0", "1", "3", "2"], "shuf_sents": ["However , the complementary problem of negative deceptive opinion spam , intended to slander competitive offerings , remains largely unstudied .", "Following an approach similar to Ott et al ( 2011 ) , in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews .", "Finally , in conjunction with the aforementioned positive review dataset , we consider the possible interactions between sentiment and deception , and present initial results that encourage further exploration of this relationship .", "Based on this dataset , we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges .", "The rising influence of user-generated online reviews ( Cone , 2011 ) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM ? fictitious reviews that have been deliberately written to sound authentic and deceive the reader .", "Recently , Ott et al ( 2011 ) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["It consists in removing pronounced pitch peaks in the original recordings , which typically lead to noticeable discontinuities in the synthesized speech .", "We present a method for improving the perceived naturalness of corpus-based speech synthesizers .", "We perceptually evaluated this method using two concatenative and two HMM-based synthesis systems , and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We show that existing methods for training preposition error correction systems , whether using well-edited text or error-annotated corpora , do not generalize across very different test sets .", "This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections .", "We present a new , large errorannotated corpus and use it to train systems that generalize across three different test sets , each from a different domain and with different error characteristics ."]}
{"orig_sents": ["0", "1", "4", "2", "3"], "shuf_sents": ["y of PennsylvaniaAbstract Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem .", "Although many techniques take advantage of a seed bilingual lexicon , this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model .", "Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task , and our approach yields an average relative gain in accuracy of nearly 50 % over an unsupervised baseline .", "Large gains in accuracy hold for all 22 languages ( low and high resource ) that we investigate .", "Even in a low resource machine translation setting , where induced translations have the potential to improve performance substantially , it is reasonable to assume access to some amount of data to perform this kind of optimization ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Our algorithms exploit the similarity between word-concept pairs using the English Wordnet to produce reverse dictionary entries .", "In this paper , we propose algorithms for creation of new reverse bilingual dictionaries from existing bilingual dictionaries in which English is one of the two languages .", "Bilingual dictionaries are expensive resources and not many are available when one of the languages is resource-poor .", "Since our algorithms rely on available bilingual dictionaries , they are applicable to any bilingual dictionary as long as one of the two languages has Wordnet type lexical ontology ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["In response , we provide an analysis of the individual features and identify differences existing between two corpora .", "This paper examines the efficacy of the application of a pre-existing technique in the area of event-event temporal relationship identification .", "We find that initially the simpler feature sets perform as expected , but that the final improvement to the feature set underperforms .", "We attempt to both reproduce the results of said technique , as well as extend the previous work with application to a newlycreated domain of biographical data ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["another highly subjective subset of adjectives that can be extracted in an unsupervised fashion .", "In order to prove the robustness of this extraction method , we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons ( as a gold standard ) .", "We examine predicative adjectives as an unsupervised criterion to extract subjective adjectives .", "We do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives , i.e ."]}
{"orig_sents": ["0", "3", "2", "4", "1"], "shuf_sents": ["Incorporating semantic structure into a linguistics-free translation model is challenging , since semantic structures are closely tied to syntax .", "Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy .", "First , we introduce linguistically motivated constraints into a hierarchical model , guiding translation phrase choices in favor of those that respect syntactic boundaries .", "In this paper , we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model .", "Second , based on such translation phrases , we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate , but also between two arguments ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This system makes for a more informative IAA evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types .", "This paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement .", "We evaluate the system on two corpora - ( 1 ) a corpus of English web text , and ( 2 ) a corpus of Modern British English ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["We propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses .", "Word sense disambiguation aims to identify which meaning of a word is present in a given usage .", "Gathering word sense annotations is a laborious and difficult task .", "Several methods have been proposed to gather sense annotations using large numbers of untrained annotators , with mixed results .", "Our findings show that given the appropriate annotation task , untrained workers can obtain at least as high agreement as annotators in a controlled setting , and in aggregate generate equally as good of a sense labeling ."]}
{"orig_sents": ["3", "2", "4", "1", "5", "0"], "shuf_sents": ["Experiments showed that the com-pound features not only improved the perfor-mances on several NLP tasks , but also ran faster , suggesting the potential of embeddings .", "The direct us-age has disadvantages such as large amount of computation , inadequacy with dealing word ambiguity and rare-words , and the problem of linear non-separability .", "In this paper , we investigated the usage of word representations learned by neural language models , i.e .", "Embedding Features for Semi-supervised Learning Mo Yu1 , Tiejun Zhao1 , Daxiang Dong2 , Hao Tian2 and Dianhai Yu2 Harbin Institute of Technology , Harbin , China Baidu Inc. , Beijing , China { yumo , tjzhao } @ mtlab.hit.edu.cn { dongdaxiang , tianhao , yudianhai } @ baidu.com Abstract To solve data sparsity problem , recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features .", "word embeddings .", "To overcome these problems , we instead built compound features from continuous word embeddings based on clustering ."]}
{"orig_sents": ["6", "4", "2", "7", "5", "3", "0", "1"], "shuf_sents": ["We explore two novel approaches using statistical classification methods and evaluate those with a preexisting corpus providing user and expert ratings .", "After analyzing the results , we eventually recommend to use expert ratings instead of user ratings in general .", "Work on automatic estimation of subjective quality usually relies on statistical models .", "In this paper , we analyze the relationship between user and expert ratings by investigating models which combine the advantages of both types of ratings .", "Deriving information about its quality can help rendering SDSs more user-adaptive .", "Here , both variants have their advantages and drawbacks .", "In the field of Intelligent User Interfaces , Spoken Dialogue Systems ( SDSs ) play a key role as speech represents a true intuitive means of human communication .", "To create those , manual data annotation is required , which may be performed by actual users or by experts ."]}
{"orig_sents": ["2", "3", "1", "0", "4"], "shuf_sents": ["We present a greedy document partitioning technique for the task .", "In this paper , we show how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node .", "Large unsupervised latent variable models ( LVMs ) of text , such as Latent Dirichlet Allocation models or Hidden Markov Models ( HMMs ) , are constructed using parallel training algorithms on computational clusters .", "The memory required to hold LVM parameters forms a bottleneck in training more powerful models .", "For large corpora , our approach reduces memory consumption by over 50 % , and trains the same models up to three times faster , when compared with existing approaches for parallel LVM training ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["In cases in which there is no standard orthography for a language or language variant , written texts will display a variety of orthographic choices .", "We show that a two-stage process can reduce divergences from this standard by 69 % , making subsequent processing of Egyptian Arabic easier .", "This is problematic for natural language processing ( NLP ) because it creates spurious data sparseness .", "We study the transformation of spontaneously spelled Egyptian Arabic into a conventionalized orthography which we have previously proposed for NLP purposes ."]}
{"orig_sents": ["3", "2", "1", "0", "10", "7", "9", "5", "11", "4", "6", "8"], "shuf_sents": ["This does not necessarily reflect the ? quality ?", "indicators of how good a published paper is .", "Existing bibliometric measures provide ? quantitative ?", "Bibliometric measures are commonly used to estimate the popularity and the impact of published research .", "We propose supervised methods for identifying citation text and analyzing it to determine the purpose ( i.e .", "aspect to biblometrics .", "author intention ) and the polarity ( i.e .", "For example , when hindex is computed for a researcher , all incoming citations are treated equally , ignoring the fact that some of these citations might be negative .", "author sentiment ) of citation .", "In this paper , we propose using NLP to add a ? qualitative ?", "of the work presented in the paper .", "We analyze the text that accompanies citations in scientific articles ( which we term citation context ) ."]}
{"orig_sents": ["6", "5", "0", "4", "1", "2", "3"], "shuf_sents": ["This short paper argues that in order to assess the robustness of NLP tools we need to evaluate them on diverse samples , and we consider the problem of finding the most appropriate way to estimate the true effect size across datasets of our systems over their baselines .", "by comparing estimated error reduction over observed error reduction on held-out datasets ?", "that this method is significantly more predictive of success than the usual practice of using macro- or micro-averages .", "Finally , we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis .", "We apply meta-analysis and show experimentally ?", "Common evaluation practice prescribes significance testing across data points in available test data , but typically we only have a single test sample .", "Most NLP tools are applied to text that is different from the kind of text they were evaluated on ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We present a systematic study of the effect of crowdsourced translations on Machine Translation performance .", "We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation .", "We compare Machine Translation systems trained on the same data but with translations obtained using Amazon ? s Mechanical Turk vs. professional translations , and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["Most experiments for which constituent-based treebanks such as the Penn Treebank are converted into dependency treebanks rely blindly on one of four-five widely used tree-to-dependency conversion schemes .", "Dependency analysis relies on morphosyntactic evidence , as well as semantic evidence .", "In some cases , however , morphosyntactic evidence seems to be in conflict with semantic evidence .", "For this reason dependency grammar theories , annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions .", "This paper evaluates the down-stream effect of choice of conversion scheme , showing that it has dramatic impact on end results ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We build a logistic regression model for predicting the singleton/coreferent distinction , drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features .", "The model is effective in its own right ( 78 % accuracy ) , and incorporating it into a state-of-the-art coreference resolution system yields a significant improvement .", "A discourse typically involves numerous entities , but few are mentioned more than once .", "Distinguishing discourse entities that die out after just one mention ( singletons ) from those that lead longer lives ( coreferent ) would benefit NLP applications such as coreference resolution , protagonist identification , topic modeling , and discourse coherence ."]}
{"orig_sents": ["0", "2", "4", "3", "1"], "shuf_sents": ["A respelling is an alternative spelling of a word in the same writing system , intended to clarify pronunciation .", "The results show that the respellings generated by our system are better on average than those found on the Web , and approach the quality of respellings designed by an expert .", "We introduce the task of automatic generation of a respelling from the word ? s phonemic representation .", "We evaluate our system both intrinsically through a human judgment experiment , and extrinsically by passing its output to a letterto-phoneme converter .", "Our approach combines machine learning with linguistic constraints and electronic resources ."]}
{"orig_sents": ["3", "2", "0", "4", "1"], "shuf_sents": ["Training the model is consistently ten times faster than Model 4 .", "An open-source implementation of the alignment model described in this paper is available from http : //github.com/clab/fast align .", "Efficient inference , likelihood evaluation , and parameter estimation algorithms are provided .", "We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1 ? s strong assumptions and Model 2 ? s overparameterization .", "On three large-scale translation tasks , systems built using our alignment model outperform IBM Model 4 ."]}
{"orig_sents": ["4", "1", "3", "0", "2"], "shuf_sents": ["Experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones .", "The proposed adaptation procedure is initialized with a standard general-domain TM , which is then used to perform phrase training on a smaller in-domain set .", "Additionally , we compare our results to mixture modeling , where we report gains when using the suggested phrase training adaptation method .", "This way , we bias the probabilities of the general TM towards the in-domain distribution .", "We present a novel approach for translation model ( TM ) adaptation using phrase training ."]}
{"orig_sents": ["6", "5", "2", "0", "3", "1", "4"], "shuf_sents": ["Our proposed method uses a weighted average of the synonyms ?", "We evaluate our method , using the synsets from the cross-lingually aligned Japanese and English WordNet .", "Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term ? s context vector does not always reliably represent a terms meaning due to the context vector ? s sparsity .", "context vectors , that is derived by inferring the mean vector of the von Mises-Fisher distribution .", "The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors .", "The motivation is that , given a certain query term , it is often possible for a user to specify one or more synonyms .", "We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora ."]}
{"orig_sents": ["0", "2", "5", "1", "4", "3"], "shuf_sents": ["We consider the task of tagging Arabic nouns with WordNet supersenses .", "The second uses unsupervised sequence modeling .", "Three approaches are evaluated .", "Analysis shows gains and remaining obstacles in four Wikipedia topical domains .", "The third and most successful approach uses machine translation to translate the Arabic into English , which is automatically tagged with English supersenses , the results of which are then projected back into Arabic .", "The first uses an expertcrafted but limited-coverage lexicon , Arabic WordNet , and heuristics ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["The objective of our method is to minimize average loss under random distribution shifts .", "We restrict the possible target distributions to mixtures of the source distribution and random Zipfian distributions .", "Inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech ( POS ) tagging that is less sensitive to domain shifts .", "Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4 % error reduction in tagging accuracy ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We show , in particular , that this model can be applied to the task of inducing stylistic lexicons , and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions .", "We adapt the popular LDA topic model ( Blei et al , 2003 ) to the representation of stylistic lexical information , evaluating our model on the basis of human-interpretability at the word and text level ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This paper presents an LDA-based approach for WSD , which is trained using any available WSD system to establish a sense per ( Latent Dirichlet alocation based ) topic .", "The topic of a document can prove to be useful information for Word Sense Disambiguation ( WSD ) since certain meanings tend to be associated with particular topics .", "The technique is tested using three unsupervised and one supervised WSD algorithms within the SPORT and FINANCE domains giving a performance increase each time , suggesting that the technique may be useful to improve the performance of any available WSD system ."]}
{"orig_sents": ["3", "0", "4", "5", "1", "2"], "shuf_sents": ["However , real-world datasets often have multiple metadata attributes that can divide the data into domains .", "Experimentally , they outperform the multi-domain learning baseline , even when it selects the single ? best ?", "attribute .", "Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains .", "It is not always apparent which single attribute will lead to the best domains , and more than one attribute might impact classification .", "We propose extensions to two multi-domain learning techniques for our multi-attribute setting , enabling them to simultaneously learn from several metadata attributes ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We propose a way to define this component of opinion and describe the challenges it poses for corpus development and sentence-level detection technologies .", "A component of these is the perspective of the user of an opinion-mining system as to what an opinion really is , which is in itself a matter of opinion ( metasubjectivity ) .", "Finally , we suggest that investment in techniques to handle metasubjectivity will likely bear costs but bring benefits in the longer term .", "This opinion piece proposes that recent advances in opinion detection are limited in the extent to which they can detect important categories of opinion because they are not designed to capture some of the pragmatic aspects of opinion ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We then conduct exploratory analysis in order to isolate factors associated with deleted posts .", "Social media users who post bullying related tweets may later experience regret , potentially causing them to delete their posts .", "In this paper , we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted .", "Finally , we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["The first system combines various lexical and prosodic features in a Conditional Random Field model for detecting edit disfluencies .", "We investigate two systems for automatic disfluency detection on English and Mandarin conversational speech data .", "The second system combines acoustic and language model scores for detecting filled pauses through constrained speech recognition .", "We compare the contributions of different knowledge sources to detection performance between these two languages ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["Although this atypicality often manifests itself in the use of unusual or unexpected words and phrases , the rate of use of such unexpected words is rarely directly measured or quantified .", "Atypical semantic and pragmatic expression is frequently reported in the language of children with autism .", "The classification of unexpected words is sufficiently accurate to distinguish the retellings of children with autism from those with typical development .", "In this paper , we use distributional semantic models to automatically identify unexpected words in narrative retellings by children with autism .", "These techniques demonstrate the potential of applying automated language analysis techniques to clinically elicited language data for diagnostic purposes ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["This work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study .", "The results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions .", "Experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase , i.e .", "Automatic assessment of reading ability builds on applying speech recognition tools to oral reading , measuring words correct per minute .", "in less appropriate locations ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper we leverage methods from submodular function optimization developed for document summarization and apply them to the problem of subselecting acoustic data .", "We evaluate our results on data subset selection for a phone recognition task .", "Our framework shows significant improvements over random selection and previously proposed methods using a similar amount of resources ."]}
{"orig_sents": ["6", "4", "2", "5", "3", "1", "0"], "shuf_sents": ["We find out that OOD data can yield improvements comparable to in-domain data .", "In this study , we investigate how semi-supervised training performs with OOD data .", "Instead , we can simulate the output of an ASR system , in which case the training becomes semisupervised .", "In typical scenarios , transcribed in-domain data is limited but large amounts of out-of-domain ( OOD ) data is available .", "However , DLM requires large amounts of ASR output to train .", "The advantage of using simulated hypotheses is that we can generate as many hypotheses as we want provided that we have enough text material .", "One way to improve the accuracy of automatic speech recognition ( ASR ) is to use discriminative language modeling ( DLM ) , which enhances discrimination by learning where the ASR hypotheses deviate from the uttered sentences ."]}
{"orig_sents": ["3", "0", "5", "1", "2", "6", "4", "7"], "shuf_sents": ["However , these approaches can be classified as weak AI systems .", "In order to accomplish this , a detailed understanding of the human techniques employed for sense disambiguation is necessary .", "Instead of building yet another WSD system that uses contextual evidence for sense disambiguation , as has been done before , we have taken a step back - we have endeavored to discover the cognitive faculties that lie at the very core of the human sense disambiguation technique .", "Word Sense Disambiguation ( WSD ) approaches have reported good accuracies in recent years .", "We also strive to find the levels of difficulties in annotating various classes of words , with senses .", "According to the classical definition , a strong AI based WSD system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings .", "In this paper , we present a hypothesis regarding the cognitive sub-processes involved in the task ofWSD.We support our hypothesis using the experiments conducted through the means of an eye-tracking device .", "We believe , once such an in-depth analysis is performed , numerous insights can be gained to develop a robust WSD system that conforms to the principle of strong AI ."]}
{"orig_sents": ["0", "4", "1", "6", "3", "5", "2"], "shuf_sents": ["Sentence Similarity computes a similarity score between two sentences .", "a sentence .", "The experiments show state-of-the-art performance among unsupervised systems on two SS datasets .", "In this paper , we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics .", "The SS task differs from document level semantics tasks in that it features the sparsity of words in a data unit , i.e .", "We incorporate both corpus-based ( selectional preference information ) and knowledge-based ( similar words extracted in a dictionary ) lexical semantics into a latent variable model .", "Accordingly it is crucial to robustly model each word in a sentence to capture the complete semantic picture of the sentence ."]}
{"orig_sents": ["1", "4", "8", "7", "6", "3", "5", "2", "0"], "shuf_sents": ["Remarkably , this method outperforms the best previous systems .", "Continuous space language models have recently demonstrated outstanding results across a variety of tasks .", "We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions .", "results in a vector very close to ? Queen . ?", "In this paper , we examine the vector-space word representations that are implicitly learned by the input-layer weights .", "We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions ( provided with this paper ) , and are able to correctly answer almost 40 % of the questions .", "For example , the male/female relationship is automatically learned , and with the induced vector representations , ? King Man + Woman ?", "This allows vector-oriented reasoning based on the offsets between words .", "We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language , and that each relationship is characterized by a relation-specific vector offset ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["TruthTeller integrates a range of semantic phenomena , such as negation , modality , presupposition , implicativity , and more , which were dealt only partly in previous works .", "We propose a novel semantic annotation type of assigning truth values to predicate occurrences , and present TruthTeller , a standalone publiclyavailable tool that produces such annotations .", "Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for NLP ."]}
{"orig_sents": ["3", "2", "1", "0", "4", "5"], "shuf_sents": ["We also release PPDB : Spa , a collection of 196 million Spanish paraphrases .", "The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words .", "Its English portion , PPDB : Eng , contains over 220 million paraphrase pairs , consisting of 73 million phrasal and 8 million lexical paraphrases , as well as 140 million paraphrase patterns , which capture many meaning-preserving syntactic transformations .", "We present the 1.0 release of our paraphrase database , PPDB .", "Each paraphrase pair in PPDB contains a set of associated scores , including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus .", "Our release includes pruning tools that allow users to determine their own precision/recall tradeoff ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper presents an approach that exploits the scope of negation cues for relation extraction ( RE ) without the need of using any specifically annotated dataset for building a separate negation scope detection classifier .", "New features are proposed which are used in two different stages .", "These also include non-target entity specific features .", "The proposed RE approach outperforms the previous state of the art for drug-drug interaction ( DDI ) extraction ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["Unfortunately , a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions .", "This paper proposes an alternative bootstrapping method , which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph .", "Iterative bootstrapping methods are widely employed for relation extraction , especially because they require only a small amount of human supervision .", "In contrast to previous bootstrapping methods , our method is not susceptible to semantic drift , and it empirically results in better extractions than iterative methods ."]}
{"orig_sents": ["1", "5", "3", "2", "0", "4"], "shuf_sents": ["Building on a state-of-the-art distantly-supervised extraction algorithm , we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level .", "Distant supervision , heuristically labeling a corpus using a knowledge base , has emerged as a popular choice for training relation extractors .", "Therefore the heuristic for generating negative examples has a serious flaw .", "examples generated by the labeling process are false negatives because the knowledge base is incomplete .", "Experimental results demonstrate its advantage over existing algorithms .", "In this paper , we show that a significant number of ? negative ?"]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Unlike TF-IDF and other content-driven measurements , RSI identifies words or phrases that are structural cues in an unstructured document .", "Experiments show that using RSI significantly improves the segmentation accuracy compared to TF-IDF , a traditional content-based feature weighting scheme .", "We show structurally motivated features with high RSI values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments .", "In this paper , we propose a novel Rhetorical Structure Index ( RSI ) to measure the structural importance of a word or a phrase ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["However , previous work has relied on simple content analysis , which conflates flu tweets that report infection with those that express concerned awareness of the flu .", "By discriminating these categories , as well as tweets about the authors versus about others , we demonstrate significant improvements on influenza surveillance using Twitter .", "Twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["Our results suggest that user responses are different when communicating with a wizarded and an automated system , indicating that wizard data may be less reliable for informing automated systems than generally assumed .", "Previous work has exposed dependencies between user behavior towards systems and user belief about whether the system is automated or human-controlled .", "Wizard-of-Oz experimental setup in a dialogue system is commonly used to gather data for informing an automated version of that system .", "We perform a posthoc experiment using generalizable prosodic and lexical features of user responses to a dialogue system backed with and without a human wizard .", "This work examines whether user behavior changes when user belief is held constant and the system ? s operator is varied ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the class based not on confidence , but on a cascade of classifiers .", "We show that it gives a minority class F-measure error reduction of 22.8 % , while also reducing the error for other classes and the overall error by about 10 % ."]}
{"orig_sents": ["0", "4", "6", "5", "1", "3", "2"], "shuf_sents": ["Document-level sentiment analysis can benefit from fine-grained subjectivity , so that sentiment polarity judgments are based on the relevant parts of the document .", "Connector-augmented transition features allow the latent variable model to learn the relevance of discourse connectors for subjectivity transitions , without subjectivity annotations .", "We also describe a simple heuristic for automatically identifying connectors when no predefined list is available .", "This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish .", "While finegrained subjectivity annotations are rarely available , encouraging results have been obtained by modeling subjectivity as a latent variable .", "We present a new method for injecting linguistic knowledge into latent variable subjectivity modeling , using discourse connectors .", "However , latent variable models fail to capitalize on our linguistic knowledge about discourse structure ."]}
{"orig_sents": ["1", "3", "2", "5", "4", "6", "0"], "shuf_sents": ["Further experiments indicate that a weighted FMeasure of 73 % can be achieved for the automated prediction of the coherence scores .", "This study focuses on modeling discourse coherence in the context of automated assessment of spontaneous speech from non-native speakers .", "However , very little research has been done to assess a speaker 's coherence in automated speech scoring systems .", "Discourse coherence has always been used as a key metric in human scoring rubrics for various assessments of spoken language .", "Then , we investigate the use of several features originally developed for essays to model coherence in spoken responses .", "To address this , we present a corpus of spoken responses that has been annotated for discourse coherence quality .", "An analysis on the annotated corpus shows that the prediction accuracy for human holistic scores of an automated speech scoring system can be improved by around 10 % relative after the addition of the coherence features ."]}
{"orig_sents": ["6", "2", "1", "7", "3", "5", "0", "4"], "shuf_sents": ["Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance .", "First , we detect filler words .", "Our method incorporates refined n-gram features step by step from different word sequences .", "In the third step , additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection .", "Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1", "We useMax-MarginMarkov Networks ( M3Ns ) as the classifier with the weighted hamming loss to balance precision and recall .", "In this paper , we propose a multi-step stacked learning model for disfluency detection .", "Second , edited words are detected using n-gram features extracted from both the original text and filler filtered text ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["Our model substantially outperforms a stateof-the-art semantic parsing baseline , yielding a 29 % absolute improvement in accuracy.1", "We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk .", "We consider the problem of translating natural language text queries into regular expressions which represent their meaning .", "However , a given regular expression can be written in many semantically equivalent forms , and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language .", "The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem ."]}
{"orig_sents": ["0", "6", "7", "3", "2", "5", "1", "4"], "shuf_sents": ["In natural-language discourse , related events tend to appear near each other to describe a larger scenario .", "The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing .", "Methods for inducing frames have been proposed recently , but they typically use ad hoc procedures and are difficult to diagnose or extend .", "Identifying frames is a prerequisite for information extraction and natural language generation , and is usually done manually .", "In end-to-end evaluations from text to induced frames and extracted facts , our method produces state-of-the-art results while substantially reducing engineering effort .", "In this paper , we propose the first probabilistic approach to frame induction , which incorporates frames , events , and participants as latent topics and learns those frame and event transitions that best explain the text .", "Such structures can be formalized by the notion of a frame ( a.k.a .", "template ) , which comprises a set of related events and prototypical participants and event transitions ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We define a dependency-based Hilbert space and show how to represent the meaning of words by density matrices that encode dependency neighborhoods .", "In this paper we explore the potential of quantum theory as a formal framework for capturing lexical meaning .", "Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models .", "We present a novel semantic space model that is syntactically aware , takes word order into account , and features key quantum aspects such as superposition and entanglement ."]}
{"orig_sents": ["4", "0", "3", "2", "1"], "shuf_sents": ["We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences , learning the association between questions and answer types .", "The developed system is open-source , and includes an implementation of the TED model that is state of the art in the task of ranking QA pairs .", "Our model is free of manually created question and answer templates , fast to run ( processing 200 QA pairs per second excluding parsing time ) , and yields an F1 of 63.3 % on a new public dataset based on prior TREC QA evaluations .", "This casts answer extraction as an answer sequence tagging problem for the first time , where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance ( TED ) .", "Our goal is to extract answers from preretrieved sentences for Question Answering ( QA ) ."]}
{"orig_sents": ["3", "9", "5", "7", "0", "10", "4", "8", "1", "2", "6"], "shuf_sents": ["One task is to determine if a sentence potentially contains a relation between two entities ?", "Our results on three datasets show that our system is superior when compared to state-of-the-art systems like REVERB and OLLIE for both tasks .", "For example , in some experiments our system achieves 33 % improvement on nominal relation extraction over OLLIE .", "Traditional relation extraction seeks to identify pre-specified semantic relations within natural language text , while open Information Extraction ( Open IE ) takes a more general approach , and looks for a variety of relations without restriction to a fixed relation set .", "We propose multiple SVM models with dependency tree kernels for both tasks .", "For example , should the more general task be restricted to relations mediated by verbs , nouns , or both ?", "In addition we propose an unsupervised rule-based approach which can serve as a strong baseline for Open IE systems .", "To help answer this question , we propose two levels of subtasks for Open IE .", "For explicit relation extraction , our system can extract both noun and verb relations .", "With this generalization comes the question , what is a relation ?", "The other task looks to confirm explicit relation words for two entities ."]}
{"orig_sents": ["5", "3", "0", "4", "1", "2"], "shuf_sents": ["The system relies on deep syntactic and semantic analysis of questions only and is independent of relevant documents .", "When used in a QA system features derived from the MMP model improve performance significantly over a state-of-the-art baseline .", "The final QA system was the best performing system in the DARPA BOLT-IR evaluation .", "We present a learnable system that can extract and rank these terms and phrases ( dubbed mandatory matching phrases or MMPs ) , and demonstrate their utility in a QA system on Internet discussion forum data sets .", "Our proposed model can predict MMPs with high accuracy .", "In natural language question answering ( QA ) systems , questions often contain terms and phrases that are critically important for retrieving or finding answers from documents ."]}
{"orig_sents": ["2", "1", "4", "0", "3"], "shuf_sents": ["The first factor is the temporal history of preceding utterances that grants higher importance to recent utterances than old ones , and the second is topic relevance that forces only the preceding utterances relevant to the current utterance to be considered in keyword extraction .", "Thus , this paper proposes a just-intime keyword extraction from meeting transcripts .", "In a meeting , it is often desirable to extract keywords from each utterance as soon as it is spoken .", "Our experiments on two data sets in English and Korean show that the consideration of the factors results in performance improvement in keyword extraction from meeting transcripts .", "The proposed method considers two major factors that make it different from keyword extraction from normal texts ."]}
{"orig_sents": ["3", "5", "0", "1", "4", "2"], "shuf_sents": ["We present a new unsupervised method for mining opaque pairs .", "Our intuition is to restrict distributional semantics to articles about the same event , thus promoting referential match .", "Our dictionary can be integrated into any coreference system ( it increases the performance of a state-of-the-art system by 1 % F1 on all measures ) and is easily extendable by using news aggregators .", "Coreference resolution systems rely heavily on string overlap ( e.g. , Google Inc. and Google ) , performing badly on mentions with very different words ( opaque mentions ) like Google and the search giant .", "Using an English comparable corpus of tech news , we built a dictionary of opaque coreferent mentions ( only 3 % are in WordNet ) .", "Yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Our model integrates global constraints on top of a rich local feature set in the framework of Markov logic networks .", "We present the first work on antecedent selection for bridging resolution without restrictions on anaphor or relation types .", "The global model improves over the local one and both strongly outperform a reimplementation of prior work ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Experiments with the TimeBank corpus demonstrate that our knowledge-rich , hybrid approach yields a 15 ? 16 % relative reduction in error over a state-of-the-art learning-based baseline system .", "We examine the task of temporal relation classification .", "Unlike existing approaches to this task , we ( 1 ) classify an event-event or eventtime pair as one of the 14 temporal relations defined in the TimeBank corpus , rather than as one of the six relations collapsed from the original 14 ; ( 2 ) employ sophisticated linguistic knowledge derived from a variety of semantic and discourse relations , rather than focusing on morpho-syntactic knowledge ; and ( 3 ) leverage a novel combination of rule-based and learning-based approaches , rather than relying solely on one or the other ."]}
{"orig_sents": ["2", "4", "3", "5", "0", "1"], "shuf_sents": ["Our constrained model improves the performance of existing fully and lightly supervised models .", "Even a fully unsupervised version of this model outperforms lightly supervised feature-based models , showing that our approach can be useful even when no labeled data is available .", "Inferring the information structure of scientific documents is useful for many downstream applications .", "Our idea is to guide feature-based models with declarative domain knowledge encoded as posterior distribution constraints .", "Existing feature-based machine learning approaches to this task require substantial training data and suffer from limited performance .", "We explore a rich set of discourse and lexical constraints which we incorporate through the Generalized Expectation ( GE ) criterion ."]}
{"orig_sents": ["2", "6", "3", "0", "4", "5", "1"], "shuf_sents": ["We find that , surprisingly , different training corpora can vary widely in their reordering characteristics for particular phrase pairs .", "Applied to mixture RMs in our experiments , these techniques ( especially smoothing ) yield significant performance improvements .", "Previous research on domain adaptation ( DA ) for statistical machine translation ( SMT ) has mainly focused on the translation model ( TM ) and the language model ( LM ) .", "In this paper , we demonstrate that mixture model adaptation of a lexicalized RM can significantly improve SMT performance , even when the system already contains a domain-adapted TM and LM .", "Furthermore , particular training corpora may be highly suitable for training the TM or the LM , but unsuitable for training the RM , or vice versa , so mixture weights for these models should be estimated separately .", "An additional contribution of the paper is to propose two improvements to mixture model adaptation : smoothing the in-domain sample , and weighting instances by document frequency .", "To the best of our knowledge , there is no previous work on reordering model ( RM ) adaptation for phrasebased SMT ."]}
{"orig_sents": ["3", "7", "4", "5", "1", "2", "0", "6"], "shuf_sents": ["This contrasts the traditional tuning where gains are usually limited to a single metric .", "We study the effectiveness of our methods through experiments on multiple as well as single reference ( s ) datasets .", "Our experiments show simultaneous gains across several metrics ( BLEU , RIBES ) , without any significant reduction in other metrics .", "This paper examines tuning for statistical machine translation ( SMT ) with respect to multiple evaluation metrics .", "Pareto-optimality is a natural way to think about multi-metric optimization ( MMO ) and our methods can effectively combine several Pareto-optimal solutions , obviating the need to choose one .", "Our best performing ensemble tuning method is a new algorithm for multi-metric optimization that searches for Pareto-optimal ensemble models .", "Our human evaluation results confirm that in order to produce better MT output , optimizing multiple metrics is better than optimizing only one .", "We propose several novel methods for tuning towards multiple objectives , including some based on ensemble decoding methods ."]}
{"orig_sents": ["2", "0", "4", "3", "1", "5"], "shuf_sents": ["In the popular cube pruning algorithm , every hypothesis is annotated with boundary words and permitted to recombine only if all boundary words are equal .", "This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand .", "We propose a new algorithm to approximately extract top-scoring hypotheses from a hypergraph when the score includes an N ? gram language model .", "We use these common boundary words to group hypotheses and do so recursively , resulting in a tree of hypotheses .", "However , many hypotheses share some , but not all , boundary words .", "Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases ."]}
{"orig_sents": ["4", "5", "1", "2", "0", "3"], "shuf_sents": ["The resulting models are highly extendible , naturally permitting the introduction of phrasal dependencies .", "In this paper we build upon this venerable base by recasting these models in the non-parametric Bayesian framework .", "By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes , and through the use of collapsed Gibbs sampling , we provide a more flexible formulation and sidestep the original heuristic optimisation techniques .", "We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++ , including significant improvements over IBM model 4 .", "The dominant yet ageing IBM and HMM word alignment models underpin most popular Statistical Machine Translation implementations in use today .", "Though beset by the limitations of implausible independence assumptions , intractable optimisation problems , and an excess of tunable parameters , these models provide a scalable and reliable starting point for inducing translation systems ."]}
{"orig_sents": ["2", "0", "5", "1", "3", "4"], "shuf_sents": ["Our system first performs hierarchical graph factorization clustering ( HGFC ) of nouns and then searches the resulting graph for metaphorical connections between concepts .", "In contrast to previous work , our method is fully unsupervised .", "We present a novel approach to automatic metaphor identification , that discovers both metaphorical associations and metaphorical expressions in unrestricted text .", "Despite this fact , it operates with an encouraging precision ( 0.69 ) and recall ( 0.61 ) .", "Our approach is also the first one in NLP to exploit the cognitive findings on the differences in organisation of abstract and concrete concepts in the human brain .", "It then makes use of the salient features of the metaphorically connected clusters to identify the actual metaphorical expressions ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains , we describe our new two-level chain annotation scheme , which rooted in the concept of cohesive harmony .", "We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents .", "Our three LDA-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin , which can be attributed to lacking coverage of the knowledge resource .", "Also , we propose a new measure for direct evaluation of lexical chains .", "Subsequent analysis shows that the three methods yield a different chaining behavior , which could be utilized in tasks that use lexical chaining as a component within NLP applications ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["When evaluated in the setting of a recently proposed SemEval-2012 task , our approach outperforms the previous best system substantially , achieving a 54.1 % relative increase in Spearman ? s rank correlation .", "Our overall system consists of two novel general-purpose relational similarity models and three specific word relation models .", "Due to the large number of possible relations , we argue that it is important to combine multiple models based on heterogeneous information sources .", "In this work , we study the problem of measuring relational similarity between two word pairs ( e.g. , silverware : fork and clothing : shirt ) ."]}
{"orig_sents": ["2", "0", "5", "3", "6", "1", "4"], "shuf_sents": ["Furthermore , users who communicate with each other often have similar hidden properties .", "The efficacy of these clusters is then evaluated on a diverse set of classification tasks that predict hidden users properties such as ethnicity , geographic location , gender , language , and race , using only profile names and locations when appropriate .", "Hidden properties of social media users , such as their ethnicity , gender , and location , are often reflected in their observed attributes , such as their first and last names .", "Attributes such as user names are grouped together if users with those names communicate with other similar users .", "Our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile , substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied .", "We propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of Twitter users .", "We separately cluster millions of unique first names , last names , and userprovided locations ."]}
{"orig_sents": ["3", "1", "2", "6", "4", "5", "0"], "shuf_sents": ["By combining structural learning and a variety of firstorder , second-order , and context-sensitive features , our system is able to outperform existing state-of-the art entity linking systems by 15 % F1 .", "As the core component of information extraction , we consider the task of Twitter entity linking in this paper .", "In the current entity linking literature , mention detection and entity disambiguation are frequently cast as equally important but distinct problems .", "Information extraction from microblog posts is an important task , as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world .", "The reason is that messages on micro-blogs are short , noisy and informal texts with little context , and often contain phrases with ambiguous meanings .", "To rigorously address the Twitter entity linking problem , we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task .", "However , in our task , we find that mention detection is often the performance bottleneck ."]}
{"orig_sents": ["1", "5", "0", "4", "3", "2"], "shuf_sents": ["To automatically discover the viewpoints or stances on hot issues from forum threads is an important and useful task .", "Threaded discussion forums provide an important social media platform .", "Evaluation results show that our model clearly outperforms a number of baseline models in terms of both clustering posts based on viewpoints and clustering users with different viewpoints .", "Our model is a principled generative latent variable model which captures three important factors : viewpoint specific topic preference , user identity and user interactions .", "In this paper , we propose a novel latent variable model for viewpoint discovery from threaded forum posts .", "Its rich user generated content has served as an important source of public feedback ."]}
{"orig_sents": ["4", "5", "7", "0", "6", "1", "2", "3"], "shuf_sents": ["This intention can be easily exploited by advertisers .", "Our research found that this problem is particularly suited to transfer learning because in different domains , people express the same intention in similar ways .", "We then propose a new transfer learning method which , unlike a general transfer learning algorithm , exploits several special characteristics of the problem .", "Experimental results show that the proposed method outperforms several strong baselines , including supervised learning in the target domain and a recent transfer learning method .", "This paper proposes to study the problem of identifying intention posts in online discussion forums .", "For example , in a discussion forum , a user wrote ? I plan to buy a camera , ?", "To the best of our knowledge , there is still no reported study of this problem .", "which indicates a buying intention ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["We describe a novel approach to detecting empty categories ( EC ) as represented in dependency trees as well as a new metric for measuring EC detection accuracy .", "The new metric takes into account not only the position and type of an EC , but also the head it is a dependent of in a dependency tree .", "We also introduce a variety of new features that are more suited for this approach .", "Tested on a subset of the Chinese Treebank , our system improved significantly over the best previously reported results even when evaluated with this more stringent metric ."]}
{"orig_sents": ["0", "1", "2", "4", "3"], "shuf_sents": ["We study multi-source transfer parsing for resource-poor target languages ; specifically methods for target language adaptation of delexicalized discriminative graph-based dependency parsers .", "We first show how recent insights on selective parameter sharing , based on typological and language-family features , can be applied to a discriminative parser by carefully decomposing its model features .", "We then show how the parser can be relexicalized and adapted using unlabeled target language data and a learning method that can incorporate diverse knowledge sources through ambiguous labelings .", "Our final model outperforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages .", "In the latter scenario , we exploit two sources of knowledge : arc marginals derived from the base parser in a self-training algorithm , and arc predictions from multiple transfer parsers in an ensemble-training algorithm ."]}
{"orig_sents": ["1", "4", "3", "5", "0", "2"], "shuf_sents": ["Our experiments on a cooperative language task show that reasoning about others ?", "Grice characterized communication in terms of the cooperative principle , which enjoins speakers to make only contributions that serve the evolving conversational goals .", "belief states , and the resulting emergent Gricean communicative behavior , leads to significantly improved task performance .", "We utilize the Decentralized Partially Observable Markov Decision Process ( Dec-POMDP ) model of multi-agent decision making which relies only on basic definitions of rationality and the ability of agents to reason about each other ? s beliefs in maximizing joint utility .", "We show that the cooperative principle and the associated maxims of relevance , quality , and quantity emerge from multi-agent decision theory .", "Our model uses cognitively-inspired heuristics to simplify the otherwise intractable task of reasoning jointly about actions , the environment , and the nested beliefs of other actors ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper explores the relationship between explicit and predictive models of incremental speech understanding in a dialogue system that supports a finite set of user utterance meanings .", "We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain .", "We show promising performance for this method in a corpus evaluation , and discuss its practical application and annotation costs in relation to some alternative approaches ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper we propose a new approach to the generation of pseudowords , i.e. , artificial words which model real polysemous words .", "Our approach simultaneously addresses the two important issues that hamper the generation of large pseudosense-annotated datasets : semantic awareness and coverage .", "We evaluate these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["This problem is approached in a weakly supervised fashion , as a sequence labeling problem with monolingual text samples for training data .", "Among the approaches evaluated , a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied .", "In this paper we consider the problem of labeling the languages of words in mixed-language documents ."]}
{"orig_sents": ["3", "0", "2", "5", "1", "6", "7", "8", "4"], "shuf_sents": ["Unfortunately , some annotators choose bad labels in order to maximize their pay .", "We match performance of more complex state-of-the-art systems and perform well even under adversarial conditions .", "Manual identification is tedious , so we experiment with an item-response model .", "Non-expert annotation services like Amazon ? s Mechanical Turk ( AMT ) are cheap and fast ways to evaluate systems and provide categorical annotations for training data .", "Our system , MACE ( Multi-Annotator Competence Estimation ) , is available for download1 .", "It learns in an unsupervised fashion to a ) identify which annotators are trustworthy and b ) predict the correct underlying labels .", "We show considerable improvements over standard baselines , both for predicted label accuracy and trustworthiness estimates .", "The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference .", "Additionally , we can achieve even higher accuracy by focusing on the instances our model is most confident in ( trading in some recall ) , and by incorporating annotated control instances ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["Robustness of our approach is demonstrated by evaluating it successfully on two different datasets .", "Instead of learning word-specific substitution patterns , a global model for lexical substitution is trained on delexicalized ( i.e. , non lexical ) features , which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set .", "This way , our approach remains technically straightforward , provides better performance and similar coverage in comparison to unsupervised approaches .", "We propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary .", "Using features from lexical resources , as well as a variety of features computed from large corpora ( n-gram counts , distributional similarity ) and a ranking method based on the posterior probabilities obtained from a Maximum Entropy classifier , we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure ."]}
{"orig_sents": ["4", "1", "0", "6", "2", "5", "3"], "shuf_sents": ["We use our method to model the composition of subject verb object triples .", "The key idea is that compositionality is modeled as a multi-way interaction between latent factors , which are automatically constructed from corpus data .", "First , we compute a latent factor model for nouns from standard co-occurrence data .", "Our model has been evaluated on a similarity task for transitive phrases , in which it exceeds the state of the art .", "In this paper , we present a novel method for the computation of compositionality within a distributional framework .", "Next , the latent factors are used to induce a latent model of three-way subject verb object interactions .", "The method consists of two steps ."]}
{"orig_sents": ["1", "0", "3", "6", "7", "5", "2", "4"], "shuf_sents": ["However , summarizing the Twitter event has been a challenging task that was not fully explored in the past .", "Twitter offers an unprecedented advantage on live reporting of the events happening around the world .", "We evaluate the proposed approach on different event types .", "In this paper , we propose a participant-based event summarization approach that ? zooms-in ?", "Results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events , yielding summaries with considerably better coverage than the state-of-the-art .", "properties of the event tweets , and generates the event summaries progressively .", "the Twitter event streams to the participant level , detects the important sub-events associated with each participant using a novel mixture model that combines the ? burstiness ?", "and ? cohesiveness ?"]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["We evaluate G-FLOW on Mechanical Turk , and find that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components , underscoring the value of our joint model .", "This graph enables G-FLOW to estimate the coherence of a candidate summary .", "G-FLOW ? s core representation is a graph that approximates the discourse relations across sentences based on indicators including discourse cues , deverbal nouns , co-reference , and more .", "This paper presents G-FLOW , a novel system for coherent extractive multi-document summarization ( MDS ) .1 Where previous work on MDS considered sentence selection and ordering separately , G-FLOW introduces a joint model for selection and ordering that balances coherence and salience ."]}
{"orig_sents": ["2", "5", "4", "1", "3", "0"], "shuf_sents": ["We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm ? s non-determinism into account .", "We call such expressions identifying descriptions .", "We introduce a novel algorithm for generating referring expressions , informed by human and computer vision and designed to refer to visible objects .", "The algorithm outperforms the well-known Incremental Algorithm ( Dale and Reiter , 1995 ) and the GraphBased Algorithm ( Krahmer et al , 2003 ; Viethen et al , 2008 ) across a variety of images in two domains .", "Expressions generated using this method are often overspecified and may be underspecified , akin to expressions produced by people .", "Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Our end-to-end system is able to predict complete paradigms with 86.1 % accuracy and individual inflected forms with 94.9 % accuracy , averaged across three languages and two parts of speech .", "Because our approach is completely data-driven and the model is trained on examples extracted from Wiktionary , our method can extend to new languages without change .", "Our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples , and then learns the contexts in which those transformations apply using a discriminative sequence model .", "We describe a supervised approach to predicting the set of all inflected forms of a lexical item ."]}
{"orig_sents": ["2", "6", "5", "4", "3", "0", "1"], "shuf_sents": ["Experiments over a data-set of 8 languages show that in all scenarios , our selection methods are effective at yielding a small , but optimal set of labelled examples .", "When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction , our methods yield average error reductions of 20 % over randomly selected examples .", "In this paper we introduce the task of unlabeled , optimal , data set selection .", "We apply these methods to the task of grapheme-to-phoneme prediction .", "For our second method , we develop the concept of feature coverage which we optimize with a greedy algorithm .", "Our first proposed method , based on the rank-revealing QR matrix factorization , selects a subset of words which span the entire word-space effectively .", "Given a large pool of unlabeled examples , our goal is to select a small subset to label , which will yield a high performance supervised model over the entire data set ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["This relaxes the word independence assumption and enables sharing of statistical strength across , for example , stems or inflectional paradigms in different contexts .", "We present a morphology-aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages .", "Our model can be used in virtually any scenario where multinomial distributions over words would be used .", "We obtain state-of-the-art results in language modeling , word alignment , and unsupervised morphological disambiguation for a variety of morphologically rich languages ."]}
{"orig_sents": ["4", "3", "2", "0", "1"], "shuf_sents": ["In conclusion we claim that loss of information value is an essential factor , insufficiently adressed in current metrics , in human perception of the degree of success or failure of coreference resolution .", "We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation .", "We examine both the correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics .", "We review the most commonly used metrics ( MUC , B3 , CEAF and BLANC ) on the basis of their evaluation of coreference resolution in five texts from the OntoNotes corpus .", "In this paper we revisit the task of quantitative evaluation of coreference resolution systems ."]}
{"orig_sents": ["3", "5", "0", "1", "2", "4"], "shuf_sents": ["In this paper , a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes .", "We demonstrate the effectiveness of our approach in the context of one form of unbalanced task : annotation of transcribed human-human dialogues for presence/absence of uncertainty .", "In two data sets , our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy .", "Annotated corpora play a significant role in many NLP applications .", "The method is able to reduce human annotation effort by about 80 % without a significant loss in data quality , as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations .", "However , annotation by humans is time-consuming and costly ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We achieved state of the art results for unigram extraction in Brazilian Portuguese .", "In this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms .", "In our preliminary experiments , we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction ."]}
{"orig_sents": ["2", "5", "1", "6", "0", "3", "4"], "shuf_sents": ["The Karminas were produced by randomly pairing the messages with the hooks , subject to the constraints imposed on the rhymes and on the structure similarity .", "One of the unique aspects of Karmina is in the absence of discourse relation between its hook and message .", "We present our work in generating Karmina , an old Malay poetic form for Indonesian language .", "Syllabifications were performed on the cue words of the hooks and messages to ensure the generated pairs have matching rhymes .", "We were able to generate a number of positive examples while still leaving room for improvement , particularly in the generation of the messages , which currently are still limited , and in filtering the negative results .", "Karmina is a poem with two lines that consists of a hook ( sampiran ) on the first line and a message on the second line .", "We approached the problem by generating the hooks and the messages in separate processes using predefined schemas and a manually built knowledge base ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["We show that native language identification accuracy can be improved by up to 6.43 % for a 9-class task , depending on the feature set , by introducing a novel method to incorporate language family information .", "In this paper we aim to identify the native language and language family of a non-native English author , given his/her English writings .", "Revealing an anonymous author ? s traits from text is a well-researched area .", "We extract features from the text based on prior work , and extend or modify it to construct different feature sets , and use support vector machines for classification .", "In addition we show that introducing grammarbased features improves accuracy of both native language and language family identification ."]}
{"orig_sents": ["1", "3", "2", "8", "0", "6", "7", "5", "4"], "shuf_sents": ["The core objective therefore , is to provide statistical machine translation ( SMT ) systems with additional context information .", "Our research investigates the translation of ontology labels , which has applications in multilingual knowledge access .", "To enable knowledge access across languages , such monolingual ontologies need to be translated into other languages .", "Ontologies are often defined only in one language , mostly English .", "This experiment showed that our approach can provide a slight improvement over standard SMT for this task , without exploiting any additional domain-specific resources .", "We applied our approach to the translation of a financial ontology , translating from English to German , using Europarl as parallel corpus .", "In our approach , we first extend standard SMT by enhancing a translation model with context information that keeps track of surrounding words for each translation .", "We compute a semantic similarity between the phrase pair context vector from the parallel corpus and a vector of noun phrases that occur in surrounding ontology labels .", "The primary challenge in ontology label translation is the lack of context , which makes this task rather different than document translation ."]}
{"orig_sents": ["4", "2", "1", "5", "0", "3"], "shuf_sents": ["In a comparison to a stateof-the-art approach , we demonstrate slightly better detokenization error rates , without the need for any hand-crafted rules .", "We review a number of detokenization schemes for Arabic , such as rule-based and table-based approaches and show their limitations .", "Unfortunately , when translating into a morphologically complex language , recombining segmented tokens to generate original word forms is not a trivial task , due to morphological , phonological and orthographic adjustments that occur during tokenization .", "We also demonstrate the effectiveness of our approach in an English-to-Arabic translation task .", "Morphological tokenization has been used in machine translation for morphologically complex languages to reduce lexical sparsity .", "We then propose a novel detokenization scheme that uses a character-level discriminative string transducer to predict the original form of a segmented word ."]}
{"orig_sents": ["6", "1", "5", "2", "0", "3", "4"], "shuf_sents": ["The theme of our approach is the integration of information from alternate data sources , other than parallel corpora , into the statistical model .", "Specifically , it aims to reduce the dependence of modern SMT systems on expensive parallel data .", "All current SMT models use parallel data during training for extracting translation rules and estimating translation probabilities .", "In particular , we focus on making use of large monolingual and comparable corpora .", "By augmenting components of the SMT framework , we hope to extend its applicability beyond the small handful of language pairs with large amounts of available parallel text .", "We define low resource settings as having only small amounts of parallel data available , which is the case for many language pairs .", "My thesis will explore ways to improve the performance of statistical machine translation ( SMT ) in low resource conditions ."]}
{"orig_sents": ["1", "0", "6", "3", "4", "2", "5"], "shuf_sents": ["We leverage bilingual parallel corpora to extract a large collection of syntactic paraphrase pairs , and introduce an adaptation scheme that allows us to tackle a variety of text transformation tasks via paraphrasing .", "We examine the application of data-driven paraphrasing to natural language understanding .", "Finally , we propose a refinement of our paraphrases by classifying them into natural logic entailment relations .", "Further , we use distributional similarity measures based on context vectors derived from large monolingual corpora to annotate our paraphrases with an orthogonal source of information .", "This yields significant improvements in our compression system ? s output quality , achieving state-of-the-art performance .", "By extending the synchronous parsing paradigm towards these entailment relations , we will enable our system to perform recognition of textual entailment .", "We evaluate our system on the sentence compression task ."]}
{"orig_sents": ["0", "5", "2", "3", "4", "1"], "shuf_sents": ["Automatically describing visual content is an extremely difficult task , with hard AI problems in Computer Vision ( CV ) and Natural Language Processing ( NLP ) at its core .", "We propose future work to improve this method , and extensions for other domains of images and natural text .", "These systems require massive amounts of hand-labeled data for training , so the number of visual classes that can be recognized is typically very small .", "We argue that these approaches place unrealistic limits on the kinds of images that can be captioned , and are unlikely to produce captions which reflect human interpretations .", "We present a framework for image caption generation that does not rely on visual recognition systems , which we have implemented on a dataset of online shopping images and product descriptions .", "Previous work relies on supervised visual recognition systems to determine the content of images ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["In this thesis , we propose a novel review summarization framework which summarizes review content under the supervision of automated assessment of review helpfulness .", "Review mining and summarization has been a hot topic for the past decade .", "A lot of effort has been devoted to aspect detection and sentiment analysis under the assumption that every review has the same utility for related tasks .", "However , reviews are not equally helpful as indicated by user-provided helpfulness assessment associated with the reviews .", "This helpfulness-guided framework can be easily adapted to traditional review summarization tasks , for a wide range of domains ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Entrainment is the phenomenon of the speech of conversational partners becoming more similar to each other .", "This thesis proposal presents a comprehensive look at entrainment in human conversations and how entrainment may be incorporated into the design of spoken dialogue systems in order to improve system performance and user satisfaction .", "We compare different kinds of entrainment in both classic and novel dimensions , provide experimental results on the utility of entrainment , and show that entrainment can be used to improve a system ? s ASR performance and turntaking decisions ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["It is believed that with the suggested various domain independent feature functions , the proposed model could better exploit not only the intra-dependencies within long ASR N-best lists but also the inter-dependencies of the observations across dialog turns , which leads to more efficient and accurate dialog state inference .", "The system first predicts the occurrence of a user goal change based on linguistic features and dialog context for each dialog turn , and then the proposed model could utilize this user goal change information to infer the most probable dialog state sequence which underlies the evolvement of user goal during the dialog .", "In this paper , a Maximum Entropy Markov Model ( MEMM ) for dialog state tracking is proposed to efficiently handle user goal evolvement in two steps ."]}
{"orig_sents": ["3", "4", "2", "1", "0", "5"], "shuf_sents": ["DALE uses the UMLS Metathesaurus as both a sense inventory and as a source of information for automatically generating labeled training examples .", "DALE ( Disambiguation using Automatically Labeled Examples ) is a supervised WSD system that can disambiguate a wide range of ambiguities found in biomedical documents .", "Word Sense Disambiguation ( WSD ) systems attempt to resolve these ambiguities but are often only able to identify the meanings for a small set of ambiguous terms .", "Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings .", "These ambiguities can still be found when language is restricted to a particular domain , such as biomedicine .", "DALE is able to disambiguate biomedical documents with the coverage of unsupervised approaches and accuracy of supervised methods ."]}
{"orig_sents": ["1", "0", "4", "3", "2"], "shuf_sents": ["Past work has relied on faceted browsing of document metadata or on natural language processing of document text .", "Effectively exploring and analyzing large text corpora requires visualizations that provide a high level summary .", "We report a user study of the usefulness of topics in our tool .", "The user can manage topics , filter documents by topic and summarize views with metadata and topic graphs .", "In this paper , we present a new web-based tool that integrates topics learned from an unsupervised topic model in a faceted browsing experience ."]}
{"orig_sents": ["2", "3", "1", "4", "0"], "shuf_sents": ["TMTprime takes confidence estimation out of the lab and provides a commercially viable platform that allows for the seamless integration of MT with legacy TM systems to provide the most effective ( least effort/cost ) translation options to human translators , based on the TMTprime confidence score .", "Recent research shows how MT systems can be combined with TMs in Computer Aided Translation ( CAT ) systems , selecting either TM or MT output based on sophisticated translation quality estimation without access to a reference .", "TMTprime is a recommender system that facilitates the effective use of both translation memory ( TM ) and machine translation ( MT ) technology within industrial language service providers ( LSPs ) localization workflows .", "LSPs have long used Translation Memory ( TM ) technology to assist the translation process .", "However , to date there are no commercially available frameworks for this ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["Anafora allows secure web-based annotation of any plaintext file with both spanned ( e.g .", "named entity or markable ) and relation annotations , as well as adjudication for both types of annotation .", "Anafora is a newly-developed open source web-based text annotation tool built to be lightweight , flexible , easy to use and capable of annotating with a variety of schemas , simple and complex .", "Anafora offers automatic set assignment and progress-tracking , centralized and humaneditable XML annotation schemas , and filebased storage and organization of data in a human-readable single-file XML format ."]}
{"orig_sents": ["0", "1", "3", "5", "4", "2"], "shuf_sents": ["Hand gesture-based input systems have been in active research , yet most of them focus only on single character recognition .", "We propose KooSHO : an environment for Japanese input based on aerial hand gestures .", "The comparison with voice recognition and a screen keyboard showed that KooSHO can be a more practical solution compared to the existing system .", "The system provides an integrated experience of character input , Kana-Kanji conversion , and search result visualization .", "The system also shows suggestions to complete the user input .", "To achieve faster input , users only have to input consonant , which is then converted directly to Kanji sequences by direct consonant decoding ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["It is written in Perl and can be used via a command line interface , an API , or a Web interface .", "UMLS : :Similarity is freely available open source software that allows a user to measure the semantic similarity or relatedness of biomedical terms found in the Unified Medical Language System ( UMLS ) ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We present KELVIN , an automated system for processing a large text corpus and distilling a knowledge base about persons , organizations , and locations .", "Our NAACL HLT 2013 demonstration permits a user to interact with a set of searchable HTML pages , which are automatically generated from the knowledge base .", "Each page contains information analogous to the semi-structured details about an entity that are present in Wikipedia Infoboxes , along with hyperlink citations to supporting text .", "We have tested the KELVIN system on several corpora , including : ( a ) the TAC KBP 2012 Cold Start corpus which consists of public Web pages from the University of Pennsylvania , and ( b ) a subset of 26k news articles taken from English Gigaword 5th edition ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We introduce an efficient , interactive framework ? Argviz ? for experts to analyze the dynamic topical structure of multi-party conversations .", "The refined topics feed into a segmentation model , whose outputs are shown to users via multiple coordinated views .", "Users inject their needs , expertise , and insights into models via iterative topic refinement ."]}
{"orig_sents": ["3", "6", "1", "2", "4", "0", "5"], "shuf_sents": ["MERT , PRO ) and outperforms ad hoc alternatives based on linear-combination of metrics .", "BLEU , TER ) focus on different aspects of translation quality ; our multi-objective approach leverages these diverse aspects to improve overall quality .", "Our approach is based on the theory of Pareto Optimality .", "We introduce an approach to optimize a machine translation ( MT ) system on multiple metrics simultaneously .", "It is simple to implement on top of existing single-objective optimization methods ( e.g .", "We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization .", "Different metrics ( e.g ."]}
{"orig_sents": ["2", "3", "0", "1", "4"], "shuf_sents": ["The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT .", "We deploy local features for SCFG-based SMT that can be read off from rules at runtime , and present a learning algorithm that applies `1/`2 regularization for joint feature selection over distributed stochastic learning processes .", "With a few exceptions , discriminative training in statistical machine translation ( SMT ) has been content with tuning weights for large feature sets on small development data .", "Evidence from machine learning indicates that increasing the training sample size results in better prediction .", "We present experiments on learning on 1.5 million training sentences , and show significant improvements over tuning discriminative models on small development sets ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["Parallel data in the domain of interest is the key resource when training a statistical machine translation ( SMT ) system for a specific purpose .", "Since ad-hoc manual translation can represent a significant investment in time and money , a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful .", "We propose methods for predicting learning curves in both these scenarios .", "In this work , we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data .", "We consider two scenarios , 1 ) Monolingual samples in the source and target languages are available and 2 ) An additional small amount of parallel corpus is also available ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition ( HVR ) , a multimodal input method designed to provide efficient text entry on modern mobile devices .", "HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition .", "Experimental results show that both the word error rate and runtime factor can be reduced by a factor of two using HVR .", "This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information .", "In addition to the acoustic and language models used in automatic speech recognition systems , HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions ."]}
{"orig_sents": ["0", "1", "4", "5", "3", "2"], "shuf_sents": ["We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable .", "We present an unsupervised model that simultaneously segments the speech , discovers a proper set of sub-word units ( e.g. , phones ) and learns a Hidden Markov Model ( HMM ) for each induced acoustic unit .", "Compared to the baselines , our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model .", "We test the quality of the learned acoustic models on a spoken term detection task .", "Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit .", "We apply our model to the TIMIT corpus , and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline ."]}
{"orig_sents": ["2", "1", "0", "3", "4"], "shuf_sents": ["Compared with the Latent Semantic Analysis with Support Vector Regression ( LSA-SVR ) method ( stands for the conventional measures ) , our FST method shows better performance especially towards the ASR transcription .", "Therefore , we introduce a framework of Finite State Transducer ( FST ) to avoid the shortcomings .", "Conventional Automated Essay Scoring ( AES ) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition ( ASR ) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription .", "In addition , we apply the synonyms similarity to expand the FST model .", "The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation ( 0.87 ) between human raters ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We significantly improve its tree-building step by incorporating our own rich linguistic features .", "We also analyze the difficulty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourseparsing performance under different discourse conditions .", "In this paper , we develop an RST-style textlevel discourse parser , based on the HILDA discourse parser ( Hernault et al , 2010b ) ."]}
{"orig_sents": ["3", "4", "0", "2", "1"], "shuf_sents": ["Annotation results show that these adaptations work well in practice .", "for English , Turkish , Hindi , and Czech ) , affords a broader perspective on how the generalized lexically grounded approach can flesh itself out in the context of cross-linguistic annotation of discourse relations .", "Our scheme , taken together with other PDTB-style schemes ( e.g .", "We describe a discourse annotation scheme for Chinese and report on the preliminary results .", "Our scheme , inspired by the Penn Discourse TreeBank ( PDTB ) , adopts the lexically grounded approach ; at the same time , it makes adaptations based on the linguistic and statistical characteristics of Chinese text ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Our analysis of the dependency parser errors gives some insights into future research directions .", "We annotate a corpus of children ? s stories with temporal dependency trees , achieving agreement ( Krippendorff ? s Alpha ) of 0.856 on the event words , 0.822 on the links between events , and of 0.700 on the ordering relation labels .", "We propose a new approach to characterizing the timeline of a text : temporal dependency structures , where all the events of a narrative are linked via partial ordering relations like BEFORE , AFTER , OVERLAP and IDENTITY .", "We compare two parsing models for temporal dependency structures , and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser , achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596 ."]}
{"orig_sents": ["1", "6", "3", "5", "4", "2", "0", "8", "7"], "shuf_sents": ["The second model learns probabilistic constraints between time expressions and the unknown document time .", "Temporal reasoners for document understanding typically assume that a document ? s creation date is known .", "This model alone improves on previous generative models by 77 % .", "Unfortunately , the timestamp is not always known , particularly on the Web .", "The first is a discriminative classifier with new features extracted from the text ? s time expressions ( e.g. , ? since 1999 ? ) .", "This paper addresses the task of automatic document timestamping , presenting two new models that incorporate rich linguistic features about time .", "Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner .", "Finally , we present a new experiment design that facilitates easier comparison by future work .", "Imposing these learned constraints on the discriminative model further improves its accuracy ."]}
{"orig_sents": ["5", "1", "2", "4", "3", "0"], "shuf_sents": ["Compared to the state of the art , the overall system achieves the highest precision reported .", "This paper proposes a methodological approach to temporally anchored relation extraction .", "Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus , and anchors each of them to an interval of temporal validity , aggregating evidence from documents supporting the relation .", "Results show that our implementation for temporal anchoring is able to achieve a 69 % of the upper bound performance imposed by the relation extraction step .", "We use a rich graphbased document-level representation to generate novel features for this task .", "Although much work on relation extraction has aimed at obtaining static facts , many of the target relations are actually fluents , as their validity is naturally anchored to a certain time period ."]}
{"orig_sents": ["0", "2", "5", "3", "4", "1"], "shuf_sents": ["Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years .", "We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically , while its output quality is close to that given by the optimal solution of the exact algorithm .", "In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates ( termed entailment graphs ) .", "property and are very similar to a novel type of graph termed forest-reducible graph .", "We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges , where each iteration takes linear time .", "We first identify that entailment graphs exhibit a ? tree-like ?"]}
{"orig_sents": ["6", "7", "8", "4", "1", "5", "0", "2", "3"], "shuf_sents": ["When applied to a complex virtual world and text describing that world , our relation extraction technique performs on par with a supervised baseline , yielding an F-measure of 66 % compared to the baseline ? s 65 % .", "Our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations .", "Additionally , we show that a high-level planner utilizing these extracted relations significantly outperforms a strong , text unaware baseline ?", "successfully completing 80 % of planning tasks as compared to 69 % for the baseline.1", "This type of grounding enables us to create high-level plans based on language abstractions .", "We implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempts .", "Comprehending action preconditions and effects is an essential step in modeling the dynamics of the world .", "In this paper , we express the semantics of precondition relations extracted from text in terms of planning operations .", "The challenge of modeling this connection is to ground language at the level of relations ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Using computer vision techniques , we build visual and multimodal distributional models and compare them to standard textual models .", "Our research aims at building computational models of word meaning that are perceptually grounded .", "Moreover , we show that visual and textual information are tapping on different aspects of meaning , and indeed combining them in multimodal models often improves performance .", "Our results show that , while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks ( accounting for semantic relatedness ) , they are as good or better models of the meaning of words with visual correlates such as color terms , even in a nontrivial task that involves nonliteral uses of such words ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number , the output commonly contains morpho-syntactic agreement errors .", "For English-to-Arabic translation , our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline .", "The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders .", "Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis .", "To address this issue , we present a target-side , class-based agreement model ."]}
{"orig_sents": ["2", "3", "1", "0", "4"], "shuf_sents": ["The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words , such as a non-parallel version of the VERBMOBIL corpus .", "On the task shown in ( Ravi and Knight , 2011 ) we obtain better results with only 5 % of the computational effort when running our method with an n-gram language model .", "In this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages .", "We present a modification of the method shown in ( Ravi and Knight , 2011 ) that is scalable to vocabulary sizes of several thousand words .", "We also report results using data from the monolingual French and English GIGAWORD corpora ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["treating MT as a problem of transformation between character strings .", "In this paper , we demonstrate that accurate machine translation is possible without the concept of ? words , ?", "We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment .", "We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model , and using this in the phrase-based MT framework .", "In an evaluation , we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs ."]}
{"orig_sents": ["1", "3", "2", "0", "4"], "shuf_sents": ["We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups , and further improve the accuracy of these tools through up-training .", "Long-span features , such as syntax , can improve language models for tasks such as speech recognition and machine translation .", "In this work , we propose substructure sharing , which saves duplicate work in processing hypothesis sets with redundant hypothesis structures .", "However , these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set .", "When using these improved tools in a language model for speech recognition , we obtain significant speed improvements with bothN -best and hill climbing rescoring , and show that up-training leads to WER reduction ."]}
{"orig_sents": ["4", "0", "2", "6", "5", "1", "3"], "shuf_sents": ["might be realized as or .", "We test the model using a corpus of child-directed speech with realistic phonetic variation and either gold standard or automatically induced word boundaries .", "Previous models of acquisition have generally tackled these problems in isolation , yet behavioral evidence suggests infants acquire lexical and phonetic knowledge simultaneously .", "In both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item has a unique pronunciation .", "During early language acquisition , infants must learn both a lexicon and a model of phonetics that explains how lexical items can vary in pronunciation ? for instance ? the ?", "The model is trained on transcribed surface pronunciations , and learns by bootstrapping , without access to the true lexicon .", "We present a Bayesian model that clusters together phonetic variants of the same lexical item while learning both a language model over lexical items and a log-linear model of pronunciation variability based on articulatory features ."]}
{"orig_sents": ["2", "6", "3", "5", "1", "4", "0"], "shuf_sents": ["We find that large-margin approaches outperform conditional random field learning , and that the Passive-Aggressive algorithm for largemargin learning is faster to converge than the Pegasos algorithm .", "We test the approach on the task of lexical access ; that is , the prediction of a word given a phonetic transcription .", "We address the problem of learning the mapping between words and their possible pronunciations in terms of sub-word units .", "We propose a discriminative , feature-rich approach using large-margin learning .", "In experiments on a subset of the Switchboard conversational speech corpus , our models thus far improve classification error rates from a previously published result of 29.1 % to about 15 % .", "This approach allows us to optimize an objective closely related to a discriminative task , to incorporate a large number of complex features , and still do inference efficiently .", "Most previous approaches have involved generative modeling of the distribution of pronunciations , usually trained to maximize likelihood ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["However , it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing .", "The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified .", "This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested .", "Firstly , we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score .", "Secondly , integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics ."]}
{"orig_sents": ["5", "9", "8", "4", "0", "6", "2", "7", "3", "1"], "shuf_sents": ["Finally , the features are efficiently integrated into the parsing model during decoding using beam search .", "The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data .", "Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus .", "We evaluate the proposed approach on English and Chinese data .", "Based on the dependency language model , we represent a set of features for the parsing model .", "Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding .", "Our approach has two advantages .", "Secondly our approach does not increase the decoding complexity .", "The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser .", "In this paper , we present an approach to enriching high-order feature representations for graph-based dependency parsing models using a dependency language model and beam search ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We introduce a spectral learning algorithm for latent-variable PCFGs ( Petrov et al , 2006 ) .", "Under a separability ( singular value ) condition , we prove that the method provides consistent parameter estimates ."]}
{"orig_sents": ["0", "2", "1", "3", "4"], "shuf_sents": ["We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging .", "Penn Chinese Treebank ( CTB ) and PKU ? s People ? s Daily ( PPD ) , on manually mapped data , and show that their linguistic annotations are systematically different and highly compatible .", "We empirically analyze the diversity between two representative corpora , i.e .", "The analysis is further exploited to improve processing accuracy by ( 1 ) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error , and ( 2 ) re-training models with high quality automatically converted data to reduce the estimation error .", "Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11 % over the best reported result in the literature ."]}
{"orig_sents": ["1", "0", "4", "2", "3"], "shuf_sents": ["Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger .", "From the perspective of structural linguistics , we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging , an important and challenging task for Chinese language processing .", "Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations .", "Our linguistically motivated approaches yield a relative error reduction of 18 % in total over a stateof-the-art baseline .", "Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination ."]}
{"orig_sents": ["4", "6", "0", "3", "5", "1", "2"], "shuf_sents": ["As we know , training a word segmentation system on large-scale datasets is already costly .", "Compared with existing training methods , our training method is an order magnitude faster in terms of training time , and can achieve equal or even higher accuracies .", "The proposed fast training method is a general purpose optimization method , and it is not limited in the specific task discussed in this paper .", "In our case , adding high dimensional new features will further slow down the training speed .", "We present a joint model for Chinese word segmentation and new word detection .", "To solve this problem , we propose a new training method , adaptive online gradient descent based on feature frequency information , for very fast online training of the parameters , even given large-scale datasets with high dimensional features .", "We present high dimensional new features , including word-based features and enriched edge ( label-transition ) features , for the joint modeling ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["Then , we design advanced similarity functions between such structures , i.e. , semantic tree kernel functions , for exploiting distributional and grammatical information in Support Vector Machines .", "In this paper , we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories , namely VerbNet and FrameNet .", "The extensive empirical analysis on VerbNet class and frame detection shows that our models capture meaningful syntactic/semantic structures , which allows for improving the state-of-the-art .", "First , syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["In this paper , we propose a method to estimate sense distributions for short queries .", "Together with the senses predicted for words in documents , we propose a novel approach to incorporate word senses into the language modeling approach to IR and also exploit the integration of synonym relations .", "Our experimental results on standard TREC collections show that using the word senses tagged by a supervised WSD system , we obtain significant improvements over a state-of-the-art IR system .", "Previous research has conflicting conclusions on whether word sense disambiguation ( WSD ) systems can improve information retrieval ( IR ) performance ."]}
{"orig_sents": ["5", "4", "2", "3", "0", "1"], "shuf_sents": ["We explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference : a gradient-style evaluation function , and a locallookahead node expansion method .", "Evaluations , using the open-source system , BIUTEE , show the contribution of these ideas to search efficiency and proof quality .", "a proof , while estimating the proof ? s validity .", "This raises a search challenge of finding the best possible proof .", "A prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations , a.k.a .", "This paper addresses the search problem in textual inference , where systems need to infer one piece of text from another ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Experimental results show that , our method can significantly improve machine translation performance on both IWSLT and NIST data , compared with a state-ofthe-art baseline .", "In this paper , we address the issue for learning better translation consensus in machine translation ( MT ) research , and explore the search of translation consensus from similar , rather than the same , source sentences or their spans .", "Unlike previous work on this topic , we formulate the problem as structured labeling over a much smaller graph , and we propose a novel structured label propagation for the task .", "We convert such graph-based translation consensus from similar source strings into useful features both for n-best output reranking and for decoding algorithm ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We explain how to implement this extension efficiently for large-scale data ( also released as a modification to GIZA++ ) and demonstrate , in experiments on Czech , Arabic , Chinese , and Urdu to English translation , significant improvements over IBM Model 4 in both word alignment ( up to +6.7 F1 ) and translation quality ( up to +1.4 Bleu ) .", "Although many models have surpassed them in accuracy , none have supplanted them in practice .", "Two decades after their invention , the IBM word-based translation models , widely available in the GIZA++ toolkit , remain the dominant approach to word alignment and an integral part of many statistical translation systems .", "In this paper , we propose a simple extension to the IBM models : an `0 prior to encourage sparsity in the word-to-word translation model ."]}
{"orig_sents": ["3", "4", "0", "1", "2"], "shuf_sents": ["movies ) , intention ( e.g. , finding a movie ) along with other semantic units ( e.g. , movie name ) .", "We inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model .", "Using utterances from five domains , our approach shows up to 4.5 % improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model ( which requires fully labeled data ) .", "We describe a joint model for understanding user actions in natural language utterances .", "Our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance ? s target domain ( e.g ."]}
{"orig_sents": ["5", "8", "3", "6", "1", "2", "4", "0", "7"], "shuf_sents": ["In this paper , we propose two statistical models to solve this seeded problem , which aim to discover exactly what the user wants .", "This setting is important because categorizing aspects is a subjective task .", "For different application purposes , different categorizations may be needed .", "By categorizing , we mean the synonymous aspects should be clustered into the same category .", "Some form of user guidance is desired .", "Aspect extraction is a central problem in sentiment analysis .", "In this paper , we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously .", "Our experimental results show that the two proposed models are indeed able to perform the task effectively .", "Current methods either extract aspects without categorizing them , or extract and categorize them using unsupervised topic modeling ."]}
{"orig_sents": ["4", "5", "1", "6", "2", "3", "0"], "shuf_sents": ["Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach .", "Human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts .", "It involves learning uncertain commonsense knowledge ( in the form of probabilistic first-order rules ) from natural language text by mining a large corpus of automatically extracted facts .", "These rules are then used to derive additional facts from extracted information using BLP inference .", "Most information extraction ( IE ) systems identify facts that are explicitly stated in text .", "However , in natural language , some facts are implicit , and identifying them requires ? reading between the lines ? .", "We propose an approach that uses Bayesian Logic Programs ( BLPs ) , a statistical relational model combining firstorder logic and Bayesian networks , to infer additional implicit information from extracted facts ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["More specifically , given a query image , we retrieve existing human-composed phrases used to describe visually similar images , then selectively combine those phrases to generate a novel description for the query image .", "Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines .", "We present a holistic data-driven approach to image description generation , exploiting the vast amount of ( noisy ) parallel image data and associated natural language descriptions available on the web .", "We cast the generation process as constraint optimization problems , collectively incorporating multiple interconnected aspects of language composition for content planning , surface realization and discourse structure ."]}
{"orig_sents": ["2", "6", "3", "1", "4", "7", "0", "5"], "shuf_sents": ["We propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting .", "into a common parsing problem .", "This paper proposes a data-driven method for concept-to-text generation , the task of automatically producing textual output from non-linguistic input .", "and surface realization ( ? how to say ? )", "We define a probabilistic context-free grammar that describes the structure of the input ( a corpus of database records and text describing some of them ) and represent it compactly as a weighted hypergraph .", "Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study .", "A key insight in our approach is to reduce the tasks of content selection ( ? what to say ? )", "The hypergraph structure encodes exponentially many derivations , which we rerank discriminatively using local and global features ."]}
{"orig_sents": ["0", "3", "5", "4", "2", "1"], "shuf_sents": ["Methods that measure compatibility between mention pairs are currently the dominant approach to coreference .", "We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise , allowing us to perform coreference on six million author mentions in under four hours on a single CPU .", "These trees succinctly summarize the mentions providing a highly compact , information-rich structure for reasoning about entities and coreference uncertainty at massive scales .", "However , they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power .", "In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities .", "As these drawbacks become increasingly restrictive , the need to replace the pairwise approaches with a more expressive , highly scalable alternative is becoming urgent ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["When added to a state-of-the-art coreference baseline , our Web features give significant gains on multiple datasets ( ACE 2004 and ACE 2005 ) and metrics ( MUC and B3 ) , resulting in the best results reported to date for the end-to-end task of coreference resolution .", "Specifically , we exploit short-distance cues to hypernymy , semantic compatibility , and semantic context , as well as general lexical co-occurrence .", "To address semantic ambiguities in coreference resolution , we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way ."]}
{"orig_sents": ["8", "7", "5", "0", "1", "4", "2", "6", "3"], "shuf_sents": ["The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups .", "In this paper , we propose an unsupervised approach for automatically detecting discussant subgroups in online communities .", "We use attitude predictions to construct an attitude vector for each discussant .", "We compare our methods to text clustering and other baselines , and show that our method achieves promising results .", "We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic .", "It is not uncommon that the participants in such discussions split into two or more subgroups .", "We use clustering techniques to cluster these vectors and , hence , determine the subgroup membership of each participant .", "Many of these groups discuss ideological and political topics .", "The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups ."]}
{"orig_sents": ["5", "3", "1", "4", "0", "2", "6", "7"], "shuf_sents": ["The framework is twofold .", "However , the performance of supervised methods highly relies on manually labeled training data .", "In the first step , we generate a few high-confidence sentiment and topic seeds in the target domain .", "Previous works have showed that supervised learning methods are superior for this task .", "In this paper , we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data , but have lots of labeled data in another related domain .", "Extracting sentiment and topic lexicons is important for opinion mining .", "In the second step , we propose a novel Relational Adaptive bootstraPping ( RAP ) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words .", "Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation ."]}
{"orig_sents": ["2", "3", "4", "5", "1", "0"], "shuf_sents": ["We conclude with future work to augment the approach .", "We discuss the advantages of graphical models for this task , in particular the ease of integrating semantic information about verbs and arguments in a principled fashion .", "We present a novel approach for building verb subcategorization lexicons using a simple graphical model .", "In contrast to previous methods , we show how the model can be trained without parsed input or a predefined subcategorization frame inventory .", "Our method outperforms the state-of-the-art on a verb clustering task , and is easily trained on arbitrary domains .", "This quantitative evaluation is complemented by a qualitative discussion of verbs and their frames ."]}
{"orig_sents": ["2", "3", "0", "4", "6", "5", "1"], "shuf_sents": ["Recent work by Chen and Mooney ( 2011 ) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs .", "We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach .", "Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language .", "It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context .", "While the algorithm produced good lexicons for the task of learning to interpret navigation instructions , it only works in batch settings and does not scale well to large datasets .", "We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon ? s Mechanical Turk we can further improve the results .", "In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results ."]}
{"orig_sents": ["4", "3", "0", "1", "2"], "shuf_sents": ["We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion .", "We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules , and develop an efficient training method based on Markov Chain Monte Carlo ( MCMC ) sampling .", "Our SR-TSG parser achieves an F1 score of 92.4 % in the Wall Street Journal ( WSJ ) English Penn Treebank parsing task , which is a 7.7 point improvement over a conventional Bayesian TSG parser , and better than state-of-the-art discriminative reranking parsers .", "An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined ( subcategorized ) to fit the training data .", "We propose Symbol-Refined Tree Substitution Grammars ( SR-TSGs ) for syntactic parsing ."]}
{"orig_sents": ["2", "3", "4", "1", "5", "0"], "shuf_sents": ["Experimental results on benchmark datasets show that our method can achieve better results than state-of-the-art methods on two sentence re-writing learning tasks : paraphrase identification and recognizing textual entailment .", "It can capture the lexical and structural similarity between two pairs of sentences without the need of constructing syntactic trees .", "Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval .", "In this paper , we propose a new class of kernel functions , referred to as string re-writing kernel , to address the problem .", "A string re-writing kernel measures the similarity between two pairs of strings , each pair representing re-writing of a string .", "We further propose an instance of string rewriting kernel which can be computed efficiently ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora .", "Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system .", "In this paper , we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora , which incorporates the topic information into translation probability estimation .", "To adapt a translation model trained from the data in one domain to another , previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["Our model is efficient , language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings .", "We propose a novel model to automatically extract transliteration pairs from parallel corpora .", "We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models .", "We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results ."]}
{"orig_sents": ["2", "1", "5", "0", "3", "4"], "shuf_sents": ["Finally we encode these reorderings by modifying selected entries of the distortion cost matrix , on a per-sentence basis .", "We address language pairs where long reordering concentrates on few patterns , and use fuzzy chunk-based rules to predict likely reorderings for these phenomena .", "This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder .", "In this way , we expand the search space by a much finer degree than if we simply raised the distortion limit .", "The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks .", "Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["In particular , this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers , producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers .", "Many semantic parsing models use tree transformations to map between natural language and meaning representation .", "However , while tree transformations are central to several state-of-the-art approaches , little use has been made of the rich literature on tree automata .", "This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework , increasing the generality of their contributions ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Optimising for one grammatical representation , but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing .", "We also present a comprehensive analysis of errors made by the C & C CCG parser , providing the first breakdown of the impact of implementation decisions , such as supertagging , on parsing accuracy .", "We find that this mismatch causes many n-best CCG parses to be semantically equivalent , and describe a hashing technique that eliminates this problem , improving oracle n-best F-score by 0.7 % and reranking accuracy by 0.4 % ."]}
{"orig_sents": ["4", "2", "5", "0", "3", "1"], "shuf_sents": ["A more powerful model , the simple context-free tree grammar , admits such a normal form .", "Thus , simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves .", "Comput .", "It can be effectively constructed and the maximal rank of the nonterminals only increases by 1 .", "Recently , it was shown ( KUHLMANN , SATTA : Tree-adjoining grammars are not closed under strong lexicalization .", "Linguist. , 2012 ) that finitely ambiguous tree adjoining grammars can not be transformed into a normal form ( preserving the generated tree language ) , in which each production contains a lexical symbol ."]}
{"orig_sents": ["4", "2", "0", "6", "5", "3", "1"], "shuf_sents": ["In this paper we propose a graph-theoretic model for tweet recommendation that presents users with items they may have an interest in .", "Experimental evaluation on a large dataset shows that our model outperforms competitive approaches by a large margin .", "Shared information through this service spreads faster than would have been possible with traditional sources , however the proliferation of user-generation content poses challenges to browsing and finding valuable information .", "We show that this framework can be parametrized to take into account user preferences , the popularity of tweets and their authors , and diversity .", "As one of the most popular micro-blogging services , Twitter attracts millions of users , producing millions of tweets daily .", "Tweet and author entities are ranked following a co-ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweets and their authors that could be reflected in the rankings .", "Our model ranks tweets and their authors simultaneously using several networks : the social network connecting the users , the network connecting the tweets , and a third network that ties the two together ."]}
{"orig_sents": ["5", "3", "0", "4", "1", "2"], "shuf_sents": ["Two main challenges are the errors propagated from named entity recognition ( NER ) and the dearth of information in a single tweet .", "Particularly , our model introduces a binary random variable for each pair of words with the same lemma across similar tweets , whose value indicates whether the two related words are mentions of the same entity .", "We evaluate our method on a manually annotated data set , and show that our method outperforms the baseline that handles these two tasks separately , boosting the F1 from 80.2 % to 83.6 % for NER , and the Accuracy from 79.4 % to 82.6 % for NEN , respectively .", "We study the problem of named entity normalization ( NEN ) for tweets .", "We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges .", "Tweets represent a critical source of fresh information , in which named entities occur frequently with rich variations ."]}
{"orig_sents": ["8", "2", "4", "6", "1", "3", "0", "7", "5"], "shuf_sents": ["Our experiments on a large Twitter dataset show that there are more meaningful and unique bursty topics in the top-ranked results returned by our model than an LDA baseline and two degenerate variations of our model .", "The former helps find eventdriven posts while the latter helps identify and filter out ? personal ?", "Bursty topics from microblogs reveal what events have attracted the most online attention .", "posts .", "Although bursty event detection from text streams has been studied before , previous work may not be suitable for microblogs because compared with other text streams such as news articles and scientific publications , microblog posts are particularly diverse and noisy .", "personal interests for bursty topic detection from microblogs .", "To find topics that have bursty patterns on microblogs , we propose a topic model that simultaneously captures two observations : ( 1 ) posts published around the same time are more likely to have the same topic , and ( 2 ) posts published by the same user are more likely to have the same topic .", "We also show some case studies that demonstrate the importance of considering both the temporal information and users ?", "Microblogs such as Twitter reflect the general public ? s reactions to major events ."]}
{"orig_sents": ["4", "2", "5", "6", "0", "1", "3"], "shuf_sents": ["In this paper , we propose a generative model that jointly identifies user-proposed refinements in instruction reviews at multiple granularities , and aligns them to the appropriate steps in the original instructions .", "Labeled data is not readily available for these tasks , so we focus on the unsupervised setting .", "In addition to providing their subjective evaluation , reviewers often provide actionable refinements .", "In experiments in the recipe domain , our model provides 90.1 % F1 for predicting refinements at the review level , and 77.0 % F1 for predicting refinement segments within reviews .", "There are a growing number of popular web sites where users submit and review instructions for completing tasks as varied as building a table and baking a pie .", "These refinements clarify , correct , improve , or provide alternatives to the original instructions .", "However , identifying and reading all relevant reviews is a daunting task for a user ."]}
{"orig_sents": ["0", "7", "5", "4", "1", "8", "3", "6", "2"], "shuf_sents": ["Online forums are becoming a popular resource in the state of the art question answering ( QA ) systems .", "In this paper , we introduce the task of sentence dependency tagging .", "This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums .", "We use linear-chain conditional random fields ( CRF ) for sentence type tagging , and a 2D CRF to label the dependency relation between sentences .", "Most prior work focused on extracting only question answering sentences from user conversations .", "However , going through tedious and redundant posts to look for answers could be very time consuming .", "Our experimental results show that our proposed approach performs well for sentence dependency tagging .", "Because of its nature as an online community , it contains more updated knowledge than other places .", "Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations ."]}
{"orig_sents": ["4", "1", "3", "0", "2"], "shuf_sents": ["Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries .", "We jointly model the interplay between latent user intents that govern queries and unobserved entity types , leveraging observed signals from query formulations and document clicks .", "We show that modeling user intent significantly improves entity type resolution for head queries over the state of the art , on several metrics , without degradation in tail query performance .", "We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base .", "We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated ."]}
{"orig_sents": ["0", "1", "3", "5", "4", "2", "8", "6", "7"], "shuf_sents": ["The amount of labeled sentiment data in English is much larger than that in other languages .", "Such a disproportion arouse interest in cross-lingual sentiment classification , which aims to conduct sentiment classification in the target language ( e.g .", "This approach suffers from the limited coverage of vocabulary in the machine translation results .", "Chinese ) using labeled data in the source language ( e.g .", "Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language .", "English ) .", "By fitting parameters to maximize the likelihood of the bilingual parallel data , the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly .", "Experiments on multiple data sets show that CLMM is consistently effective in two settings : ( 1 ) labeled data in the target language are unavailable ; and ( 2 ) labeled data in the target language are also available .", "In this paper , we propose a generative cross-lingual mixture model ( CLMM ) to leverage unlabeled bilingual parallel data ."]}
{"orig_sents": ["5", "1", "4", "2", "7", "6", "3", "0"], "shuf_sents": ["Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task .", "of a complex multi-sentence question misses valuable information that is contained in other answers .", "Various textual and non-textual QA features are explored .", "Experimental results on a Yahoo !", "In order to automatically generate a novel and non-redundant community answer summary , we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field ( CRF ) based answer summary method with group L1 regularization .", "We present a novel answer summarization method for community Question Answering services ( cQAs ) to address the problem of ? incomplete answer ? , i.e. , the ? best answer ?", "To further unleash the potential of the abundant cQA features , we introduce the group L1 regularization for feature learning .", "Specifically , we explore four different types of contextual factors , namely , the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data .", "In recent years , error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons .", "We show that this tree mining algorithm permits identifying not only errors in the generation system ( grammar , lexicon ) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data .", "In this paper , we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure ."]}
{"orig_sents": ["2", "1", "3", "4", "0"], "shuf_sents": ["We find that by fusing local and global information , we can exceed 50 % on this task ( chance baseline is 20 % ) , and we suggest some avenues for further research .", "These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena .", "This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions .", "We tackle the problem with two approaches : methods that use local lexical information , such as the n-grams of a classical language model ; and methods that evaluate global coherence , such as latent semantic analysis .", "We evaluate these methods on a suite of practice SAT questions , and on a recently released sentence completion task based on data taken from five Conan Doyle novels ."]}
{"orig_sents": ["1", "4", "3", "2", "5", "0"], "shuf_sents": ["This algorithm makes real-time large-scale tagging applications with thousands of labels feasible .", "Sequential modeling has been widely used in a variety of important applications including named entity recognition and shallow parsing .", "We show the efficiency of these proposed algorithms for five NLP tagging tasks .", "In this paper we propose 1-best A* , 1-best iterative A* , k-best A* and k-best iterative Viterbi A* algorithms for sequential decoding .", "However , as more and more real time large-scale tagging applications arise , decoding speed has become a bottleneck for existing sequential tagging algorithms .", "In particular , we show that iterative Viterbi A* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets .", "It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function .", "This paper introduces a novel variant of the Yarowsky algorithm based on this view .", "Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them ."]}
{"orig_sents": ["6", "5", "2", "7", "1", "0", "4", "8", "3"], "shuf_sents": ["In contrast , the ordering decisions are only influenced by languages with similar properties .", "Being largely languageuniversal , the selection component is learned in a supervised fashion from all the training languages .", "The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly .", "The largest improvement is achieved on the non Indo-European languages yielding a gain of 14.4 % .1", "We systematically model this cross-lingual sharing using typological features .", "Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource-rich languages .", "We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language .", "The model factorizes the process of generating a dependency tree into two steps : selection of syntactic dependents and their ordering .", "In our experiments , the model consistently outperforms a state-of-the-art multilingual parser ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["This paper describes the creation of the first tagged and delineated corpus of English metalanguage , accompanied by an explicit definition and a rubric for identifying the phenomenon in text .", "Metalanguage is an essential linguistic mechanism which allows us to communicate explicit information about language itself .", "This resource will provide a basis for further studies of metalanguage and enable its utilization in language technologies .", "However , it has been underexamined in research in language technologies , to the detriment of the performance of systems that could exploit it ."]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations .", "Specifically , we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model .", "When applied to German-English parallel data , our method obtains a substantial improvement over a model trained without using the agreement signal , when both are tested on non-parallel sentences .", "We propose a simple approximate learning algorithm for our set-up which results in efficient inference .", "At inference time , instead of only seeking the model which explains the monolingual data available for each language , we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This algorithm handles global structures , such as clause and coordination , better than shift-reduce or other bottom-up algorithms .", "This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis .", "Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["The language MIX consists of all strings over the three-letter alphabet { a , b , c } that contain an equal number of occurrences of each letter .", "We prove Joshi ? s ( 1985 ) conjecture that MIX is not a tree-adjoining language ."]}
{"orig_sents": ["3", "0", "5", "4", "2", "1"], "shuf_sents": ["Several types of transformation patterns ( TP ) are designed to capture the systematic annotation inconsistencies among different treebanks .", "Moreover , an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion .", "The improvements are respectively 1.37 % and 1.10 % with automatic part-of-speech tags .", "We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing .", "Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target treebanks ( Penn Chinese Treebank 5.1 and 6.0 ) using the Chinese Dependency Treebank as the source treebank .", "Based on such TPs , we design quasisynchronous grammar features to augment the baseline parsing models ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We evaluate our model and its components on two datasets collected from political blogs and sports news , finding that it outperforms a simple agglomerative clustering approach and previous work .", "The model is novel in that it incorporates entity context , surface features , firstorder dependencies among attribute-parts , and a notion of noise .", "We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes ( or parts of attributes ) .", "Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata .", "The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence .", "In this paper we propose a method to automatically label multi-lingual data with named entity tags .", "We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["In this paper , we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined .", "We describe all the linguistic resources and natural language processing techniques that we have exploited for this task .", "Then , we analyze the performance of the system that we have developed .", "The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation of the naming process ."]}
{"orig_sents": ["1", "0", "2", "4", "6", "5", "3"], "shuf_sents": ["In practice this assumption is often violated .", "To discover relation types from text , most methods cluster shallow or syntactic patterns of relation mentions , but consider only one possible sense per pattern .", "In this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns .", "Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation , and that incorporating global features , such as the document theme , is crucial .", "In particular , we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features .", "We compare against several baselines : a generative latent-variable model , a clustering method that does not disambiguate between path senses , and our own approach but with only local features .", "We merge these sense clusters into semantic relations using hierarchical agglomerative clustering ."]}
{"orig_sents": ["0", "5", "4", "8", "7", "6", "2", "1", "3"], "shuf_sents": ["In relation extraction , distant supervision seeks to extract relations between entities from text by using a knowledge base , such as Freebase , as a source of supervision .", "Our experimental results show that this model detected wrong labels with higher performance than baseline methods .", "The model predicts whether assigned labels are correct or wrong via its hidden variables .", "In the experiment , we also found that our wrong label reduction boosted the performance of relation extraction .", "However , this heuristic can fail with the result that some sentences are labeled wrongly .", "When a sentence and a knowledge base refer to the same entity pair , this approach heuristically labels the sentence with the corresponding relation in the knowledge base .", "We present a novel generative model that directly models the heuristic labeling process of distant supervision .", "In this paper , we propose a method to reduce the number of wrong labels .", "This noisy labeled data causes poor extraction performance ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["In order to extract salient dates that warrant inclusion in an event timeline , we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic .", "the name of an event or person , etc . ) .", "We present an approach for detecting salient ( important ) dates in texts in order to automatically build event timelines from a search query ( e.g .", "This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse ( AFP ) .", "We focused only on extracting the dates and not the events to which they are related ."]}
{"orig_sents": ["5", "6", "0", "4", "1", "2", "3"], "shuf_sents": ["To test this , we collect a corpus of slavery-related United States property law judgements sampled from the years 1730 to 1866 .", "Because this is a longitudinal data set , we are also interested in understanding how these opinions change over the course of decades .", "We show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identification tasks .", "Experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting , and that these improvements are robust across different parameter settings .", "We study the language use in these legal cases , with a special focus on shifts in opinions on controversial topics across different regions .", "We propose a latent variable model to enhance historical analysis of large corpora .", "This work extends prior work in topic modelling by incorporating metadata , and the interactions between the components in metadata , in a general way ."]}
{"orig_sents": ["5", "2", "1", "0", "3", "4"], "shuf_sents": ["We associate each synchronous rule with a topic distribution , and select desirable rules according to the similarity of their topic distributions with given documents .", "We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation .", "However , SMT has been advanced from word-based paradigm to phrase/rule-based paradigm .", "We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments .", "Our model also achieves a better performance and a faster speed than previous approaches that work at the word level .", "Previous work using topic model for statistical machine translation ( SMT ) explore topic information at the word level ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["In this paper , we encode topic dependencies in hierarchical multi-label Text Categorization ( TC ) by means of rerankers .", "We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes .", "Additionally , to better investigate the role of category relationships , we consider two interesting cases : ( i ) traditional schemes in which node-fathers include all the documents of their child-categories ; and ( ii ) more general schemes , in which children can include documents not belonging to their fathers .", "The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art ."]}
{"orig_sents": ["3", "5", "4", "2", "1", "0"], "shuf_sents": ["Furthermore , conjunctions are attached with an accuracy of 90.8 % , and prepositions with an accuracy of 87.4 % .", "By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions , we achieve a new state-of-the-art for English dependencies with 93.55 % correct attachments on the current standard .", "As lexical statistics based on the training set only are sparse , unlabeled data can help ameliorate this sparsity problem .", "Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing .", "Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution .", "Across various existing parsers , these two categories have the lowest accuracies , and mistakes made have consequences for downstream applications ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Treebanks are not large enough to reliably model precise lexical phenomena .", "Experiments on the French Treebank showed a relative decrease of the error rate of 7.1 % Labeled Accuracy Score yielding the best parsing results on this treebank .", "We propose in this paper to compute lexical affinities , on large corpora , for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser .", "This deficiency provokes attachment errors in the parsers trained on such data ."]}
{"orig_sents": ["4", "2", "1", "5", "3", "0"], "shuf_sents": ["The experimental results show that the second approach compares favorably against the first approach .", "We then experimented with two supervised learning methods that automatically disambiguate the Chinese comma based on this classification .", "In this work , we propose a discourse structureoriented classification of the comma that can be automatically extracted from the Chinese Treebank based on syntactic patterns .", "approach that extracts features from automatic parses to train a classifier .", "The Chinese comma signals the boundary of discourse units and also anchors discourse relations between adjacent text spans .", "The first method integrates comma classification into parsing , and the second method adopts a ? post-processing ?"]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["Our approach strongly outperforms reimplementations of previous work .", "We here introduce the task of classifying finegrained information status and work on written text .", "We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions .", "Previous work on classifying information status ( Nissim , 2006 ; Rahman and Ng , 2011 ) is restricted to coarse-grained classification and focuses on conversational dialogue .", "We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus ."]}
{"orig_sents": ["4", "0", "5", "1", "2", "3"], "shuf_sents": ["While some sellers provide rich , structured descriptions of their items , a vast majority of them provide unstructured natural language descriptions .", "The first step consists in unsupervised property discovery and extraction .", "The second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm .", "We evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall .", "Large e-commerce enterprises feature millions of items entered daily by a large variety of sellers .", "In the paper we present a 2 steps method for structuring items into descriptive properties ."]}
{"orig_sents": ["2", "6", "0", "3", "1", "5", "4"], "shuf_sents": ["High-quality senseannotated data , however , are hard to be obtained in streaming environments , since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream .", "Producing binary classifiers directly from this data , however , leads to poor disambiguation performance .", "The named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique realworld entity .", "On the other hand , few positive examples plus large amounts of unlabeled data may be easily acquired .", "We conducted a systematic evaluation using Twitter streaming data and the results show that our classifiers are extremely effective , providing improvements ranging from 1 % to 20 % , when compared to the current state-of-the-art biased SVMs , being more than 120 times faster .", "Thus , we propose to enhance the quality of the classifiers using finer-grained variations of the well-known ExpectationMaximization ( EM ) algorithm .", "This task can be modeled as a classification problem , provided that positive and negative examples are available for learning binary classifiers ."]}
{"orig_sents": ["6", "5", "0", "3", "2", "4", "1"], "shuf_sents": ["There is , however , no study comparing the relative impact of these two sources on the precision and recall of post-learning answers .", "In contrast , human feedback has a positive and statistically significant , but lower , impact on precision and recall .", "We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples .", "To fill this gap , we empirically study how state-of-the-art techniques are affected by scaling these two sources .", "Our experiments show that increasing the corpus size for distant supervision has a statistically significant , positive impact on quality ( F1 score ) .", "To mitigate this cost , NLU researchers have considered two newly available sources of less expensive ( but potentially lower quality ) labeled data from distant supervision and crowd sourcing .", "Classically , training relation extractors relies on high-quality , manually annotated training data , which can be expensive to obtain ."]}
{"orig_sents": ["0", "3", "1", "4", "5", "6", "2"], "shuf_sents": ["This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts .", "This paper addresses the event extraction problem in a primarily unsupervised setting , where no labeled training instances are available .", "Our model , trained without annotated data and with a small number of structured preferences , yields performance competitive to some baseline supervised approaches .", "The model takes in coarse mention and type information and predicts argument roles for a given event template .", "Our key contribution is a novel learning framework called structured preference modeling ( PM ) , that allows arbitrary preference to be assigned to certain structures during the learning procedure .", "We establish and discuss connections between this framework and other existing works .", "We show empirically that the structured preferences are crucial to the success of our task ."]}
{"orig_sents": ["4", "3", "0", "2", "1"], "shuf_sents": ["Such an approach can , for example , learn likely event durations and the fact that start times should come before end times .", "Empirical results in two benchmark domains demonstrate consistently strong performance on both mention detection and template filling tasks .", "While the joint inference space is large , we demonstrate effective learning with a Perceptron-style approach that uses simple , greedy beam decoding .", "The approach models mention detection , unification and field extraction in a flexible , feature-rich model that allows for joint modeling of interdependencies at all levels and across fields .", "This paper presents a joint model for template filling , where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70 .", "We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use ( i ) of a neural clustering method which associates clusters with features , ( ii ) of several supervised and unsupervised evaluation metrics and ( iii ) of various existing syntactic and semantic lexical resources ."]}
{"orig_sents": ["4", "0", "5", "3", "2", "1", "6"], "shuf_sents": ["Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences .", "Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models .", "The new framework allows for large scale tuning and testing of Sentence Similarity models .", "In the process , we propose a new evaluation framework for sentence similarity : Concept Definition Retrieval .", "Sentence Similarity is the process of computing a similarity score between two sentences .", "In this paper , we show that by carefully handling words that are not in the sentences ( missing words ) , we can train a reliable latent variable model on sentences .", "Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity ."]}
{"orig_sents": ["2", "4", "0", "1", "5", "3"], "shuf_sents": ["This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings .", "We present a new neural network architecture which 1 ) learns word embeddings that better capture the semantics of words by incorporating both local and global document context , and 2 ) accounts for homonymy and polysemy by learning multiple embeddings per word .", "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems .", "1", "However , most of these models are built with only local context and one representation per word .", "We introduce a new dataset with human judgments on pairs of words in sentential context , and evaluate our model on it , showing that our model outperforms competitive baselines and other neural language models ."]}
{"orig_sents": ["3", "5", "0", "1", "2", "4"], "shuf_sents": ["Each object is annotated by social cues , indicating e.g. , whether the caregiver is looking at or touching the object .", "We show how to model the task of inferring which objects are being talked about ( and which words refer to which objects ) as standard grammatical inference , and describe PCFG-based unigram models and adaptor grammar-based collocation models for the task .", "Exploiting social cues improves the performance of all models .", "This paper uses an unsupervised model of grounded language acquisition to study the role that social cues play in language acquisition .", "Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure , consistent with the idea that children could discover the importance of particular social information sources during word learning .", "The input to the model consists of ( orthographically transcribed ) child-directed utterances accompanied by the set of objects present in the non-linguistic context ."]}
{"orig_sents": ["5", "3", "2", "1", "0", "4"], "shuf_sents": ["The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data .", "The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features .", "The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate .", "In this paper , we propose two discriminative , feature-based models to exploit predicateargument structures for statistical machine translation : 1 ) a predicate translation model and 2 ) an argument reordering model .", "Experimental results demonstrate that the two models significantly improve translation accuracy .", "Predicate-argument structure contains rich semantic information of which statistical machine translation hasn ? t taken full advantage ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks , and show that it can significantly outperform the baseline phrasebased SMT system .", "Long distance word reordering is a major challenge in statistical machine translation research .", "Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference .", "The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features .", "In this work , we further extend this line of exploration and propose a novel but simple approach , which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence ."]}
{"orig_sents": ["1", "3", "4", "0", "2"], "shuf_sents": ["By reformulating the problem in the linear programming framework , TESLACELAB addresses several drawbacks of the character-level metrics , in particular the modeling of synonyms spanning multiple characters .", "In this work , we introduce the TESLACELAB metric ( Translation Evaluation of Sentences with Linear-programming-based Analysis ?", "We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks .", "Character-level Evaluation for Languages with Ambiguous word Boundaries ) for automatic machine translation evaluation .", "For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy , TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation ."]}
{"orig_sents": ["0", "3", "1", "7", "5", "6", "4", "2"], "shuf_sents": ["Many machine translation ( MT ) evaluation metrics have been shown to correlate better with human judgment than BLEU .", "However , due to issues such as speed , requirements for linguistic resources , and optimization difficulty , they have not been widely adopted for tuning .", "PORT tuning achieves consistently better performance than BLEU tuning , according to four automated metrics ( including BLEU ) and to human evaluation : in comparisons of outputs from 300 source sentences , human judges preferred the PORT-tuned output 45.3 % of the time ( vs. 32.7 % BLEU tuning preferences and 22.0 % ties ) .", "In principle , tuning on these metrics should yield better systems than tuning on BLEU .", "We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs .", "PORT does not require external resources and is quick to compute .", "It has a better correlation with human judgment than BLEU .", "This paper presents PORT 1 , a new MT evaluation metric which combines precision , recall and an ordering metric and which is primarily designed for tuning MT systems ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models , the current state-of-the-art for domain adaptation in machine translation .", "Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain .", "We propose a novel approach , ensemble decoding , which combines a number of translation systems dynamically at the decoding step .", "In this paper , we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Under the weighed synchronous context-free grammar defined by these rules , our model searches for the best translation derivation and yields target translation simultaneously .", "With the help of shallow parsing , our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion .", "Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks .", "We present a hierarchical chunk-to-string translation model , which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model , to combine the merits of the two models ."]}
{"orig_sents": ["2", "4", "0", "3", "1"], "shuf_sents": ["We evaluate on perplexity and a range of grammaticality tasks , and find that we perform as well or better than n-gram models and other generative baselines .", "We also show fluency improvements in a preliminary machine translation experiment .", "We propose a simple generative , syntactic language model that conditions on overlapping windows of tree context ( or treelets ) in the same way that n-gram language models condition on overlapping windows of linear context .", "Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks , despite training on positive data alone .", "We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques , allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Empirical results demonstrating the potential of this approach are presented for experiments using texts taken from the Universal Declaration of Human Rights and Wikipedia , covering more than 200 languages .", "The problem was motivated by an attempt to collect a large amount of linguistic data for non-major languages from the web .", "The problem is formulated in terms of obtaining the minimum description length of a text , and the proposed solution finds the segments and their languages through dynamic programming .", "The problem addressed in this paper is to segment a given multilingual document into segments for each language and then identify the language of each segment ."]}
{"orig_sents": ["4", "0", "2", "3", "1"], "shuf_sents": ["In particular , evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks .", "The paper includes a description of the approach , tips for how to use AdWords for scientific research , and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach .", "In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost .", "The central idea is exploiting existing commercial tools for advertising on the web , such as Google AdWords , to measure message impact in an ecological setting .", "In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks ."]}
{"orig_sents": ["2", "0", "5", "4", "7", "3", "1", "6"], "shuf_sents": ["A number of sentiment word/sense dictionaries have been manually or ( semi ) automatically constructed .", "We reduce the polarity consistency problem to the satisfiability problem and utilize a fast SAT solver to detect inconsistencies in a sentiment dictionary .", "Polarity classification of words is important for applications such as Opinion Mining and Sentiment Analysis .", "We show that the consistency problem is NP-complete .", "Besides obvious instances , where the same word appears with different polarities in different dictionaries , the dictionaries exhibit complex cases , which can not be detected by mere manual inspection .", "The dictionaries have substantial inaccuracies .", "We perform experiments on four sentiment dictionaries and WordNet .", "We introduce the concept of polarity consistency of words/senses in sentiment dictionaries in this paper ."]}
{"orig_sents": ["5", "0", "1", "4", "3", "2"], "shuf_sents": ["Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles .", "A current research focus is to process these sentences so that they read fluently as a whole .", "The results show significantly improved performance over AESOP 2011 submitted metrics .", "In this work , we adapt a machine translation metric to measure content coverage , apply an enhanced discourse coherence model to evaluate summary readability , and combine both in a trained regression model to evaluate overall responsiveness .", "The current AESOP task encourages research on evaluating summaries on content , readability , and overall responsiveness .", "An ideal summarization system should produce summaries that have high content coverage and linguistic quality ."]}
{"orig_sents": ["1", "2", "3", "4", "0"], "shuf_sents": ["We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems .", "In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation , augmented with a re-ranking heuristic based on dissimilarity , and trained on a monolingual parallel corpus .", "We compare our system to a word-substitution baseline and two state-of-the-art systems , all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia .", "Human test subjects judge the output of the different systems .", "Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems , while generating better formed output ."]}
{"orig_sents": ["5", "1", "6", "4", "3", "2", "0"], "shuf_sents": ["In addition , the experimental result on text chunking shows that fewer serious errors help to improve the performance of subsequent NLP tasks .", "However , the errors are not equally important , since some errors affect the performance of subsequent natural language processing ( NLP ) tasks seriously while others do not .", "Through a set of POS tagging experiments , it is shown that the classifier trained with the proposed loss functions reduces serious errors compared to state-of-the-art POS taggers .", "They are designed to assign a larger cost to serious errors and a smaller one to minor errors .", "Two gradient loss functions are proposed to reflect the different types of errors .", "All types of part-of-speech ( POS ) tagging errors have been equally treated by existing taggers .", "This paper aims to minimize these serious errors while retaining the overall performance of POS tagging ."]}
{"orig_sents": ["0", "3", "4", "2", "1", "5"], "shuf_sents": ["Social media language contains huge amount and wide variety of nonstandard tokens , created both intentionally and unintentionally by the users .", "The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets .", "In this paper , we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens , including the enhanced letter transformation , visual priming , and string/phonetic similarity .", "It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques .", "A major challenge facing this task is the system coverage , i.e. , for any user-created nonstandard term , the system should be able to restore the correct word within its top n output candidates .", "Results show that our system achieves over 90 % word-coverage across all data sets ( a 10 % absolute increase compared to state-ofthe-art ) ; the broad word-coverage can also successfully translate into message-level performance gain , yielding 6 % absolute increase compared to the best prior approach ."]}
{"orig_sents": ["0", "2", "3", "1", "4"], "shuf_sents": ["We propose the first joint model for word segmentation , POS tagging , and dependency parsing for Chinese .", "In experiments using the Chinese Treebank ( CTB ) , we show that the accuracies of the three tasks can be improved significantly over the baseline models , particularly by 0.6 % for POS tagging and 2.4 % for dependency parsing .", "Based on an extension of the incremental joint model for POS tagging and dependency parsing ( Hatori et al , 2011 ) , we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation , POS tagging , and dependency parsing models .", "We also describe our method to align comparable states in the beam , and how we can combine features of different characteristics in our incremental framework .", "We also perform comparison experiments with the partially joint models ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree .", "It achieves its best F1 scores of 91.86 % and 85.58 % on the two languages , respectively , and further pushes them to 92.80 % and 85.60 % via combination with other highperformance parsers .", "Experiments on English and Chinese treebanks confirm its advantage over its first-order version ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios ( assuming gold segmented and tagged input ) and realistic ones ( not assuming gold segmentation and tags ) .", "Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios .", "We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance .", "The protocol uses distance-based metrics defined for the space of trees over lattices ."]}
{"orig_sents": ["1", "5", "3", "0", "4", "2"], "shuf_sents": ["Our comparison of performance and efficiency across seven popular open source parsers ( four constituent and three dependency ) shows , by contrast , that recent higher-order graph-based techniques can be more accurate , though somewhat slower , than constituent parsers .", "Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation , commonly generated either by ( i ) converting the output of a constituent parser , or ( ii ) predicting dependencies directly .", "Finally , we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice .", "In this paper , we re-evaluate both methods for Chinese , using more accurate dependency parsers than in previous work .", "We demonstrate also that n-way jackknifing is a useful technique for producing automatic ( rather than gold ) partof-speech tags to train Chinese dependency parsers .", "Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["On sentences of up to length 40 , LLCCM outperforms CCM by 13.9 % bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not .", "LLCCM retains the simplicity of the original CCM but extends robustly to long sentences .", "We present LLCCM , a log-linear variant of the constituent context model ( CCM ) of grammar induction ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["Some Statistical Machine Translation systems never see the light because the owner of the appropriate training data can not release them , and the potential user of the system can not disclose what should be translated .", "We propose a simple and practical encryption-based method addressing this barrier ."]}
{"orig_sents": ["3", "1", "4", "0", "2"], "shuf_sents": ["We compare our approach with Moses and observe the same performance , but a substantially better trade-off between translation quality and speed .", "Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2 .", "At a speed of roughly 70 words per second , Moses reaches 17.2 % BLEU , whereas our approach yields 20.0 % with identical models .", "In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation ( SMT ) , aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions .", "Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents an extension of Chiang ? s hierarchical phrase-based ( HPB ) model , called Head-Driven HPB ( HD-HPB ) , which incorporates head information in translation rules to better capture syntax-driven information , as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space .", "Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang ? s model with average gains of 1.91 points absolute in BLEU ."]}
{"orig_sents": ["5", "6", "4", "0", "1", "3", "2"], "shuf_sents": ["Paraphrase criteria especially the paraphrase rate is not able to be ensured in that way .", "In this paper , we propose a joint learning method of two SMT systems to optimize the process of paraphrase generation .", "Our experiments on NIST 2008 testing data with automatic evaluation as well as human judgments suggest that the proposed method is able to enhance the paraphrase quality by adjusting between semantic equivalency and surface dissimilarity .", "In addition , a revised BLEU score ( called iBLEU ) which measures the adequacy and diversity of the generated paraphrase sentence is proposed for tuning parameters in SMT systems .", "Existing work that uses two independently trained SMT systems can not directly optimize the paraphrase results .", "SMT has been used in paraphrase generation by translating a source sentence into another ( pivot ) language and then back into the source .", "The resulting sentences can be used as candidate paraphrases of the source sentence ."]}
{"orig_sents": ["5", "3", "2", "0", "1", "4"], "shuf_sents": ["BurstVSM corresponds dimensions to bursty features instead of terms , which can capture semantic and temporal information .", "Meanwhile , it significantly reduces the number of non-zero entries in the representation .", "To address it , we proposed a novel burst-based text representation model , denoted as BurstVSM .", "Classic text representation model ( i.e. , vector space model ) can not model temporal aspects of documents .", "We test it via scalable event detection , and experiments in a 10-year news archive show that our methods are both effective and efficient .", "Mining retrospective events from text streams has been an important research topic ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Although researchers have conducted extensive studies on relation extraction in the last decade , supervised approaches are still limited because they require large amounts of training data to achieve high performances .", "This paper proposes a novel graph-based projection approach and demonstrates the merits of it by using a Korean relation extraction system based on projected dataset from an English-Korean parallel corpus .", "To build a relation extractor without significant annotation effort , we can exploit cross-lingual annotation projection , which leverages parallel corpora as external resources for supervision ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations .", "Results show that the learned patterns can be used to extract new relations with good precision .", "We leverage distant supervision using relations from the knowledge base FreeBase , but do not require any manual heuristic nor manual seed list selections ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["Our framework uses text mining techniques to discover topics , emotions , sentiments , lexical patterns , as well as personally identifiable information ( PII ) and personally embarrassing information ( PEI ) .", "In social psychology , it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship .", "Our preliminary results illustrate that in relationships with high relationship strength , Twitter users show significantly more frequent behaviors of self-disclosure .", "We present a computational framework for automatically analyzing such self-disclosure behavior in Twitter conversations ."]}
{"orig_sents": ["1", "3", "2", "5", "4", "0"], "shuf_sents": ["Experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes ( expressed via sentiment ) and it can improve the sub-group detection performance independent of genre .", "We describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread .", "Sentiment tags play an important role in this detection , but we also note another dimension to the detection of people ? s attitudes in a discussion : if two persons share the same opinion , they tend to use similar language content .", "An intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion .", "In this paper , we investigate the impact of implicit and explicit attitude in two genres of social media discussion data , more formal wikipedia discussions and a debate discussion forum that is much more informal .", "We consider the latter to be an implicit attitude ."]}
{"orig_sents": ["1", "4", "3", "2", "0"], "shuf_sents": ["This finding has important implications for styles of data representation and resources used for temporal relation learning : clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank , implying that the field may need to look at a wider range of domains to fully understand the nature of temporal ordering .", "We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence .", "Interestingly , we observe that this methodology performs better than a classification-based approach for this domain , but worse on the relationships found in the Timebank corpus .", "Such a representation allows us to learn all of Allen ? s temporal relations between medical events .", "We represent each medical event as a time duration , with a corresponding start and stop , and learn to rank the starts/stops based on their proximity to the admission date ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["-sensitive , Multi-faceted model of Lexico-Conceptual Affect Tony Veale Web Science and Technology Division , KAIST , Daejeon , South Korea .", "words and concepts to suit our affective needs , context is a major determinant of the perceived affect of a word or concept .", "We show how a large body of affective stereotypes can be acquired from the web , and also show how these are used to create and interpret affective metaphors .", "We view this re-profiling as a selective emphasis or de-emphasis of the qualities that underpin our shared stere-otype of a concept or a word meaning , and construct our model of the affective lexicon accordingly .", "Tony.Veale @ gmail.com Abstract Since we can ? spin ?"]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We consider a different type of classical encoding scheme known as the running key cipher , and propose a search solution using Gibbs sampling with a word language model .", "We evaluate our method on synthetic ciphertexts of different lengths , and find that it outperforms previous work that employs Viterbi decoding with character-based models .", "There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in Goldwater ( 2006 ) .", "By adding rejuvenation to a particle filter , we are able to considerably improve its performance , both in terms of finding higher probability and higher accuracy solutions ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Based on these observations , we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets , sometimes providing a new state-of-the-art performance level .", "Variants of Naive Bayes ( NB ) and Support Vector Machines ( SVM ) are often used as baseline methods for text classification , but their performance varies greatly depending on the model variant , features used and task/ dataset .", "We show that : ( i ) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks ; ( ii ) for short snippet sentiment tasks , NB actually does better than SVMs ( while for longer documents the opposite result holds ) ; ( iii ) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets ."]}
{"orig_sents": ["4", "0", "2", "1", "3"], "shuf_sents": ["A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments .", "We then learn a more general developmental index by applying our method across children , predicting relative temporal orderings of speech samples .", "In the first experiment , we learn a child-specific metric and predicts the ages at which speech samples were produced .", "In both cases we compare our results with established measures of language development , showing improvements in age prediction performance .", "We propose a new approach for the creation of child language development metrics ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["Besides using traditional dependency parsers , we also use the dependency structures transformed from PCFG trees and predicate-argument structures ( PASs ) which are generated by an HPSG parser and a CCG parser .", "This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers .", "Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation .", "The experiments on Chinese-to-English translation show that the HPSG parser ? s PASs achieved the best dependency and translation accuracies ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["In contrast with past work ( Clark and Curran , 2009 ) , which used simple transductions on category pairs , our approach uses richer transductions attached to single categories .", "Our conversion preserves more sentences under round-trip conversion ( 51.1 % vs. 39.6 % ) and is more robust .", "We propose an improved , bottom-up method for converting CCG derivations into PTB-style phrase structure trees .", "In particular , unlike past methods , ours does not require ad-hoc rules over non-local features , and so can be easily integrated into a parser ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Under our general variant of TIG , grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal , which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom ( 2010 ) .", "We present a Bayesian nonparametric model for estimating tree insertion grammars ( TIG ) , building upon recent work in Bayesian inference of tree substitution grammars ( TSG ) via Dirichlet processes .", "We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features .", "We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation .", "Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We address a core aspect of the multilingual content synchronization task : the identification of novel , more informative or semantically equivalent pieces of information in two documents about the same topic .", "Using a combination of lexical , syntactic , and semantic features to train a cross-lingual textual entailment system , we report promising results on different datasets .", "This can be seen as an application-oriented variant of textual entailment recognition where : i ) T and H are in different languages , and ii ) entailment relations between T and H have to be checked in both directions ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Evaluation shows that the system reliably discards dispreferred parses from the raw parser output , which results in a pre-selection that can speed up manual treebanking .", "We present a system for cross-lingual parse disambiguation , exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities .", "We simultaneously reduce ambiguity in multiple languages in a fully automatic way ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["In this paper , we present a new method for learning to finding translations and transliterations on the Web for a given term .", "At runtime , the model is used to extracting translation candidates for a given term .", "Preliminary experiments and evaluation show our method cleanly combining various features , resulting in a system that outperforms previous work .", "The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine , and automatically annotating the snippets with tags and features for training a conditional random field model ."]}
{"orig_sents": ["0"], "shuf_sents": ["We investigate how novel English-derived words ( anglicisms ) are used in a Germanlanguage Internet hip hop forum , and what factors contribute to their uptake ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We learn low-dimensional latent semantic vectors of concept definitions to construct a more robust sense similarity measure wmfvec .", "In this paper we study unsupervised word sense disambiguation ( WSD ) based on sense definition .", "Experiments on four all-words WSD data sets show significant improvement over the baseline WSD systems and LDA based similarity measures , achieving results comparable to state of the art WSD systems ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We propose a probabilistic generative model for unsupervised semantic role induction , which integrates local role assignment decisions and a global role ordering decision in a unified model .", "The role sequence is divided into intervals based on the notion of primary roles , and each interval generates a sequence of secondary roles and syntactic constituents using local features .", "The global role ordering consists of the sequence of primary roles only , thus making it a partial ordering ."]}
{"orig_sents": ["4", "1", "3", "0", "2"], "shuf_sents": ["Initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles .", "Of the scarce amount of research in computational humor , no research had focused on humor generation beyond simple puns and punning riddles .", "We evaluate the said metrics through a user-assessed quality of the generated two-liners .", "We propose an algorithm for mining simple humorous scripts from a semantic network ( ConceptNet ) by specifically searching for dual scripts that jointly maximize overlap and incongruity metrics in line with Raskin ? s Semantic-Script Theory of Humor .", "This work presents a first step to a general implementation of the Semantic-Script Theory of Humor ( SSTH ) ."]}
{"orig_sents": ["5", "0", "4", "2", "3", "1"], "shuf_sents": ["However , evaluating such resources has turned out to be a non-trivial task , slowing progress in the field .", "We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time , without requiring training expert annotators .", "Our framework simplifies a previously proposed ? instance-based evaluation ?", "method that involved substantial annotator training , making it suitable for crowdsourcing .", "In this paper , we suggest a framework for evaluating inference-rule resources .", "The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Using our new gold standard , we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems .", "This work , however , has been hampered by a lack of data .", "Many researchers have attempted to predict the Enron corporate hierarchy from the data .", "We present a new , large , and freely available gold-standard hierarchy ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["For evaluation , we compare our system output with multiple human references .", "In the first step , we use a sequence labeling method to determine if a word in the utterance can be removed , and generate n-best compressed sentences .", "This paper presents a two-step approach to compress spontaneous spoken utterances .", "In the second step , we use a discriminative training approach to capture sentence level global information from the candidates and rerank them .", "Our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set , and reranking is able to yield additional gain , especially when training is performed to take into account multiple references ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Over four different datasets spanning from the product review to the essay domain , we demonstrate that features driven from Context Free Grammar ( CFG ) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features .", "This paper investigates syntactic stylometry for deception detection , adding a somewhat unconventional angle to prior literature .", "Most previous studies in computerized deception detection have relied only on shallow lexico-syntactic patterns .", "Our results improve the best published result on the hotel review data ( Ott et al , 2011 ) reaching 91.2 % accuracy with 14 % error reduction ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We compare several algorithms for automatically segmenting and discretizing this data into ( utterance , reaction ) pairs and training a classifier to predict reactions given the next utterance .", "Previous approaches to instruction interpretation have required either extensive domain adaptation or manually annotated corpora .", "Our empirical analysis shows that the best algorithm achieves 70 % accuracy on this task , with no manual annotation required .", "This paper presents a novel approach to instruction interpretation that leverages a large amount of unannotated , easy-to-collect data from humans interacting with a virtual world ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["Experiments show that using question reformulation patterns can significantly improve the search performance of natural language questions .", "However , various questions can be formulated to convey the same information need , which poses a great challenge to search systems .", "Natural language questions have become popular in web search .", "The question reformulations generated from these patterns are further incorporated into the retrieval model .", "In this paper , we automatically mined 5w1h question reformulation patterns from large scale search log data ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We compare two state of the art methods for Tree Substitution Grammar induction and show that features from both methods outperform previous state of the art results at native language detection .", "We investigate the potential of Tree Substitution Grammars as a source of features for native language detection , the task of inferring an author ? s native language from text in a different language .", "Furthermore , we contrast these two induction algorithms and show that the Bayesian approach produces superior classification results with a smaller feature set ."]}
{"orig_sents": ["5", "6", "3", "2", "4", "0", "1"], "shuf_sents": ["In order to take global information into account , we regard the task as sequence labeling : each verb phrase in a document is labeled with tense/aspect depending on surrounding labels .", "Our experiments show that the global context makes a moderate contribution to tense/aspect error correction .", "One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context .", "However , most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important .", "Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently , and thus fails to capture the information of neighboring labels .", "As the number of learners of English is constantly growing , automatic error correction of ESL learners ?", "writing is an increasingly active area of research ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes Movie-DiC a Movie Dialogue Corpus recently collected for research and development purposes .", "The collected dataset comprises 132,229 dialogues containing a total of 764,146 turns that have been extracted from 753 movies .", "Details on how the data collection has been created and how it is structured are provided along with its main statistics and characteristics ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["However , a user that wants to cut in on a debate may experience some difficulties in extracting the current accepted positions , and can be discouraged from interacting through these applications .", "In our paper , we combine textual entailment with argumentation theory to automatically extract the arguments from debates and to evaluate their acceptability .", "Blogs and forums are widely adopted by online communities to debate about various issues ."]}
{"orig_sents": ["4", "0", "5", "2", "3", "1"], "shuf_sents": ["A corpus of more than 14 million tweets containing temporal duration information was collected .", "This automatically generated duration information is broadly comparable to hand-annotation .", "For each verb lemma , associated duration information was collected for episodic and habitual uses of the verb .", "Summary statistics for 483 verb lemmas and their typical habit and episode durations has been compiled and made available .", "We seek to automatically estimate typical durations for events and habits described in Twitter tweets .", "These tweets were classified as to their habituality status using a bootstrapped , decision tree ."]}
{"orig_sents": ["2", "3", "1", "0", "4"], "shuf_sents": ["This implicitly relaxes coreference to co-reporting , and will practically enable augmenting news archives with semantic hyperlinks .", "We introduce event linking , which canonically labels an event reference with the article where it was first reported .", "Interpreting news requires identifying its constituent events .", "Events are complex linguistically and ontologically , so disambiguating their reference is challenging .", "We annotate and analyse a corpus of 150 documents , extracting 501 links to a news archive with reasonable inter-annotator agreement ."]}
{"orig_sents": ["3", "6", "1", "2", "5", "0", "4"], "shuf_sents": ["Label propagation aggressively gathers fact candidates , and an Integer Linear Program is used to clean out false hypotheses that violate temporal constraints .", "However , in reality most knowledge is transient , i.e .", "changes over time , requiring a temporal dimension in fact extraction .", "The Web and digitized text sources contain a wealth of information about named entities such as politicians , actors , companies , or cultural landmarks .", "Our method is able to improve on recall while keeping up with precision , which we demonstrate by experiments with biography-style Wikipedia pages and a large corpus of news articles .", "In this paper we develop a methodology that combines label propagation with constraint reasoning for temporal fact extraction .", "Extracting this information has enabled the automated construction of large knowledge bases , containing hundred millions of binary relationships or attribute values about these named entities ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["We propose a simple , efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time .", "Additionally , we annotate a corpus of search queries with partof-speech tags , providing a resource for future work on syntactic query analysis .", "Unlike previous work , our final model does not require any additional resources at run-time .", "Syntactic analysis of search queries is important for a variety of information-retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .", "Compared to a state-ofthe-art approach , we achieve more than 20 % relative error reduction ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["In particular , I propose using Sumerian and Akkadian ideogrammatic signs within Hittite texts to improve the performance of Naive Bayes and Maximum Entropy classifiers .", "This paper presents the problem within Hittite and Ancient Near Eastern studies of fragmented and damaged cuneiform texts , and proposes to use well-known text classification metrics , in combination with some facts about the structure of Hittite-language cuneiform texts , to help classify a number of fragments of clay cuneiform-script tablets into more complete texts .", "The performance in some cases is improved , and in some cases very much not , suggesting that the variable frequency of occurrence of these ideograms in individual fragments makes considerable difference in the ideal choice for a classification method .", "Further , complexities of the writing system and the digital availability of Hittite texts complicate the problem ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The corpus is intended to support research on textual revision by language learners , and how it is influenced by feedback .", "Furthermore , the sentences in the drafts are annotated with comments from teachers .", "This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative ( TEI ) .", "This paper describes the creation of the first large-scale corpus containing drafts and final versions of essays written by non-native speakers , with the sentences aligned across different versions ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["? Lightweight ?", "semantic annotation of text calls for a simple representation , ideally without requiring a semantic lexicon to achieve good coverage in the language and domain .", "The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement .", "In this paper , we repurpose WordNet ? s supersense tags for annotation , developing specific guidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Through experiments run using word usage examples collected from three major periods of time ( 1800 , 1900 , 2000 ) , we show that the task is feasible , and significant differences can be observed between occurrences of words in different periods of time .", "defined as the problem of identifying changes in word usage over time .", "In this paper we introduce the novel task of ? word epoch disambiguation , ?"]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Building on our earlier finding that the Latent Dirichlet Allocation ( LDA ) topic model can be used to improve authorship attribution accuracy , we show that employing a previously-suggested Author-Topic ( AT ) model outperforms LDA when applied to scenarios with many authors .", "In addition , we define a model that combines LDA and AT by representing authors and documents over two disjoint topic sets , and show that our model outperforms LDA , AT and support vector machines on datasets with many authors .", "Authorship attribution deals with identifying the authors of anonymous texts ."]}
{"orig_sents": ["3", "4", "0", "2", "1"], "shuf_sents": ["consistency for target data with source training data by identifying the correlations of domain-specific features from different domains .", "Experiments show that IMAM significantly outperforms state-of-the-art baselines .", "We present an Information-theoretic Multi-view Adaptation Model ( IMAM ) based on a multi-way clustering scheme , where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views .", "We use multiple views for cross-domain document classification .", "The main idea is to strengthen the views ?"]}
{"orig_sents": ["4", "3", "5", "0", "1", "2"], "shuf_sents": ["This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments .", "We further improve performance by iteratively refining the sampling distribution only when needed .", "Experiments show that the proposed techniques dramatically improve the computation time .", "However , its expressive power comes at the cost of more complicated inference .", "Topic modeling with a tree-based prior has been used for a variety of applications because it can encode correlations between words that traditional topic modeling can not .", "We extend the SPARSELDA ( Yao et al , 2009 ) inference scheme for latent Dirichlet alocation ( LDA ) to tree-based topic models ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences .", "Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["To tackle this , we propose a framework that uses two different segmentation specifications for alignment and translation respectively : we use Chinese character as the basic unit for alignment , and then convert this alignment to conventional word alignment for translation rule induction .", "Experimentally , our approach outperformed two baselines : fully word-based system ( using word for both alignment and translation ) and fully character-based system , in terms of alignment quality and translation performance .", "The dominant practice of statistical machine translation ( SMT ) uses the same Chinese word segmentation specification in both alignment and translation rule induction steps in building Chinese-English SMT system , which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["On the contrary , the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules , and thus preserve the search space of SMT decoders as much as possible .", "Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance .", "In this paper , we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems .", "Previous approaches try to prune infrequent entries or unreliable entries based on statistics , but cause a problem of reducing the translation coverage ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size .", "Empirically , we show a gain in running time of a standard machine translation system , at a small loss in accuracy ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline .", "We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources .", "We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU , which we further augment with character-based transliteration at the word level and combine with a word-level translation model ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm , by placing prior probabilities on the model parameters .", "We apply one such Bayesian technique , variational Bayes , to the IBM models of word alignment for statistical machine translation .", "We show that using variational Bayes improves the performance of the widely used GIZA++ software , as well as improving the overall performance of the Moses machine translation system in terms of BLEU score ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We employ the postordering framework proposed by ( Sudoh et al. , 2011b ) for Japanese to English translation and improve upon the reordering method .", "Reordering is a difficult task in translating between widely different languages such as Japanese and English .", "The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT , while our method reorders the sequence by : 1 ) parsing the sequence to obtain syntax structures similar to a source language structure , and 2 ) transferring the obtained syntax structures into the syntax structures of the target language ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses .", "In this paper , we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself .", "We find that it is primarily the latter , but that under the right conditions , the translation stage does correct parser errors , improving parsing accuracy on the Chinese Treebank ."]}
{"orig_sents": ["0", "2", "3", "1", "4"], "shuf_sents": ["If unsupervised morphological analyzers could approach the effectiveness of supervised ones , they would be a very attractive choice for improving MT performance on low-resource inflected languages .", "Our approach gives an 18 % relative BLEU gain for Levantine dialectal Arabic .", "In this paper , we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers , using a state-of-theart Arabic-to-English MT system .", "We apply maximum marginal decoding to the unsupervised analyzer , and show that this yields the best published segmentation accuracy for Arabic , while also making segmentation output more stable .", "Furthermore , it gives higher gains for Modern Standard Arabic ( MSA ) , as measured on NIST MT-08 , than does MADA ( Habash and Rambow , 2005 ) , a leading supervised MSA segmenter ."]}
{"orig_sents": ["2", "1", "0", "4", "3"], "shuf_sents": ["The primary advantages of our model are two-fold : first , it performs document-level and sentence-level sentiment polarity classification jointly ; second , it is able to find informative sentences that are closely related to some respects in a review , which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization .", "Our model aims to identify highly informative sentences that are aspect-specific in online custom reviews .", "In this paper , we present a structural learning model for joint sentiment classification and aspect analysis of text at various levels of granularity .", "Preliminary experiments demonstrate that our model obtains promising performance .", "The proposed method was evaluated with 9,000 Chinese restaurant reviews ."]}
{"orig_sents": ["2", "0", "1", "3", "4"], "shuf_sents": ["However , such models are highly sensitive to the type and size of syntactic structure used .", "It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task .", "Convolution kernels support the modeling of complex syntactic information in machinelearning tasks .", "In this paper we present a systematic study investigating ( combinations of ) sequence and convolution kernels using different types of substructures in document-level sentiment classification .", "We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["For sentence compression , we propose new semantic constraints to directly capture the relations between a predicate and its arguments , whereas the existing approaches have focused on relatively shallow linguistic properties , such as lexical and syntactic information .", "Our empirical evaluation on the Written News Compression Corpus ( Clarke and Lapata , 2008 ) demonstrates that our system achieves results comparable to other state-of-the-art techniques .", "These constraints are based on semantic roles and superior to the constraints of syntactic dependencies ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper shows that full abstraction can be accomplished in the context of guided summarization .", "We describe a work in progress that relies on Information Extraction , statistical content selection and Natural Language Generation .", "Early results already demonstrate the effectiveness of the approach ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Using Text Analysis Conference data , we measure annotator consistency based on human scoring of summaries for Responsiveness , Readability , and Pyramid scoring .", "We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques .", "Finally , we examine the stability of automatic metrics ( ROUGE and CLASSY ) with respect to the inconsistent assessments .", "We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems ."]}
{"orig_sents": ["0", "2", "3", "4", "6", "5", "1"], "shuf_sents": ["This paper presents a novel way of improving POS tagging on heterogeneous data .", "We believe that this model selection approach can be applied to more sophisticated tagging algorithms and improve their robustness even further .", "First , two separate models are trained ( generalized and domain-specific ) from the same data set by controlling lexical items with different document frequencies .", "During decoding , one of the models is selected dynamically given the cosine similarity between each sentence and the training data .", "This dynamic model selection approach , coupled with a one-pass , leftto-right POS tagging algorithm , is evaluated on corpora from seven different genres .", "Furthermore , our system is able to tag about 32K tokens per second .", "Even with this simple tagging algorithm , our system shows comparable results against other state-of-the-art systems , and gives higher accuracies when evaluated on a mixture of the data ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["We present a novel approach to the task of word lemmatisation .", "We formalise lemmatisation as a category tagging task , by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a specific language .", "We test our approach on eight languages reaching a new state-of-the-art level for the lemmatisation task .", "In this way , a lemmatisation system can be trained and tested using any supervised tagging model .", "In contrast to previous approaches , the proposed technique allows us to easily integrate relevant contextual information ."]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["This paper presents a comparative study of spelling errors that are corrected as you type , vs. those that remain uncorrected .", "Our analysis shows a clear distinction between the types of errors that are generated and those that remain uncorrected , as well as across languages .", "First , we generate naturally occurring online error correction data by logging users ?", "keystrokes , and by automatically deriving pre- and postcorrection strings from them .", "We then perform an analysis of this data against the errors that remain in the final text as well as across languages ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["to genreor domain-specific idiosyncrasies ) .", "We examine some of the frequently disregarded subtleties of tokenization in Penn Treebank style , and present a new rule-based preprocessing toolkit that not only reproduces the Treebank tokenization with unmatched accuracy , but also maintains exact stand-off pointers to the original text and allows flexible configuration to diverse use cases ( e.g ."]}
{"orig_sents": ["4", "0", "1", "2", "3"], "shuf_sents": ["Following Harris 's Hypothesis in Kempe ( 1999 ) and Tanaka-Ishii 's ( 2005 ) reformulation , we base our work on the Variation of Branching Entropy .", "We improve on ( Jin and Tanaka-Ishii , 2006 ) by adding normalization and viterbidecoding .", "This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results ( Wang et al , 2011 ) with a simpler system .", "We provide evaluation on different corpora available from the Segmentation bake-off II ( Emerson , 2005 ) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf ( Zhang and Clark , 2010 ; Zhao and Kit , 2008 ; Huang and Zhao , 2007 )", "In this paper , we present an unsupervized segmentation system tested on Mandarin Chinese ."]}
{"orig_sents": ["2", "1", "0", "3", "4"], "shuf_sents": ["We tackle this problem by using pseudoerror sentences generated automatically .", "The error correction task is hindered by the difficulty of collecting large error corpora .", "This paper presents grammar error correction for Japanese particles that uses discriminative sequence conversion , which corrects erroneous particles by substitution , insertion , and deletion .", "Furthermore , we apply domain adaptation , the pseudo-error sentences are from the source domain , and the real-error sentences are from the target domain .", "Experiments show that stable improvement is achieved by using domain adaptation ."]}
{"orig_sents": ["7", "0", "4", "1", "5", "3", "6", "2"], "shuf_sents": ["The knowledge about the grapheme-phoneme conversion ( GPC ) rules of languages has been shown to be highly correlated to the ability of reading alphabetic languages and Chinese .", "Results of a preliminary evaluation of our games indicated significant improvement in learners ?", "Evaluation of the authoring environment with 20 subjects showed that our system made the authoring of games more effective and efficient .", "In addition , we construct a Webbased open system for teachers to prepare their own games to best meet their teaching goals .", "We build and will demo a game platform for strengthening the association of phonological components in Chinese characters with the pronunciations of the characters .", "response times in Chinese naming tasks .", "Techniques for decomposing Chinese characters and for comparing the similarity between Chinese characters were employed to recommend lists of Chinese characters for authoring the games .", "We demonstrate applications of psycholinguistic and sublexical information for learning Chinese characters ."]}
{"orig_sents": ["3", "5", "1", "4", "2", "0"], "shuf_sents": ["We also con-sider the kinds of deep and shallow stereotypical knowledge that are needed for such a system , and demonstrate how they can be acquired from corpora and the web .", "Tony.Veale @ gmail.com Yanfen.Hao @ UCD.ie Abstract Metaphors pervade our language because they are elastic enough to allow a speaker to express an affective viewpoint on a topic without committing to a specific meaning .", "We explore here , via a demonstration of a system for metaphor interpretation and generation called Metaphor Magnet , the practical uses of metaphor as a basis for formulating af-fective information queries .", "g Viewpoint and Information Need with Affective Metaphors A System Demonstration of the Metaphor Magnet Web App/Service Tony Veale Guofu Li Web Science and Technology Division , School of Computer Science & Informatics , KAIST , Daejeon , University College Dublin , South Korea .", "This balance of expressiveness and inde-terminism means that metaphors are just as useful for eliciting information as they are for conveying information .", "Belfield , Dublin D4 , Ireland ."]}
{"orig_sents": ["2", "3", "0", "4", "1"], "shuf_sents": ["We present QuickView , an NLP-based tweet search platform to tackle this issue .", "Then , non-noisy tweets , together with the mined information , are indexed , on top of which two brand new scenarios are enabled , i.e. , categorized browsing and advanced search , allowing users to effectively access either the tweets or fine-grained information they are interested in .", "Tweets have become a comprehensive repository for real-time information .", "However , it is often hard for users to quickly get information they are interested in from tweets , owing to the sheer volume of tweets as well as their noisy and informal nature .", "Specifically , it exploits a series of natural language processing technologies , such as tweet normalization , named entity recognition , semantic role labeling , sentiment analysis , tweet classification , to extract useful information , i.e. , named entities , events , opinions , etc. , from a large volume of tweets ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["The toolkit supports several state-of-the-art models developed in statistical machine translation , including the phrase-based model , the hierachical phrase-based model , and various syntaxbased models .", "The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms , such as phrase-based decoding , decoding as parsing/tree-parsing and forest-based decoding .", "We present a new open source toolkit for phrase-based and syntax-based machine translation .", "Moreover , several useful utilities were distributed with the toolkit , including a discriminative reordering model , a simple and fast language model , and an implementation of minimum error rate training for weight tuning ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We discuss the design and implementation of langid.py , and provide an empirical comparison on 5 longdocument datasets , and 2 datasets from the microblog domain .", "We present langid.py , an off-the-shelf language identification tool .", "We find that langid.py maintains consistently high accuracy across all domains , making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Due to the lack of training data and the variations of short-forms used among different social communities , it is hard to normalize and translate chat messages if user uses vocabularies outside the training data and create short-forms freely .", "We develop a personalized chat normalizer for English and integrate it with a multilingual chat system , allowing user to create and use personalized short-forms in multilingual chat .", "This paper describes the personalized normalization of a multilingual chat system that supports chatting in user defined short-forms or abbreviations .", "One of the major challenges for multilingual chat realized through machine translation technology is the normalization of non-standard , self-created short-forms in the chat message to standard words before translation ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed .", "The system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples .", "This system demonstration paper presents IRIS ( Informal Response Interactive System ) , a chat-oriented dialogue system based on the vector space model framework ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["platform , its main features , architecture , and an evaluation in a practical use case .", "To facilitate the creation and usage of custom SMT systems we have created a cloud-based platform for do-it-yourself MT .", "The platform is developed in the EU collaboration project LetsMT ! .", "This system demonstration paper presents the motivation in developing the LetsMT !"]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We describe the environment and components , the metrics that can be used for the evaluation of pedestrian route instruction-giving systems , and the shared challenge which is being organised using this environment .", "We demonstrate a web-based environment for development and testing of different pedestrian route instruction-giving systems .", "The environment contains a City Model , a TTS interface , a game-world , and a user GUI including a simulated street-view ."]}
{"orig_sents": ["3", "0", "1", "4", "2"], "shuf_sents": ["Given a multi-word expression as a query , the system involves retrieving sentence pairs from a bilingual corpus , identifying translation equivalents of the query in the sentence pairs ( translation spotting ) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents .", "To provide high-precision translation spotting for domain-specific translation tasks , we exploited a normalized correlation method to spot the translation equivalents .", "The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively .", "In this paper , we propose a web-based bilingual concordancer , DOMCAT 1 , for domain-specific computer assisted translation .", "To ranking the retrieved sentence pairs , we propose a correlation function modified from the Dice coefficient for assessing the correlation between the query and the translation equivalents ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The OpenGrm libraries use the OpenFst library to provide an efficient encoding of grammars and general algorithms for building , modifying and applying models .", "In this paper , we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules into finite-state transducers , and for n-gram language modeling ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["a wide-coverage multilingual lexical knowledge base ?", "and multilingual knowledge-rich Word Sense Disambiguation ( WSD ) .", "Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction .", "In this paper we present an API for programmatic access to BabelNet ?"]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["This paper introduces BIUTEE1 , an opensource system for recognizing textual entailment .", "Its main advantages are its ability to utilize various types of knowledge resources , and its extensibility by which new knowledge resources and inference components can be easily integrated .", "their knowledge resources and inference components .", "Notable assistance for these researchers is provided by a visual tracing tool , by which researchers can refine and ? debug ?", "These abilities make BIUTEE an appealing RTE system for two research communities : ( 1 ) researchers of end applications , that can benefit from generic textual inference , and ( 2 ) RTE researchers , who can integrate their novel algorithms and knowledge resources into our system , saving the time and effort of developing a complete RTE system from scratch ."]}
{"orig_sents": ["4", "0", "3", "1", "2"], "shuf_sents": ["The proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process .", "As a prominent use case , we apply our exploration system and illustrate its benefit on the health-care domain .", "To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation .", "A user of our system can explore the result space of a query by drilling down/up from one statement to another , according to entailment relations specified by an entailment graph and an optional concept taxonomy .", "We present a novel text exploration model , which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We present CSNIPER ( Corpus Sniper ) , a tool that implements ( i ) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and ( ii ) evaluation of annotation quality by measuring inter-rater agreement .", "This annotationby-query approach efficiently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools ."]}
{"orig_sents": ["0", "4", "3", "1", "5", "6", "2"], "shuf_sents": ["We present IlluMe , a software tool pack which creates a personalized ambient using the music and lighting .", "The ambient lighting can sparkle with different forms of light and the smart phone can broadcast music respectively according to different atmosphere .", "It works in a Chinese chatting environment to illustrate the language technology in life .", "The software analyzes emotional changes from instant message logs and corresponds the detected emotion to the best sound and light settings .", "IlluMe includes an emotion analysis software , the small space ambient lighting , and a multimedia controller .", "All settings can be modified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software .", "The IlluMe system , equipped with the learning function , provides a link between residential situation and personal emotion ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This component can be used to increase the responsivity and naturalness of spoken interactive systems .", "We present a component for incremental speech synthesis ( iSS ) and a set of applications that demonstrate its capabilities .", "While iSS can show its full strength in systems that generate output incrementally , we also discuss how even otherwise unchanged systems may profit from its capabilities ."]}
{"orig_sents": ["4", "1", "5", "2", "3", "0"], "shuf_sents": ["Preliminary results indicate that WizIE is a step forward towards enabling extractor development for novice IE developers .", "In order to satisfy the increasing text analytics demands of enterprise applications , it is crucial to enable developers with general computer science background to develop high quality IE extractors .", "WizIE provides an integrated wizard-like environment that guides IE developers step-by-step throughout the entire development process , based on best practices synthesized from the experience of expert developers .", "In addition , WizIE reduces the manual effort involved in performing key IE development tasks by offering automatic result explanation and rule discovery functionality .", "Information extraction ( IE ) is becoming a critical building block in many enterprise applications .", "In this demonstration , we present WizIE , an IE development environment intended to reduce the development life cycle and enable developers with little or no linguistic background to write high quality IE rules ."]}
{"orig_sents": ["3", "4", "1", "0", "5", "2"], "shuf_sents": ["In addition , sentiment analysis can help explore how these events affect public opinion .", "Emerging events or news are often followed almost instantly by a burst in Twitter volume , providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events .", "It offers the public , the media , politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion .", ", Dogan Can** , Abe Kazemzadeh** , Fran ? ois Bar* and Shrikanth Narayanan** Annenberg Innovation Laboratory ( AIL ) * Signal Analysis and Interpretation Laboratory ( SAIL ) ** University of Southern California , Los Angeles , CA { haowang @ , dogancan @ , kazemzad @ , fbar @ , shri @ sipi } .usc.edu Abstract This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter , a micro-blogging service .", "Twitter has become a central site where people express their opinions and views on political parties and candidates .", "While traditional content analysis takes days or weeks to complete , the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election , delivering results instantly and continuously ."]}
{"orig_sents": ["1", "0", "5", "6", "4", "3", "2"], "shuf_sents": ["The workbench is intended for specialists and nontechnical audiences alike , and provides the ever expanding library of analytics compliant with the Unstructured Information Management Architecture , a widely adopted interoperability framework .", "Argo is a web-based NLP and text mining workbench with a convenient graphical user interface for designing and executing processing workflows of various complexity .", "The experimental results performed on two tagging tasks , chunking and named entity recognition , showed that a tagger with a generic set of features built in Argo is capable of competing with taskspecific solutions .", "Users define the features of their choice directly from Argo ? s graphical interface , without resorting to programming ( a commonly used approach to feature engineering ) .", "The learning and tagging components are based on an implementation of conditional random fields ( CRF ) ; whereas the feature generation component is an analytic capable of extending basic token information to a comprehensive set of features .", "We explore the flexibility of this framework by demonstrating workflows involving three processing components capable of performing self-contained machine learning-based tagging .", "The three components are responsible for the three distinct tasks of 1 ) generating observations or features , 2 ) training a statistical model based on the generated features , and 3 ) tagging unlabelled data with the model ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["In terms of tree-to-string translation rule extraction , the toolkit implements the traditional maximum likelihood algorithm using PCFG trees ( Galley et al , 2004 ) and HPSG trees/forests ( Wu et al , 2010 ) .", "Akamon implements all of the algorithms required for tree/forestto-string decoding using tree-to-string translation rules : multiple-thread forest-based decoding , n-gram language model integration , beam- and cube-pruning , k-best hypotheses extraction , and minimum error rate training .", "We describe Akamon , an open source toolkit for tree and forest-based statistical machine translation ( Liu et al , 2006 ; Mi et al , 2008 ; Mi and Huang , 2008 ) ."]}
{"orig_sents": ["3", "0", "4", "1", "2"], "shuf_sents": ["The system uses attitude predictions to detect the split of discussants into subgroups of opposing views .", "The system is open source and is freely available for download .", "An online demo of the system is available at : http : //clair.eecs.umich.edu/SubgroupDetector/", "We present Subgroup Detector , a system for analyzing threaded discussions and identifying the attitude of discussants towards one another and towards the discussion topic .", "The system uses an unsupervised approach based on rule-based opinion target detecting and unsupervised clustering techniques ."]}
{"orig_sents": ["1", "3", "0", "4", "2"], "shuf_sents": ["To do so , we have set up an online graphical interface for the ASIYA toolkit , a rich repository of evaluation measures working at different linguistic levels .", "Error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the MT systems under development and allow fair comparisons among them .", "The intelligent visualization of the linguistic structures used by the metrics , as well as a set of navigational functionalities , may lead towards advanced methods for automatic error analysis .", "This work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations .", "The current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations , and word alignments ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["This paper explains how link prediction , information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia .", "We present UWN , a large multilingual lexical knowledge base that describes the meanings and relationships of words in over 200 languages .", "We additionally introduce extensions to cover lexical relationships , frame-semantic knowledge , and language data .", "An online interface provides human access to the data , while a software API enables applications to look up over 16 million words and names ."]}
{"orig_sents": ["5", "3", "4", "0", "1", "2"], "shuf_sents": ["On the other hand , people can also obtain various opinions from others in a few seconds even though they do not know each other .", "A typical approach to obtain required information is to use a search engine with some relevant keywords .", "We thus take the social media and forum as our major data source and aim at collecting specific issues efficiently and effectively in this work .", "strategy .", "With the rapid development of social network , people can now easily publish their opinions on the Internet .", "Social Event Radar is a new social networking-based service platform , that aim to alert as well as monitor any merchandise flaws , food-safety related issues , unexpected eruption of diseases or campaign issues towards to the Government , enterprises of any kind or election parties , through keyword expansion detection module , using bilingual sentiment opinion analysis tool kit to conclude the specific event social dashboard and deliver the outcome helping authorities to plan ? risk control ?"]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .", "This new edition introduces syntactic annotations : words are tagged with their part-of-speech , and headmodifier relationships are recorded .", "We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .", "The annotations are produced automatically with statistical models that are specifically adapted to historical text ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["As the algorithm generates dependency trees for partial translations left-to-right in decoding , it allows for efficient integration of both n-gram and dependency language models .", "To resolve conflicts in shift-reduce parsing , we propose a maximum entropy model trained on the derivation graph of training data .", "As our approach combines the merits of phrase-based and string-todependency models , it achieves significant improvements over the two baselines on the NIST Chinese-English datasets .", "We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation ."]}
{"orig_sents": ["1", "0", "3", "4", "2"], "shuf_sents": ["Unlike previous multi-stage pipeline approaches , which directly merge TM result into the final output , the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding .", "Since statistical machine translation ( SMT ) and translation memory ( TM ) complement each other in matched and unmatched regions , integrated models are proposed in this paper to incorporate TM information into phrase-based SMT .", "Besides , the proposed models also outperform previous approaches significantly .", "On a Chinese ? English TM database , our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4 .", "Furthermore , integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system ."]}
{"orig_sents": ["0", "6", "1", "2", "5", "4", "7", "3"], "shuf_sents": ["We derive variants of the fertility based models IBM-3 and IBM-4 that , while maintaining their zero and first order parameters , are nondeficient .", "The arising M-step energies are non-trivial and handled via projected gradient ascent .", "Our evaluation on gold alignments shows substantial improvements ( in weighted Fmeasure ) for the IBM-3 .", "BLEU scores .", "Training the nondeficient IBM-5 in the regular way gives surprisingly good results .", "For the IBM4 there are no consistent improvements .", "Subsequently , we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training .", "Using the resulting alignments for phrasebased translation systems offers no clear insights w.r.t ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning , and consistently outperform strong baselines .", "We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour .", "Annotating linguistic data is often a complex , time consuming and expensive endeavour .", "These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models , while explicitly representing correlations between models which can be learned directly from data .", "Even with strict annotation guidelines , human subjects often deviate in their analyses , each bringing different biases , interpretations of the task and levels of consistency ."]}
{"orig_sents": ["4", "0", "3", "1", "2"], "shuf_sents": ["Unlike Kneser-Ney , our approach is designed to be applied to any given smoothed backoff model , including models that have already been heavily pruned .", "We present experimental results for heavily pruned backoff ngram models , and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods .", "An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1", "As a result , the algorithm avoids issues observed when pruning Kneser-Ney models ( Siivola et al , 2007 ; Chelba et al , 2010 ) , while retaining the benefits of such marginal distribution constraints .", "We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions , along the lines of wellknown Kneser-Ney ( 1995 ) smoothing ."]}
{"orig_sents": ["3", "2", "0", "1", "4"], "shuf_sents": ["Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels , our labels are sentences containing nouns , verbs , prepositions , adjectives , and adverbs .", "The correspondence between words and concepts in the video is learned in an unsupervised fashion , even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences .", "Unlike prior work on learning language from symbolic input , our input consists of video of people interacting with multiple complex objects in outdoor environments .", "We present a method that learns representations for word meanings from short video clips paired with sentences .", "The learned word meanings can be subsequently used to automatically generate description of new video ."]}
{"orig_sents": ["0", "2", "1", "3", "5", "4"], "shuf_sents": ["Recent work on statistical quantifier scope disambiguation ( QSD ) has improved upon earlier work by scoping an arbitrary number and type of noun phrases .", "In this paper we report early , though promising , results for automatic QSD when handling both phenomena .", "No corpusbased method , however , has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation .", "We also present a general model for learning to build partial orders from a set of pairwise preferences .", "Finally , we significantly improve the performance of the previous model using a rich set of automatically generated features .", "We give an n log n algorithm for finding a guaranteed approximation of the optimal solution , which works very well in practice ."]}
{"orig_sents": ["3", "1", "4", "0", "2"], "shuf_sents": ["Experimental results show that our joint approach with local features outperforms the pipelined baseline , and adding global features further improves the performance significantly .", "By contrast , we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved .", "Our approach advances state-ofthe-art sentence-level event extraction , and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents .", "Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages , which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers .", "In addition , we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments ."]}
{"orig_sents": ["4", "3", "1", "2", "0"], "shuf_sents": ["We achieve state-of-the-art accuracy on all languages in the TempEval2 temporal normalization task , reporting a 4 % improvement in both English and Spanish accuracy , and to our knowledge the first results for four other languages .", "We make use of a latent parse that encodes a language-flexible representation of time , and extract rich features over both the parse and associated temporal semantics .", "The parameters of the model are learned using a weakly supervised bootstrapping approach , without the need for manually tuned parameters or any other language expertise .", "We present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference .", "Temporal resolution systems are traditionally tuned to a particular language , requiring significant human effort to translate them to new languages ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems .", "We propose a computationally efficient graph-based approach for local coherence modeling .", "We evaluate our system on three tasks : sentence ordering , summary coherence rating and readability assessment ."]}
{"orig_sents": ["0", "1", "3", "4", "2", "5"], "shuf_sents": ["Automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data .", "Important behavioral categories , though , are often sparse and often appear only in specific subsections of a conversation .", "We show the effectiveness of this technique in automated annotation of empowerment language in online support group chatrooms .", "This makes supervised machine learning difficult , through a combination of noisy features and unbalanced class distributions .", "We propose within-instance content selection , using cue features to selectively suppress sections of text and biasing the remaining representation towards minority classes .", "Our technique is significantly more accurate than multiple baselines , especially when prioritizing high precision ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["We describe an end-to-end discriminative probabilistic model for coreference that , along with standard pairwise features , enforces structural agreement constraints between specified properties of coreferent mentions .", "We show that our method can use entity-level information to outperform a basic pairwise system .", "Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions .", "This model can be represented as a factor graph for each document that admits efficient inference via belief propagation ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation , part-of-speech ( POS ) tagging and phrase-structure parsing system that integrates character-structure features .", "Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test , and gives the best published results for Chinese parsing .", "Characters play an important role in the Chinese language , yet computational processing of Chinese has been dominated by word-based approaches , with leaves in syntax trees being words .", "We investigate Chinese parsing from the character-level , extending the notion of phrase-structure trees by annotating internal structures of words ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The new strategy allows the parser to postpone difficult decisions until the relevant information becomes available .", "The novel parser has a ? 12 % error reduction in unlabeled attachment score over an arc-eager parser , with a slow-down factor of 2.8 .", "We present a novel transition-based , greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation .", "We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter .", "Binarization of grammars is crucial for improving the complexity and performance of parsing and translation ."]}
{"orig_sents": ["3", "2", "5", "4", "1", "0"], "shuf_sents": ["In our experiments , our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models .", "It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data .", "In decoding , a distortion model estimates the source word position to be translated next ( NP ) given the last translated source word position ( CP ) .", "This paper proposes new distortion models for phrase-based SMT .", "Moreover , we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates .", "We propose a distortion model that can consider the word at the CP , a word at an NP candidate , and the context of the CP and the NP candidate simultaneously ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We describe in detail how we adapt and extend the CD-DNNHMM ( Dahl et al , 2012 ) method introduced in speech recognition to the HMMbased word alignment model , in which bilingual word embedding is discriminatively learnt to capture lexical translation information , and surrounding words are leveraged to model context information in bilingual sentences .", "Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score .", "While being capable to model the rich bilingual correspondence , our method generates a very compact model with much fewer parameters .", "In this paper , we explore a novel bilingual word alignment approach based on DNN ( Deep Neural Network ) , which has been proven to be very effective in various machine learning tasks ( Collobert et al , 2011 ) ."]}
{"orig_sents": ["5", "2", "3", "0", "4", "1"], "shuf_sents": ["We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo ( the Chinese counterpart of Twitter ) using only their public APIs .", "The resources in described in this paper are available at http : //www.cs.cmu.edu/ ? lingwang/utopia .", "translations .", "We present an efficient method for detecting these messages and extracting parallel segments from them .", "As a supplement to existing parallel training data , our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary .", "In the ever-expanding sea of microblog data , there is a surprising amount of naturally occurring parallel text : some users create post multilingual messages targeting international audiences while others ? retweet ?"]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use : 1 ) response variables are usually over-weighted by document word counts ; and 2 ) existing variational inference methods make strict mean-field assumptions .", "We address these issues by : 1 ) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference ; and 2 ) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables .", "Empirical results demonstrate significant improvements on prediction performance and time efficiency .", "Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["In addition , we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression .", "We present a dual decomposition framework for multi-document summarization , using a model that jointly extracts and compresses sentences .", "Compared with previous work based on integer linear programming , our approach does not require external solvers , is significantly faster , and is modular in the three qualities a summary should have : conciseness , informativeness , and grammaticality .", "Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date , with runtimes that rival those of extractive summarizers ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["By jointly modeling the text of the document and the noisy ( but regular ) process of rendering glyphs , our unsupervised system is able to decipher font structure and more accurately transcribe images into text .", "We present a generative probabilistic model , inspired by historical printing processes , for transcribing images of documents from the printing press era .", "Overall , our system substantially outperforms state-of-the-art solutions for this task , achieving a 31 % relative reduction in word error rate over the leading commercial system for historical transcription , and a 47 % relative reduction over Tesseract , Google ? s open source OCR system ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["Instead , we show how the weak supervision of response feedback ( e.g .", "We adapt discriminative reranking to improve the performance of grounded language acquisition , specifically the task of learning to follow navigation instructions from observation .", "Unlike conventional reranking used in syntactic and semantic parsing , gold-standard reference trees are not naturally available in a grounded setting .", "successful task completion ) can be used as an alternative , experimentally demonstrating that its performance is comparable to training on gold-standard parse trees ."]}
{"orig_sents": ["4", "1", "2", "3", "0"], "shuf_sents": ["We also show that UCCA can be effectively and quickly learned by annotators with no linguistic background , and describe the compilation of a UCCAannotated corpus .", "This renders them sensitive to formal variation both within and across languages , and limits their value to semantic applications .", "We present UCCA , a novel multi-layered framework for semantic representation that aims to accommodate the semantic distinctions expressed through linguistic utterances .", "We demonstrate UCCA ? s portability across domains and languages , and its relative insensitivity to meaning-preserving syntactic variation .", "Syntactic structures , by their nature , reflect first and foremost the formal constructions used for expressing meanings ."]}
{"orig_sents": ["2", "7", "5", "0", "3", "6", "1", "4"], "shuf_sents": ["This requires robust modeling and understanding of the semantics of short texts .", "We show that using tweet specific feature ( hashtag ) and news specific feature ( named entities ) as well as temporal constraints , we are able to extract text-to-text correlations , and thus completes the semantic picture of a short text .", "Many current Natural Language Processing techniques work well assuming a large context of text as input data .", "The contribution of the paper is two-fold : 1. we introduce the Linking-Tweets-toNews task as well as a dataset of linked tweet-news pairs , which can benefit many NLP applications ; 2. in contrast to previous research which focuses on lexical features within the short texts ( text-to-word information ) , we propose a graph based latent variable model that models the inter short text correlations ( text-to-text information ) .", "Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task .", "To overcome the issue , we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks .", "This is motivated by the observation that a tweet usually only covers one aspect of an event .", "However they become ineffective when applied to short texts such as Twitter feeds ."]}
{"orig_sents": ["3", "4", "0", "5", "6", "1", "2"], "shuf_sents": ["These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory , such as indirection , deference , impersonalization and modality .", "We see a similar negative correlation between politeness and power on Stack Exchange , where users at the top of the reputation scale are less polite than those at the bottom .", "Finally , we apply our classifier to a preliminary analysis of politeness variation by gender and community .", "We propose a computational framework for identifying linguistic aspects of politeness .", "Our starting point is a new corpus of requests annotated for politeness , which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context .", "Our classifier achieves close to human performance and is effective across domains .", "We use our framework to study the relationship between politeness and social power , showing that polite Wikipedia editors are more likely to achieve high status through elections , but , once elevated , they become less polite ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Recently , researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence , technical errors , and relevance to prompt , but there is relatively little work on modeling thesis clarity .", "Additionally , in order to provide more valuable feedback on why an essay is scored as it is , we propose a second learning-based approach to identifying what kinds of errors an essay has that may lower its thesis clarity score .", "We present a new annotated corpus and propose a learning-based approach to scoring essays along the thesis clarity dimension ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["Initial experiments to learn the four possible translations with Decision Trees give promising results .", "We present a corpus analysis of how Italian connectives are translated into LIS , the Italian Sign Language .", "Since corpus resources are scarce , we propose an alignment method between the syntactic trees of the Italian sentence and of its LIS translation .", "We translate these findings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system .", "This method , and clustering applied to its outputs , highlight the different ways a connective can be rendered in LIS : with a corresponding sign , by affecting the location or shape of other signs , or being omitted altogether ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In this paper , we exploit a prior knowledge of STOP-probabilities ( whether a given word has any children in a given direction ) , which is obtained from a large raw corpus using the reducibility principle .", "By incorporating this knowledge into Dependency Model with Valence , we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks .", "Even though the quality of unsupervised dependency parsers grows , they often fail in recognition of very basic dependencies ."]}
{"orig_sents": ["0", "6", "7", "2", "1", "3", "4", "5"], "shuf_sents": ["In this paper , we consider the problem of cross-formalism transfer in parsing .", "The model includes features of both parses , allowing transfer between the formalisms , while preserving parsing efficiency .", "To handle this apparent discrepancy , we design a probabilistic model that jointly generates CFG and target formalism parses .", "We evaluate our approach on three constituency-based grammars ?", "CCG , HPSG , and LFG , augmented with the Penn Treebank-1 .", "Our experiments show that across all three formalisms , the target parsers significantly benefit from the coarse annotations.1", "We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism , and a large quantity of coarse CFG annotations from the Penn Treebank .", "While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG , they also encode additional constraints and semantic features ."]}
{"orig_sents": ["4", "2", "1", "3", "0"], "shuf_sents": ["Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible .", "This collapsed context-free form is used to implement efficient grammar estimation and parsing algorithms .", "We provide a transformation to context-free form , and a further reduction in probabilistic model size through factorization and pooling of parameters .", "We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design .", "We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity ."]}
{"orig_sents": ["1", "4", "0", "5", "3", "2"], "shuf_sents": ["Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems .", "We present a fast and scalable online method for tuning statistical machine translation models with large feature sets .", "Equally important is our analysis , which suggests techniques for mitigating overfitting and domain mismatch , and applies to other recent discriminative methods for machine translation .", "Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features .", "The standard tuning algorithm ? MERT ? only scales to tens of features .", "Our method , which is based on stochastic gradient descent with an adaptive learning rate , scales to millions of features and tuning sets with tens of thousands of sentences , while still converging after only a few epochs ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["In this paper , we propose a novel reordering model based on sequence labeling techniques .", "Our model converts the reordering problem into a sequence labeling problem , i.e .", "Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average .", "a tagging task .", "Results of comparative study with other seven widely used reordering models will also be reported ."]}
{"orig_sents": ["3", "2", "5", "0", "4", "1"], "shuf_sents": ["Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing .", "Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines , of up to +3.4 BLEU .", "However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs .", "Most modern machine translation systems use phrase pairs as translation units , allowing for accurate modelling of phraseinternal translation and reordering .", "This mechanism implicitly supports not only traditional phrase pairs , but also gapping phrases which are non-consecutive in the source .", "We propose a new model to address this imbalance , based on a word-based Markov model of translation which generates target translations left-to-right ."]}
{"orig_sents": ["4", "3", "5", "2", "0", "1"], "shuf_sents": ["We present a novel learning algorithm , which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus .", "In experiments with text topic classification and sentiment analysis , we show that our method is both more scalable and more accurate than SSL techniques from previous work .", "In this paper , we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets .", "SSL techniques are often effective in text classification , where labeled data is scarce but large unlabeled corpora are readily available .", "Semi-supervised learning ( SSL ) methods augment standard machine learning ( ML ) techniques to leverage unlabeled data .", "However , existing SSL techniques typically require multiple passes over the entirety of the unlabeled data , meaning the techniques are not applicable to large corpora being produced today ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["As the first attempt to solve this problem explicitly , we also present a new dataset for the text-driven analysis of film , along with a benchmark testbed to help drive future work in this area .", "We present two latent variable models for learning character types , or personas , in film , in which a persona is defined as a set of mixtures over latent lexical classes .", "These lexical classes capture the stereotypical actions of which a character is the agent and patient , as well as attributes by which they are described ."]}
{"orig_sents": ["2", "4", "1", "5", "3", "0"], "shuf_sents": ["We also report for the first time ? BLEU score results for a largescale MT task using only non-parallel data ( EMEA corpus ) .", "In order to perform fast , efficient Bayesian inference in this framework , we then derive a hash sampling strategy that is inspired by the work of Ahmed et al ( 2012 ) .", "In this paper , we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora .", "We show empirical results on the OPUS data ? our method yields the best BLEU scores compared to existing approaches , while achieving significant computational speedups ( several orders faster ) .", "Following a probabilistic decipherment approach , we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features ( besides simple bag-of-words ) as side-information used for estimating translation models .", "The new translation hash sampler enables us to scale elegantly to complex models ( for the first time ) and large vocabulary/corpora sizes ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This paper describes the creation of a semantic relation inventory covering the use of ? s , an inter-annotator agreement study to calculate how well humans can agree on the relations , a large collection of possessives annotated according to the relations , and an accurate automatic annotation system for labeling new examples .", "The English ? s possessive construction occurs frequently in text and can encode several different semantic relations ; however , it has received limited attention from the computational linguistics community .", "Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of , and both our automatic classification system , which achieves 87.4 % accuracy in our classification experiment , and our annotation data are publicly available ."]}
{"orig_sents": ["2", "6", "0", "1", "5", "4", "3"], "shuf_sents": ["We explore two approaches for acquiring numerical common sense .", "Both approaches start with extracting numerical expressions and their context from the Web .", "This paper presents novel methods for modeling numerical common sense : the ability to infer whether a given number ( e.g. , three billion ) is large , small , or normal for a given context ( e.g. , number of people facing a water shortage ) .", "Experimental results demonstrate the effectiveness of both approaches .", "Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression .", "One approach estimates the distribution of numbers co-occurring within a context and examines whether a given value is large , small , or normal , based on the distribution .", "We first discuss the necessity of numerical common sense in solving textual entailment problems ."]}
{"orig_sents": ["0", "2", "4", "3", "1"], "shuf_sents": ["Generative probabilistic models have been used for content modelling and template induction , and are typically trained on small corpora in the target domain .", "In a subsequent extrinsic evaluation , we show that these improvements are also reflected in multi-document summarization .", "In contrast , vector space models of distributional semantics are trained on large corpora , but are typically applied to domaingeneral lexical disambiguation tasks .", "Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain .", "We introduce Distributional Semantic Hidden Markov Models , a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions ."]}
{"orig_sents": ["3", "1", "4", "5", "0", "2", "6"], "shuf_sents": ["The performance of our classifier reaches the 100 % precision level for many language pairs .", "In our approach we treat bilingual term extraction as a classification problem .", "We also perform manual evaluation on bilingual terms extracted from English-German term-tagged comparable corpora .", "In this paper we present a method for extracting bilingual terminologies from comparable corpora .", "For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus .", "We test our approach on a held-out test set from EUROVOC and perform precision , recall and f-measure evaluations for 20 European language pairs .", "The results of this manual evaluation showed 60-83 % of the term pairs generated are exact translations and over 90 % exact or partial translations ."]}
{"orig_sents": ["0", "1", "4", "3", "5", "2"], "shuf_sents": ["Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification .", "A plausible reason for such a performance improvement is the reduction in data sparsity .", "Similar idea is applied to Cross Lingual Sentiment Analysis ( CLSA ) , and it is shown that reduction in data sparsity ( after translation or bilingual-mapping ) produces accuracy higher than Machine Translation based CLSA and sense based CLSA .", "In this paper , the problem of data sparsity in sentiment analysis , both monolingual and cross-lingual , is addressed through the means of clustering .", "However , such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering .", "Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Supervised training procedures for semantic parsers produce high-quality semantic parsers , but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data .", "Leveraging techniques from each of these areas , we develop a semantic parser for Freebase that is capable of parsing questions with an F1 that improves by 0.42 over a purely-supervised learning algorithm .", "We present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms , schema matching , and pattern learning ."]}
{"orig_sents": ["5", "4", "0", "1", "3", "2"], "shuf_sents": ["This turns out to have a large empirical impact on the framework of global training and beam search .", "We propose a simple yet effective extension to the shift-reduce process , which eliminates size differences between action sequences in beam-search .", "With linear run-time complexity , our parser is over an order of magnitude faster than the fastest chart parser .", "Our parser gives comparable accuracies to the state-of-the-art chart parsers .", "One important reason is the existence of unary nodes in phrase structure trees , which leads to different numbers of shift-reduce actions between different outputs for the same input .", "Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts , yet the best shiftreduce constituent parsers still lag behind the state-of-the-art ."]}
{"orig_sents": ["1", "5", "3", "7", "6", "2", "4", "9", "8", "0"], "shuf_sents": ["Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem , they sometimes find better solutions than Viterbi EM with random restarts , in the same time .", "Many models in NLP involve latent variables , such as unknown parses , tags , or alignments .", "As an illustrative case , we study a generative model for dependency parsing .", "The usual practice is to settle for local optimization methods such as EM or gradient ascent .", "We search for the maximum-likelihood model parameters and corpus parse , subject to posterior constraints .", "Finding the optimal model parameters is then usually a difficult nonconvex optimization problem .", "Our method would eventually find the global maximum ( up to a user-specified ) if run for long enough , but at any point can return a suboptimal solution together with an upper bound on the global maximum .", "We explore how one might instead search for a global optimum in parameter space , using branch-and-bound .", "We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound .", "We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints ."]}
{"orig_sents": ["4", "2", "0", "1", "3"], "shuf_sents": ["The CVG improves the PCFG of the Stanford Parser by 3.8 % to obtain an F1 score of 90.4 % .", "It is fast to train and implemented approximately as an efficient reranker it is about 20 % faster than the current Stanford factored parser .", "Instead , we introduce a Compositional Vector Grammar ( CVG ) , which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic , compositional vector representations .", "The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments .", "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP , but this representation does not capture the full syntactic nor semantic richness of linguistic phrases , and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness ."]}
{"orig_sents": ["4", "3", "0", "1", "2"], "shuf_sents": ["In this paper we discuss these limitations and introduce a new approach for discriminative state tracking that overcomes them by leveraging the problem structure .", "An offline evaluation with dialog data collected from real users shows improvements in both state tracking accuracy and the quality of the posterior probabilities .", "Features that encode speech recognition error patterns are particularly helpful , and training requires relatively few dialogs .", "Current approaches based on generative or discriminative models have different but important shortcomings that limit their accuracy .", "In spoken dialog systems , statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states ."]}
{"orig_sents": ["2", "0", "3", "1", "4"], "shuf_sents": ["However , a previous study ( Sporleder and Lascarides , 2008 ) showed that models trained on these synthetic data do not generalize very well to natural ( i.e .", "In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition .", "To overcome the shortage of labeled data for implicit discourse relation recognition , previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples .", "genuine ) implicit discourse data .", "Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks , can achieve an averaged F1 improvement of 5.86 % over baseline models ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields : one for intrasentential parsing and the other for multisentential parsing .", "We present two approaches to combine these two stages of discourse parsing effectively .", "A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art , often by a wide margin .", "We propose a novel approach for developing a two-stage document-level discourse parser ."]}
{"orig_sents": ["1", "4", "0", "3", "2", "5"], "shuf_sents": ["In effect , our approach finds an optimal feature space ( derived from a base feature set and indicator set ) for discriminating coreferential mention pairs .", "This paper proposes a new method for significantly improving the performance of pairwise coreference models .", "Our experiments on the CoNLL-2012 Shared Task English datasets ( gold mentions ) indicate that our method is robust relative to different clustering strategies and evaluation metrics , showing large and consistent improvements over a single pairwise model using the same base features .", "Although our approach explores a very large space of possible feature spaces , it remains tractable by exploiting the structure of the hierarchies built from the indicators .", "Given a set of indicators , our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models .", "Our best system obtains a competitive 67.2 of average F1 over MUC , B3 , and CEAF which , despite its simplicity , places it above the mean score of other systems on these datasets ."]}
{"orig_sents": ["0", "2", "5", "3", "1", "4"], "shuf_sents": ["Techniques that compare short text segments using dependency paths ( or simply , paths ) appear in a wide range of automated language processing applications including question answering ( QA ) .", "Informative catenae are selected using supervised machine learning with linguistically informed features and compared to both non-linguistic terms and catenae selected heuristically with filters derived from work on paths .", "However , few models in ad hoc information retrieval ( IR ) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection .", "These chains , or catenae , are readily applied in standard IR models .", "Automatically selected catenae of 1-2 words deliver significant performance gains on three TREC collections .", "In this paper , we introduce a flexible notion of paths that describe chains of words on a dependency path ."]}
{"orig_sents": ["0", "4", "5", "2", "3", "1"], "shuf_sents": ["Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms .", "In addition , empirical observations on convertibility between selected styles of representations are shown too .", "This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures .", "We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages .", "This has painful consequences such as high frequency of parsing errors related to coordination .", "In other words , coordination is a pending problem in dependency analysis of natural languages ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages .", "For each language of interest , given a small number of hypernymy relation seeds concerning a target domain , we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns .", "We present GlossBoot , an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages ."]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["Yet , to create annotated linguistic resources from this data , we face the challenge of having to combine the judgements of a potentially large group of annotators .", "In this paper we investigate how to aggregate individual annotations into a single collective annotation , taking inspiration from the field of social choice theory .", "We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule .", "Crowdsourcing , which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online , has revolutionised the collection of labelled data .", "We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation ."]}
{"orig_sents": ["0", "5", "3", "1", "2", "4"], "shuf_sents": ["This paper discusses the construction of a parallel treebank currently involving ten languages from six language families .", "This output forms the basis of a parallel treebank covering a diverse set of phenomena .", "The treebank is publicly available via the INESS treebanking environment , which also allows for the alignment of language pairs .", "The grammars produce output that is maximally parallelized across languages and language families .", "We thus present a unique , multilayered parallel treebank that represents more and different types of languages than are available in other treebanks , that represents deep linguistic knowledge and that allows for the alignment of sentences at several levels : dependency structures , constituency structures and POS information .", "The treebank is based on deep LFG ( LexicalFunctional Grammar ) grammars that were developed within the framework of the ParGram ( Parallel Grammar ) effort ."]}
{"orig_sents": ["3", "2", "0", "6", "5", "1", "4"], "shuf_sents": ["As a consequence , improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures .", "Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors .", "However , they are far from containing only interesting semantic relations .", "Distributional thesauri are now widely used in a large number of Natural Language Processing tasks .", "We evaluate the interest of this method for a large set of English nouns with various frequencies .", "This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts .", "In this article , we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images .", "We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes .", "We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning .", "We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data ."]}
{"orig_sents": ["1", "5", "3", "0", "4", "2"], "shuf_sents": ["We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy ( as well as English ) and evaluate how the amounts of various kinds of data affect performance of a trained POS-tagger .", "Developing natural language processing tools for low-resource languages often requires creating resources from scratch .", "We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available .", "We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging .", "Our results show that annotation of word types is the most important , provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus .", "While a variety of semi-supervised methods exist for training from incomplete data , there are open questions regarding what types of training data should be used and how much is necessary ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper demonstrates the need and impact of subcategorization information for SMT .", "A manual evaluation of an English-toGerman translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case .", "We combine ( i ) features on sourceside syntactic subcategorization and ( ii ) an external knowledge base with quantitative , dependency-based information about target-side subcategorization frames ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Additionally , we also propose a new MT metric to appropriately evaluate the translation quality of informative words , by assigning different weights to different words according to their importance values in a document .", "We propose a Name-aware Machine Translation ( MT ) approach which can tightly integrate name processing into MT model , by jointly annotating parallel corpora , extracting name-aware translation grammar and rules , adding name phrase table and name translation driven decoding .", "Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation , name translation and word alignment over a high-quality MT baseline1 ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["To the best of our knowledge , this connection between the QAP and the decipherment problem has not been known in the literature before .", "In this paper we show that even for the case of 1:1 substitution ciphers ? which encipher plaintext symbols by exchanging them with a unique substitute ? finding the optimal decipherment with respect to a bigram language model is NP-hard .", "We show that in this case the decipherment problem is equivalent to the quadratic assignment problem ( QAP ) ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper studies the problem of nonmonotonic sentence alignment , motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically , and proposes a semisupervised learning approach based on two assumptions : ( 1 ) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other ; and ( 2 ) initial alignment is readily available with existing alignment techniques .", "The evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity , this approach can work effectively on both monotonic and non-monotonic data .", "They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Our experimental results on English-Chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for organization names , and 0.14 in a low comparability case .", "found in parallel corpora , the proposed method is tolerant to asymmetry often found in comparable corpora , by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora .", "Unlike the previous approaches relying on the ? symmetry ?", "This paper studies the problem of mining named entity translations from comparable corpora with some ? asymmetry ? ."]}
{"orig_sents": ["5", "1", "2", "4", "3", "0"], "shuf_sents": ["Our experimental results demonstrate that WikiCiKE outperforms the monolingual knowledge extraction method and the translation-based method .", "However , infobox information is very incomplete and imbalanced among the Wikipedias in different languages .", "It is a promising but challenging problem to utilize the rich structured knowledge from a source language Wikipedia to help complete the missing infoboxes for a target language .", "An instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors .", "In this paper , we formulate the problem of cross-lingual knowledge extraction from multilingual Wikipedia sources , and present a novel framework , called WikiCiKE , to solve this problem .", "Wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing ."]}
{"orig_sents": ["4", "3", "0", "1", "5", "2"], "shuf_sents": ["In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents , and investigate the performance of a model built on English data , on Italian test data ( and viceversa ) .", "The results show not only statistically significant , but a large improvement ?", "over the raw ( vanilla bag-ofwords ) representation .", "We support this hypothesis with experiments in crosslanguage ( English-Italian ) document categorization .", "We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages .", "a jump of almost 40 points in F1-score ?"]}
{"orig_sents": ["8", "6", "10", "1", "3", "2", "4", "9", "7", "0", "5"], "shuf_sents": ["After modeling , we use the modeling results to classify the nature of interaction of each user pair .", "In this paper , we focus on identifying the nature of interactions among user pairs .", "Does the pair of users mostly agree or disagree ?", "The central questions are : How does each pair of users interact with each other ?", "What is the lexicon that people often use to express agreement and disagreement ?", "Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques .", "In forums where users discuss social , political , or religious issues , there are often heated debates among users or participants .", "Since agreement and disagreement expressions are usually multiword phrases , we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling .", "Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest .", "We present a topic model based approach to answer these questions .", "Existing research has studied mining of user stances or camps on certain issues , opposing perspectives , and contention points ."]}
{"orig_sents": ["2", "3", "5", "6", "0", "7", "1", "4"], "shuf_sents": ["is an important challenging problem , which has been of great interest to the research community .", "We present novel algorithms that integrate triggers for cognitive , affective , perceptual and social processes with stylistic and lexical information .", "Metaphor is an important way of conveying the affect of people , hence understanding how people use metaphors to convey affect is important for the communication between individuals and increases cohesion if the perceived affect of the concrete example is the same for the two individuals .", "Therefore , building computational models that can automatically identify the affect in metaphor-rich texts like ? The team captain is a rock .", "By running evaluations on datasets in English , Spanish , Russian and Farsi , we show that the developed affect polarity and valence prediction technology of metaphor-rich texts is portable and works equally well for different languages .", "? , ? Time is money .", "? , ? My lawyer is a shark . ?", "To solve this task , we have collected and manually annotated the affect of metaphor-rich texts for four languages ."]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages ( which require large lexical tagset inventories ) .", "For this reason , a number of alternative methods have been proposed over the years .", "In this paper we present an alternative method to Tiered Tagging , based on local optimizations with Neural Networks and we show how , by properly encoding the input sequence in a general Neural Network architecture , we achieve results similar to the Tiered Tagging methodology , significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method .", "A second phase is aimed at recovering the full set of morpho-syntactic features .", "One of the most successful methods used for this task , FDOOHG7LHUHG7DJJLQJ 7XIL , 1999 ) , exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We perform evaluation , which shows results similar to those obtained earlier by a rule-based system , while our approach allows to separate chunking from lemmatisation .", "We present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem .", "The idea draws on the observation that the lemmatisation of almost all Polish noun phrases may be decomposed into transformation of singular words ( tokens ) that make up each phrase ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["For example , we learn that people in the Female class tend to have maiden names and engagement rings .", "We describe a novel approach for automatically predicting the hidden demographic properties of social media users .", "Our novel approach enables substantial improvements on the widelystudied task of user gender prediction , obtaining a 20 % relative error reduction over the current state-of-the-art .", "We then show that this knowledge can be used in the analysis of first-person communication ; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples .", "Building on prior work in common-sense knowledge acquisition from third-person text , we first learn the distinguishing attributes of certain classes of people ."]}
{"orig_sents": ["5", "1", "3", "6", "2", "0", "4"], "shuf_sents": ["We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles .", "However , only a small amount of annotated data is available for training quality assessment systems .", "We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results .", "Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws .", "This approach better reflects the situation a classifier would face in a real-life application .", "With the increasing amount of user generated reference texts in the web , automatic quality assessment has become a key challenge .", "We show that the distribution of these labels is topically biased , since they can not be applied freely to any arbitrary article ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["A key problem is the lack of word delimiters in Chinese .", "We exploit this reliance as an opportunity : recognizing the relation between informal word recognition and Chinese word segmentation , we propose to model the two tasks jointly .", "Our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially .", "We address the problem of informal word recognition in Chinese microblogs ."]}
{"orig_sents": ["1", "3", "4", "2", "0"], "shuf_sents": ["We tested the suggestions generated by our algorithm via a Mechanical Turk experiment , which showed a significant improvement over the strongest baseline of more than 45 % in all metrics .", "We introduce the novel task of automatically generating questions that are relevant to a text but do not appear in it .", "We present the first algorithm for the task , which consists of : ( a ) offline construction of a comparable question template database ; ( b ) ranking of relevant templates to a given article ; and ( c ) instantiation of templates only with entities in the article whose comparison under the template ? s relation makes sense .", "One motivating example of its application is for increasing user engagement around news articles by suggesting relevant comparable questions , such as ? is Beyonce a better singer than Madonna ?", "? , for the user to answer ."]}
{"orig_sents": ["2", "0", "3", "4", "1"], "shuf_sents": ["This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts .", "The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text .", "Punctuations are not available in automatic speech recognition outputs , which could create barriers to many subsequent text processing tasks .", "Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right .", "It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["With Chinese word segmentation as a case study , experiments show that the segmenter enhanced with the Chinese wikipedia achieves significant improvement on a series of testing sets from different domains , even with a single classifier and local features .", "In this paper we propose a discriminative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the Internet .", "Structural information in web text provides natural annotations for NLP problems such as word segmentation and parsing .", "It utilizes the Internet as an external corpus with massive ( although slight and sparse ) natural annotations , and enables a classifier to evolve on the large-scaled and real-time updated web text ."]}
{"orig_sents": ["1", "2", "3", "4", "0", "5"], "shuf_sents": ["An inductive character-based joint model is obtained eventually .", "This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging .", "The proposed approach is based on a graph-based label propagation technique .", "One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information , i.e. , label distributions .", "The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields ( CRFs ) on unlabeled data .", "Empirical results on Chinese tree bank ( CTB-7 ) and Microsoft Research corpora ( MSR ) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task ."]}
{"orig_sents": ["2", "1", "4", "0", "3"], "shuf_sents": ["Overall this leads to a model which learns translations of entire sentences , while also learning their decomposition into smaller units ( phrase-pairs ) recursively , terminating at word translations .", "This is problematic , as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation .", "Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora .", "Our experiments on Arabic , Urdu and Farsi to English demonstrate improvements over competitive baseline systems .", "This paper presents a novel method for inducing phrase-based translation units directly from parallel data , which we frame as learning an inverse transduction grammar ( ITG ) using a recursive Bayesian prior ."]}
{"orig_sents": ["3", "2", "6", "5", "4", "0", "7", "1"], "shuf_sents": ["additive neural networks , for SMT to go beyond the log-linear translation model .", "Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks .", "Although the log-linear model achieves success in SMT , it still suffers from some limitations : ( 1 ) the features are required to be linear with respect to the model itself ; ( 2 ) features can not be further interpreted to reach their potential .", "Most statistical machine translation ( SMT ) systems are modeled using a loglinear framework .", "In this paper , we propose a variant of a neural network , i.e .", "However , modeling SMT with a neural network is not trivial , especially when taking the decoding efficiency into consideration .", "A neural network is a reasonable method to address these pitfalls .", "In addition , word embedding is employed as the input to the neural network , which encodes each word as a feature vector ."]}
{"orig_sents": ["6", "5", "1", "4", "3", "0", "2"], "shuf_sents": ["The performance measured by BLEU is at least as comparable to the traditional batch training method .", "In face of the problem , we propose an efficient phrase table combination method .", "Furthermore , each phrase table is trained separately in each domain , and while computational overhead is significantly reduced by training them in parallel .", "The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process .", "In particular , we train a Bayesian phrasal inversion transduction grammars for each domain separately .", "With the growth of the available data across different domains , it is computationally demanding to perform batch training every time when new data comes .", "Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation .", "We present a new translation model integrating the shallow local multi bottomup tree transducer .", "We perform a largescale empirical evaluation of our obtained system , which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English ? German translation task ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We show that the recovered empty categories not only improve the word alignment quality , but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system .", "ECs are ubiquitous in languages like Chinese , but they are tacitly ignored in most machine translation ( MT ) work because of their elusive nature .", "Empty categories ( EC ) are artificial elements in Penn Treebanks motivated by the government-binding ( GB ) theory to explain certain language phenomena such as pro-drop .", "In this paper we present a comprehensive treatment of ECs by first recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features , and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches , including the extraction of EC-specific sparse features ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["While domain adaptation techniques for SMT have proven to be effective at improving translation quality , their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains .", "We also describe a method for unsupervised adaptation with development and test data from multiple domains .", "We present an architecture that delays the computation of translation model features until decoding , allowing for the application of mixture-modeling techniques at decoding time .", "Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering , with gains of up to 1 BLEU over unadapted systems and single-domain adaptation ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["In particular , we extend the monolingual infinite tree model ( Finkel et al , 2007 ) to a bilingual scenario : each hidden state ( POS tag ) of a source-side dependency tree emits a source word together with its aligned target word , either jointly ( joint model ) , or independently ( independent model ) .", "Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system .", "This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech ( POS ) tags in dependency trees to improve the performance of statistical machine translation ( SMT ) .", "Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model ."]}
{"orig_sents": ["5", "0", "3", "2", "4", "8", "6", "1", "7"], "shuf_sents": ["In this paper , we focus on the problem of question retrieval .", "Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization .", "However , the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA .", "Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users .", "State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models .", "Community question answering ( CQA ) has become an increasingly popular research topic .", "In this work , we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages .", "Experiments conducted on a real CQA data show that our proposed approach is promising .", "While useful , the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora ( e.g. , question-answer pairs ) in the absence of which they are troubled by noise issue ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1 .", "We present the first unified framework for unsupervised learning of these three types of information .", "We show how to utilize Determinantal Point Processes ( DPPs ) , elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets , for clustering .", "Subcategorization frames ( SCFs ) , selectional preferences ( SPs ) and verb classes capture related aspects of the predicateargument structure .", "Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs ."]}
{"orig_sents": ["2", "0", "3", "5", "4", "6", "1"], "shuf_sents": ["There has been much work on semantic frame parsers , but less that applies them to general NLP problems .", "Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task .", "Semantic frames are a rich linguistic resource .", "We address a task to predict change in stock price from financial news .", "We introduce a novel tree representation , and use it to train predictive models with tree kernels using support vector machines .", "Semantic frames help to generalize from specific sentences to scenarios , and to detect the ( positive or negative ) roles of specific companies .", "Our experiments test multiple text representations on two binary classification tasks , change of price and polarity ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached .", "This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora .", "Through the smoothing , all the optimal senses are obtained at one time under maximum marginal likelihood criterion , by competitive probabilistic kernels made to reinforce one another among nearby words , and to suppress conflicting sense hypotheses within the same word .", "By generalizing discrete senses to a continuum , we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense , as well as to exploit senseinterdependency among the words in the same text string ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders .", "This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence .", "We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks , demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general .", "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing ."]}
{"orig_sents": ["5", "7", "3", "2", "4", "0", "1", "6"], "shuf_sents": ["These parts are then combined to a global coherent solution using Lagrangian relaxation .", "In our experiments , using the NLP tasks of semantic role labeling and entityrelation extraction , we demonstrate that with the margin-based algorithm , we need to call the inference engine only for a third of the test examples .", "Second , we introduce decomposed amortized inference , which is designed to address very large inference problems , where earlier amortization methods become less effective .", "In this paper , first , we introduce a new amortized inference algorithm called the Margin-based Amortized Inference , which uses the notion of structured margin to identify inference problems for which previous solutions are provably optimal .", "This approach works by decomposing the output structure and applying amortization piece-wise , thus increasing the chance that we can re-use previous solutions for parts of the output structure .", "Given that structured output prediction is typically performed over entire datasets , one natural question is whether it is possible to re-use computation from earlier inference instances to speed up inference for future instances .", "Further , we show that the decomposed variant of margin-based amortized inference achieves a greater reduction in the number of inference calls .", "Amortized inference has been proposed as a way to accomplish this ."]}
{"orig_sents": ["2", "6", "5", "0", "7", "3", "4", "1"], "shuf_sents": ["It can efficiently handle semantic ambiguity by extending standard topic models with two new features .", "Our new SSL approach improves semantic tagging performance by 3 % absolute over the baseline models , and also compares favorably on semi-supervised syntactic tagging .", "Finding concepts in natural language utterances is a challenging task , especially given the scarcity of labeled data for learning semantic ambiguity .", "Second , by going beyond a bag-of-words approach , it takes into account the inherent sequential nature of utterances to learn semantic classes based on context .", "( ii ) Retrospective Learner is a new learning technique that adapts to the unlabeled target data .", "To deal with these issues , we describe an efficient semisupervised learning ( SSL ) approach which has two components : ( i ) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags ( concepts ) .", "Furthermore , data mismatch issues , which arise when the expected test ( target ) data does not exactly match the training data , aggravate this scarcity problem .", "First , it encodes word n-gram features from labeled source and unlabeled target data ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree .", "We present a more precise characterization of the algorithm ? s complexity , an optimization analogous to binarization of contextfree grammars , and some important implementation details , resulting in an algorithm that is practical for natural-language applications .", "Hyperedge replacement grammar ( HRG ) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation .", "The algorithm is part of Bolinas , a new software toolkit for HRG processing ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states , and learns a probabilistic grammar using EM .", "On the challenging ATIS dataset , GUSP attained an accuracy of 84 % , effectively tying with the best published results by supervised approaches .", "We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries .", "To compensate for the lack of example annotations or question-answer pairs , GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision ."]}
{"orig_sents": ["3", "2", "4", "0", "1"], "shuf_sents": ["These evaluations are performed using a novel set of syntactic features , including measures of complexity .", "Our results show that sentence length , the mean number of clauses per utterance , and the StajnerMitkov measure of complexity are highly informative syntactic features , that classification accuracy varies greatly by the age of the speaker , and that accuracy up to 91.7 % can be achieved by support vector machines given a sufficient amount of data .", "Unfortunately , children can be misled by interrogators or might offer false information , with dire consequences .", "It is important that the testimony of children be admissible in court , especially given allegations of abuse .", "In this work , we evaluate various parameterizations of five classifiers ( including support vector machines , neural networks , and random forests ) in deciphering truth from lies given transcripts of interviews with 198 victims of abuse between the ages of 4 and 7 ."]}
{"orig_sents": ["4", "0", "3", "1", "2"], "shuf_sents": ["We propose a new concept , sentiment relevance , to make this distinction and argue that it better reflects the requirements of sentiment analysis systems .", "Since no large amount of labeled training data for our new notion of sentiment relevance is available , we investigate two semi-supervised methods for creating sentiment relevance classifiers : a distant supervision approach that leverages structured information about the domain of the reviews ; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer .", "We show that both methods learn sentiment relevance classifiers that perform well .", "We demonstrate experimentally that sentiment relevance and subjectivity are related , but different .", "A number of different notions , including subjectivity , have been proposed for distinguishing parts of documents that convey sentiment from those that do not ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["While there have been many attempts to estimate the emotion of an addresser from her/his utterance , few studies have explored how her/his utterance affects the emotion of the addressee .", "The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by five human workers .", "We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the predictors and generators .", "This has motivated us to investigate two novel tasks : predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee ? s mind ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["This paper presents a method for multimodal sentiment classification , which can identify the sentiment expressed in utterance-level visual datastreams .", "Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews , we show that multimodal sentiment analysis can be effectively performed , and that the joint use of visual , acoustic , and linguistic modalities can lead to error rate reductions of up to 10.5 % as compared to the best performing individual modality .", "With the recent growth of social websites such as YouTube , Facebook , and Amazon , video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques .", "During real-life interactions , people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions ."]}
{"orig_sents": ["6", "4", "2", "3", "1", "0", "5"], "shuf_sents": ["We apply the proposed approach to address two main NLP tasks , namely , Indirect yes/no Question Answer Pairs inference and Sentiment Orientation prediction .", "The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs .", "To achieve this aim , we propose a probabilistic emotionbased approach that is built on a hidden emotional model .", "The model aims to predict a vector of basic human emotions for each sense of the words .", "This paper aims to infer the sentiment similarity between word pairs with respect to their senses .", "Extensive experiments demonstrate the effectiveness of the proposed approach .", "Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments ."]}
{"orig_sents": ["0", "2", "3", "4", "1", "5"], "shuf_sents": ["Social Media contain a multitude of user opinions which can be used to predict realworld phenomena in many domains including politics , finance and health .", "We also consider the problem of modelling groups of related output variables , using a structured multi-task regularisation method .", "Most existing methods treat these problems as linear regression , learning to relate word frequencies and other simple features to a known response variable ( e.g. , voting intention polls or financial indicators ) .", "These techniques require very careful filtering of the input texts , as most Social Media posts are irrelevant to the task .", "In this paper , we present a novel approach which performs high quality filtering automatically , through modelling not just words but also users , framed as a bilinear model with a sparse regulariser .", "Our experiments on voting intention prediction demonstrate strong performance over large-scale input from Twitter on two distinct case studies , outperforming competitive baselines ."]}
{"orig_sents": ["4", "3", "2", "5", "0", "1"], "shuf_sents": ["We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets , and performs competitively compared to the best results in the TAC evaluations .", "We also conducted various analysis to show the impact of bigram selection , weight estimation , and ILP setup .", "The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary .", "For each bigram , a regression model is used to estimate its frequency in the reference summary .", "In this paper , we propose a bigram based supervised method for extractive document summarization in the integer linear programming ( ILP ) framework .", "During testing , the sentence selection problem is formulated as an ILP problem to maximize the bigram gains ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We conduct experiments on two corpora ? DUC 2004 and user comments on news articles ? and show that the performance of our algorithm outperforms those that rely only on submodularity .", "In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function , which we call dispersion ; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary .", "We propose a new optimization framework for summarization by generalizing the submodular framework of ( Lin and Bilmes , 2011 ) .", "We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases ."]}
{"orig_sents": ["4", "1", "3", "0", "5", "2"], "shuf_sents": ["In order to handle the subtree extraction problem , we investigate a new class of submodular maximization problem , and a new algorithm that has the approximation ratio 12 ( 1 ?", "We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster .", "Ourexperiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm .", "We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary .", "This study proposes a text summarization model that simultaneously performs sentence extraction and compression .", "e ? 1 ) ."]}
{"orig_sents": ["6", "5", "1", "0", "2", "3", "4"], "shuf_sents": ["We begin by presenting the notion of ? almost everywhere tight grammars ?", "This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs .", "and show that linear CFGs follow it .", "We then propose three different ways of reinterpreting non-tight PCFGs to make them tight , show that the Bayesian estimators in Johnson et al ( 2007 ) are correct under one of them , and provide MCMC samplers for the other two .", "We conclude with a discussion of the impact of tightness empirically .", "of the trees the grammar generates can be less than one ) .", "Probabilistic context-free grammars have the unusual property of not always defining tight distributions ( i.e. , the sum of the ? probabilities ?"]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing .", "While deep parsers with corpusinduced grammars have been emerging for some languages , those for Japanese have not been widely studied , mainly because most Japanese syntactic resources are dependency-based .", "Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations .", "This paper describes a method of inducing wide-coverage CCG resources for Japanese ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We also present a new transition-based dependency parsing algorithm that gives a complexity of O ( n ) for projective parsing and an expected linear time speed for non-projective parsing .", "With the standard setup , our parser shows an unlabeled attachment score of 92.96 % and a parsing speed of 9 milliseconds per sentence , which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search .", "Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately .", "We present a novel approach , called selectional branching , which uses confidence estimates to decide when to employ a beam , providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We induced dependency grammar for five different languages under the guidance of dependency information projected from the parsed English translation , experiments show that the bilinguallyguided method achieves a significant improvement of 28.5 % over the unsupervised baseline and 3.0 % over the best projection baseline on average .", "This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency .", "By moderately leveraging the dependency information projected from the parsed counterpart language , and simultaneously mining the underlying syntactic structure of the language considered , it effectively integrates the advantages of bilingual projection and unsupervised induction , so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction ."]}
{"orig_sents": ["0", "3", "5", "2", "1", "4"], "shuf_sents": ["Translated bi-texts contain complementary language cues , and previous work on Named Entity Recognition ( NER ) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages .", "We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space .", "We introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions .", "However , most previous approaches to bilingual tagging assume word alignments are given as fixed input , which can cause cascading errors .", "Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines .", "We observe that NER label information can be used to correct alignment mistakes , and present a graphical model that performs bilingual NER tagging jointly with word alignment , by combining two monolingual tagging models with two unidirectional alignment models ."]}
{"orig_sents": ["7", "5", "6", "4", "1", "2", "3", "0"], "shuf_sents": ["Experimental results on Chinese Sina Weibo data demonstrate that our approach is promising and significantly outperforms baseline methods1 .", "In this paper we aim to solve a new problem of resolving entity morphs to their real targets .", "We exploit temporal constraints to collect crosssource comparable corpora relevant to any given morph query and identify target candidates .", "Then we propose various novel similarity measurements including surface features , meta-path based semantic features and social correlation features and combine them in a learning-to-rank framework .", "to avoid active censorship or achieve other communication goals .", "? Peace West King ?", "to refer to ? Bo Xilai ? )", "In some societies , internet users have to create information morphs ( e.g ."]}
{"orig_sents": ["0", "3", "1", "4", "5", "2"], "shuf_sents": ["We describe a new probabilistic model for extracting events between major political actors from news corpora .", "temporal and dyad dependence ? to infer latent event classes .", "A supplementary appendix , and replication software/data are available online , at : http : //brenocon.com/irevents", "Our unsupervised model brings together familiar components in natural language processing ( like parsers and topic models ) with contextual political information ?", "We quantitatively evaluate the model ? s performance on political science benchmarks : recovering expert-assigned event class valences , and detecting real-world conflict .", "We also conduct a small case study based on our model ? s inferences ."]}
{"orig_sents": ["2", "3", "4", "0", "1"], "shuf_sents": ["Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step ( from oov directly to a source language phrase that has a translation ) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations .", "Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics .", "Out-of-vocabulary ( oov ) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data .", "In this paper , we propose a novel approach to finding translations for oov words .", "We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks , each with small and large feature sets , and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set .", "However , these solutions are impractical in complex structured prediction problems such as statistical machine translation .", "We present an online gradient-based algorithm for relative margin maximization , which bounds the spread of the projected data while maximizing the margin .", "Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization , such as the spread of the data ."]}
{"orig_sents": ["2", "4", "1", "6", "0", "5", "3"], "shuf_sents": ["In this way , we incorporate rich context information of PAS for disambiguation .", "In this paper , we group PAS ambiguities into two types : role ambiguity and gap ambiguity .", "Predicate-argument structure ( PAS ) has been demonstrated to be very effective in improving SMT performance .", "Experiments show that our approach helps to achieve significant improvements on translation quality .", "However , since a sourceside PAS might correspond to multiple different target-side PASs , there usually exist many PAS ambiguities during translation .", "Then we integrate the two methods into a PASbased translation framework .", "Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly : 1 ) inside context integration ; 2 ) a novel maximum entropy PAS disambiguation ( MEPD ) model ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language .", "To address these questions , this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers .", "Although there has been plenty of work on mother tongue interference , very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues .", "This paper further explores linguistic features that explain why certain relations are preserved in English writing , and which contribute to related tasks such as native language identification ."]}
{"orig_sents": ["4", "1", "2", "3", "0"], "shuf_sents": ["Finally , we use word association profiles to improve a system for automated scoring of essays .", "We illustrate the shape of the distirbution and observe variation with genre and target audience .", "We present a study of the relationship between quality of writing and word association profiles .", "For a set of essays written by college graduates on a number of general topics , we show that the higher scoring essays tend to have higher percentages of both highly associated and dis-associated pairs , and lower percentages of mildly associated pairs of words .", "We describe a new representation of the content vocabulary of a text we call word association profile that captures the proportions of highly associated , mildly associated , unassociated , and dis-associated pairs of words that co-exist in the given text ."]}
{"orig_sents": ["2", "3", "1", "4", "5", "0"], "shuf_sents": ["Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques , but also manual word-to-word annotations .", "In this paper , we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text .", "Text normalization is an important first step towards enabling many Natural Language Processing ( NLP ) tasks over informal text .", "While many of these tasks , such as parsing , perform the best over fully grammatically correct text , most existing text normalization approaches narrowly define the task in the word-to-word sense ; that is , the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms .", "To understand the real effect of normalization on the parser , we tie normalization performance directly to parser performance .", "Additionally , we design a customizable framework to address the often overlooked concept of domain adaptability , and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Based on the measure of preferences between predicates and arguments , the model aggregates all the transitions from a given predicate to its nearby predicates , and propagates their argument preferences as the given predicate ? s smoothed preferences .", "This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences .", "Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task , and it better correlates with human plausibility judgements ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling .", "The algorithm solves the implicit arguments sequentially , exploiting not only explicit but also the implicit arguments previously solved .", "In addition , we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data .", "The system exploits a very simple but relevant discursive property , the argument coherence over different instances of a predicate ."]}
{"orig_sents": ["2", "1", "0", "3", "5", "4"], "shuf_sents": ["This paper describes a rule-based framework for inducing derivational families ( i.e. , clusters of lemmas in derivational relationships ) and its application to create a highcoverage German resource , DERIVBASE , mapping over 280k lemmas into more than 17k non-singleton clusters .", "Even for German , a rather resourcerich language , there is a lack of largecoverage derivational knowledge .", "Derivational models are still an underresearched area in computational morphology .", "We focus on the rule component and a qualitative and quantitative evaluation .", "We attribute the high precision to the fact that our rules are based on information from grammar books .", "Our approach achieves up to 93 % precision and 71 % recall ."]}
{"orig_sents": ["6", "0", "2", "4", "1", "3", "5"], "shuf_sents": ["The corpus compiles manually written documents obtained from a completely controlled , yet representative environment that emulates the web .", "Part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents .", "Each of the 297 documents in the corpus is about one of the 150 topics used at the TREC Web Tracks 2009 ? 2011 , thus forming a strong connection with existing evaluation efforts .", "This will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third party ? a setting which has not been studied so far .", "Writers , hired at the crowdsourcing platform oDesk , had to retrieve sources for a given topic and to reuse text from what they found .", "In addition , the corpus provides an original resource for the evaluation of text reuse and plagiarism detectors , where currently only less realistic resources are employed .", "We report on the construction of the Webis text reuse corpus 2012 for advanced research on text reuse ."]}
{"orig_sents": ["4", "2", "5", "0", "3", "1"], "shuf_sents": ["argument .", "Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner ? s Dictionary with high precision and recall , and perform well against the most similar approach .", "We start from existing collocations to form lexical predicates ( e.g. , break ? )", "To do this , we extract all the occurrences in Wikipedia which match the predicate and abstract its arguments to general semantic classes ( e.g. , break BODY PART , break AGREEMENT , etc . ) .", "We present SPred , a novel method for the creation of large repositories of semantic predicates .", "and learn the semantic classes that best fit the ?"]}
{"orig_sents": ["5", "4", "2", "1", "0", "3"], "shuf_sents": ["We show that model summaries ( 1 ) are more abstractive and make use of more sentence aggregation , ( 2 ) do not contain as many topical caseframes as system summaries , and ( 3 ) can not be reconstructed solely from the source text , but can be if texts from in-domain documents are added .", "We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes .", "In this paper , we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text .", "These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria , but rather more domain knowledge is needed .", "Current systems use centrality , along with redundancy avoidance and some sentence compression , to produce mostly extractive summaries .", "In automatic summarization , centrality is the notion that a summary should contain the core parts of the source text ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["At inference time , we query the model with the patterns observed in an unseen news collection , identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a headline .", "HEADY improves over a state-of-theart open-domain title abstraction method , bridging half of the gap that separates it from extractive methods using humangenerated titles in manual evaluations , and performs comparably to human-generated headlines as evaluated with ROUGE .", "This paper presents HEADY : a novel , abstractive approach for headline generation from news collections .", "From a web-scale corpus of English news , we mine syntactic patterns that a Noisy-OR model generalizes into event descriptions ."]}
{"orig_sents": ["5", "4", "2", "6", "1", "0", "3"], "shuf_sents": ["It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation .", "This leads to more natural and less repetitive surface realisation .", "We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields ( CRFs ) with semantic trees .", "Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher ( up to 14 % ) than those for taking only local context into account .", "They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs .", "Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers .", "Due to their extended notion of context , CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers ."]}
{"orig_sents": ["4", "0", "3", "2", "1"], "shuf_sents": ["In this paper , we propose Two-Neighbor Orientation ( TNO ) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries .", "On NIST MT08 set , our most advanced model brings around +2.0 BLEU and -1.0 TER improvement .", "We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task .", "We explicitly model the longest span of such chunks , referred to as Maximal Orientation Span , to serve as a global parameter that constrains underlying local decisions .", "Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units ."]}
{"orig_sents": ["6", "3", "2", "0", "1", "5", "4"], "shuf_sents": ["The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy .", "To mitigate the effect of noisy machine alignments , we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model .", "In this paper , we focus on further improving the performance of the reordering model ( and thereby machine translation ) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available .", "Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance .", "The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments , and a gain of 5.2 BLEU points over a standard phrase-based baseline .", "This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner .", "Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems ."]}
{"orig_sents": ["5", "8", "0", "6", "7", "2", "3", "1", "4"], "shuf_sents": ["set .", "Experiments on large scale NIST evaluation data show improvements over strong baselines : +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline , and significant improvements in most circumstances over baselines with linear mixture model adaptation .", "Thus , we obtain a decoding feature whose value represents the phrase pair ? s closeness to the dev .", "This is a simple , computationally cheap form of instance weighting for phrase pairs .", "An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning , on the basis of style and genre .", "This paper proposes a new approach to domain adaptation in statistical machine translation ( SMT ) based on a vector space model ( VSM ) .", "This profile might , for instance , be a vector with a dimensionality equal to the number of training subcorpora ; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set .", "Then , for each phrase pair extracted from the training data , we create a vector with features defined in the same way , and calculate its similarity score with the vector representing the dev set .", "The general idea is first to create a vector profile for the in-domain development ( ? dev ? )"]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["We present a method for automatically generating input parsers from English specifications of input file formats .", "Our results show that our approach achieves 80.0 % F-Score accuracy compared to an F-Score of 66.7 % produced by a state-of-the-art semantic parser on a dataset of input format specifications from the ACM International Collegiate Programming Contest ( which were written in English for humans with no intention of providing support for automated processing ) .1", "We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree , which is then translated into a C++ input parser .", "We model the problem as a joint dependency parsing and semantic role labeling task .", "Our method is based on two sources of information : ( 1 ) the correlation between the text and the specification tree and ( 2 ) noisy supervision as determined by the success of the generated C++ parser in reading input examples ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations .", "We study the task of entity linking for tweets , which tries to associate each mention in a tweet with a knowledge base entry .", "Particularly , our model integrates three kinds of similarities , i.e. , mention-entry similarity , entry-entry similarity , and mention-mention similarity , to enrich the context for entity linking , and to address irregular mentions that are not covered by the entity-variation dictionary .", "To address these challenges , we propose a collective inference method that simultaneously resolves a set of mentions .", "We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["The experimental results show that our method is more accurate and general than previous approaches to the problem .", "Speaker identification is the task of attributing utterances to characters in a literary narrative .", "In this paper , we present a supervised machine learning approach for the task that incorporates several novel features .", "It is challenging to automate because the speakers of the majority of utterances are not explicitly identified in novels ."]}
{"orig_sents": ["1", "5", "6", "4", "2", "3", "0"], "shuf_sents": ["Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs .", "Hierarchical Bayesian Models ( HBMs ) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition .", "This paper presents such an evaluation for a language acquisition domain where explicit HBMs have been proposed : the acquisition of English dative constructions .", "In particular , we present a detailed , empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning ( LCL ) .", "Consequently , it remains crucial to carefully assess the use of HBMs along with alternative , possibly simpler , candidate models .", "However , as is well known , HBMs are ? ideal ?", "learning systems , assuming access to unlimited computational resources that may not be available to child language learners ."]}
{"orig_sents": ["4", "3", "0", "2", "1"], "shuf_sents": ["We propose a novel two-level model , which computes similarities between word-level vectors that are biased by topic-level context representations .", "We also release a first context-sensitive inference rule set .", "Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models .", "A recent line of work , which addresses context sensitivity of rules , represented contexts in a latent topic space and computed similarity over topic vectors .", "Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words , operating at the word space level ."]}
{"orig_sents": ["4", "3", "2", "1", "0"], "shuf_sents": ["This unified representation shows state-ofthe-art performance on three tasks : semantic textual similarity , word similarity , and word sense coarsening .", "Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data .", "We present a unified approach to semantic similarity that operates at multiple levels , all the way from comparing word senses to comparing text documents .", "However , prior methods for computing semantic similarity often operate at different levels , e.g. , single words or entire documents , which requires adapting the method for each data type .", "Semantic similarity is an essential component of many Natural Language Processing applications ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages .", "It is made by combining wordnets with open licences , data from Wiktionary and the Unicode Common Locale Data Repository .", "Overall there are over 2 million senses for over 100 thousand concepts , linking over 1.4 million words in hundreds of languages ."]}
{"orig_sents": ["5", "2", "0", "3", "1", "4"], "shuf_sents": ["Our approach is based on a sense alignment of FrameNet and Wiktionary , and subsequent translation disambiguation into the target language .", "The created resource is publicly available at http : //www .", "It is created through a simple , but powerful approach to construct a FrameNet in any language using Wiktionary as an interlingual representation .", "We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexicalsemantic resources .", "ukp.tu-darmstadt.de/fnwkde/ .", "We present a new bilingual FrameNet lexicon for English and German ."]}
{"orig_sents": ["4", "3", "2", "6", "0", "1", "5"], "shuf_sents": ["Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres , some previously unavailable in curated datasets .", "Even with minimal cleaning and filtering , the resulting data boosts translation performance across the board for five different language pairs in the news domain , and on open domain test sets we see improvements of up to 5 BLEU .", "We bring web-scale parallel text to the masses by mining the Common Crawl , a public Web crawl hosted on Amazon ? s Elastic Cloud .", "The Web is a comprehensive source of preexisting parallel text , but crawling the entire web is impossible for all but the largest companies .", "Parallel text is the fuel that drives modern machine translation systems .", "We make our code and data available for other researchers seeking to mine this rich new data resource.1", "Starting from nothing more than a set of common two-letter language codes , our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day , at a cost of about $ 500 ."]}
{"orig_sents": ["1", "0", "2", "4", "3", "5"], "shuf_sents": ["We present a sentence-compression-based framework for the task , and design a series of learning-based compression models built on parse trees .", "We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization .", "An innovative beam search decoder is proposed to efficiently find highly probable compressions .", "Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics ( e.g .", "Under this framework , we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function .", "8.0 % and 5.4 % improvements in ROUGE-2 respectively ) for the DUC 2006 and 2007 summarization task ."]}
{"orig_sents": ["3", "4", "0", "2", "1"], "shuf_sents": ["An Overgenerateand-Rank strategy is utilized to produce and rank candidate abstracts .", "In addition , human judges rate our system summaries significantly higher than compared systems in fluency and overall quality .", "Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches .", "We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion .", "We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains ."]}
{"orig_sents": ["5", "13", "2", "12", "11", "3", "9", "1", "0", "6", "4", "7", "10", "8"], "shuf_sents": ["syntactic variability ?", "We also introduce a novel automatic metric ?", "First , we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis .", "At generation time , a set of input data , the collection of semantically organized templates , and the model weights are used to select optimal templates .", "The metrics for generated weather and biography texts fall within acceptable ranges .", "We present a hybrid natural language generation ( NLG ) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process .", "that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents .", "In sum , we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time .", "*Ravi Kondadadi is now affiliated with Nuance Communications , Inc .", "Our system is evaluated with automatic , non ? expert crowdsourced and expert evaluation metrics .", "?", "After this semi-automatic processing ( human review of cluster assignments ) , a number of corpus ? level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus .", "Each sentence is then organized into semantically similar groups ( representing a domain specific concept ) by k-means clustering .", "Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain ."]}
{"orig_sents": ["4", "3", "1", "0", "5", "2"], "shuf_sents": ["It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result .", "Recently , some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora .", "We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality .", "However , when it comes to a language pair or a different domain without any bilingual resources , the traditional SMT loses its power .", "Currently , almost all of the statistical machine translation ( SMT ) models are trained with the parallel corpora in some specific domains .", "In this paper , we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary ."]}
{"orig_sents": ["2", "4", "3", "1", "0", "5"], "shuf_sents": ["Our system is able to achieve F-measures of as much as 80 % , when applied to word types it has never seen before .", "Instead of difficult and expensive annotation , we build a goldstandard by leveraging cheaply available parallel corpora , targeting our approach to the problem of domain adaptation for machine translation .", "Words often gain new senses in new domains .", "We define a task , SENSESPOTTING , in which we build systems to spot tokens that have new senses in new domain text .", "Being able to automatically identify , from a corpus of monolingual text , which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics .", "Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We evaluate its performance on a creative sentence generation task , showing its capability of generating well-formed , catchy and effective sentences that have all the good qualities of slogans produced by human copywriters .", "We present BRAINSUP , an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions , namely emotions , colors , domain relatedness and phonetic properties ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["We propose a joint inference algorithm for grammatical error correction .", "Experimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems .", "Different from most previous work where different error types are corrected independently , our proposed inference process considers all possible errors in a unied framework .", "We use integer linear programming ( ILP ) to model the inference process , which can easily incorporate both the power of existing error classiers and prior knowledge on grammatical error correction ."]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document , or metadata such as population .", "This paper shows that text-driven disambiguation for toponyms is far more effective .", "We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution , and show that textual cues can be straightforwardly integrated with other commonly used ones .", "Toponym resolvers identify the specific locations referred to by ambiguous placenames in text .", "Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline .", "To resolve such problem , this paper proposes a novel global argument inference model to explore specific relationships , such as Coreference , Sequence and Parallel , among relevant event mentions to recover those intersentence arguments in the sentence , discourse and document layers which represent the cohesion of an event or a topic .", "As a paratactic language , sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments ."]}
{"orig_sents": ["3", "4", "1", "2", "0"], "shuf_sents": ["Our experimental evaluation , based on crowdsourced user studies , show our method performing significantly better than prior work .", "In this paper , we present a method for discovering and semantically typing newly emerging out-ofKB entities , thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE .", "Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints .", "Methods for information extraction ( IE ) and knowledge base ( KB ) construction have been intensively studied .", "However , a largely under-explored case is tapping into highly dynamic sources like news streams and social media , where new entities are continuously emerging ."]}
{"orig_sents": ["1", "5", "3", "0", "4", "2"], "shuf_sents": ["This is the problem of domain adaptation .", "Relation Extraction ( RE ) is the task of extracting semantic relationships between entities in text .", "The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation .", "The clear drawback of supervised methods is the need of training data : labeled data is expensive to obtain , and there is often a mismatch between the training data and the data the system will be applied to .", "In this paper , we propose to combine ( i ) term generalization approaches such as word clustering and latent semantic analysis ( LSA ) and ( ii ) structured kernels to improve the adaptability of relation extractors to new text genres/domains .", "Recent studies on relation extraction are mostly supervised ."]}
{"orig_sents": ["3", "5", "4", "6", "1", "2", "0", "7"], "shuf_sents": ["We analyse how our model handles /t/-deletion on a large corpus of transcribed speech , and show that the joint model can perform word segmentation and recover underlying /t/s .", "Current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation .", "We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model .", "Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as /wEst/ ? west ?", "in certain contexts .", "are pronounced as ? wes ?", "Phonological variation like this is common in naturally occurring speech .", "We find that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts.1"]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data .", "Speakers of a language can construct an unlimited number of new words through morphological derivation .", "We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts .", "Our results constitute a novel evaluation of the proposed composition methods , in which the full additive model achieves the best performance , and demonstrate the usefulness of a compositional morphology component in distributional semantics .", "This is a major cause of data sparseness for corpus-based approaches to lexical semantics , such as distributional semantic models of word meaning ."]}
{"orig_sents": ["1", "4", "3", "0", "2"], "shuf_sents": ["We further show that our methodology can be used to predict more fine-grained phonetic distinctions .", "In this paper , we present a solution to one aspect of the decipherment task : the prediction of consonants and vowels for an unknown language and alphabet .", "On a three-way classification task between vowels , nasals , and nonnasal consonants , our model yields unsupervised accuracy of 89 % across the same set of languages .", "We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99 % across 503 languages .", "Adopting a classical Bayesian perspective , we performs posterior inference over hundreds of languages , leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters ."]}
{"orig_sents": ["3", "1", "2", "4", "5", "0"], "shuf_sents": ["Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams .", "Unlike some text-to-text translation tasks , text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model .", "We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each .", "In this paper we examine language modeling for text simplification .", "We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012 .", "We find that a combined model using both simplified and normal English data achieves a 23 % improvement in perplexity and a 24 % improvement on the lexical simplification task over a model trained only on simple data ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Our experiments compare several architectures varying the order of a set of trainable modules .", "We present a data set of German articles annotated with deep syntax and referents , including some types of implicit referents .", "The results suggest that a revision-based pipeline , with intermediate linearization , significantly outperforms standard pipelines or a parallel architecture .", "We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization ."]}
{"orig_sents": ["6", "2", "0", "1", "5", "4", "3"], "shuf_sents": ["In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links .", "We show that such features have a dramatic positive effect on recall .", "One such language is Arabic , which : a ) lacks a capitalization feature ; and b ) has relatively small knowledge bases , such as Wikipedia .", "The features led to improvements of 17.1 % and 20.5 % on the new news and microblogs test sets respectively .", "On the standard dataset , we achieved a 4.1 % relative improvement in Fmeasure over the best reported result in the literature .", "We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs .", "Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition ( NER ) that can generalize to previously unseen named entities ."]}
{"orig_sents": ["3", "5", "1", "2", "0", "4"], "shuf_sents": ["Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments , our approach only uses a letterbased 6-gram language model .", "We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6 % decipherment error , while our approach achieves 4.13 % decipherment error in a fraction of time by using a 6-gram language model .", "We also apply our approach to the famous Zodiac-408 cipher and obtain slightly better ( and near to optimal ) results than previously published .", "In this paper we address the problem of solving substitution ciphers using a beam search approach .", "Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8 % to 6.0 % error rate .", "We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models ."]}
{"orig_sents": ["4", "5", "0", "3", "1", "2"], "shuf_sents": ["The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus .", "When used as a preprocessing step for a state-of-the-art machine translation system , the translation quality on social media text improved by 6 % .", "The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text .", "We show that the proposed approach has a very high precision of ( 92.43 ) and a reasonable recall of ( 56.4 ) .", "We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text .", "The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text ."]}
{"orig_sents": ["5", "4", "3", "2", "1", "0"], "shuf_sents": ["We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish , Chinese-English and GermanEnglish translation .", "2 ) Extend the chart decoder to incorporate features from the phrase-based path .", "The work consists of two parts : 1 ) for each Hiero translation derivation , find its corresponding discontinuous phrase-based path .", "Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder .", "We propose an extension of Hiero called PhrasalHiero to address Hiero ? s second problem .", "Hiero translation models have two limitations compared to phrase-based models : 1 ) Limited hypothesis space ; 2 ) No lexicalized reordering model ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation ( SMT ) systems .", "Unlike previous approaches that require bilingual data , our method uses only monolingual text as input ; therefore it is applicable for refining data produced by a variety of Web-mining activities .", "Evaluation results show that the proposed method achieves an accuracy of 95.8 % for sentences and 80.6 % for text in noisy Web pages .", "We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We study question answering as a machine learning problem , and induce a function that maps open-domain questions to queries over a database of web extractions .", "Our approach automatically generalizes a seed lexicon and includes a scalable , parallelized perceptron parameter estimation scheme .", "Experiments show that our approach more than quadruples the recall of the seed lexicon , with only an 8 % loss in precision .", "Given a large , community-authored , question-paraphrase corpus , we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions ."]}
{"orig_sents": ["4", "5", "2", "0", "1", "3"], "shuf_sents": ["As a result , victims could not receive necessary aid and humanitarian organizations wasted resources on redundant efforts .", "In this paper , we propose a method for discovering matches between problem reports and aid messages .", "However , most problem reports and corresponding aid messages were not successfully exchanged between victims and local governments or humanitarian organizations , overwhelmed by the vast amount of information .", "Our system contributes to problem-solving in a large scale disaster situation by facilitating communication between victims and humanitarian organizations .", "The 2011 Great East Japan Earthquake caused a wide range of problems , and as countermeasures , many aid activities were carried out .", "Many of these problems and aid activities were reported via Twitter ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We propose a joint model for unsupervised induction of sentiment , aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model , we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level .", "The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure .", "We deviate from the traditional view of discourse , as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task ; consequently , the induced discourse relations play the role of opinion and aspect shifters ."]}
{"orig_sents": ["3", "0", "4", "2", "1"], "shuf_sents": ["the identification of opinion-related entities : the opinion expressions , the opinion holders , and the targets of the opinions , and the relations between opinion expressions and their targets and holders .", "Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction .", "We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction , and seeks a globally optimal solution .", "This paper addresses the task of finegrained opinion extraction ?", "Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner , where the interdependencies among different extraction stages are not captured ."]}
{"orig_sents": ["0", "5", "6", "2", "4", "1", "3"], "shuf_sents": ["Unbiased language is a requirement for reference sources like encyclopedias and scientific texts .", "These insights help us develop features for a model to solve a new prediction task of practical importance : given a biased sentence , identify the bias-inducing word .", "The analysis uncovers two classes of bias : framing bias , such as praising or perspective-specific words , which we link to the literature on subjectivity ; and epistemological bias , related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true .", "Our linguistically-informed model performs almost as well as humans tested on the same task .", "We identify common linguistic cues for these classes , including factive verbs , implicatives , hedges , and subjective intensifiers .", "Bias is , nonetheless , ubiquitous , making it crucial to understand its nature and linguistic realization and hence detect bias automatically .", "To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["They also rated our system above Google search ( with the Samsung S-Voice interface ) for tourist information tasks .", "We present a city navigation and tourist information mobile dialogue app with integrated question-answering ( QA ) and geographic information system ( GIS ) modules that helps pedestrian users to navigate in and learn about urban environments .", "We evaluated our system in comparison with Samsung S-Voice ( which interfaces to Google navigation and Google search ) with 17 users and found that users judged our system to be significantly more interesting to interact with and learn from .", "In contrast to existing mobile apps which treat these problems independently , our Android app addresses the problem of navigation and touristic questionanswering in an integrated fashion using a shared dialogue context ."]}
{"orig_sents": ["1", "5", "4", "3", "0", "2"], "shuf_sents": ["To learn from such textual resources , we describe a novel approach that first automatically extracts task knowledge from instructions , then learns a dialog manager over this task knowledge to provide assistance .", "Procedural dialog systems can help users achieve a wide range of goals .", "Evaluation in a Microsoft Office domain shows that the individual components are highly accurate and can be integrated into a dialog system that provides effective help to users .", "We consider domains where the required task knowledge exists in textual form ( e.g. , instructional web pages ) and where system builders have access to statements of user intent ( e.g. , search query logs or dialog interactions ) .", "In this paper , we demonstrate that it is possible to learn procedural dialog systems given only light supervision , of the type that can be provided by non-experts .", "However , such systems are challenging to build , currently requiring manual engineering of substantial domain-specific task knowledge and dialog management strategies ."]}
{"orig_sents": ["0", "2", "1", "6", "5", "8", "7", "4", "3"], "shuf_sents": ["Social media platforms have enabled people to freely express their views and discuss issues of interest with others .", "There are many questions that can be asked .", "While it is important to discover the topics in discussions , it is equally useful to mine the nature of such discussions or debates and the behavior of the participants .", "Our experiments using real-life discussions demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions .", "To the best of our knowledge , this is the first such study .", "The central idea of this question is tolerance , which is a key concept in the field of communications .", "One key question is whether the participants give reasoned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies .", "We aim to identify tolerant vs. intolerant participants and investigate how disagreement affects tolerance in discussions in a quantitative framework .", "In this work , we perform a computational study of tolerance in the context of online discussions ."]}
{"orig_sents": ["1", "5", "0", "2", "4", "3"], "shuf_sents": ["We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted .", "Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work .", "Moreover , investigating these deviations provides new insights and a deeper understanding of the examined techniques .", "Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field .", "We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers .", "We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Existing segmentation metrics such as Pk , WindowDiff , and Segmentation Similarity ( S ) are all able to award partial credit for near misses between boundaries , but are biased towards segmentations containing few or tightly clustered boundaries .", "Despite S ? s improvements , its normalization also produces cosmetically high values that overestimate agreement & performance , leading this work to propose a solution .", "This work proposes a new segmentation evaluation metric , named boundary similarity ( B ) , an inter-coder agreement coefficient adaptation , and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen ( 2012 ) ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["Query segmentation , like text chunking , is the first step towards query understanding .", "Similarly , in the case of hierarchical or nested segmentation , turkers have a strong preference towards balanced binary trees .", "In this study , we explore the effectiveness of crowdsourcing for this task .", "Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data , we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal ( often only two ) parts ."]}
{"orig_sents": ["1", "5", "6", "8", "9", "0", "4", "7", "2", "3"], "shuf_sents": ["To measure the user similarity , we propose a new user preference graph based on the answer preference expressed by users , such as ? helpful ?", "In Community question answering ( QA ) sites , malicious users may provide deceptive answers to promote their products or services .", "The user preference graph is incorporated into traditional supervised learning framework with the graph regularization technique .", "The experiment results demonstrate that the user preference graph can indeed help improve the performance of deceptive answer prediction .", "voting and ? best answer ?", "It is important to identify and filter out these deceptive answers .", "In this paper , we first solve this problem with the traditional supervised learning methods .", "selection .", "Two kinds of features , including textual and contextual features , are investigated for this task .", "We further propose to exploit the user relationships to identify the deceptive answers , based on the hypothesis that similar users will have similar behaviors to post deceptive or authentic answers ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["In this paper , we explore the utility of intra- and inter-sentential causal relations between terms or clauses as evidence for answering why-questions .", "By applying these ideas to Japanese why-QA , we improved precision by 4.4 % against all the questions in our test set over the current state-of-theart system for Japanese why-QA .", "In addition , unlike the state-of-the-art system , our system could achieve very high precision ( 83.2 % ) for 25 % of all the questions in the test set by restricting its output to the confident answers only .", "We also propose a method for assessing the appropriateness of causal relations as answers to a given question using the semantic orientation of excitation proposed by Hashimoto et al ( 2012 ) .", "To the best of our knowledge , this is the first work that uses both intra- and inter-sentential causal relations for why-QA ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["In this paper , we study the answer sentence selection problem for question answering .", "Unlike previous work , which primarily leverages syntactic analysis through dependency tree matching , we focus on improving the performance using models of lexical semantic resources .", "When evaluated on a benchmark dataset , the MAP and MRR scores are increased by 8 to 10 points , compared to one of our baseline systems using only surface-form matching .", "Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information , regardless of the choice of learning algorithms .", "Moreover , our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin ."]}
{"orig_sents": ["2", "3", "1", "0", "5", "4", "6", "7"], "shuf_sents": ["In contrast , alignment based methods used word alignment model to fulfill this task , which could avoid parsing errors without using parsing .", "Syntax based methods usually exploited syntactic patterns to extract opinion targets , which were however prone to suffer from parsing errors when dealing with online informal texts .", "Mining opinion targets is a fundamental and important task for opinion mining from online reviews .", "To this end , there are usually two kinds of methods : syntax based and alignment based methods .", "To fill this gap , this paper empirically studies how the performance of these two kinds of methods vary when changing the size , domain and language of the corpus .", "However , there is no research focusing on which kind of method is more better when given a certain amount of reviews .", "We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not .", "In our experiments , we verify that our combination is effective on the corpus with small and medium size ."]}
{"orig_sents": ["1", "2", "0", "3", "4"], "shuf_sents": ["Then random walking is employed to estimate confidence of candidates , which improves extraction accuracy by considering confidence of patterns .", "This paper proposes a novel two-stage method for mining opinion words and opinion targets .", "In the first stage , we propose a Sentiment Graph Walking algorithm , which naturally incorporates syntactic patterns in a Sentiment Graph to extract opinion word/target candidates .", "In the second stage , we adopt a self-learning strategy to refine the results from the first stage , especially for filtering out high-frequency noise terms and capturing the long-tail terms , which are not investigated by previous methods .", "The experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["minds .", "We propose induction algorithms encoding a diverse set of linguistic insights ( semantic prosody , distributional similarity , semantic parallelism of coordination ) and prior knowledge drawn from lexical resources , resulting in the first broad-coverage connotation lexicon .", "Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text , as seemingly objective statements often allude nuanced sentiment of the writer , and even purposefully conjure emotion from the readers ?", "The focus of this paper is drawing nuanced , connotative sentiments from even those words that are objective on the surface , such as ? intelligence ? , ? human ? , and ? cheesecake ? ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["The transformation reduces the out-of-vocabulary ( OOV ) words from 5.2 % to 2.6 % and gives a gain of 1.87 BLEU points .", "In contrast to previous work , we first narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG ? , which looks similar to MSA .", "We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic ( MSA ) adaptation .", "The transformations include morphological , phonological and spelling changes .", "Further , adapting large MSA/English parallel data increases the lexical coverage , reduces OOVs to 0.7 % and leads to an absolute BLEU improvement of 2.73 points ."]}
{"orig_sents": ["5", "2", "1", "4", "3", "0"], "shuf_sents": ["Finding the best alignment appears important , as this model leads to a substantial improvement in alignment quality .", "Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation .", "Initial attempts at modeling fertility used heuristic search methods .", "Building on recent advances in dual decomposition , this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM .", "Yet in practice we also need the single best alignment , which is difficult to find using Gibbs .", "The notion of fertility in word alignment ( the number of words emitted by a single state ) is useful but difficult to model ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Experiments on two well-studied NLP tasks , dependency parsing and NER , demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small , i.e. , 8 or even 2 .", "The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique .", "This significant benefit enables us to provide compact model representation , which is especially useful in actual use .", "This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models ."]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["This paper proposes a technique to leverage topic based sentiments from Twitter to help predict the stock market .", "Experiments on real-life S & P100 Index show that our approach is effective and performs better than existing state-of-the-art non-topic based methods .", "We first utilize a continuous Dirichlet Process Mixture model to learn the daily topic set .", "Then , for each topic we derive its sentiment according to its opinion words distribution to build a sentiment time series .", "We then regress the stock index and the Twitter sentiment time series to predict the market ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["We propose a novel entity disambiguation model , based on Deep Neural Network ( DNN ) .", "A supervised fine-tuning stage follows to optimize the representation towards the similarity measure .", "Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features , even beating complex collective approaches .", "Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage .", "Instead of utilizing simple similarity measures and their disjoint combinations , our method directly optimizes document and entity representations for a given similarity measure ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["Statistical language models have successfully been used to describe and analyze natural language documents .", "We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors , and show that using a comment completion tool can save up to 47 % of the comment typing .", "Recent work applying language models to programming languages is focused on the task of predicting code , while mainly ignoring the prediction of programmer comments .", "In this work , we predict comments from JAVA source files of open source projects , using topic models and n-grams , and we analyze the performance of the models given varying amounts of background data on the project being predicted ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In order to narrow down such mismatch , in this paper , we present an in-depth investigation on adapting a paraphrasing technique to web search from three aspects : a search-oriented paraphrasing model ; an NDCG-based parameter optimization algorithm ; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries .", "Experiments performed on the large scale query-document data set show that , the search performance can be significantly improved , with +3.28 % and +1.14 % NDCG gains on dev and test sets respectively .", "Mismatch between queries and documents is a key issue for the web search task ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art , and in some cases achieves higher accuracy than recently proposed purpose-built systems .", "These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations , and suggest that research in semantic parsing could benefit from advances in machine translation .", "Here we approach it as a straightforward machine translation task , and demonstrate that standard machine translation components can be adapted into a semantic parser .", "Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role , namely determiner phrases ( DPs ) .", "Distributional models of semantics capture word meaning very effectively , and they have been recently extended to account for compositionally-obtained representations of phrases made of content words .", "We introduce a new publicly available dataset to test distributional representations of DPs , and we evaluate state-of-the-art models on this set ."]}
{"orig_sents": ["0", "1", "2", "3", "4"], "shuf_sents": ["Uncertainty text detection is important to many social-media-based applications since more and more users utilize social media platforms ( e.g. , Twitter , Facebook , etc . )", "as information source to produce or derive interpretations based on them .", "However , existing uncertainty cues are ineffective in social media context because of its specific characteristics .", "In this paper , we propose a variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets .", "We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We introduce PARMA , a system for crossdocument , semantic predicate and argument alignment .", "We suggest that previous efforts have focussed on data that is biased and too easy , and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17 % F1 .", "PARMA achieves state of the art results on an existing and a new dataset .", "Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering , integrating them into a simple discriminative model ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features .", "In addition , we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation .", "We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Conversational implicatures involve reasoning about multiply nested belief structures .", "We show that agents in the multi-agent DecentralizedPOMDP reach implicature-rich interpretations simply as a by-product of the way they reason about each other to maximize joint utility .", "Our simulations involve a reference game of the sort studied in psychology and linguistics as well as a dynamic , interactional scenario involving implemented artificial agents .", "This complexity poses significant challenges for computational models of conversation and cognition ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures .", "In this paper , we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution .", "Most coreference resolvers rely heavily on string matching , syntactic properties , and semantic attributes of words , but they lack the ability to make decisions based on individual words ."]}
{"orig_sents": ["6", "5", "0", "4", "2", "3", "1"], "shuf_sents": ["adjacent sentences ) of each sentence in the source document .", "Evaluation results on a news corpus show the effectiveness of our proposed method .", "We introduce a set of features to characterize the order and coherence of natural language texts , and use the learning to rank technique to determine the order of any two sentences .", "We also propose to use the genetic algorithm to determine the total order of all sentences .", "In this paper , we investigate a more challenging task of ordering a set of unordered sentences without any contextual information .", "Most previous works on summary sentence ordering rely on the contextual information ( e.g .", "Ordering texts is an important task for many NLP applications ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["This ? universal ?", "To show the usefulness of such a resource , we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before .", "treebank is made freely available in order to facilitate research on multilingual dependency parsing.1", "We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["Aspects of Chinese syntax result in a distinctive mix of parsing challenges .", "However , the contribution of individual sources of error to overall difficulty is not well understood .", "We conduct a comprehensive automatic analysis of error types made by Chinese parsers , covering a broad range of error types for large sets of sentences , enabling the first empirical ranking of Chinese error types by their performance impact .", "We also investigate which error types are resolved by using gold part-of-speech tags , showing that improving Chinese tagging only addresses certain error types , leaving substantial outstanding challenges ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["Different from stacked learning methods ( Nivre and McDonald , 2008 ; Martins et al , 2008 ) , which process the dependency parsing in a pipelined way ( e.g. , a second level uses the first level outputs ) , in our method , multiple dependency parsing models are coordinated to exchange consensus information .", "This paper is concerned with the problem of heterogeneous dependency parsing .", "We conduct experiments on Chinese Dependency Treebank ( CDT ) and Penn Chinese Treebank ( CTB ) , experimental results show that joint inference can bring significant improvements to all state-of-the-art dependency parsers .", "In this paper , we present a novel joint inference scheme , which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase ."]}
{"orig_sents": ["5", "2", "0", "1", "4", "3"], "shuf_sents": ["to ensure valid update in the training process .", "The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity .", "We propose a simple variant of ? early-update ?", "On PTB , we also achieve state-of-the-art performance .", "On CTB , we achieve 94.01 % tagging accuracy and 86.33 % unlabeled attachment score with a relatively small beam width .", "In this paper , we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We use this model to study whether representing an argument/modifier distinction helps in learning argument structure , and whether a linguistically-natural argument/modifier distinction can be induced from distributional data alone .", "Our results provide evidence for both hypotheses .", "We present a model for inducing sentential argument structure , which distinguishes arguments from optional modifiers ."]}
{"orig_sents": ["0", "2", "1", "3", "4"], "shuf_sents": ["This paper presents an annotation scheme for events that negatively or positively affect entities ( benefactive/malefactive events ) and for the attitude of the writer toward their agents and objects .", "However , many attitudes are conveyed implicitly , and benefactive/malefactive events are important for inferring implicit attitudes .", "Work on opinion and sentiment tends to focus on explicit expressions of opinions .", "We describe an annotation scheme and give the results of an inter-annotator agreement study .", "The annotated corpus is available online ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Performance of each configuration is evaluated and experiment shows that the off-the-shelf AR system can be effectively used for Indic languages .", "Anaphora resolution module is also modified or replaced in order to realize different configurations of GuiTAR .", "The language specific preprocessing modules of GuiTAR ( v3.0.3 ) are identified and suitably designed for Bengali .", "This paper attempts to use an off-the-shelf anaphora resolution ( AR ) system for Bengali ."]}
{"orig_sents": ["1", "2", "0", "4", "3"], "shuf_sents": ["Using four years of data from the Text Analysis Conference , we analyze the performance of eight ROUGE variants in terms of accuracy , precision and recall in finding significantly different systems .", "How good are automatic content metrics for news summary evaluation ?", "Here we provide a detailed answer to this question , with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content .", "We also test combinations of ROUGE variants and find that they considerably improve the accuracy of automatic prediction .", "Our experiments show that some of the neglected variants of ROUGE , based on higher order n-grams and syntactic dependencies , are most accurate across the years ; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["To reduce the cost of collecting these multiple ratings , we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings .", "This paper tackles the problem of collecting reliable human assessments .", "We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality .", "Even if prediction performance is pretty low , decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Of three methods tested here , the one that performs best relies on latent semantics .", "summaries .", "The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students ?", "This motivates the development of a more accurate automated method to compute pyramid scores ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics .", "We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback , and we measure the semantic coherence of those topics .", "Results show that retrieval performances tend to be better when using topics with higher semantic coherence .", "We perform a first experimental evaluation using two major TREC test collections .", "More , the semantic coherence of the topics has never been considered in this field ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Results obtained with the definition of a new stopping criterion over the ODP-239 and the MORESQUE gold standard datasets evidence that our proposal outperforms all reported text-based approaches .", "Within this context , we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity measure initially developed for NLP tasks .", "Post-retrieval clustering is the task of clustering Web search results ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Information Retrieval ( IR ) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering ( QA ) , with repeated overengineering on IR , and not necessarily performance gain for QA .", "Our method is very quick to implement , and significantly improves IR for QA ( measured in Mean Average Precision and Mean Reciprocal Rank ) by 10 % -20 % against an uncoupled retrieval baseline in both document and passage retrieval , which further leads to a downstream 20 % improvement in QA F1 .", "We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm , called regularized compression .", "Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information .", "The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark ."]}
{"orig_sents": ["7", "3", "1", "5", "2", "0", "6", "4"], "shuf_sents": ["The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data .", "between the two different types of view are used to overcome the scarcity of the label information on unlabeled data .", "Then , the two models are constantly updated using unlabeled examples , where the learning objective is maximizing their segmentation agreements .", "Similarly to multi-view learning , the ? segmentation agreements ?", "The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature .", "The proposed approach trains a character-based and word-based model on labeled data , respectively , as the initial models .", "The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models .", "This paper presents a semi-supervised Chinese word segmentation ( CWS ) approach that co-regularizes character-based and word-based models ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Offline approaches have been proposed to split them using word statistics , but they rely on static lexicon , limiting their use .", "We propose an online approach , integrating source LM , and/or , back-transliteration and English LM .", "The experiments on Japanese and Chinese WS have shown that the proposed models achieve significant improvement over state-of-the-art , reducing 16 % errors in Japanese .", "Transliterated compound nouns not separated by whitespaces pose difficulty on word segmentation ( WS ) ."]}
{"orig_sents": ["5", "2", "0", "4", "3", "6", "1"], "shuf_sents": ["We employ Laplacian Eigenmaps ( LE ) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure .", "The proposed approach provides the best performance with the highest F1-measure of 0.7860 .", "The latent topic distribution estimated by Latent Dirichlet Allocation ( LDA ) is used to represent each text block .", "The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied .", "We evaluate two approaches employing LDA and probabilistic latent semantic analysis ( PLSA ) distributions respectively .", "We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions .", "Experimental results show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["We compare phone-phone substitution and wordphone mapping for pronunciation of English words using Telugu phones .", "In this paper , we relook at the problem of pronunciation of English words using native phone set .", "This differentiates our approach from other works in polyglot speech synthesis .", "We are not considering other than native language phoneset in all our experiments .", "Specifically , we investigate methods of pronouncing English words using Telugu phoneset in the context of Telugu Text-to-Speech ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["entities .", "This paper studies named entity translation and proposes ? selective temporality ?", "Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1 % .", "as a new feature , as using temporal features may be harmful for translating ? atemporal ?"]}
{"orig_sents": ["3", "1", "5", "0", "2", "4"], "shuf_sents": ["Finally , we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance .", "We present a way to integrate partof-speech tags ( POS ) and language information ( LID ) into these models which leads to significant improvements in terms of perplexity .", "The final combined language model provides 37.8 % relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7 % on the evaluation set compared to the traditional n-gram language model .", "In this paper , we investigate the application of recurrent neural network language models ( RNNLM ) and factored language models ( FLM ) to the task of language modeling for Code-Switching speech .", "Index Terms : multilingual speech processing , code switching , language modeling , recurrent neural networks , factored language models", "Furthermore , a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed ."]}
{"orig_sents": ["0", "2", "4", "1", "3"], "shuf_sents": ["Unsupervised object matching ( UOM ) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition , parallel corpus construction , and cross-language text categorization , because it does not require labor-intensive linguistic resources .", "We demonstrate the effectiveness of our method on cross-language text categorization .", "However , UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains , and this prevents us from applying UOM to real-world cross-language natural language processing tasks .", "The results show that our method outperforms conventional unsupervised object matching methods .", "To alleviate these limitations , we proposes latent semantic matching , which embeds objects in both source and target language domains into a shared latent topic space ."]}
{"orig_sents": ["3", "1", "0", "2", "4"], "shuf_sents": ["As a result , the task of deceptive review detection has been gaining increasing attention .", "And because of the profits at stake , people have been known to try to game the system by writing fake reviews to promote target products .", "In this paper , we propose a generative LDA-based topic modeling approach for fake review detection .", "Product reviews are now widely used by individuals and organizations for decision making ( Litvin et al , 2008 ; Jansen , 2010 ) .", "Our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95 % accuracy on review spam datasets , outperforming existing baselines by a large margin ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["We present a method to construct hypergraphs from sets of utterances , and evaluate this method on a simple recognition task .", "Lattices compactly represent lexical variation , but word order variation leads to a combinatorial explosion of states .", "Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks , including paraphrase generation , paraphrase recognition , and machine translation evaluation .", "We advocate hypergraphs as compact representations for sets of utterances describing the same event or object .", "Given a set of utterances that describe a single object or event , we construct such a hypergraph , and demonstrate that it can recognize novel descriptions of the same event with high accuracy ."]}
{"orig_sents": ["4", "1", "5", "0", "2", "3"], "shuf_sents": ["Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y , Z jokes , where X , Y , and Z are variables to be filled in .", "It is difficult to say exactly what makes a joke funny , and solving this problem algorithmically is assumed to require deep semantic understanding , as well as cultural and other contextual cues .", "This is , to the best of our knowledge , the first fully unsupervised humor generation system .", "Our model significantly outperforms a competitive baseline and generates funny jokes 16 % of the time , compared to 33 % for human-generated jokes .", "Humor generation is a very hard problem .", "We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["In this paper , we explore the use of distance and co-occurrence information of word-pairs for language modeling .", "Evaluated on the WSJ corpus , bigram and trigram model perplexity were reduced up to 23.5 % and 14.0 % , respectively .", "We attempt to extract this information from history-contexts of up to ten words in size , and found it complements well the n-gram model , which inherently suffers from data scarcity in learning long history-contexts .", "Compared to the distant bigram , we show that word-pairs can be more effectively modeled in terms of both distance and occurrence ."]}
{"orig_sents": ["3", "2", "1", "0", "4"], "shuf_sents": ["proficiency .", "Unlike previous studies , the proposed methods aim at satisfying both reliability and validity of generated distractors ; distractors should be exclusive against answers to avoid multiple answers in one quiz , and distractors should discriminate learners ?", "corpus .", "We propose discriminative methods to generate semantic distractors of fill-in-theblank quiz for language learners using a large-scale language learners ?", "Detailed user evaluation with 3 native and 23 non-native speakers of English shows that our methods achieve better reliability and validity than previous methods ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor .", "Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly .", "We propose three types of lexical constraints as building blocks of humorous word substitution : constraints concerning the similarity of sounds or spellings of the original word and the substitute , a constraint requiring the substitute to be a taboo word , and constraints concerning the position and context of the replacement ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Our approach divides this problem into two steps , using a graph-based approach for each step : ( 1 ) factoid discovery , finding groups of words that correspond to the same factoid , and ( 2 ) factoid assignment , using these groups of words to mark collective discourse units that contain the respective factoids .", "Factoids are information units that are shared between instances of collective discourse and may have many different ways of being realized in words .", "In this paper , we study the problem of automatically annotating the factoids present in collective discourse .", "We study this on two novel data sets : the New Yorker caption contest data set , and the crossword clues data set ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["With our approach , we were able to contrast the performance of our method and define language-specific features for these typologically different languages .", "Our presented method proved to be sufficiently robust as it achieved approximately the same scores on the two typologically different languages .", "Here , we introduce a machine learningbased approach that allows us to identify light verb constructions ( LVCs ) in Hungarian and English free texts .", "We also present the results of our experiments on the SzegedParalellFX English ? Hungarian parallel corpus where LVCs were manually annotated in both languages ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper presents the settings and the results of the ROMIP 2013 MT shared task for the English ? Russian language direction .", "The quality of generated translations was assessed using automatic metrics and human evaluation .", "We also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["This standardized version of lexical knowledge base of Indian Languages can now easily be linked to similar global resources .", "We present IndoNet , a multilingual lexical knowledge base for Indian languages .", "It is a linked structure of wordnets of 18 different Indian languages , Universal Word dictionary and the Suggested Upper Merged Ontology ( SUMO ) .", "The system is encoded in Lexical Markup Framework ( LMF ) and we propose modifications in LMF to accommodate Universal Word Dictionary and SUMO .", "We discuss various benefits of the network and challenges involved in the development ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper proposes a methodology for generating specialized Japanese data sets for textual entailment , which consists of pairs decomposed into basic sentence relations .", "We compared our methodology with existing studies in terms of agreement , frequencies and times , and we evaluated its validity by investigating recognition accuracy .", "We experimented with our methodology over a number of pairs taken from the RITE-2 data set ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages .", "Comparable corpora are important basic resources in cross-language information processing .", "Experiments show that the novel method can obtain similar documents with consistent topics own better adaptability and stability performance .", "However , the existing methods of building comparable corpora , which use intertranslate words and relative features , can not evaluate the topical relation between document pairs ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["To improve the learning of such rules in an unsupervised way , we propose to lexically expand sparse argument word vectors with semantically similar words .", "Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words .", "In this scheme , prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability .", "Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["This paper studies the task of mining equivalent relations from Linked Data , which was insufficiently addressed before .", "Early experiments have shown encouraging results with an average of 0.75~0.87 precision in predicting relation pair equivalency and 0.78~0.98 precision in relation clustering .", "We introduce an unsupervised method to measure equivalency of relation pairs and cluster equivalent relations .", "Linking heterogeneous resources is a major research challenge in the Semantic Web ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["Based on the early encouraging results , the context-dependent lexical lookup tool may be developed further into an intelligent reading aid , to help users grasp the gist of a second or foreign language text .", "Evaluations on a prototype lookup tool , trained on a English ? Malay bilingual Wikipedia corpus , show a precision score of 0.65 ( baseline 0.55 ) and mean reciprocal rank score of 0.81 ( baseline 0.771 ) .", "Current approaches for word sense disambiguation and translation selection typically require lexical resources or large bilingual corpora with rich information fields and annotations , which are often infeasible for under-resourced languages .", "The multilingual lexicon can then be used to perform context-dependent lexical lookup on texts of any language , including under-resourced ones .", "We extract translation context knowledge from a bilingual comparable corpora of a richer-resourced language pair , and inject it into a multilingual lexicon ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Resource scarcity along with diversity ?", "In this paper we aim at addressing these two problems by ( i ) building a text corpus for Sorani and Kurmanji , the two main dialects of Kurdish , and ( ii ) highlighting some of the orthographic , phonological , and morphological differences between these two dialects from statistical and rule-based perspectives .", "both in dialect and script ? are the two primary challenges in Kurdish language processing ."]}
{"orig_sents": ["3", "1", "0", "2", "4"], "shuf_sents": ["This assumption is useful but often leads to errors in projection .", "These algorithms , however , make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another .", "In this paper , we will address this weakness by using trees created from instances of Interlinear Glossed Text ( IGT ) to discover patterns of divergence between the languages .", "As most of the world ? s languages are under-resourced , projection algorithms offer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment .", "We will show that this method improves the performance of projection algorithms significantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages .", "To make the projection method well-generalize to diverse languages pairs , we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations .", "Experiments showed that our methods improve the performances greatly on projections between English and Chinese .", "However , these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly ."]}
{"orig_sents": ["1", "0", "4", "3", "2"], "shuf_sents": ["Since a word or a phrase has different meanings in different contexts , we should map source and target phrases in an intelligent way .", "Mapping phrases between languages as translation of each other by using an intermediate language ( pivot language ) may generate translation pairs that are wrong .", "Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved .", "We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs .", "We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["By iteratively alternating between the tasks of retrieval and translation , an initial general-domain model is allowed to adapt to in-domain data .", "Our setup is time- and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences .", "We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval ( CLIR ) .", "Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before ."]}
{"orig_sents": ["2", "3", "4", "1", "0"], "shuf_sents": ["We further show how such a representation could help us with the design of computer aided SL verification tools , which in turn would bring us closer to the development of an automatic recognition system for these languages .", "Here we propose a formal representation of SL signs , that will help us with the analysis of automatically-collected hand tracking data from French Sign Language ( FSL ) video corpora .", "This paper explores the use of Propositional Dynamic Logic ( PDL ) as a suitable formal framework for describing Sign Language ( SL ) , the language of deaf people , in the context of natural language processing .", "SLs are visual , complete , standalone languages which are just as expressive as oral languages .", "Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements , which make reference to real world objects , characters or situations ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["A diverse ensemble of weak learners is created using the same SMT engine ( a hierarchical phrase-based system ) by manipulating the training data and a strong model is created by combining the weak models on-the-fly .", "We propose the use of stacking , an ensemble learning technique , to the statistical machine translation ( SMT ) models .", "Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model ."]}
{"orig_sents": ["1", "2", "4", "0", "3", "5"], "shuf_sents": ["To reduce the reliance on labeled examples , we propose an unsupervised method to clean bilingual data .", "The quality of bilingual data is a key factor in Statistical Machine Translation ( SMT ) .", "Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance .", "The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs , based on the observation that better sentence pairs often lead to better phrase extraction and vice versa .", "Previous work often used supervised learning methods to filter lowquality data , but a fair amount of human labeled examples are needed which are not easy to obtain .", "End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks ."]}
{"orig_sents": ["7", "6", "9", "12", "13", "14", "5", "0", "3", "1", "11", "8", "2", "4", "10"], "shuf_sents": ["We train a Support Vector Regression ( SVR ) system to predict TDIs for new sentences using these features as input .", "The primary use of our work is a way of ? binning ?", "categories as per their predicted TDI .", "The prediction done by our framework is well correlated with the empirical gold standard data , which is a repository of < L , DP , SC > and TDI pairs for a set of sentences .", "This can decide pricing of any translation task , especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing .", "length ( L ) , degree of polysemy ( DP ) and structural complexity ( SC ) .", "We first define and quantify translation difficulty in terms of TDI .", "In this paper we introduce Translation Difficulty Index ( TDI ) , a measure of difficulty in text translation .", "and ? hard ?", "We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism .", "This can also provide a way of monitoring progress of second language learners .", "sentences ( to be translated ) in ? easy ? , ? medium ?", "We , rather , rely on cognitive evidences from eye tracking .", "TDI is measured as the sum of fixation ( gaze ) and saccade ( rapid eye movement ) times of the eye .", "We then establish that TDI is correlated with three properties of the input sentence , viz ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["The proposed method is easy to implement , orthogonal to cube pruning and additive to its pruning power .", "On a full-scale Englishto-German experiment with a string-totree model , we obtain a speed-up of more than 60 % over a strong baseline , with no loss in BLEU .", "Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus , and speed-up is obtained by pruning corresponding chart cells .", "We present a context-sensitive chart pruning method for CKY-style MT decoding ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["In this paper , we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model , which plays a critical role in word alignment .", "However , estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem , which is computationally infeasible .", "Therefore , we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs .", "The experiments show that our approach outperforms both 1-best and n-best alignments ."]}
{"orig_sents": ["3", "1", "0", "4", "2"], "shuf_sents": ["We employ stem as the atomic translation unit to alleviate data spareness .", "In this paper , we propose a novel approach for translating agglutinative languages by treating stems and affixes differently .", "Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese .", "Current translation models are mainly designed for languages with limited morphology , which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated .", "In addition , we associate each stemgranularity translation rule with a distribution of related affixes , and select desirable rules according to the similarity of their affix distributions with given spans to be translated ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Rhetorical structure theory ( RST ) is widely used for discourse understanding , which represents a discourse as a hierarchically semantic structure .", "In our framework , the translation process mainly includes three steps : 1 ) Source RST-tree acquisition : a source sentence is parsed into an RST tree ; 2 ) Rule extraction : translation rules are extracted from the source tree and the target string via bilingual word alignment ; 3 ) RST-based translation : the source RST-tree is translated with translation rules .", "In this paper , we propose a novel translation framework with the help of RST .", "Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively ."]}
{"orig_sents": ["2", "3", "1", "4", "0"], "shuf_sents": ["Tuning a machine translation system against a semantic frame based objective function is independent of the translation model paradigm , so , any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach .", "We argue that by preserving themeaning of the translations as captured by semantic frames right in the training process , an MT system is constrained to make more accurate choices of both lexical and reordering rules .", "We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function , MEANT , produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation .", "Moreover , for informal web forum data , human evaluators preferredMEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire ? even though automatic semantic parsing might be expected to fare worse on informal language .", "As a result , MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets .", "In this paper , we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation ( SMT ) .", "Furthermore , , we consider multiple permutations of all the matching rules , and select the final reordering path based on the weighed sum of reordering probabilities of these rules .", "Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach : we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric .", "Instead of solely relying on the top-1 best-matching rule for source sentence preordering , we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage ."]}
{"orig_sents": ["3", "5", "0", "4", "2", "1"], "shuf_sents": ["Recently , transliteration models based on Bayesian learning have overcome issues with over-fitting allowing for many-to-many alignment in the training of transliteration models .", "The experimental results show that our method considerably outperforms conventional alignment models .", "The unified model decomposes into two classes of non-parametric Bayesian component models : a Dirichlet process mixture model for clustering , and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster .", "Machine Transliteration is an essential task for many NLP applications .", "We propose a novel coupled Dirichlet process mixture model ( cDPMM ) that simultaneously clusters and bilingually aligns transliteration data within a single unified model .", "However , names and loan words typically originate from various languages , obey different transliteration rules , and therefore may benefit from being modeled independently ."]}
{"orig_sents": ["2", "5", "4", "0", "3", "1"], "shuf_sents": ["A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model .", "A large scale evaluation over 8 language pairs shows that performance does significantly improve .", "The phrase-based and N-gram-based SMT frameworks complement each other .", "We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption .", "Some work has been done to combine insights from these two frameworks .", "While the former is better able to memorize , the latter provides a more principled model that captures dependencies across phrasal boundaries ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["In this paper we show how to automatically induce non-linear features for machine translation .", "The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases , which guarantees that the asymptotic complexity of machine translation decoding does not increase .", "Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks .", "We achieve this by applying gradient boosting machines ( Friedman , 2000 ) to learn newweak learners ( features ) in the form of regression trees , using a differentiable loss function related to BLEU ."]}
{"orig_sents": ["2", "5", "1", "0", "4", "3"], "shuf_sents": ["In this paper , we present two language-independent features to improve the quality of phrase-pivot based SMT .", "Although pivoting is a robust technique , it introduces some low quality translations .", "An important challenge to statistical machine translation ( SMT ) is the lack of parallel data for many language pairs .", "We show positive results ( 0.6 BLEU points ) on Persian-Arabic SMT as a case study .", "The features , source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table .", "One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al ( 2004 ) .", "We compare methods based on augmenting the set of nonterminals by adding semantic role labels , and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure .", "Our results demonstrate that the second approach is effective in increasing the quality of translations ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Evaluations are performed on factoid questions selected from two different domains : Jeopardy !", "The first approach re-ranks single QA system ? s outputs by using a traditional MBR model , by measuring correlations between answer candidates ; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model .", "and Web , and significant improvements are achieved on all data sets .", "This paper presents two minimum Bayes risk ( MBR ) based Answer Re-ranking ( MBRAR ) approaches for the question answering ( QA ) task ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["By translating the training corpus , we obtain results close to a monolingual setting .", "In particular , for question classification , no labeled question corpus is available for French , so this paper studies the possibility to use existing English corpora and transfer a classification by translating the question and their labels .", "Question answering systems have been developed for many languages , but most resources were created for English , which can be a problem when developing a system in another language such as French ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Thus , our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts .", "In this paper , we propose a unified question retrieval model based on latent semantic indexing with tensor analysis , which can capture word associations among different parts of CQA triples simultaneously .", "Retrieving similar questions is very important in community-based question answering ( CQA ) .", "The experimental result shows that our method outperforms the traditional methods ."]}
{"orig_sents": ["2", "4", "6", "3", "1", "0", "7", "5"], "shuf_sents": ["In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs , the KL divergence achieves close to 80 % precision but does not outperform a simpler ( linguistically unmotivated ) frequency measure .", "We investigate the hypothesis that semantic content can be computed using the KullbackLeibler ( KL ) divergence , an informationtheoretic measure of the relative entropy of two distributions .", "Some words are more contentful than others : for instance , make is intuitively more general than produce and fifteen is more ? precise ?", "of lexical items , as modelled by distributional representations .", "than a group .", "aspect of distributions .", "In this paper , we propose to measure the ? semantic content ?", "We suggest that this result illustrates the rather ? intensional ?"]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We first analyze a labeled RTE-5 test set and find that the negative entailment phenomena are very effective features for TE recognition .", "Then , a method is proposed to extract this kind of phenomena from text-hypothesis pairs automatically .", "This paper aims at understanding what human think in textual entailment ( TE ) recognition process and modeling their thinking process to deal with this problem .", "We evaluate the performance of using the negative entailment phenomena on both the English RTE-5 dataset and Chinese NTCIR-9 RITE dataset , and conclude the same findings ."]}
{"orig_sents": ["4", "0", "6", "1", "5", "3", "2"], "shuf_sents": ["It thus can not capture the notion that the target fragment is ? almost entailed ?", "The recently suggested idea of partial textual entailment may remedy this problem .", "We also provide a preliminary assessment of how partial entailment may be used for recognizing ( complete ) textual entailment .", "Indeed , our results show that these methods are useful for recognizing partial entailment .", "Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other .", "We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting .", "by the given text ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["This paper introduces a supervised approach for performing sentence level dialect identification between Modern Standard Arabic and Egyptian Dialectal Arabic .", "The system achieves an accuracy of 85.5 % on an Arabic online-commentary dataset outperforming a previously proposed approach achieving 80.9 % and reflecting a significant gain over a majority baseline of 51.9 % and two strong baseline systems of 78.5 % and 80.4 % , respectively .", "These features are then used with other core and meta features to train a generative classifier that predicts the correct label for each sentence in the given input text .", "We use token level labels to derive sentence-level features ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains .", "Semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols .", "Motivated by the observation that interpretation can be decomposed into domain-dependent and independent components , we suggest a novel interpretation model , which augments a domain dependent model with abstract information that can be shared by multiple domains ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods .", "We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models .", "We test our hypothesis on the problem of judging event coreferentiality , which involves compositional interactions in the predicate-argument structure of sentences , and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper addresses the problem of dealing with a collection of labeled training documents , especially annotating negative training documents and presents a method of text classification from positive and unlabeled data .", "We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines ( SVM ) .", "The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614 ."]}
{"orig_sents": ["1", "2", "0", "4", "3"], "shuf_sents": ["Once we have an idea of who a character is speaking to , the sentiment in his or her speech can be attributed accordingly , allowing us to generate lists of a character ? s enemies and allies as well as pinpoint scenes critical to a character ? s emotional development .", "We present an automatic method for analyzing sentiment dynamics between characters in plays .", "This literary format ? s structured dialogue allows us to make assumptions about who is participating in a conversation .", "novels ) .", "Results of experiments on Shakespeare ? s plays are presented along with discussion of how this work can be extended to unstructured texts ( i.e ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The performance of the experiments on two datasets indicates feasibility and potentiality of the quantum classifier .", "In this article , we propose a novel classifier based on quantum computation theory .", "Different from existing methods , we consider the classification as an evolutionary process of a physical system and build the classifier by using the basic quantum mechanics equation ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["Recently , with an increase in computing resources , it became possible to learn rich word embeddings from massive amounts of unlabeled data .", "We show improvement on the task of sentiment classification with respect to several baselines , and observe that the approach is most useful when the training set is sufficiently small .", "However , some methods take days or weeks to learn good embeddings , and some are notoriously difficult to train .", "We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task .", "We propose a method that takes as input an existing embedding , some labeled data , and produces an embedding in the same space , but with a better predictive performance in the supervised task ."]}
{"orig_sents": ["5", "4", "0", "1", "3", "2"], "shuf_sents": ["We investigate the properties of the the dataset , and present its statistics .", "We explore using the dataset for two tasks : sentiment polarity classification and rating classification .", "We run baseline experiments on the dataset to establish a benchmark .", "We provide standard splits of the dataset into training and testing , for both polarity and rating classification , in both balanced and unbalanced settings .", "It consists of over 63,000 book reviews , each rated on a scale of 1 to 5 stars .", "We introduce LABR , the largest sentiment analysis dataset to-date for the Arabic language ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We demonstrate our approach on a new dataset just released by Yelp , and release a new sentiment lexicon with 1329 adjectives for the restaurant domain .", "Recommendation dialog systems help users navigate e-commerce listings by asking questions about users ?", "We present a framework for generating and ranking fine-grained , highly relevant questions from user-generated reviews .", "preferences toward relevant domain attributes ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Our experiments on English , Spanish and Russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media .", "We study subjective language in social media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams .", "Starting with a domain-independent , highprecision sentiment lexicon and a large pool of unlabeled data , we bootstrap Twitter-specific sentiment lexicons , using a small amount of labeled data to guide the process ."]}
{"orig_sents": ["2", "4", "0", "3", "5", "1"], "shuf_sents": ["On the basis , we propose a respective way to jointly model these two tasks .", "*", "Emotion classification can be generally done from both the writer ? s and reader ? s perspectives .", "In particular , a cotraining algorithm is proposed to improve semi-supervised learning of the two tasks .", "In this study , we find that two foundational tasks in emotion classification , i.e. , reader ? s emotion classification on the news and writer ? s emotion classification on the comments , are strongly related to each other in terms of coarse-grained emotion categories , i.e. , negative and positive .", "Experimental evaluation shows the effectiveness of our joint modeling approach ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["However , labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on .", "Using this we construct a corpus covering 7 topics with 2,228 quotes .", "We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic , called a position statement .", "Quotes are used in news articles as evidence of a person ? s opinion , and thus are a useful target for opinion mining ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["However , the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model .", "The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion , and then leverage both the original and opposite samples for ( dual ) training and ( dual ) prediction .", "Bag-of-words ( BOW ) is now the most popular way to model text in machine learning based sentiment classification .", "Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification .", "In this paper , we focus on the polarity shift problem , and propose a novel approach , called dual training and dual prediction ( DTDP ) , to address it ."]}
{"orig_sents": ["4", "5", "3", "0", "2", "1"], "shuf_sents": ["German ) .", "Evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results .", "We propose a new coregression algorithm to address this task by leveraging unlabeled reviews .", "English ) to predict the rating scores of unrated reviews in a target language ( e.g .", "The task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings .", "In this paper , we aim to investigate a more challenging task of crosslanguage review rating prediction , which makes use of only rated reviews in a source language ( e.g ."]}
{"orig_sents": ["3", "2", "4", "0", "1"], "shuf_sents": ["Afterwards , we transform such syntactic contexts in abstract representations , that are then fed into a Support Vector Machine classifier .", "The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques .", "Instead of using pattern matching methods that rely on lexico-syntactic patterns , we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser .", "In this paper we present a technique to reveal definitions and hypernym relations from text .", "The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences ."]}
{"orig_sents": ["3", "4", "0", "2", "6", "1", "5"], "shuf_sents": ["Recently an unsupervised bilingual EM based algorithm has been proposed , which makes use only of the raw counts of the translations in comparable corpora ( Marathi and Hindi ) .", "An improvement of 17 % 35 % in the accuracy of verb WSD is obtained compared to the existing EM based approach .", "But the performance of this approach is poor on verbs with accuracy level at 25-38 % .", "Word Sense Disambiguation ( WSD ) is one of the toughest problems in NLP , and in WSD , verb disambiguation has proved to be extremely difficult , because of high degree of polysemy , too fine grained senses , absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation .", "Unsupervised WSD has received widespread attention , but has performed poorly , specially on verbs .", "On a general note , the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization .", "We suggest a modification to this mentioned formulation , using context and semantic relatedness of neighboring words ."]}
{"orig_sents": ["3", "0", "4", "2", "1"], "shuf_sents": ["They are usually trained on humanannotated datasets , which are very costly due to its task-specific nature .", "In other words , our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors .", "Experiments on a number of datasets show that with as little as 25 % of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets .", "Quality estimation models provide feedback on the quality of machine translated texts .", "We investigate active learning techniques to reduce the size of these datasets and thus annotation effort ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["To do so we follow an n-best list reranking approach that exploits recent advances in learning to rank techniques .", "We achieve 10.1 % and 11.4 % reduction in recognition word error rate ( WER ) relative to a standard baseline system on typewritten and handwritten Arabic respectively .", "In this paper we incorporate linguistically and semantically motivated features to an existing OCR system .", "Optical Character Recognition ( OCR ) systems for Arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize fluency ."]}
{"orig_sents": ["5", "0", "1", "3", "2", "4"], "shuf_sents": ["It is a new challenge which combines salience ranking problem with novelty detection .", "Previous researches in this field seldom explore the evolutionary pattern of topics such as birth , splitting , merging , developing and death .", "In EHDP , time varying information is formulated as a series of HDPs by considering time-dependent information .", "In this paper , we develop a novel model called Evolutionary Hierarchical Dirichlet Process ( EHDP ) to capture the topic evolution pattern in timeline summarization .", "Experiments on 6 different datasets which contain 3156 documents demonstrates the good performance of our system with regard to ROUGE scores .", "Timeline summarization aims at generating concise summaries and giving readers a faster and better access to understand the evolution of news ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Unlike pipeline architectures , our model jointly considers the choices in content selection , lexicalization , and aggregation to avoid greedy decisions and produce more compact texts .", "We present an ILP model of concept-totext generation ."]}
{"orig_sents": ["2", "0", "5", "1", "4", "3", "6"], "shuf_sents": ["Among them , Twitter is the most popular service by far due to its ease for realtime sharing of information .", "Then the question is how users can understand a topic in a short time when they are frustrated with the overwhelming and unorganized tweets .", "The growth of the Web 2.0 technologies has led to an explosion of social networking media sites .", "Both the number and the content of sub-summaries are automatically identified by the proposed stream-based and semantic-based approaches .", "In this paper , this problem is approached by sequential summarization which aims to produce a sequential summary , i.e. , a series of chronologically ordered short subsummaries that collectively provide a full story about topic development .", "It collects millions of tweets per day and monitors what people are talking about in the trending topics updated timely .", "These approaches are evaluated in terms of sequence coverage , sequence novelty and sequence correlation and the effectiveness of their combination is demonstrated ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["We present evaluation results for the performance of our system using this annotated data .", "We have manually annotated 2,625 sentences with these factoids ( around 375 sentences per topic ) to build an evaluation corpus for this task .", "In this paper , we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user .", "We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids , or information units , from 47 gold standard documents ( surveys and tutorials ) on seven topics in Natural Language Processing .", "We present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents , and then selecting relevant sentences from those documents ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We create a new resource composed of U-SDannotated constituency and dependency treebanks for the MRL Modern Hebrew , and present two systems that can automatically predict U-SD annotations , for gold segmented input as well as raw texts , with high baseline accuracy .", "Stanford Dependencies ( SD ) provide a functional characterization of the grammatical relations in syntactic parse-trees .", "We present a novel extension of SD , called Unified-SD ( U-SD ) , which unifies the annotation of structurally- and morphologically-marked relations via an inheritance hierarchy .", "The SD representation is useful for parser evaluation , for downstream applications , and , ultimately , for natural language understanding , however , the design of SD focuses on structurally-marked relations and under-represents morphosyntactic realization patterns observed in Morphologically Rich Languages ( MRLs ) ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Our approach achieves significant improvement on all the three pairs of data sets .", "This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data .", "In this paper , we propose a simple and effective approach to domain adaptation for dependency parsing .", "To demonstrate the effectiveness of the proposed approach , we evaluate it on three pairs of source-target data , compared with several common baseline systems and previous approaches ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents an effective algorithm of annotation adaptation for constituency treebanks , which transforms a treebank from one annotation guideline to another with an iterative optimization procedure , thus to build a much larger treebank to train an enhanced parser without increasing model complexity .", "Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In the line of research extending statistical parsing to more expressive grammar formalisms , we demonstrate for the first time the use of tree-adjoining grammars ( TAG ) .", "We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus , along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing .", "Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data , but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems , thereby enabling further experimentation along these lines ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank , using a CCG parser .", "Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies , for which it is known to have weak rates of recovery .", "We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar ( CCG ) can improve dependency parsing of Hindi , a free word order language .", "We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser ( Malt ) ."]}
{"orig_sents": ["1", "3", "0", "4", "2"], "shuf_sents": ["We find considerable gains in accuracy on the range of standard metrics .", "Higher-order dependency features are known to improve dependency parser accuracy .", "This suggests that higher-order dependency features are not simply overfitting the training material .", "We investigate the incorporation of such features into a cube decoding phrase-structure parser .", "What is especially interesting is that we find strong , statistically significant gains on dependency recovery on out-of-domain tests ( Brown vs. WSJ ) ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Experiments in fourteen languages yield parsing speeds competitive to projective parsers , with state-ofthe-art accuracies for the largest datasets ( English , Czech , and German ) .", "Our approach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models .", "We present fast , accurate , direct nonprojective dependency parsers with thirdorder features ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks .", "A strategy is designed to exploit the complementary strengths of the tagger and parser , and encourage them to predict agreed structures .", "For the cascaded task of Chinese word segmentation , POS tagging and parsing , the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space .", "In this paper , we present a novel lattice-based framework in which a Chinese sentence is first segmented into a word lattice , and then a lattice-based POS tagger and a lattice-based parser are used to process the lattice from two different viewpoints : sequential POS tagging and hierarchical tree building ."]}
{"orig_sents": ["3", "2", "0", "4", "1"], "shuf_sents": ["We present an improved implementation , based on Tree Structured Stack ( TSS ) , in which a transition is performed in O ( 1 ) , resulting in a real lineartime algorithm , which is verified empirically .", "Practically , our methods combined offer a speedup of ? 2x over strong baselines on Penn Treebank sentences , and are orders of magnitude faster on much longer sentences .", "We demonstrate that , contrary to popular belief , most current implementations of beam parsers in fact run in O ( n2 ) , rather than linear time , because each statetransition is actually implemented as an O ( n ) operation .", "Beam search incremental parsers are accurate , but not as fast as they could be .", "We further improve parsing speed by sharing feature-extraction and dotproduct across beam items ."]}
{"orig_sents": ["5", "4", "0", "3", "2", "1"], "shuf_sents": ["Many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features .", "While previous approaches do not improve on our supervised baseline , our approach is better across the board with an average 4 % error reduction .", "We present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilingual cross-domain part-of-speech tagging datasets .", "Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts .", "The performance loss observed in such cross-domain applications is often attributed to covariate shifts , with out-of-vocabulary effects as an important subclass .", "Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development ."]}
{"orig_sents": ["4", "3", "2", "1", "0"], "shuf_sents": ["Using machine learning , we improve upon prior approaches to the problem , achieving over 80 % accuracy at labelling the types of temporal relation between events and times that are related by temporal signals .", "We investigate the ro ? le that these co-ordinating temporal signals have in determining the type of temporal relations in discourse .", "and ? as soon as ? .", "Sometimes events and times are related through use of an explicit co-ordination which gives information about the temporal relation : expressions like ? before ?", "Automatically determining the temporal order of events and times in a text is difficult , though humans can readily perform this task ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Inspired from summarization , the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments .", "The results demonstrate that the method outperforms two competitive baselines .", "The method is evaluated on excerpts of the Fisher and AMI corpora , using a crowdsourcing platform to elicit comparative relevance judgments .", "A new method for keyword extraction from conversations is introduced , which preserves the diversity of topics that are mentioned ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Tabular information in text documents contains a wealth of information , and so tables are a natural candidate for information extraction .", "We study how natural-language tools , such as part-of-speech tagging , dependency paths , and named-entity recognition , can be used to improve the quality of relation extraction from tables .", "There are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table .", "In three domains we show that ( 1 ) a model that performs joint probabilistic inference across tabular and natural language features achieves an F1 score that is twice as high as either a puretable or pure-text system , and ( 2 ) using only shallower features or non-joint inference results in lower quality ."]}
{"orig_sents": ["4", "1", "3", "0", "2"], "shuf_sents": ["We adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases , assuming entity pairs in top-ranked passages are more likely to express a relation .", "However , previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases .", "Our proposed technique significantly improves the quality of distantly supervised relation extraction , boosting recall from 47.7 % to 61.2 % with a consistently high level of precision of around 93 % in the experiments .", "To tackle this problem , we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features .", "Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Our joint log-linear model outperforms the state-of-the-art Favre and Hakkani-Tu ? r ( 2009 ) model by ? 10 % on Broadcast News , and achieves 54.3 % Fscore on multiple genres .", "We propose systems exploiting syntactic and semantic constraints to extract appositions from OntoNotes .", "Appositions are adjacent NPs used to add information to a discourse ."]}
{"orig_sents": ["3", "5", "2", "6", "1", "0", "4"], "shuf_sents": ["In a comprehensive evaluation of 4 language pairs ( English to German , French , Russian , Spanish ) , we found that neural language models are indeed viable tools for data selection : while the improvements are varied ( i.e .", "We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts , which are prevalent in general-domain text .", "Substantial gains have been demonstrated in previous works , which employ standard ngram language models .", "Data selection is an effective approach to domain adaptation in statistical machine translation .", "0.1 to 1.7 gains in BLEU ) , they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams .", "The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora , which are then incorporated into the training data .", "Here , we explore the use of neural language models for data selection ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In this study , we revisit this learning paradigm and apply it to the transliteration task .", "Analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language .", "We show that alone , it performs worse than a statistical phrase-based machine translation engine , but the combination of both approaches outperforms each one taken separately , demonstrating the usefulness of the information captured by a so-called formal analogy ."]}
{"orig_sents": ["1", "0", "5", "4", "2", "3"], "shuf_sents": ["Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk .", "We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation .", "Our algorithm is also faster for small models : we estimated a model on 302 million tokens using 7.7 % of the RAM and 14.0 % of the wall time taken by SRILM .", "The code is open source as part of KenLM .", "Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs .", "Using one machine with 140 GB RAM for 2.8 days , we built an unpruned model on 126 billion tokens ."]}
{"orig_sents": ["4", "3", "2", "0", "1", "5"], "shuf_sents": ["Thus , our approach is well-suited to the causal constraint of spoken conversations .", "On an English-to-Iraqi CSLT task , the proposed approach gives significant improvements over a baseline system as measured by BLEU , TER , and NIST .", "A significant novelty of our adaptation technique is its incremental nature ; we continuously update the topic distribution on the evolving test conversation as new utterances become available .", "Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations , which is used to bias translation choices towards the current context .", "We describe a translation model adaptation approach for conversational spoken language translation ( CSLT ) , which encourages the use of contextually appropriate translation options from relevant training conversations .", "Interestingly , the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["But in the setting of monolingual alignment , previous work has not been able to align more than one sentence pair per second .", "We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences .", "Fast alignment is essential for many natural language tasks .", "Using just part-of-speech tags and WordNet as external resources , our aligner gives state-of-the-art result , while being an order-of-magnitude faster than the previous best performing system ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion .", "The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners .", "We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners .", "Furthermore , the model is trained using both a native corpus and the learner corpus via a domain adaptation technique ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Different from the majority of approaches , where a large number of pairwise similarity features are used to represent a text pair , our model features the following : ( i ) it directly encodes input texts into relational syntactic structures ; ( ii ) relies on tree kernels to handle feature engineering automatically ; ( iii ) combines both structural and feature vector representations in a single scoring model , i.e. , in Support Vector Regression ( SVR ) ; and ( iv ) delivers significant improvement over the best STS systems .", "Measuring semantic textual similarity ( STS ) is at the cornerstone of many NLP applications ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We present results from our study of which uses syntactically and semantically motivated information to group segments of sentences into unbreakable units for the purpose of typesetting those sentences in a region of a fixed width , using an otherwise standard dynamic programming line breaking algorithm , to minimize raggedness .", "We also use a simple genetic algorithm to search for a subset of the features optimizing F1 , to arrive at a set of features that delivers 89.2 % Precision , 90.2 % Recall ( 89.7 % F1 ) on a test set , improving the rule-based baseline by about 11 points and the classifier trained on all features by about 1 point in F1 .", "In addition to a rule-based baseline segmenter , we use a very modest size text , manually annotated with positions of breaks , to train a maximum entropy classifier , relying on an extensive set of lexical and syntactic features , which can then predict whether or not to break after a certain word position in a sentence ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English , Danish and Spanish .", "This article describes the annotation process , the results in terms of inter-encoder agreement , and the sense distributions obtained with two methods : majority voting with a theory-compliant backoff strategy , and MACE , an unsupervised system to choose the most likely sense from all the annotations ."]}
{"orig_sents": ["0", "1", "2", "3", "4", "5"], "shuf_sents": ["Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces ( Baroni and Lenci , 2010 ) .", "However , they are also sparse , with resulting reliability and coverage problems .", "We address this problem by derivational smoothing , which uses knowledge about derivationally related words ( oldish ?", "old ) to improve semantic similarity estimates .", "We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German .", "Even for models built from very large corpora , simple derivational smoothing can improve coverage considerably ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows , using a state-of-the-art verb clustering system , that features based on alternation approximation outperform those based on independent subcategorization frames .", "Although diathesis alternations have been used as features for manual verb classification , and there is recent work on incorporating such features in computational models of human language acquisition , work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process .", "Our alternation-based approach is particularly adept at leveraging information from less frequent data ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We compare two approaches : the first one is the standard annotation methodology of lexical units and frame elements in two steps , while the second is a novel approach aimed at acquiring frames in a bottom-up fashion , starting from frame element annotation .", "We show that our methodology , relying on a single annotation step and on simplified role definitions , outperforms the standard one both in terms of accuracy and time .", "We present the first attempt to perform full FrameNet annotation with crowdsourcing techniques ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["In this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures .", "The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation .", "We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study .", "However , there is no widely-used metric to evaluate wholesentence semantic structures ."]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["Despite only using a fraction of the available hyperplanes , VBQ outperforms uniform quantisation by up to 168 % for retrieval across standard text and image datasets .", "Previous approaches assign a constant number of bits per hyperplane .", "Our method , dubbed Variable Bit Quantisation ( VBQ ) , provides a datadriven non-uniform bit allocation across hyperplanes .", "We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane .", "This neglects the fact that a subset of hyperplanes may be more informative than others ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["On two specialized FrenchEnglish comparable corpora , empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches .", "This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora .", "We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors ."]}
{"orig_sents": ["4", "6", "1", "2", "0", "7", "3", "5"], "shuf_sents": ["We present DAVID , a simple , lexical resource-based preference violation detector .", "This practice risks missing significant performance gains and even entire techniques .", "This paper addresses the importance of resource quality through the lens of a challenging NLP task : detecting selectional preference violations .", "When the resource entries and parser outputs for a small sample are corrected , however , the F1-measure on that sample jumps from 40 % to 61.54 % , and performance on other examples rises , suggesting that the algorithm becomes practical given refined resources .", "Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks , as are annotated corpora such as treebanks .", "More broadly , this paper shows that resource quality matters tremendously , sometimes even more than algorithmic improvements .", "Often , the resources are used as-is , without question or examination .", "With asis lexical resources , DAVID achieves an F1-measure of just 28.27 % ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["Focusing on the cross-lingual textual entailment task , we contribute with a novel method that : i ) significantly outperforms the state of the art , and ii ) is portable , with limited loss in performance , to language pairs where training data are not available .", "the number of alignments ) , disregarding qualitative aspects ( the importance of aligned terms ) .", "The use of automatic word alignment to capture sentence-level semantic relations is common to a number of cross-lingual NLP applications .", "In this paper we demonstrate that integrating qualitative information can bring significant performance improvements with negligible impact on system complexity .", "Despite its proved usefulness , however , word alignment information is typically considered from a quantitative point of view ( e.g ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language , while the bilingual component is the average mutual information of the aligned clusters .", "To evaluate our method , we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters .", "We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["The resource is freely available .", "We report on the first structured distributional semantic model for Croatian , DM.HR .", "It is constructed after the model of the English Distributional Memory ( Baroni and Lenci , 2010 ) , from a dependencyparsed Croatian web corpus , and covers about 2M lemmas .", "An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns .", "We give details on the linguistic processing and the design principles ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer .", "We address this challenge with contributions in two folds : first , we introduce the new task of image caption generalization , formulated as visually-guided sentence compression , and present an efficient algorithm based on dynamic beam search with dependency-based constraints .", "The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision .", "Second , we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text .", "However , the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text ."]}
{"orig_sents": ["1", "4", "3", "0", "2"], "shuf_sents": ["We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs .", "Identifying news stories that discuss the same real-world events is important for news tracking and retrieval .", "Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model , and is especially suitable for distinguishing between topically similar , yet non-identical events .", "We propose an approach for recognizing identical real-world events based on a structured , event-oriented document representation .", "Most existing approaches rely on the traditional vector space model ."]}
{"orig_sents": ["1", "3", "2", "0", "4"], "shuf_sents": ["To address the term ambiguity detection problem , we employ a model that combines data from language models , ontologies , and topic modeling .", "While the resolution of term ambiguity is important for information extraction ( IE ) systems , the cost of resolving each instance of an entity can be prohibitively expensive on large datasets .", "By making a judgment about the general ambiguity of a term , a system is able to handle ambiguous and unambiguous cases differently , improving throughput and quality .", "To combat this , this work looks at ambiguity detection at the term , rather than the instance , level .", "Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96 ."]}
{"orig_sents": ["4", "2", "1", "3", "0"], "shuf_sents": ["By experimenting on Wikipedia articles to extract the facts in Freebase ( the top 92 relations ) , we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach .", "In this paper , we point out and analyze some critical factors in DS which have great impact on accuracy , including valid entity type detection , negative training examples construction and ensembles .", "However , the accuracy is still not satisfying .", "We propose an approach to handle these factors .", "Distant supervision ( DS ) is an appealing learning method which learns from existing relational facts to extract more from a text corpus ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification .", "We seek to improve Anand et al ? s ( 2011 ) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts , user-interaction constraints and ideology constraints .", "Determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem ."]}
{"orig_sents": ["4", "2", "5", "3", "6", "1", "0"], "shuf_sents": ["Narrative and quantitative experiments show positive and promising results to the questions raised above .", "SOT distinguishes between two types of school-ofthought words for either the general background of a school of thought or the original ideas each paper contributes to its school of thought .", "This paper makes the first attempt at this problem .", "To answer these questions , we propose a probabilistic generative School-Of-Thought ( SOT ) model to simulate the scientific authoring process based on several assumptions .", "School of thought analysis is an important yet not-well-elaborated scientific knowledge discovery task .", "We focus on one aspect of the problem : do characteristic school-of-thought words exist and whether they are characterizable ?", "SOT defines a school of thought as a distribution of topics and assumes that authors determine the school of thought for each sentence before choosing words to deliver scientific ideas ."]}
{"orig_sents": ["5", "2", "6", "3", "1", "4", "0"], "shuf_sents": ["We evaluate the system using a data set of labeled discussions and show that it achieves good results .", "We opinion predictions to represent the discussion in one of two formal representations : signed attitude network or a space of attitude vectors .", "The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions .", "We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets .", "We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation .", "In this paper , we use Arabic natural language processing techniques to analyze Arabic debates .", "The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Building on this , an event date extraction system learns to integrate the likelihood of candidate dates extracted from time-rich sentences with temporal constraints extracted from eventrelated sentences .", "A temporal tagger retrieves and normalizes dates mentioned informally in social media to actual month and year referents .", "We present a system for extracting the dates of illness events ( year and month of the event occurrence ) from posting histories in the context of an online medical support community .", "Our integrated model achieves 89.7 % of the maximum performance given the performance of the temporal expression retrieval step ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["In this paper , we address the problem for predicting cQA answer quality as a classification task .", "Extensive experimental results conducted on two cQA datasets demonstrate the effectiveness of our proposed approach .", "We propose a multimodal deep belief nets based approach that operates in two stages : First , the joint representation is learned by taking both textual and non-textual features into a deep learning network .", "Then , the joint representation learned by the network is used as input features for a linear classifier ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We report results on two public datasets ( cameras and cars ) , showing that our model outperforms state-ofthe-art models , as well as on a new dataset consisting of Twitter posts .", "Intuitively , these three variables are bidirectionally interdependent , but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that can not model the bidirectional interaction between these variables .", "Opinion mining is often regarded as a classification or segmentation task , involving the prediction of i ) subjective expressions , ii ) their target and iii ) their polarity .", "Towards better understanding the interaction between these variables , we propose a model that allows for analyzing the relation of target and subjective phrases in both directions , thus providing an upper bound for the impact of a joint model in comparison to a pipeline model ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words .", "Unlike previous approaches , our model exploits the sentiment labels of documents instead of seed words .", "Sentiment Word Identification ( SWI ) is a basic technique in many sentiment analysis applications .", "In this paper , we propose a novel optimization-based model for SWI .", "Most existing researches exploit seed words , and lead to low robustness ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Among these , tree kernels ( Collins and Duffy , 2001 ) have been perhaps the most robust and effective syntactic tool , appealing for their empirical success , but also because they do not require an answer to the difficult question of which tree features to use for a given task .", "Since explicit features are easy to generate and use ( with publicly available tools ) , we suggest they should always be included as baseline comparisons in tree kernel method evaluations .", "We compare tree kernels to different explicit sets of tree features on five diverse tasks , and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time , and with smaller models .", "Syntactic features are useful for many text classification tasks ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Computational models of infant word segmentation have not been tested on a wide range of languages .", "In contrast to the undersegmentation pattern previously found in English and Russian , the model exhibited more oversegmentation errors and more errors overall .", "Despite the high error rate , analysis suggested that lexical acquisition might not be problematic , provided that infants attend only to frequently segmented items .", "This paper applies a phonotactic segmentation model to Korean ."]}
{"orig_sents": ["0", "4", "2", "5", "3", "1"], "shuf_sents": ["We investigated the effect of word surprisal on the EEG signal during sentence reading .", "These findings provide support for surprisal as a generally applicable measure of processing difficulty during language comprehension .", "Four event-related potential components were extracted from the EEG of 24 readers of the same sentences .", "This effect was mostly due to content words .", "On each word of 205 experimental sentences , surprisal was estimated by three types of language model : Markov models , probabilistic phrasestructure grammars , and recurrent neural networks .", "Surprisal estimates under each model type formed a significant predictor of the amplitude of the N400 component only , with more surprising words resulting in more negative N400s ."]}
{"orig_sents": ["0", "5", "4", "1", "3", "2"], "shuf_sents": ["We present a system for automated phonetic clustering analysis of cognitive tests of phonemic verbal fluency , on which one must name words starting with a specific letter ( e.g. , ? F ? )", "Our system provides an automated alternative .", "These findings are preliminary , but strongly suggest that our system can be used to detect subtle signs of brain damage due to repetitive head trauma in individuals that are otherwise unimpaired .", "In a pilot study , we applied this system to tests of 55 novice and experienced professional fighters ( boxers and mixed martial artists ) and found that experienced fighters produced significantly longer chains of phonetically similar words , while no differences were found in the total number of words produced .", "Test responses are typically subjected to manual phonetic clustering analysis that is labor-intensive and subject to inter-rater variability .", "for one minute ."]}
{"orig_sents": ["4", "0", "3", "5", "2", "1"], "shuf_sents": ["Judgments from individual subjects in our study exhibit high average correlation to the resulting relatedness means ( r = 0.77 , ?", "We also offer a critique of the field ? s reliance upon similarity norms to evaluate relatedness measures .", "We compare the results of several WordNet-based similarity and relatedness measures to our Rel-122 norms and demonstrate the limitations of WordNet for discovering general indications of semantic relatedness .", "= 0.09 , N = 73 ) , although not as high as Resnik ? s ( 1995 ) upper bound for expected average human correlation to similarity means ( r = 0.90 ) .", "We have elicited human quantitative judgments of semantic relatedness for 122 pairs of nouns and compiled them into a new set of relatedness norms that we call Rel-122 .", "This suggests that human perceptions of relatedness are less strictly constrained than perceptions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["We aim to show the effects of using varying degrees of morphological information .", "We use stems and word categories that are extracted with morphological analysis as main features and compare them with fixed length stemmers in a bag of words approach with several learning algorithms .", "Morphologically rich languages such as Turkish may benefit from morphological analysis in natural language tasks .", "In this study , we examine the effects of morphological analysis on text categorization task in Turkish ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["In order to explore them , we introduce a traversal algorithm based on user pages .", "We present a way to extract links from messages published on microblogging platforms and we classify them according to the language and possible relevance of their target in order to build a text corpus .", "Using mature open-source software from the NLP research field , a spell checker ( aspell ) and a language identification system ( langid.py ) , our case study and our benchmarks give an insight into the linguistic structure of the considered services .", "As we target lesser-known languages , we try to focus on non-English posts by filtering out English text .", "Three platforms are taken into consideration : FriendFeed , identi.ca and Reddit , as they account for a relative diversity of user profiles and more importantly user languages ."]}
{"orig_sents": ["1", "5", "4", "6", "2", "3", "0"], "shuf_sents": ["adoption , current use , discontinuation and switching , and demonstrates the utility of such an application for drug safety monitoring in online discussion forums .", "Though there has been substantial research concerning the extraction of information from clinical notes , to date there has been less work concerning the extraction of useful infor-mation from patient-generated content .", "and affect , and suggests possible applications of such techniques to re-search concerning online social support , as well as integration into search interfaces for patients .", "Additionally , the paper demonstrates the extraction of side effects and sentiment at different phases in patient medication use , e.g .", "With regard to the former , the pa-per describes an approach involving the pair-ing of important figures ( e.g .", "Using a dataset comprised of online support group discussion content , this paper investigates two dimensions that may be important in the ex-traction of patient-generated experiences from text ; significant individuals/groups and medi-cation use .", "family , hus-bands , doctors , etc . )"]}
{"orig_sents": ["1", "0"], "shuf_sents": ["This paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context .", "As one of the most challenging issues in NLP , metaphor identification and its interpretation have seen many models and methods proposed ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper we focus on practical issues of data representation for dependency parsing .", "We carry out an experimental comparison of ( a ) three syntactic dependency schemes ; ( b ) three data-driven dependency parsers ; and ( c ) the influence of two different approaches to lexical category disambiguation ( aka tagging ) prior to parsing .", "Comparing parsing accuracies in various setups , we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser ."]}
{"orig_sents": ["3", "5", "4", "0", "1", "2"], "shuf_sents": ["We create several baselines and experiment with various parameter settings for three algorithms , i.e. , Conditional Random Fields ( CRF ) , Support Vector Machines ( SVM ) and Random Forests ( RF ) .", "Also , we evaluate the impact of lexical , syntactic and semantic features on each of the algorithms and look at errors .", "The best performance of 79.35 % F-score is achieved by CRFs when using all three feature types .", "Current domain-specific information extraction systems represent an important resource for biomedical researchers , who need to process vaster amounts of knowledge in short times .", "We here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning .", "Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models ."]}
{"orig_sents": ["3", "5", "2", "6", "0", "7", "4", "1"], "shuf_sents": ["Furthermore , before clustering documents , we refine the target documents by representing them as a collection of important sentences in each document .", "We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction , using latent and surface information for clustering , and have confirmed that our proposed method provides better result among various conditions for clustering .", "In this case , tf.idf is often used to decide important words .", "In this paper , we propose a method to raise the accuracy of text classification based on latent topics , reconsidering the techniques necessary for good classification ?", "As a clustering method , we employ the k-means algorithm and investigate how our proposed method works for good clustering .", "for example , to decide important sentences in a document , the sentences with important words are usually regarded as important sentences .", "On the other hand , we apply the PageRank algorithm to rank important words in each document .", "We then classify the documents based on latent information in the documents ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["Our results show that we get better results compared to other methods that use only wellformed text .", "Japanese learners may be able to construct grammatically correct sentences , however , these may sound ? unnatural ? .", "In this work , we analyze correct word combinations using different collocation measures and word similarity methods .", "This study addresses issues of Japanese language learning concerning word combinations ( collocations ) .", "While other methods use well-formed text , our approach makes use of a large Japanese language learner corpus for generating collocation candidates , in order to build a system that is more sensitive to constructions that are difficult for learners ."]}
{"orig_sents": ["0", "3", "5", "2", "1", "4"], "shuf_sents": ["Natural language can be easily understood by everyone irrespective of their differences in age or region or qualification .", "We have identified seven primitive overlapping verb senses which substantiate our claim .", "We claim that this new set of primitives captures the meaning inherent in verbs and help in forming an inter-lingual and computable ontological classification of verbs .", "The existence of a conceptual base that underlies all natural languages is an accepted claim as pointed out by Schank in his Conceptual Dependency ( CD ) theory .", "The percentage of coverage of these primitives is 100 % for all verbs in Sanskrit and Hindi and 3750 verbs in English .", "Inspired by the CD theory and theories in Indian grammatical tradition , we propose a new set of meaning primitives in this paper ."]}
{"orig_sents": ["6", "2", "9", "3", "8", "7", "1", "5", "0", "4"], "shuf_sents": ["And , finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset .", "Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes .", "Some of these data are in the form of free text and require preprocessing to be able to used in automated systems .", "As a case study , we analyzed classification of CT imaging reports into binary categories .", "Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation .", "A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class .", "Electronic health records ( EHRs ) contain important clinical information about patients .", "Topic modeling of the corpora provides interpretable themes that exist in these reports .", "In addition to regular text classification , we utilized topic modeling of the entire dataset in various ways .", "Efficient and effective use of this data could be vital to the speed and quality of health care ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["In order to facilitate annotation and to avoid bias , two alternative automatic pre-taggings are presented to the annotator , without revealing which of them is given a higher confidence by the pre-tagging system .", "To minimise the instances in which none of the presented pre-taggings is correct , the texts presented to the annotator are actively selected from a pool of unlabelled text , with the selection criterion that one of the presented pre-taggings should have a high probability of being correct , while still being useful for improving the result of an automatic classifier .", "The task of the annotator is to select the correct version among these two alternatives .", "For expanding a corpus of clinical text , annotated for named entities , a method that combines pre-tagging with a version of active learning is proposed ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The model outperforms most systems participating in the English track of the CoNLL ? 12 shared task .", "We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We propose an approach to comparison recognition through the use of syntactic patterns .", "This paper presents work in progress towards automatic recognition and classification of comparisons and similes .", "Keeping in mind the requirements of autistic readers , we discuss the properties relevant for distinguishing semantic criteria like figurativeness and abstractness .", "Among possible applications , we discuss the place of this task in text simplification for readers with Autism Spectrum Disorders ( ASD ) , who are known to have deficits in comprehending figurative language ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["The goal of the question analysis is to determine its general structure , type of an expected answer and create a search query for finding relevant documents in a textual knowledge base .", "The paper contains an overview of available solutions of these problems , description of their implementation and presents an evaluation based on a set of 1137 questions from a Polish quiz TV show .", "This study is devoted to the problem of question analysis for a Polish question answering system .", "The results help to understand how an environment of a Slavonic language affects the performance of methods created for English ."]}
{"orig_sents": ["2", "8", "3", "5", "1", "0", "6", "9", "7", "4"], "shuf_sents": ["Firstly , the corpus design is explained and the results of the validation experiments using human judges are reported .", "A corpus of sentences with annotated CWs is mined from Simple Wikipedia edit histories , which is then used as the basis for several experiments .", "Identifying complex words ( CWs ) is an important , yet often overlooked , task within lexical simplification ( The process of automatically replacing CWs with simpler alternatives ) .", "If too few words are identified then those which impede a user ? s understanding may be missed , resulting in a complex final text .", "The support vector machine achieves a slight increase in precision over the other two methods , but at the cost of a dramatic trade off in recall .", "This paper addresses the task of evaluating different methods for CW identification .", "Experiments are carried out into the CW identification techniques of : simplifying everything , frequency thresholding and training a support vector machine .", "? ve technique of simplifying everything .", "If too many words are identified then substitutions may be made erroneously , leading to a loss of meaning .", "These are based upon previous approaches to the task and show that thresholding does not perform significantly differently to the more na ?"]}
{"orig_sents": ["5", "0", "1", "4", "6", "3", "2"], "shuf_sents": ["We are working to automatically detect these chronic critics to prevent the spread of bad rumors about the reputation of the entity .", "In social media , most comments are informal , and , there are sarcastic and incomplete contexts .", "Our experimental results show that the proposed method outperforms analysis based only on opinion mining techniques .", "Thus , we propose a method that combines opinion mining with graph analysis for the connections between users to identify the chronic critics .", "This means that it is difficult for current NLP technology such as opinion mining to recognize the complaints .", "There are some chronic critics who always complain about the entity in social media .", "As an alternative approach for social media , we can assume that users who share the same opinions will link to each other ."]}
{"orig_sents": ["4", "0", "2", "6", "5", "3", "1"], "shuf_sents": ["Part-of-speech tagging is a crucial preliminary process in many natural language processing applications .", "Experiments show that our methodology works for words with high ambiguity .", "Because many words in natural languages have more than one part-of-speech tag , resolving part-of-speech ambiguity is an important task .", "This study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction .", "We study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting .", "A substitute vector is constructed with possible substitutes of a target word .", "We claim that partof-speech ambiguity can be solved using substitute vectors ."]}
{"orig_sents": ["5", "4", "1", "3", "2", "0"], "shuf_sents": ["We observed the same phenomena for Bangla verb sequences where experiments showed noncompositional verb sequences are in general stored as a whole in the ML and low traces of compositional verbs are found in the mental lexicon .", "For this , we have conducted a series of psycholinguistic experiments to build up hypothesis on the possible organizational structure of the mental lexicon .", "We observed that derivationally suffixed Bangla words are in general decomposed during processing and compositionality between the stem and the suffix plays an important role in the decomposition process .", "Next , we develop computational models based on the collected dataset .", "Our goal is to identify whether morphologically complex words are stored as a whole or are they organized along the morphological line .", "In this work we present psycholinguistically motivated computational models for the organization and processing of Bangla morphologically complex words in the mental lexicon ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["Machine translation ( MT ) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation .", "This comparison can be performed on multiple levels : lexical , syntactic or semantic .", "Based on experiments performed on English to German translations , we show that the new metric correlates well with human judgments at the system level .", "In this paper , we propose a new syntactic metric for MT evaluation based on the comparison of the dependency structures of the reference and the candidate translations .", "The dependency structures are obtained by means of a Weighted Constraints Dependency Grammar parser ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["In a multi-class document categorization using graph-based semi-supervised learning ( GBSSL ) , it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm .", "Furthermore , it is also important to provide high-quality correct data as training data .", "Experimenting on Reuters21578 corpus , we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization .", "In this context , we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageRank algorithm ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Our approach is rated as equally grammatical and beginner reader appropriate as a supervised SMT-based baseline system by native speakers , but our setup performs more radical changes that better resembles the variation observed in human generated simplifications .", "We present experiments using a new unsupervised approach to automatic text simplification , which builds on sampling and ranking via a loss function informed by readability research .", "The main idea is that a loss function can distinguish good simplification candidates among randomly sampled sub-sentences of the input sentence ."]}
{"orig_sents": ["8", "1", "7", "0", "3", "5", "2", "6", "4"], "shuf_sents": ["Then probabilistic inference enables us to see the strength of such relationships : given the observed value of one feature ( or combination of features ) , the probabilities of values of other features can be calculated .", "We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph ( DAG ) .", "Model averaging technique solves the problem of limited data .", "Our model is not restricted to using only two values of a feature .", "III and Campbell ( 2007 ) .", "Using imputation technique and EM algorithm it can handle missing values well .", "In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daum ?", "Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features .", "In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals ."]}
{"orig_sents": ["6", "2", "0", "5", "1", "3", "4"], "shuf_sents": ["Our system uses a trainable classifier to predict ? edit scripts ?", "Suffixes of lemmas are included as features to achieve robustness .", "the prediction of inflected word forms given lemma , part-of-speech and morphological features , aimed at robustness to unseen inputs .", "We evaluate our system on 6 languages with a varying degree of morphological richness .", "The results show that the system is able to learn most morphological phenomena and generalize to unseen inputs , producing significantly better results than a dictionarybased baseline .", "that are then used to transform lemmas into inflected word forms .", "We present a novel method of statistical morphological generation , i.e ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We show that it enables to predict two behaviorbased measures across a range of parameters in a Latent Semantic Analysis model .", "Evaluation methods for Distributional Semantic Models typically rely on behaviorally derived gold standards .", "We introduce a corpus-based measure that evaluates the stability of the lexical semantic similarity space using a pseudo-synonym same-different detection task and no external resources .", "These methods are difficult to deploy in languages with scarce linguistic/behavioral resources ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency .", "On the English-to-Czech translation pair , we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge .", "Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs ."]}
{"orig_sents": ["0", "3", "4", "2", "1", "5"], "shuf_sents": ["We present WebAnno , a general purpose web-based annotation tool for a wide range of linguistic annotations .", "Currently WebAnno allows part-ofspeech , named entity , dependency parsing and co-reference chain annotations .", "It supports arbitrarily large documents , pluggable import/export filters , the curation of annotations across various users , and an interface to farming out annotations to a crowdsourcing platform .", "WebAnno offers annotation project management , freely configurable tagsets and the management of users in different roles .", "WebAnno uses modern web technology for visualizing and editing annotations in a web browser .", "The architecture design allows adding additional modes of visualization and editing , when new kinds of annotations are to be supported ."]}
{"orig_sents": ["3", "1", "4", "0", "5", "2"], "shuf_sents": ["We further evaluate our method on a recent crawl of Twitter data to investigate the impact of temporal factors on model generalisation .", "The system infers a user ? s location based on both tweet text and user-declared metadata using a stacking approach .", "We also describe two ways of accessing/demoing our system .", "We implement a city-level geolocation prediction system for Twitter users .", "We demonstrate that the stacking method substantially outperforms benchmark methods , achieving 49 % accuracy on a benchmark dataset .", "Our results suggest that user-declared location metadata is more sensitive to temporal change than the text of Twitter messages ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["This toolkit also serves as an interface between existing software packages and frequently used data formats , and it provides implementations of new and existing algorithms within a homogeneous framework .", "Given the increasing interest and development of computational and quantitative methods in historical linguistics , it is important that scholars have a basis for documenting , testing , evaluating , and sharing complex workflows .", "We then illustrate evaluation metrics on gold standard datasets that are provided with the toolkit .", "We illustrate the toolkit ? s functionality with an exemplary workflow that starts with raw language data and ends with automatically calculated phonetic alignments , cognates and borrowings .", "We present a novel open-source toolkit for quantitative tasks in historical linguistics that offers these features ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Utilising AnnoMarket is straightforward , since cloud infrastructural issues are dealt with by the platform , completely transparently to the user : load balancing , efficient data upload and storage , deployment on the virtual machines , security , and fault tolerance .", "This paper presents AnnoMarket , an open cloud-based platform which enables researchers to deploy , share , and use language processing components and resources , following the data-as-a-service and software-as-a-service paradigms .", "The focus is on multilingual text analysis resources and services , based on an opensource infrastructure and compliant with relevant NLP standards .", "We demonstrate how the AnnoMarket platform can be used to develop NLP applications with little or no programming , to index the results for enhanced browsing and search , and to evaluate performance ."]}
{"orig_sents": ["4", "2", "1", "3", "0"], "shuf_sents": ["This system will be part of the EMM media monitoring framework working live and it will be demonstrated using Google Earth .", "This paper describes a system that links the main events detected from clusters of newspaper articles to tweets related to them , detects complementary information sources from the links they contain and subsequently applies sentiment analysis to classify them into positive , negative and neutral .", "As such , people no loger remain mere spectators to the events that happen in the world , but become part of them , commenting on their developments and the entities involved , sharing their opinions and distributing related content .", "In this manner , readers can follow the main events happening in the world , both from the perspective of mainstream as well as social media and the public ? s perception on them .", "Nowadays , the importance of Social Media is constantly growing , as people often use such platforms to share mainstream media news and comment on the events that they relate to ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Furthermore , DISSECT can be useful to researchers and practitioners who need models of word meaning ( without composition ) as well , as it supports various methods to construct distributional semantic spaces , assessing similarity and even evaluating against benchmarks , that are independent of the composition infrastructure .", "The toolkit focuses in particular on compositional meaning , and implements a number of composition methods that have been proposed in the literature .", "We introduce DISSECT , a toolkit to build and explore computational models of word , phrase and sentence meaning based on the principles of distributional semantics ."]}
{"orig_sents": ["4", "5", "0", "1", "2", "3"], "shuf_sents": ["In this paper we present DKPro WSD , a freely licensed , general-purpose framework for WSD which is both modular and extensible .", "DKPro WSD abstracts the WSD process in such a way that test corpora , sense inventories , and algorithms can be freely swapped .", "Its UIMA-based architecture makes it easy to add support for new resources and algorithms .", "Related tasks such as word sense induction and entity linking are also supported .", "Implementations of word sense disambiguation ( WSD ) algorithms tend to be tied to a particular test corpus format and sense inventory .", "This makes it difficult to test their performance on new data sets , or to compare them against past algorithms implemented for different data sets ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["The enhancements exploit the UIMA Subject of Analysis ( Sofa ) mechanism , that allows different facets of the input data to be represented .", "U-Compare is a UIMA-based workflow construction platform for building natural language processing ( NLP ) applications from heterogeneous language resources ( LRs ) , without the need for programming skills .", "U-Compare has been adopted within the context of the METANET Network of Excellence , and over 40 LRs that process 15 European languages have been added to the U-Compare component library .", "We demonstrate how our customised extensions to U-Compare allow the construction and testing of NLP applications that transform the input data in different ways , e.g. , machine translation , automatic summarisation and text-to-speech .", "In line with METANET ? s aims of increasing communication between citizens of different European countries , U-Compare has been extended to facilitate the development of a wider range of applications , including both multilingual and multimodal workflows ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["The growing need for Chinese natural language processing ( NLP ) is largely in a range of research and commercial applications .", "However , most of the currently Chinese NLP tools or components still have a wide range of issues need to be further improved and developed .", "FudanNLP is an open source toolkit for Chinese natural language processing ( NLP ) , which uses statistics-based and rule-based methods to deal with Chinese NLP tasks , such as word segmentation , part-ofspeech tagging , named entity recognition , dependency parsing , time phrase recognition , anaphora resolution and so on ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["ICARUS also ships with plugins that enable it to interface with tool chains running either locally or remotely .", "We present ICARUS , a versatile graphical search tool to query dependency treebanks .", "Search results can be inspected both quantitatively and qualitatively by means of frequency lists , tables , or dependency graphs ."]}
{"orig_sents": ["6", "1", "5", "4", "3", "2", "0"], "shuf_sents": ["questions about MONSERRATE .", "The platform is developed within a game environment , and currently allows speech recognition and synthesis in Portuguese , English and Spanish .", "In this paper we also introduce EDGAR , the butler of MONSERRATE , which was developed in the aforementioned platform , and that answers tourists ?", "community .", "Most indomain interactions are answered using different similarity metrics , which compare the perceived utterances with questions/sentences in the agent ? s knowledge base ; small-talk capabilities are mainly due to AIML , a language largely used by the chatbots ?", "In this paper we focus on its understanding component that supports in-domain interactions , and also small talk .", "In this paper we describe a platform for embodied conversational agents with tutoring goals , which takes as input written and spoken questions and outputs answers in both forms ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The strategies used by PAL , including semantic-extension-based question matching , solution management with personal information consideration , and XML-based knowledge pattern construction , are described and discussed .", "This system focuses on providing primary suggestions or helping people relieve pressure by extracting knowledge from online forums , based on which the chatterbot system is constructed .", "We also conduct a primary test for the feasibility of our system .", "In this paper , we propose PAL , a prototype chatterbot for answering non-obstructive psychological domain-specific questions ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The usefulness of the tool is demonstrated with three case studies that deal with vowel harmony and similar place avoidance patterns .", "The cooccurrence counts from the user-specified context are statistically analyzed according to a number of association measures that can be selected by the user .", "The statistical values then serve as the input for a matrix visualization where rows and columns represent the relevant sounds under investigation and the matrix cells indicate whether the respective ordered pair of sounds occurs more or less frequently than expected .", "This paper describes the online tool PhonMatrix , which analyzes a word list with respect to the co-occurrence of sounds in a specified context within a word ."]}
{"orig_sents": ["4", "3", "0", "2", "1"], "shuf_sents": [") , as well as language tools ( parsers , part-of-speech tags , etc . ) .", "We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms .", "It also provides machine learning algorithms to build quality estimation models .", "The framework allows the extraction of several quality indicators from source segments , their translations , external resources ( corpora , language models , topic models , etc .", "We describe QUEST , an open source framework for machine translation quality estimation ."]}
{"orig_sents": ["2", "4", "5", "0", "6", "1", "3", "7"], "shuf_sents": ["One way to handle such texts is to modify them prior to translation .", "In this paper we present an interactive system where source modifications are induced by confidence estimates that are derived from the translation model in use .", "The quality of automatic translation is affected by many factors .", "Modifications are automatically generated and proposed for the user ? s approval .", "One is the divergence between the specific source and target languages .", "Another lies in the source text itself , as some texts are more complex than others .", "Yet , an important factor that is often overlooked is the source translatability with respect to the specific translation system and the specific model that are being used .", "Such a system can reduce postediting effort , replacing it by cost-effective pre-editing that can be done by monolinguals ."]}
{"orig_sents": ["6", "0", "3", "1", "4", "2", "5"], "shuf_sents": ["It provides an open-source C++ implementation for the entire forest-to-string MT pipeline , including rule extraction , tuning , decoding , and evaluation .", "The training pipeline is modeled after that of the popular Moses decoder , so users familiar with Moses should be able to get started quickly .", "As auxiliary results , we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder .", "There are a number of options for model training , and tuning includes advanced options such as hypergraph MERT , and training of sparse features through online learning .", "We perform a validation experiment of the decoder on EnglishJapanese machine translation , and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation .", "Travatar is available under the LGPL at http : //phontron.com/travatar", "In this paper we describe Travatar , a forest-to-string machine translation ( MT ) engine based on tree transducers ."]}
{"orig_sents": ["2", "0", "3", "4", "1"], "shuf_sents": ["We provide PLIS with two probabilistic implementation of this framework .", "PLIS includes an online interactive viewer , which is a powerful tool for investigating lexical inference processes .", "This paper presents PLIS , an open source Probabilistic Lexical Inference System which combines two functionalities : ( i ) a tool for integrating lexical inference knowledge from diverse resources , and ( ii ) a framework for scoring textual inferences based on the integrated knowledge .", "PLIS is available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications .", "PLIS is easily configurable , components can be extended or replaced with user generated ones to enable system customization and further research ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia .", "We release a Java API for the programmatic use of multilingual WCLs in three languages ( English , French and Italian ) , as well as a Web application for definition and hypernym extraction from user-provided sentences .", "In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices ( WCLs ) , a supervised lattice-based model used to identify textual definitions and extract hypernyms from them ."]}
{"orig_sents": ["5", "4", "1", "0", "6", "3", "7", "2"], "shuf_sents": ["In this paper , we demonstrate Argo , a Web-based workbench for the development and processing of NLP pipelines/workflows .", "The architecture has been gaining attention from industry and academia alike , resulting in a large volume of UIMA-compliant processing components .", "The distributed development feature allows users to seamlessly connect their tools to workflows running in Argo , and thus take advantage of both the available library of components ( without the need of installing them locally ) and the analytical tools .", "We present features , and show examples , of facilitating the distributed development of components and the analysis of processing results .", "The Unstructured Information Management Architecture ( UIMA ) is an industry standard whose aim is to ensure such interoperability by defining common data structures and interfaces .", "Developing sophisticated NLP pipelines composed of multiple processing tools and components available through different providers may pose a challenge in terms of their interoperability .", "The workbench is based upon UIMA , and thus has the potential of using many of the existing UIMA resources .", "The latter includes annotation visualisers and editors , as well as serialisation to RDF format , which enables flexible querying in addition to data manipulation thanks to the semantic query language SPARQL ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["In order to promote the reproducibility of experimental results and to provide reliable , permanent experimental conditions for future studies , DKPro Similarity additionally comes with a set of full-featured experimental setups which can be run out-of-the-box and be used for future systems to built upon .", "DKPro Similarity comprises a wide variety of measures ranging from ones based on simple n-grams and common subsequences to high-dimensional vector comparisons and structural , stylistic , and phonetic measures .", "Our goal is to provide a comprehensive repository of text similarity measures which are implemented using standardized interfaces .", "We present DKPro Similarity , an open source framework for text similarity ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["Fluid Construction Grammar ( FCG ) is an open-source computational grammar formalism that is becoming increasingly popular for studying the history and evolution of language .", "This demonstration shows how FCG can be used to operationalise the cultural processes and cognitive mechanisms that underly language evolution and change ."]}
{"orig_sents": ["4", "7", "6", "0", "3", "2", "1", "5"], "shuf_sents": ["As a consequence , existing systems are incapable of performing ad-hoc type classification on arbitrary input texts .", "Our system offers an online interface where natural-language text can be inserted , which returns semantic type labels for entity mentions .", "Thanks to its efficient implementation and compacted feature representation , the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations .", "In this demo , we present a novel Webbased tool that is able to perform domain independent entity type classification under real time conditions .", "Recent research has shown progress in achieving high-quality , very fine-grained type classification in hierarchical taxonomies .", "Further more , the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy .", "In order to achieve high-precision in type classification , current approaches are either limited to certain domains or require time consuming multistage computations .", "Within such a multi-level type hierarchy with several hundreds of types at different levels , many entities naturally belong to multiple types ."]}
{"orig_sents": ["5", "2", "6", "7", "4", "9", "1", "0", "3", "8"], "shuf_sents": ["In addition , Linggle provides example sentences from The New York Times on demand .", "Clusters of synonyms or conceptually related words are also provided .", "30013 { joanne.boisson , maxis1718 , wujc86 , joseph.yen , jason.jschang } @ gmail.com Abstract In this paper , we introduce a Web-scale lin-guistics search engine , Linggle , that retrieves lexical bundles in response to a given query .", "The current implementation of Linggle is the most functionally comprehen-sive , and is in principle language and dataset independent .", "The method in-volves parsing the query to transforming it in-to several keyword retrieval commands .", "a Web-scale Linguistic Search Engine for Words in Context Joanne Boisson+ , Ting-Hui Kao* , Jian-Cheng Wu* , Tzu-His Yen* , Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu , Taiwan , R.O.C .", "The query might contain keywords , wildcards , wild parts of speech ( PoS ) , synonyms , and ad-ditional regular expression ( RE ) operators .", "In our approach , we incorporate inverted file in-dexing , PoS information from BNC , and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T .", "We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major lan-guages in the world .", "Word chunks are retrieved with counts , further filter-ing the chunks with the query as a RE , and fi-nally displaying the results according to the counts , similarities , and topics ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition .", "all within a single interface .", "Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications , it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections .", "We present ParaQuery , a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection , analyze its utility for a particular domain , and compare it to other popular lexical similarity resources ?"]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana , the European Library .", "The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system .", "Europeana contains over 20 million artefacts with meta-data in a wide range of European languages .", "The system currently provides access to Europeana content with meta-data in English and Spanish ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We introduce the proposed five step workflow for creating information extractors , the graph query based rule language , as well as the core features of the PROPMINER tool .", "The use of deep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction .", "In this system demonstration , we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees .", "Despite this potential , the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["It is available as a Java library and as a Java standalone ap-plication offering GUI-based access to the implemented semantic similarity methods .", "We present in this paper SEMILAR , the SEMantic simILARity toolkit .", "SEMILAR implements a number of algorithms for assessing the semantic similarity between two texts .", "Furthermore , it offers facilities for manual se-mantic similarity annotation by experts through its component SEMILAT ( a SEMantic simILarity Annotation Tool ) ."]}
{"orig_sents": ["1", "3", "4", "2", "0"], "shuf_sents": ["We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual , and constructs a story around its use of the landscape .", "The aim of the Tag2Blog system is to bring satellite tagged wild animals ? to life ?", "We are working with one of the largest nature conservation charities in Europe in this regard , focusing on a single species , the red kite .", "through narratives that place their movements in an ecological context .", "Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme , although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags ."]}
{"orig_sents": ["3", "5", "2", "1", "4", "0"], "shuf_sents": ["For a complex domain like judicial proceedings , the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work .", "TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing .", "Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks : ( a ) translation of constituent phrases of the sentence ; ( b ) validation of quality of the phrase translations ; and ( c ) composition of complete sentence translations from phrase translations .", "Large amount of parallel corpora is required for building Statistical Machine Translation ( SMT ) systems .", "We have evaluated the crowd ? s output using the METEOR metric .", "We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators ."]}
{"orig_sents": ["4", "0", "2", "1", "3"], "shuf_sents": ["tSEARCH uses the evaluation results obtained with the ASIYA toolkit for MT evaluation and it is connected to its on-line GUI , which makes possible a graphical visualization and interactive access to the evaluation results .", "Its database design permits a fast response time for all queries supported on realistic-size test beds .", "The search engine offers a flexible query language allowing to find translation examples matching a combination of numerical and structural features associated to the calculation of the quality metrics .", "In summary , tSEARCH , used with ASIYA , offers developers of MT systems and evaluation metrics a powerful tool for helping translation and error analysis .", "This work presents tSEARCH , a web-based application that provides mechanisms for doing complex searches over a collection of translation cases evaluated with a large set of diverse measures ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["VSEM is entirely written in MATLAB and its objectoriented design allows a large flexibility and reusability .", "Starting from a collection of tagged images , it is possible to automatically construct an image-based representation of concepts by using off-theshelf VSEM functionalities .", "VSEM is an open library for visual semantics .", "The software is accompanied by a website with supporting documentation and examples ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data .", "We present an open-source framework for large-scale online structured learning .", "Although designed with a focus on SMT , the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing .", "Developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation ( SMT ) , our large-margin learner can be used with any decoder ."]}
{"orig_sents": ["0", "3", "2", "5", "4", "1"], "shuf_sents": ["We consider the construction of part-of-speech taggers for resource-poor languages .", "We further present successful results on seven additional languages from different families , empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages .", "In this paper , we show that additional token constraints can be projected from a resourcerich source language to a resource-poor target language via word-aligned bitext .", "Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .", "Averaged across eight previously studied Indo-European languages , our model achieves a 25 % relative error reduction over the prior state of the art .", "We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees .", "1-EndpointCrossing trees also have natural connections to linguistics and another class of graphs that has been studied in NLP .", "For 95.899.8 % of dependency parses in various natural language treebanks , whenever an edge is crossed , the edges that cross it all have a common vertex .", "The optimal dependency tree that satisfies this 1-Endpoint-Crossing property can be found with an O ( n4 ) parsing algorithm that recursively combines forests over intervals with one exterior point ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["In this paper , we consider the problem of grounding sentences describing actions in visual information extracted from videos .", "Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions , but so far only visual information extracted from static images has been used .", "We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos , together with an annotation of how similar the action descriptions are to each other .", "Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions ."]}
{"orig_sents": ["4", "5", "0", "1", "2", "6", "3"], "shuf_sents": ["Dynamic programming is used to search the upper bound .", "Experiments are conducted on English PTB and Chinese CTB datasets .", "We achieved competitive Unlabeled Attachment Score ( UAS ) when no additional resources are available : 93.17 % for English and 87.25 % for Chinese .", "Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models .", "Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference .", "In this paper , we proposed an exact and efficient decoding algorithm based on the Branch and Bound ( B & B ) framework where nonlocal features are bounded by a linear combination of local features .", "Parsing speed is 177 words per second for English and 97 words per second for Chinese ."]}
{"orig_sents": ["1", "0", "2", "3", "4"], "shuf_sents": ["In this paper , we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions , using various types of weak supervision .", "The context in which language is used provides a strong signal for learning to recover its meaning .", "The joint nature provides crucial benefits by allowing situated cues , such as the set of visible objects , to directly influence learning .", "It also enables algorithms that learn while executing instructions , for example by trying to replicate human actions .", "Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision , including correctly executing 60 % more instruction sets relative to the previous state of the art ."]}
{"orig_sents": ["5", "0", "4", "2", "3", "1"], "shuf_sents": ["Progress has been made on this task using text-based models , but few computational approaches have considered how infants might benefit from acoustic cues .", "These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants , and motivate the use of word duration cues in NLP tasks with speech .", "We describe how duration information can be incorporated into an unsupervised Bayesian dependency parser whose only other source of information is the words themselves ( without punctuation or parts of speech ) .", "Our results , evaluated on both adult-directed and child-directed utterances , show that using word duration can improve parse quality relative to words-only baselines .", "This paper explores the hypothesis that word duration can help with learning syntax .", "Unsupervised parsing is a difficult task that infants readily perform ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We introduce a novel nonparametric Bayesian model for the induction of Combinatory Categorial Grammars from POS-tagged text .", "It achieves state of the art performance on a number of languages , and induces linguistically plausible lexicons ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In this paper , we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way , thus taking advantages of both topic model and feature based supervised learning methods .", "Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization .", "Experimental results on DUC2007 , TAC2008 and TAC2009 demonstrate the effectiveness of our approach ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing .", "Profile enrichment alone yields up to 15 % improvement in the accuracy of the seed lexicon on 3way sentence-level sentiment polarity classification of essay data .", "Using lexical expansion in addition to sentiment profiles provides a further 7 % improvement in performance .", "Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application ( product reviews ) ."]}
{"orig_sents": ["3", "4", "5", "2", "0", "1"], "shuf_sents": ["Unlike existing parsers , our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence .", "We show that it outperforms an n-gram model in predicting more than one upcoming word .", "In addition to whole-sentence F-score , we also evaluate the partial trees that the parser constructs for sentence prefixes ; partial trees play an important role in incremental interpretation , language modeling , and psycholinguistics .", "In this paper , we present the first incremental parser for Tree Substitution Grammar ( TSG ) .", "A TSG allows arbitrarily large syntactic fragments to be combined into complete trees ; we show how constraints ( including lexicalization ) can be imposed on the shape of the TSG fragments to enable incremental processing .", "We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers ."]}
{"orig_sents": ["5", "1", "0", "2", "4", "3"], "shuf_sents": ["Our corpus consists of child sentences with corrected adult forms .", "In this paper , we introduce a data set and approach for systematically modeling this child-adult grammar divergence .", "We bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances .", "Our model allows us to chart specific aspects of grammar development in longitudinal studies of children , and investigate the hypothesis that children share a common developmental path in language acquisition .", "Our method outperforms MT and ESL baselines , reducing child error by 20 % .", "During the course of first language acquisition , children produce linguistic forms that do not conform to adult grammar ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["A dynamic programming shift-reduce parser produces a packed derivation forest which is then scored by a discriminative reranker , using the 1-best tree output by the shift-reduce parser as guide features in addition to third-order graph-based features .", "This paper proposes a discriminative forest reranking algorithm for dependency parsing that can be seen as a form of efficient stacked parsing .", "Testing on the English Penn Treebank data , forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12 .", "To improve efficiency and accuracy , this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arcstandard transition systems ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall .", "In this paper , we present Dijkstra-WSA , a novel graph-based algorithm for word sense alignment .", "We evaluate it on four different pairs of lexical-semantic resources with different characteristics ( WordNet-OmegaWiki , WordNet-Wiktionary , GermaNet-Wiktionary and WordNet-Wikipedia ) and show that it achieves competitive performance on 3 out of 4 datasets .", "Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity ."]}
{"orig_sents": ["2", "1", "4", "5", "0", "3"], "shuf_sents": ["A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks , demonstrating that even newcomers to the field can help improve the state-ofthe-art on hard NLP problems while simultaneously learning a great deal .", "There are excellent pedagogical texts , but problems in MT and current algorithms for solving them are best learned by doing .", "Machine translation ( MT ) draws from several different disciplines , making it a complex subject to teach .", "The problems , baseline code , and results are freely available .", "As a centerpiece of our MT course , we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks : alignment , decoding , evaluation , and reranking .", "Students brought a diverse set of techniques to the problems , including some novel solutions which performed remarkably well ."]}
{"orig_sents": ["4", "1", "2", "5", "3", "0"], "shuf_sents": ["We outperform a variety of existing approaches on a wide-coverage question answering task , and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite .", "Distributional models have been successful in modelling the meanings of content words , but logical semantics is necessary to adequately represent many function words .", "We follow formal semantics in mapping language to logical representations , but differ in that the relational constants used are induced by offline distributional clustering at the level of predicateargument structure .", "Different senses of a word are disambiguated based on their induced types .", "We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics .", "Our clustering algorithm is highly scalable , allowing us to run on corpora the size of Gigaword ."]}
{"orig_sents": ["10", "4", "7", "9", "1", "6", "8", "2", "0", "3", "5"], "shuf_sents": ["We perform experiments on two applications : scene understanding and geographical question answering .", "? mug ? )", "We further introduce a weakly supervised training procedure that estimates LSP ? s parameters using annotated referents for entire statements , without annotated referents for individual words or the parse structure of the statement .", "We find that LSP outperforms existing , less expressive models that can not represent relational language .", "For example , given an image , LSP can map the statement ? blue mug on the table ?", "We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort .", "and relational ( ? on ? )", "to the set of image segments showing blue mugs on tables .", "language , and also learns to compose these representations to produce the referents of entire statements .", "LSP learns physical representations for both categorical ( ? blue , ?", "This paper introduces Logical Semantics with Perception ( LSP ) , a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment ."]}
{"orig_sents": ["2", "3", "4", "1", "0"], "shuf_sents": ["As a result , this training process is as efficient as existing online learning methods , and yet derives consistently better models , as evaluated on four benchmark NLP datasets for part-ofspeech tagging , named-entity recognition and dependency parsing .", "Unlike algorithms such as Perceptron and stochastic gradient descent , our method keeps track of dual variables and updates the weight vector more aggressively .", "Due to the nature of complex NLP problems , structured prediction algorithms have been important modeling tools for a wide range of tasks .", "While there exists evidence showing that linear Structural Support Vector Machine ( SSVM ) algorithm performs better than structured Perceptron , the SSVM algorithm is still less frequently chosen in the NLP community because of its relatively slow training speed .", "In this paper , we propose a fast and easy-toimplement dual coordinate descent algorithm for SSVMs ."]}
{"orig_sents": ["3", "2", "5", "1", "0", "4"], "shuf_sents": ["Finally , we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations .", "For the syntactic part , we define a standard arc-factored dependency model that predicts the full syntactic tree .", "The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments .", "In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing .", "In experiments on the CoNLL-2009 English benchmark we observe very competitive results .", "This process is framed as a linear assignment task , which allows to control some well-formedness constraints ."]}
{"orig_sents": ["3", "5", "0", "6", "4", "1", "2"], "shuf_sents": ["Given a preposition in a sentence , our computational task to jointly model the preposition relation and its arguments along with their semantic types , as a way to support the relation prediction .", "Our generalization of latent structure SVM gives close to 90 % accuracy on relation labeling .", "Further , by jointly predicting the relation , arguments , and their types along with preposition sense , we show that we can not only improve the relation accuracy , but also significantly improve sense prediction accuracy .", "This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations , their arguments and the semantic types of the arguments .", "We address this by presenting two models for preposition relation labeling .", "We define an inventory of 32 relations , building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions .", "The annotated data , however , only provides labels for the relation label , and not the arguments and types ."]}
{"orig_sents": ["0", "4", "5", "3", "2", "1"], "shuf_sents": ["In current research , most tree-based translation models are built directly from parse trees .", "Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees .", "The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes .", "To train the model efficiently , we develop a Gibbs sampler with three novel Gibbs operators .", "In this study , we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model .", "In the model , we utilize synchronous tree substitution grammars ( STSG ) to capture the bilingual mapping between language pairs ."]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["This paper explores the use of Adaptor Grammars , a nonparametric Bayesian modelling framework , for minimally supervised morphological segmentation .", "Moreover , this method provides the potential to tune performance according to different evaluation metrics or downstream tasks .", "We compare three training methods : unsupervised training , semisupervised training , and a novel model selection method .", "In the model selection method , we train unsupervised Adaptor Grammars using an over-articulated metagrammar , then use a small labelled data set to select which potential morph boundaries identified by the metagrammar should be returned in the final output .", "We evaluate on five languages and show that semi-supervised training provides a boost over unsupervised training , while the model selection method yields the best average results over all languages and is competitive with state-ofthe-art semi-supervised systems ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In this article we extend these techniques to a class of non-projective dependency trees , called well-nested dependency trees with block-degree at most 2 , which has been previously investigated in the literature .", "We define a structural property that allows head splitting for these trees , and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage .", "Head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees , under the arc-factored model ."]}
{"orig_sents": ["2", "4", "3", "5", "0", "1", "6"], "shuf_sents": ["When ranking English adjectives , our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics ( specifically , 70 % pairwise accuracy as compared to only 56 % by previous work ) .", "Moreover , our approach can incorporate external synonymy information ( increasing its pairwise accuracy to 78 % ) and extends easily to new languages .", "Adjectives like good , great , and excellent are similar in meaning , but differ in intensity .", "In this paper , we present a primarily unsupervised approach that uses semantics from Web-scale data ( e.g. , phrases like good but not excellent ) to rank words by assigning them positions on a continuous scale .", "Intensity order information is very useful for language learners as well as in several NLP tasks , but is missing in most lexical resources ( dictionaries , WordNet , and thesauri ) .", "We rely on Mixed Integer Linear Programming to jointly determine the ranks , such that individual decisions benefit from global information .", "We also make our code and data freely available.1"]}
{"orig_sents": ["3", "0", "4", "1", "5", "2"], "shuf_sents": ["It has been verified to be a useful constraint for word alignment .", "In this paper , we take dependency cohesion as a soft constraint , and integrate it into a generative model for large-scale word alignment experiments .", "Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality .", "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language .", "However , previous work either treats this as a hard constraint or uses it as a feature in discriminative models , which is ineffective for large-scale tasks .", "We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["Inspired by the impact of a constituency grammar on dependency parsing , we propose several strategies to acquire pseudo CFGs only from dependency annotations .", "We present a comparative study of transition- , graph- and PCFG-based models aimed at illuminating more precisely the likely contribution of CFGs in improving Chinese dependency parsing accuracy , especially by combining heterogeneous models .", "Moreover , pseudo grammars increase the diversity of base models ; therefore , together with all other models , further improve system combination .", "Compared to linguistic grammars learned from rich phrase-structure treebanks , well designed pseudo grammars achieve similar parsing accuracy and have equivalent contributions to parser ensemble .", "Based on automatic POS tagging , our final model achieves a UAS of 87.23 % , resulting in a significant improvement of the state of the art ."]}
{"orig_sents": ["5", "2", "1", "4", "6", "0", "3"], "shuf_sents": ["By casting grounded language learning as a grammatical inference task , we use our parser to extend the work of Johnson et al ( 2012 ) , investigating the importance of discourse continuity in children ? s language acquisition and its interaction with social cues .", "In the context of language acquisition , this independence assumption discards cues that are important to the learner , e.g. , the fact that consecutive utterances are likely to share the same referent ( Frank et al , 2013 ) .", "In most work on this topic , however , utterances in a conversation are treated independently and discourse structure information is largely ignored .", "Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators .", "The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels .", "Grounded language learning , the task of mapping from natural language to a representation of meaning , has attracted more and more interest in recent years .", "We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens , creating parse trees that represent full discourses ."]}
{"orig_sents": ["4", "1", "5", "0", "3", "2"], "shuf_sents": ["The space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another .", "In fact , the optimal tradeoff between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space .", "Significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline , while BLEU and METEOR remain stable , or even increase , at a very high distortion limit .", "The integration of this model into a phrase-based decoder improves a strong Arabic-English baseline already including state-of-the-art early distortion cost ( Moore and Quirk , 2007 ) and hierarchical phrase orientation models ( Galley and Manning , 2008 ) .", "Defining the reordering search space is a crucial issue in phrase-based SMT between distant languages .", "We propose a method to dynamically shape such space and , thus , capture long-range word movements without hurting translation quality nor decoding time ."]}
{"orig_sents": ["6", "0", "1", "5", "4", "3", "2"], "shuf_sents": ["Readers seek out articles that are beautifully written , informative and entertaining .", "Yet information-access technologies lack capabilities for predicting article quality at this level .", "We show that the distinction between great and typical articles can be detected fairly accurately , and that the entire spectrum of our features contribute to the distinction .", "We implement features to capture aspects of great writing , including surprising , visual and emotional content , as well as general features related to discourse organization and sentence structure .", "We introduce a corpus of great pieces of science journalism , along with typical articles from the genre .", "In this paper we present first experiments on article quality prediction in the science journalism domain .", "Great writing is rare and highly admired ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["Linking implicit semantic roles is a challenging problem in discourse processing .", "Unlike prior work inspired by SRL , we cast this problem as an anaphora resolution task and embed it in an entity-based coreference resolution ( CR ) architecture .", "We achieve performance beyond state-of-the art .", "Our experiments clearly show that CR-oriented features yield strongest performance exceeding a strong baseline .", "We address the problem of data sparsity by applying heuristic labeling techniques , guided by the anaphoric nature of the phenomenon ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement , and over 5 % improvement in F1 measure for the target pronouns when evaluated on the ACE 2004 newswire corpus .", "A significant advantage of the new approach is that the expert rules can be easily augmented with new semantic features .", "We present a novel adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters .", "We demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper explores the hypothesis that semantic relatedness may be more reliably inferred by using a multilingual space , as compared to the typical monolingual representation .", "Through evaluations using several stateof-the-art semantic relatedness systems , applied on standard datasets , we show that a multilingual approach is better suited for this task , and leads to improvements of up to 47 % with respect to the monolingual baseline ."]}
{"orig_sents": ["4", "2", "0", "3", "1", "5", "6"], "shuf_sents": ["These links serve as an excellent resource for obtaining lexical translations , or building multilingual dictionaries and semantic networks .", "This paper describes a supervised learning method for generating new links and detecting existing incorrect links .", "Wikipedia articles on the same topic in different languages are connected via interlingual ( or translational ) links .", "As these links are manually built , many links are missing or simply wrong .", "Wikipedia is a Web based , freely available multilingual encyclopedia , constructed in a collaborative effort by thousands of contributors .", "Since there is no dataset available to evaluate the resulting interlingual links , we create our own gold standard by sampling translational links from four language pairs using distance heuristics .", "We manually annotate the sampled translation links and used them to evaluate the output of our method for automatic link detection and correction ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents a novel sentence clustering scheme based on projecting sentences over term clusters .", "The scheme incorporates external knowledge to overcome lexical variability and small corpus size , and outperforms common sentence clustering methods on two reallife industrial datasets ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["We present the results of several machine learning tasks designed to predict rhetorical relations that hold between clauses in discourse .", "We demonstrate that organizing rhetorical relations into different granularity categories ( based on relative degree of detail ) increases average prediction accuracy from 58 % to 70 % .", "Accuracy further increases to 80 % with the inclusion of clause types .", "These results , which are competitive with existing systems , hold across several modes of written discourse and suggest that features of information structure are an important consideration in the machine learnability of discourse ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We conduct experiments using syntactic information and immediate context for Dutch and English .", "The correct choice of words has proven challenging for learners of a second language and errors of this kind form a separate category in error typology .", "This paper focuses on one known example of two verbs that are often confused by non-native speakers of Germanic languages , to make and to do .", "Our results show that the methods exploiting syntactic information and distributional similarity yield the best results ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["Text reuse is common in many scenarios and documents are often based , at least in part , on existing documents .", "This paper reports an approach to detecting text reuse which identifies not only documents which have been reused verbatim but is also designed to identify cases of reuse when the original has been rewritten .", "The approach is applied to a corpus of newspaper stories and found to outperform a previously reported method .", "The approach identifies reuse by comparing word n-grams in documents and modifies these ( by substituting words with synonyms and deleting words ) to identify when text has been altered ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In this paper we explore alternative term representations , complemented by clustering of morphological variants .", "Corpus-based thesaurus construction for Morphologically Rich Languages ( MRL ) is a complex task , due to the morphological variability of MRL .", "We introduce a generic algorithmic scheme for thesaurus construction in MRL , and demonstrate the empirical benefit of our methodology for a Hebrew thesaurus ."]}
{"orig_sents": ["3", "5", "6", "4", "2", "0", "1"], "shuf_sents": ["The classifier overall achieves 79.4 % accuracy , 41.1 % error deduction compared to the corpus majority baseline 65 % .", "However , it is even more interesting to discover that the classifier learns more from the more compositional examples than those idiomatic ones .", "We build a discriminative classifier with easily available lexical and syntactic features and test it over the datasets .", "In this paper , we investigate a full-fledged supervised machine learning framework for identifying English phrasal verbs in a given context .", "This dataset is further split into two groups , more idiomatic group which consists of those that tend to be used as a true phrasal verb and more compositional group which tends to be used either way .", "We concentrate on those that we define as the most confusing phrasal verbs , in the sense that they are the most commonly used ones whose occurrence may correspond either to a true phrasal verb or an alignment of a simple verb with a preposition .", "We construct a benchmark dataset1 with 1,348 sentences from BNC , annotated via an Internet crowdsourcing platform ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modifiers , and adjective-noun selectional preference .", "We analyse the effect of lexical context on these relationships , and the efficacy of the latent semantic representation for disambiguating word meaning .", "Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised .", "We investigate the semantic relationship between a noun and its adjectival modifiers ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We also propose a new directional measure that achieves the best performance in hypernym identification .", "of Linguisticsvia S. Maria 36I-56126 , Pisa , Italymezzanine.g @ gmail.comAbstract In this paper we apply existing directional similarity measures to identify hypernyms with a state-of-the-art distributional semantic model .", "Pisa , Italyalessandro.lenci @ ling.unipi.it Giulia BenottoUniversity of Pisa , Dept ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We describe algorithms for generation and resolution of colour descriptions and report results of experiments on how humans use colour terms for reference in production and comprehension .", "We report ongoing work on the development of agents that can implicitly coordinate with their partners in referential tasks , taking as a case study colour terms ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We extend previous work in unsupervised text-only disambiguation with methods that integrate text and images .", "We construct a corpus by using Amazon Mechanical Turk to caption sensetagged images gathered from ImageNet .", "Given a set of images with related captions , our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small , as this sort of data is common in news and social media .", "Using a Yarowsky-inspired algorithm , we show that gains can be made over text-only disambiguation , as well as multimodal approaches such as Latent Dirichlet Allocation ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Our framework rests on lexical semantic association networks derived from encoding , via bilingual corpora , each language in a common reference language , the tertium comparationis , so that distances between languages can easily be determined .", "We present a framework , based on Sejane and Eger ( 2012 ) , for inducing lexical semantic typologies for groups of languages ."]}
{"orig_sents": ["1", "0", "2", "3", "4"], "shuf_sents": ["In this paper , we demonstrate how the frame-to-frame relations described in the FrameNet ontology can be used to improve the performance of a FrameNet-based semantic role classifier for Swedish , a low-resource language .", "Semantic role classification accuracy for most languages other than English is constrained by the small amount of annotated data .", "In order to make use of the FrameNet relations , we cast the semantic role classification task as a non-atomic label prediction task .", "The experiments show that the cross-frame generalization methods lead to a 27 % reduction in the number of errors made by the classifier .", "For previously unseen frames , the reduction is even more significant : 50 % ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We find that constituent modifiability in an MWE-type is more predictive of the idiomaticity of its tokens than other constituent characteristics such as semantic class or part of speech .", "To this end we combine gold-standard idiomaticity of tokens in the OpenMWE corpus with MWE-type-level information drawn from the recently-published JDMWE lexicon .", "We study the task of automatically disambiguating word combinations such as jump the gun which are ambiguous between a literal and MWE interpretation , focusing on the utility of type-level features from an MWE lexicon for the disambiguation task ."]}
{"orig_sents": ["4", "0", "5", "1", "2", "3"], "shuf_sents": ["Since agents do not come with their preferences transparently given in advance , we have only two means to determine what they are if we wish to exploit them in reasoning : we can infer them from what an agent says or from his nonlinguistic actions .", "To this end , we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres .", "This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre .", "Our results show that preferences can be easily annotated by humans .", "Modeling user preferences is crucial in many real-life problems , ranging from individual and collective decision-making to strategic interactions between agents and game theory .", "In this paper , we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation ."]}
{"orig_sents": ["4", "0", "2", "1", "3"], "shuf_sents": ["Different approaches have been used to represent individual concepts , but current state-of-the-art techniques require extensive manual intervention to scale to arbitrary words and domains .", "We find that dependency parse-based features are the most effective , achieving accuracies similar to the leading semi-manual approaches and higher than any published for a corpus-based model .", "To overcome this challenge , we initiate a systematic comparison of automatically-derived corpus representations , based on various types of textual co-occurrence .", "We also find that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost .", "Neurosemantics aims to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper complements a series of works on implicative verbs such as manage to and fail to .", "It extends the description of simple implicative verbs to phrasal implicatives as take the time to and waste the chance to .", "It shows that the implicative signatures of over 300 verb-noun collocations depend both on the semantic type of the verb and the semantic type of the noun in a systematic way ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We propose an unsupervised system that learns continuous degrees of lexicality for noun-noun compounds , beating a strong baseline on several tasks .", "We demonstrate that the distributional representations of compounds and their parts can be used to learn a finegrained representation of semantic contribution .", "Finally , we argue such a representation captures compositionality better than the current status-quo which treats compositionality as a binary classification problem ."]}
{"orig_sents": ["1", "4", "2", "3", "0"], "shuf_sents": ["We define a superset of the previous frameworks , which is solvable by similar algorithms with the same time and space complexity .", "Over the past decade , several underspecification frameworks have been proposed that efficiently solve a big subset of scopeunderspecified semantic representations within the realm of the most popular constraint-based formalisms .", "It has remained an open question whether there exists a tractable superset of these frameworks , covering this family .", "In this paper , we show that the answer to this question is yes .", "However , there exists a family of coherent natural language sentences whose underspecified representation does not belong to this subset ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["We evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines .", "This paper presents ( a ) a general framework which grounds sense alternations in corpus data , generalizes them above individual words , and allows the prediction of alternations for new words ; and ( b ) a concrete unsupervised implementation of the framework , the Centroid Attribute Model .", "Despite their pervasiveness , regular alternations have been mostly ignored in empirical computational semantics .", "Many types of polysemy are not word specific , but are instances of general sense alternations such as ANIMAL-FOOD ."]}
{"orig_sents": ["1", "6", "0", "2", "4", "5", "3"], "shuf_sents": ["Our objective is , therefore , to contribute enriching the available set of resources by taking advantage of reliable lexicographic data and formalizing it with the well-established lexical functions formalism .", "We present a rule-based method to automatically create a large-coverage semantic lexicon of French adjectives by extracting paradigmatic relations from lexicographic definitions .", "The resulting semantic lexicon of French adjectives can be used in NLP tasks such as word sense disambiguation or machine translation .", "We discuss the results of the evaluation and conclude on some perspectives .", "After presenting related work , we describe the extraction method and the formalization procedure of the data .", "Our method is then quantitatively and qualitatively evaluated .", "Formalized adjectival resources are , indeed , scarce for French and they mostly focus on morphological and syntactic information ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["This paper describes Bayesian selectional preference models that incorporate knowledge from a lexical hierarchy such as WordNet .", "In an evaluation comparing against human plausibility judgements , we show that the models presented here outperform previously proposed comparable WordNet-based models , are competitive with state-of-the-art selectional preference models and are particularly wellsuited to estimating plausibility for items that were not seen in training .", "model that selects a path from the root to a leaf .", "Inspired by previous work on modelling with WordNet , these approaches are based either on ? cutting ?", "the hierarchy at an appropriate level of generalisation or on a ? walking ?"]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["By learning such linkings , we do not need to model individual semantic roles independently of one another , and we can exploit the relation between different mappings for the same verb , or between mappings for different verbs .", "We learn linkings , i.e. , mappings from the syntactic arguments and adjuncts of a verb to its semantic roles .", "We present a method for learning syntaxsemantics mappings for verbs from unannotated corpora .", "We present an evaluation on a standard test set for semantic role labeling ."]}
{"orig_sents": ["2", "1", "3", "0", "5", "4"], "shuf_sents": ["We consider an alternate method of annotating graded senses using Word Sense Induction , which automatically learns the senses and their features from corpus properties .", "Graded word sense disambiguation relaxes the single label assumption , allowing for multiple sense labels with varying degrees of applicability .", "Word Sense Disambiguation aims to label the sense of a word that best applies in a given context .", "Training multi-label classifiers for such a task requires substantial amounts of annotated data , which is currently not available .", "We demonstrate that sense induction offers significant promise for accurate graded sense annotation .", "Our work proposes three objective to evaluate performance on the graded sense annotation task , and two new methods for mapping between sense inventories using parallel graded sense annotations ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We present an ensemble-based framework for semantic lexicon induction that incorporates three diverse approaches for semantic class identification .", "The three methods are embedded in a bootstrapping architecture where they produce independent hypotheses , consensus words are added to the lexicon , and the process repeats .", "Our results show that the ensemble outperforms individual methods in terms of both lexicon quality and instance-based semantic tagging .", "Our architecture brings together previous bootstrapping methods for pattern-based semantic lexicon induction and contextual semantic tagging , and incorporates a novel approach for inducing semantic classes from coreference chains ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["The task is to find the best matching between semantic roles and sentential spans , subject to structural constraints that come from expert linguistic knowledge ( e.g. , in the FrameNet lexicon ) .", "Runtime is nine times faster than a proprietary ILP solver .", "We present a novel technique for jointly predicting semantic arguments for lexical predicates .", "Compared to a baseline that makes local predictions , we achieve better argument identification scores and avoid all structural violations .", "We formulate this task as an integer linear program ( ILP ) ; instead of using an off-the-shelf tool to solve the ILP , we employ a dual decomposition algorithm , which we adapt for exact decoding via a branch-and-bound technique ."]}
{"orig_sents": ["5", "3", "2", "6", "4", "1", "7", "8", "0", "9"], "shuf_sents": ["Initial experiments on the task of predicting predicate alignments across text pairs show promising results .", "The resulting alignments and their contexts can then be used for developing a coherence model for argument realization .", "In particular , we aim to study the case of non-realized arguments as a coherence inducing factor .", "Our aim is to learn factors that constitute coherent discourse from data , with a focus on how to realize predicateargument structures ( PAS ) in a model that exceeds the sentence level .", "The first aligns predicates across comparable texts , admitting partial argument structure correspondence .", "Discourse coherence is an important aspect of natural language that is still understudied in computational linguistics .", "This task can be broken down into two subtasks .", "This paper introduces a large corpus of comparable monolingual texts as a prerequisite for approaching this task , including an evaluation set with manual predicate alignments .", "We illustrate the potential of this new resource for the empirical investigation of discourse coherence phenomena .", "Our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["The semantic annotations are coarse semantic categories or entries from a distributional thesaurus , assigned either heuristically or by a pre-trained tagger .", "We test this using two test corpora in different domains with various sources of training data .", "We investigate the effects of adding semantic annotations including word sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar .", "The best reduces error rate in dependency Fscore by 1 % on average , while some methods produce substantial decreases in performance ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["Commonly , it is approached either by generative syntactic-based methods or by ? lightweight ?", "We suggest a model which is confined to simple lexical information , but is formulated as a principled generative probabilistic model .", "Identifying textual inferences , where the meaning of one text follows from another , is a general underlying task within many natural language applications .", "We focus our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set .", "heuristic lexical models ."]}
{"orig_sents": ["4", "3", "1", "5", "2", "0"], "shuf_sents": ["Finally , we extract a word ? emotion association lexicon from this Twitter corpus , and show that it leads to significantly better results than the manually crafted WordNet Affect lexicon in an emotion classification task.1", "In this paper , we describe how we created such a corpus from Twitter posts using emotionword hashtags .", "We also show how the Twitter emotion corpus can be used to improve emotion classification accuracy in a different domain .", "However , there exists no microblog corpus with instances labeled for emotions for developing supervised systems .", "Detecting emotions in microblogs and social media posts has applications for industry , health , and security .", "We conduct experiments to show that the self-labeled hashtag annotations are consistent and match with the annotations of trained judges ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Previous work on paraphrase extraction and application has relied on either parallel datasets , or on distributional similarity metrics over large text corpora .", "Our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system ? s log-linear model .", "We compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression , achieving stateof-the-art quality ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["This paper presents the specifications , datasets and evaluation criteria of the task .", "An overview of participating systems is provided and their results are summarized .", "In its first edition held in 2012 , the shared task was dedicated to resolving the scope and focus of negation .", "The Joint Conference on Lexical and Computational Semantics ( *SEM ) each year hosts a shared task on semantic related topics ."]}
{"orig_sents": ["2", "3", "1", "0", "4"], "shuf_sents": ["Then , it applies machine learning approaches to detect the scope and negated event for each negation cue identified in the first phase .", "In the first phase , the system creates a lexicon of negation signals from the training data and uses the lexicon to identify the negation cues .", "This paper describes our participation in the closed track of the *SEM 2012 Shared Task of finding the scope of negation .", "To perform the task , we propose a system that has three components : negation cue detection , scope of negation detection , and negated event detection .", "Using a preliminary approach , our system achieves a reasonably good accuracy in identifying the scope of negation ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["It was initially intended for processing negation in opinionated texts , and has been adapted to fit the task requirements .", "This paper presents one of the two contributions from the Universidad Complutense de Madrid to the *SEM Shared Task 2012 on Resolving the Scope and Focus of Negation .", "It next uses the information from the syntax tree of the sentence in which the negation arises to get a first approximation to the negation scope , which is later refined using a set of post-processing rules that bound or expand such scope .", "It first detects negation cues using a list of explicit negation markers ( such as not or nothing ) , and infers other implicit negations ( such as affixal negations , e.g , undeniable or improper ) by using semantic information from WordNet concepts and relations .", "We describe a rule-based system for detecting the presence of negations and delimitating their scope ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["An initial version of the system was developed to handle the annotations of the Bioscope corpus .", "It first makes use of an algorithm that detects negation cues , like no , not or nothing , and the words affected by them by traversing Minipar dependency structures .", "UCM-2 infers the words that are affected by negations by browsing dependency syntactic structures .", "Second , the scope of these negation cues is computed by using a post-processing rulebased approach that takes into account the information provided by the first algorithm and simple linguistic clause boundaries .", "For the present version , we have changed , omitted or extended the rules and the lexicon of cues ( allowing prefix and suffix negation cues , such as impossible or meaningless ) , to make it suitable for the present task ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["While scope detection has recently seen repeated attention , the linguistic notion of focus is only now being introduced into computational work .", "Simply detecting negation cues is not sufficient to determine the semantics of negation , scope and focus must be taken into account .", "CLaC ? s NegFocus system is a solid baseline approach to the task .", "The *Sem2012 Shared Task is pioneering this effort by introducing a suitable dataset and annotation guidelines ."]}
{"orig_sents": ["0", "4", "1", "2", "5", "3", "6"], "shuf_sents": ["We use the NLP toolchain that is used to construct the Groningen Meaning Bank to address the task of detecting negation cue and scope , as defined in the shared task ? Resolving the Scope and Focus of Negation ? .", "For negation cue detection , the DRSs are converted to flat , non-recursive structures , called Discourse Representation Graphs ( DRGs ) .", "DRGs simplify cue detection by means of edge labels representing relations .", "The result is a system that is fairly reliable for cue detection and scope detection .", "This toolchain applies the C & C tools for parsing , using the formalism of Combinatory Categorial Grammar , and applies Boxer to produce semantic representations in the form of Discourse Representation Structures ( DRSs ) .", "Scope detection is done by gathering the tokens that occur within the scope of a negated DRS .", "Furthermore , it provides a fairly robust algorithm for detecting the negated event or property within the scope ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper describes the first of two systems submitted from the University of Oslo ( UiO ) to the 2012 *SEM Shared Task on resolving negation .", "Our submission is an adaption of the negation system of Velldal et al ( 2012 ) , which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution .", "While submitted for the closed track , the system was the top performer in the shared task overall .", "The approach further extends our prior work in that we also identify factual negated events ."]}
{"orig_sents": ["1", "2", "0", "4", "3"], "shuf_sents": ["Models for scopes and events are created using lexical and syntactic features , together with a fine-grained set of labels that capture the scopal behavior of certain tokens .", "This paper describes the second of two systems submitted from the University of Oslo ( UiO ) to the 2012 *SEM Shared Task on resolving negation .", "The system combines SVM cue classification with CRF sequence labeling of events and scopes .", "The system was ranked first in the open track and third in the closed track , and was one of the top performers in the scope resolution sub-task overall .", "Following labeling , negated tokens are assigned to their respective cues using simple post-processing heuristics ."]}
{"orig_sents": ["1", "6", "4", "2", "3", "7", "0", "5"], "shuf_sents": ["It identifies negation scope with 82.70 % F1 on token-bytoken level and 64.78 % F1 on full scope level .", "In this paper , we present a system for detecting negation in English text .", "For each task , we train a Conditional Random Field ( CRF ) model on lexical , structural , and syntactic features extracted from labeled data .", "The models are trained and tested using the dataset distributed with the *sem Shared Task 2012 on resolving the scope and focus of negation .", "We pose these tasks as sequence labeling problems .", "Negated events are detected with 51.10 % F1 measure .", "We address three tasks : negation cue detection , negation scope resolution and negated event identification .", "The system detects negation cues with 90.98 % F1 measure ( 94.3 % and 87.88 % recall ) ."]}
{"orig_sents": ["1", "3", "4", "0", "2"], "shuf_sents": ["namely the SimpleTagger library in the MALLET machine learning toolkit .", "This paper reports on a simple system for resolving the scope of negation in the closed track of the *SEM 2012 Shared Task .", "The full negation F1 score obtained for the task evaluation is 48.09 % ( P=74.02 % , R=35.61 % ) which ranks this system fourth among the six submitted for the closed track .", "Cue detection is performed using regular expression rules extracted from the training data .", "Both scope tokens and negated event tokens are resolved using a Conditional Random Field ( CRF ) sequence tagger ?"]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["This paper presents a system for this task that exploits phrasal and contextual clues apart from various token specific features .", "The system was developed for the participation in the Task 1 ( closed track ) of the *SEM 2012 Shared Task ( Resolving the Scope and Focus of Negation ) , where it is ranked 3rd among the participating teams while attaining the highest F1 score for negation cue detection .", "Automatic detection of negation cues along with their scope and corresponding negated events is an important task that could benefit other natural language processing ( NLP ) tasks such as extraction of factual information from text , sentiment analysis , etc ."]}
{"orig_sents": ["5", "6", "0", "1", "7", "4", "3", "2"], "shuf_sents": ["The task requires that annotators and systems rank a number of alternative substitutes ?", "all deemed adequate ?", "Out of nine participating systems , the best scoring ones combine contextdependent and context-independent information , with the strongest individual contribution given by the frequency of the substitute regardless of its context .", "The notion of simplicity is biased towards non-native speakers of English .", "these substitutes are .", "We describe the English Lexical Simplification task at SemEval-2012 .", "This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context-aware lexical simplification approaches .", "for a target word in context , according to how ? simple ?"]}
{"orig_sents": ["2", "0", "1", "4", "3"], "shuf_sents": ["However , instances of a single relation class may still have significant variability in how characteristic they are of that class .", "We present a new SemEval task based on identifying the degree of prototypicality for instances within a given class .", "Up to now , work on semantic relations has focused on relation classification : recognizing whether a given instance ( a word pair such as virus : flu ) belongs to a specific relation class ( such as CAUSE : EFFECT ) .", "Three teams submitted six systems , which were evaluated using two methods .", "As a part of the task , we have assembled the first dataset of graded relational similarity ratings across 79 relation categories ."]}
{"orig_sents": ["4", "5", "2", "1", "0", "3"], "shuf_sents": ["We have one participant system with two runs .", "The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark .", "In addition to these major components , the links between them and the general-type of spatial relationships including region , direction and distance are targeted .", "The participant ? s runs are compared to the system in ( Kordjamshidi et al , 2011c ) which is provided by task organizers .", "This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling .", "The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language : trajectors , landmarks and spatial indicators ."]}
{"orig_sents": ["1", "7", "5", "6", "4", "3", "0", "2"], "shuf_sents": ["Four systems participating in this task return their results .", "This task focuses on evaluating word similarity computation in Chinese .", "We evaluate their results on gold standard data in term of Kendall 's tau value , and the results show three of them have a positive correlation with the rank manually created while the taus ' value is very small .", "This data is used as gold standard .", "We rank the word pairs by the average value of similar scores among the twenty annotators .", "Then we organize twenty undergraduates who are major in Chinese linguistics to annotate the data .", "Each pair is assigned a similarity score by each annotator .", "We follow the way of Finkelstein et al ( 2002 ) to select word pairs ."]}
{"orig_sents": ["2", "3", "4", "1", "0"], "shuf_sents": ["At last , we briefly describe the submitted systems and analyze these results .", "Over ten thousand sentences were labeled for participants to train and evaluate their systems .", "The paper presents the SemEval-2012 Shared Task 5 : Chinese Semantic Dependency Parsing .", "The goal of this task is to identify the dependency structure of Chinese sentences from the semantic view .", "We firstly introduce the motivation of providing Chinese semantic dependency parsing task , and then describe the task in detail including data preparation , data format , task evaluation , and so on ."]}
{"orig_sents": ["6", "0", "7", "5", "1", "4", "3", "2"], "shuf_sents": ["This paper presents the results of the STS pilot task in Semeval .", "The similarity of pairs of sentences was rated on a 0-5 scale ( low to high similarity ) by human judges using Amazon Mechanical Turk , with high Pearson correlation scores , around 90 % .", "This pilot task opens an exciting way ahead , although there are still open issues , specially the evaluation metric .", "The best results scored a Pearson correlation > 80 % , well above a simple lexical baseline that only scored a 31 % correlation .", "35 teams participated in the task , submitting 88 runs .", "The test data also comprised 2000 sentences pairs for those datasets , plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise .", "Semantic Textual Similarity ( STS ) measures the degree of semantic equivalence between two texts .", "The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["We report on the training and test data used for evaluation , the process of their creation , the participating systems ( 10 teams , 92 runs ) , the approaches adopted and the results achieved .", "The task was designed to promote research on semantic inference over texts written in different languages , targeting at the same time a real application scenario .", "Participants were presented with datasets for different language pairs , where multi-directional entailment relations ( ? forward ? , ? backward ? , ? bidirectional ? , ? no entailment ? )", "This paper presents the first round of the task on Cross-lingual Textual Entailment for Content Synchronization , organized within SemEval-2012 .", "had to be identified ."]}
{"orig_sents": ["2", "1", "4", "5", "0", "3"], "shuf_sents": ["Our features include n-gram probabilities of candidate and context in a web corpus , distributional differences of candidate in a corpus of ? easy ?", "For this we learn a binary classifier .", "Our system breaks down the problem of ranking a list of lexical substitutions according to how simple they are in a given context into a series of pairwise comparisons between candidates .", "sentences and a corpus of normal sentences , syntactic complexity of documents that are similar to the given context , candidate length , and letter-wise recognizability of candidate as measured by a trigram character language model .", "As only very little training data is provided , we describe a procedure for generating artificial unlabeled data from Wordnet and a corpus and approach the classification task as a semisupervised machine learning problem .", "We use a co-training procedure that lets each classifier increase the other classifier ? s training set with selected instances from an unlabeled data set ."]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["In this paper we present our approach for assigning degrees of relational similarity to pairs of words in the SemEval-2012 Task 2 .", "This approach achieved the best results on the SemEval 2012 Task 2 , obtaining a Spearman correlation of 0.229 and an accuracy on reproducing human answers to MaxDiff questions of 39.4 % .", "To measure relational similarity we employed lexical patterns that can match against word pairs within a large corpus of 12 million documents .", "Patterns are weighted by obtaining statistically estimated lower bounds on their precision for extracting word pairs from a given relation .", "Finally , word pairs are ranked based on a model predicting the probability that they belong to the relation of interest ."]}
{"orig_sents": ["1", "4", "3", "0", "2"], "shuf_sents": ["This joint approach allows for a rich feature set based on the complete relation instead of individual relation arguments .", "We present a joint approach for recognizing spatial roles in SemEval-2012 Task 3 .", "Our best official submission achieves an F1measure of 0.573 on relation recognition , best in the task and outperforming the previous best result on the same data set ( 0.500 ) .", "The joint classification of spatial roles is then cast as a binary classification over the candidates .", "Candidate spatial relations , in the form of triples , are heuristically extracted from sentences with high recall ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["These systems are performed on SemEval-2012 Task 4 : Evaluating Chinese Word Similarity .", "One is based on Machine Readable Dictionaries and the others utilize both MRDs and Corpus .", "This document describes three systems calculating semantic similarity between two Chinese words ."]}
{"orig_sents": ["3", "0", "4", "2", "1"], "shuf_sents": ["Our system extends the second-order MST model by adding two third-order features .", "After using the selected third-order features , our system presently achieves LAS of 61.58 % ignoring punctuation tokens which is 0.15 % higher than the result of purely second-order model on the test dataset .", "In the decoding phase , we keep the k best results for each span .", "This paper presents our system participated on SemEval-2012 task : Chinese Semantic Dependency Parsing .", "The two third-order features are grand-sibling and tri-sibling ."]}
{"orig_sents": ["1", "2", "4", "3", "0"], "shuf_sents": ["Our final models , one per dataset , consist of a log-linear combination of about 20 features , out of the possible 300+ features implemented .", "We present the UKP system which performed best in the Semantic Textual Similarity ( STS ) task at SemEval-2012 in two out of three metrics .", "It uses a simple log-linear regression model , trained on the training data , to combine multiple text similarity measures of varying complexity .", "Further , we employ a lexical substitution system and statistical machine translation to add additional lexemes , which alleviates lexical gaps .", "These range from simple character and word n-grams and common subsequences to complex features such as Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources ."]}
{"orig_sents": ["2", "5", "0", "3", "1", "6", "4", "7"], "shuf_sents": ["However , a fair amount of information is condensed into short text snippets such as social media posts , image captions , and scientific abstracts .", "Out of 89 systems submitted , our two systems ranked in the top 5 , for the three overall evaluation metrics used ( overall Pearson ?", "This paper describes the two systems for determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6 .", "We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity .", "1st and 3rd , weighted mean ?", "Most of the research on semantic similarity of textual content focuses on large documents .", "2nd and 3rd , normalized Pearson ?", "2nd and 5th ) ."]}
{"orig_sents": ["2", "1", "4", "0", "5", "3"], "shuf_sents": ["Experimentally , we observed that a performance correlation function in a space defined by all parameters was relatively smooth and had a single maximum achievable by ? hill climbing . ?", "Our approach provides a consistent and recursive model , varying levels of granularity from sentences to characters .", "We present an approach for the construction of text similarity functions using a parameterized resemblance coefficient in combination with a softened cardinality function called soft cardinality .", "The proposed method ranked 3rd ( average ) , 5th ( normalized correlation ) , and 15th ( aggregated correlation ) among 89 systems submitted by 31 teams .", "Therefore , our model was used to compare sentences divided into words , and in turn , words divided into q-grams of characters .", "Our approach used only surface text information , a stop-word remover , and a stemmer to tackle the semantic text similarity task 6 at SEMEVAL 2012 ."]}
{"orig_sents": ["2", "1", "0", "4", "3"], "shuf_sents": ["Our runs focus on combining standard similarity measures for Machine Translation .", "Our contribution consists of an unsupervised method , Heterogeneity Based Ranking ( HBR ) , to combine similarity measures .", "This paper describes the participation of UNED NLP group in the SEMEVAL 2012 Semantic Textual Similarity task .", "However , the combination of system outputs that participated in the campaign produces three interesting results : ( i ) Combining all systems without considering any kind of human assessments achieve a similar performance than the best peers in all test corpora , ( ii ) combining the 40 less reliable peers in the evaluation campaign achieves similar results ; and ( iii ) the correlation between peers and HBR predicts , with a 0.94 correlation , the performance of measures according to human assessments .", "The Pearson correlation achieved is outperformed by other systems , due to the limitation of MT evaluation measures in the context of this task ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["The system relies on features extracted with statistical machine translation methods and tools , combining monolingual and cross-lingual word alignments as well as standard textual entailment distance and bag-of-words features in a statistical learning framework .", "We learn separate binary classifiers for each entailment direction and combine them to obtain four entailment relations .", "Our system yielded the best overall score for three out of four language pairs .", "We describe the Heidelberg University system for the Cross-lingual Textual Entailment task at SemEval-2012 ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We use sub-segment translations from different MT systems available online as a source of crosslingual knowledge .", "We presented this system to the SemEval 2012 task 8 obtaining an accuracy up to 59.8 % on the English ? Spanish test set , the second best performing approach in the contest .", "In this work we describe and evaluate different features derived from these sub-segment translations , which are used by a support vector machine classifier to detect CLTEs .", "This paper describes a new method for crosslingual textual entailment ( CLTE ) detection based on machine translation ( MT ) ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["It operates on the basis of a linear weighted ranking function composed of context sensitive and psycholinguistic features .", "The system outperforms a very strong baseline , and ranked first on the shared task .", "This paper describes SimpLex,1 a Lexical Simplification system that participated in the English Lexical Simplification shared task at SemEval-2012 ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["In this paper , we describe the system we submitted to the SemEval-2012 Lexical Simplification Task .", "We believe that the proposed approach might help to shed light on the interplay between linguistic features and lexical complexity in general .", "in descending order of ( cognitive ) simplicity .", "Our system ( mmSystem ) combines word frequency with decompositional semantics criteria based on syntactic structure in order to rank candidate substitutes of lexical forms of arbitrary syntactic complexity ( oneword , multi-word , etc . )"]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["On the evaluation corpus , we achieved a 0.465 score with the first system .", "Our first system relies on n-grams frequencies computed from the Simple English Wikipedia version , ranking each substitution term by decreasing frequency of use .", "We experimented with several other systems , based on term frequencies , or taking into account the context in which each substitution term occurs .", "This paper presents the systems we developed while participating in the first task ( English Lexical Simplification ) of SemEval 2012 ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["Notably , the thirdbest system is very close to the second-best , and at the same time much more resource-light in comparison .", "This paper presents three systems that took part in the lexical simplification task at SEMEVAL 2012 .", "One of the systems performs second-best ( statistically significant ) and another one performs third-best out of 9 systems and 3 baselines .", "Speculating on what the concept of simplicity might mean for a word , the systems apply different approaches to rank the given candidate lists ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes the Duluth systems that participated in Task 2 of SemEval ? 2012 .", "These systems were unsupervised and relied on variations of the Gloss Vector measure found in the freely available software package WordNet : :Similarity .", "This method was moderately successful for the Class-Inclusion , Similar , Contrast , and Non-Attribute categories of semantic relations , but mimicked a random baseline for the other six categories ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The approach presented is based on a vectorial representation using the following features : i ) the context surrounding the words with a windows size = 3 , ii ) knowledge extracted from WordNet to discover several semantic relationships , such as meronymy , hyponymy , hypernymy , and part-whole between pair of words , iii ) the description of the pairs with their POS tag , morphological information ( gender , person ) , and iv ) the average number of words separating the two words in text .", "We describe a system proposed for measuring the degree of relational similarity beetwen a pair of words at the Task # 2 of Semeval 2012 ."]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["We will report detailed experimental results with correct program as a comparison standard for further research .", "To attain this goal , we concentrate on obtaining better dependency structure to predict better semantic relations , and propose a method to combine the results of three state-of-the-art dependency parsers .", "After giving golden testing set , we fix the bug and rerun the evaluation script , this time we obtain the score of 62.8 % which is consistent with the results on developing set .", "The goal of semantic dependency parsing is to build dependency structure and label semantic relation between a head and its modifier .", "Unfortunately , we made a mistake when we generate the final output that results in a lower score of 56.31 % in term of Labeled Attachment Score ( LAS ) , reported by organizers ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Our system is based on MSTParser and two effective methods are proposed : splitting sentence by punctuations and extracting last character of word as lemma .", "We also try to handle the multilevel labels , but with no improvement .", "In this paper , we introduce our work on SemEval-2012 task 5 : Chinese Semantic Dependency Parsing .", "The experiments show that , with a combination of the two proposed methods , our system can improve LAS about one percent and finally get the second prize out of nine participating systems ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The semantic vector is used to compute similarities between sentence pairs using the lexical database WordNet and the Wikipedia corpus .", "This paper presents the work of the Hong Kong Polytechnic University ( PolyUCOMP ) team which has participated in the Semantic Textual Similarity task of SemEval-2012 .", "The use of skip bigram is to introduce the order of words in measuring sentence similarity .", "The PolyUCOMP system combines semantic vectors with skip bigrams to determine sentence similarity ."]}
{"orig_sents": ["2", "3", "0", "1", "4"], "shuf_sents": ["In this work , we extend an existing machine translation metric , TERp ( Snover et al , 2009a ) , by adding support for more detailed feature types and by implementing a discriminative learning algorithm .", "These additions facilitate applications of our system , called PERP , to similarity tasks other than machine translation evaluation , such as paraphrase recognition .", "Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts .", "However , many systems that address these tasks focus on a single task and may or may not generalize well .", "In the SemEval 2012 Semantic Textual Similarity task , PERP performed competitively , particularly at the two surprise subtasks revealed shortly before the submission deadline ."]}
{"orig_sents": ["6", "4", "5", "1", "0", "3", "2"], "shuf_sents": ["The proposed rules enable the system to adequately handle domain knowledge gaps that are inherent when working with knowledge resources .", "The first system ( rule-based ) combines both semantic and syntax features to arrive at the overall similarity .", "The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair .", "As such one of its main goals , the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences .", "The goal of this challenge is to accurately identify five levels of semantic similarity between two sentences : equivalent , mostly equivalent , roughly equivalent , not equivalent but sharing the same topic and no equivalence .", "Our participations were two systems .", "In this paper , we describe the system architecture used in the Semantic Textual Similarity ( STS ) task 6 pilot challenge ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Despite ignoring text preprocessing and dispensing with semantic resources , the approach was ranked as high as 22nd among 89 participants in the SemEval-2012 Task6 : Semantic Textual Similarity .", "Unlike classical RI , we use only those context vector features that are informative for the semantics modeled .", "We propose a semantic similarity learning method based on Random Indexing ( RI ) and ranking with boosting ."]}
{"orig_sents": ["5", "6", "1", "4", "3", "0", "2"], "shuf_sents": ["Both approaches were evaluated against the test data for the SemEval 2012 Semantic Text Similarity task .", "The first was a corpus-based method that extracted lexical and semantic features from pairs of chunks from each sentence that were associated through a chunk alignment algorithm .", "The results show that the rule-based chunk approach is superior .", "The second approach involved breadth-first chunk association and the application of a rule-based scoring algorithm .", "The features were used as input to a classifier trained on the same features ex-tracted from a corpus of gold standard training data .", "Chunk-based Determination of Semantic Text Similarity Demetrios Glinos Advanced Text Analytics , LLC Orlando , Florida , USA demetrios.glinos @ advancedtextanalytics.com Abstract This paper describes investigations into using syntactic chunk information as the basis for determining the similarity of candidate texts at the semantic level .", "Two approaches were con-sidered ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The method used consists of a n-gram based comparison method combined with a conceptual similarity measure that uses WordNet to calculate the similarity between a pair of concepts .", "This paper describes the participation of the IRIT team to SemEval 2012 Task 6 ( Semantic Textual Similarity ) ."]}
{"orig_sents": ["10", "3", "2", "8", "7", "9", "4", "5", "1", "0", "6"], "shuf_sents": ["The results are promising , with Pearson ? s coefficients on each individual dataset ranging from .3765 to .7761 for our relatively simple heuristics based systems that do not require training on different datasets .", "For this reason we only provide a brief description of that .", "Currently we use words ( that is word forms ) , lemmas , distributional similar words and grammatical relations identified with a dependency parser .", "Our systems are all based on a simple process of identifying the components that correspond between two sentences .", "Our second system ( wordsim ) uses a different algorithm and unlike alignheuristic , it does not use the dependency information .", "The third system ( average ) simply takes the average of the scores for each item from the other two systems to take advantage of the merits of both systems .", "We provide some analysis of the results and also provide results for our data using Spearman ? s , which as a nonparametric measure which we argue is better able to reflect the merits of the different systems ( average is ranked between the others ) .", "All systems only use open class words .", "We submitted three systems .", "Our first system ( alignheuristic ) tries to obtain a mapping between every open class token using all the above sources of information .", "In this paper we present our systems for the STS task ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["State-of-the-art results are obtained for semantic similarity computation at the word level , however , the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval ? 12 .", "We estimate the semantic similarity between two sentences using regression models with features : 1 ) n-gram hit rates ( lexical matches ) between sentences , 2 ) lexical semantic similarity between non-matching words , and 3 ) sentence length .", "Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric .", "Despite the simple features used , regression models provide good performance , especially for shorter sentences , reaching correlation of 0.62 on the SemEval test set ."]}
{"orig_sents": ["3", "1", "2", "0", "7", "8", "5", "4", "6"], "shuf_sents": ["Similarity score is one kind of multi way classification in the form of grade between 0 to 5 .", "Task-6 of SemEval- 2012 focused on semantic relations of text pair .", "Task-6 provides five different text pair files to compare different semantic relations and judge these relations through a similarity and confidence score .", "This article presents the experiments carried out at Jadavpur University as part of the participation in Semantic Textual Similarity ( STS ) of Task 6 @ Semantic Evaluation Exercises ( SemEval-2012 ) .", "The scores from each module are identified using rule based techniques .", "Similarity score given to a pair is the average of the scores of the above-mentioned modules .", "The Pearson Correlation of our system in the task is 0.3880 .", "We have submitted one run for the STS task .", "Our system has two basic modules - one deals with lexical relations and another deals with dependency based syntactic relations of the text pair ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["This paper briefly reports our submissions to the Semantic Textual Similarity ( STS ) task in the SemEval 2012 ( Task 6 ) .", "We also consider word order similarity from the structure of the sentence .", "We first use knowledge-based methods to compute word semantic similarity as well as Word Sense Disambiguation ( WSD ) .", "Finally we sum up several aspects of similarity with different coefficients and get the sentence similarity score ."]}
{"orig_sents": ["3", "4", "2", "0", "1"], "shuf_sents": ["Two token similarity measures are used for the task - WordNet based similarity , and a statistical word similarity measure which overcomes the shortcomings of WordNet based similarity .", "As part of three systems created for the task , we explore a simple bag of words tokenization scheme , a more careful tokenization scheme which captures named entities , times , dates , monetary entities etc. , and finally try to capture context around tokens using grammatical dependencies .", "The tokens include single words , or multiwords in case of Named Entitites , adjectivally and numerically modified words .", "The paper aims to come up with a system that examines the degree of semantic equivalence between two sentences .", "At the core of the paper is the attempt to grade the similarity of two sentences by finding the maximal weighted bipartite match between the tokens of the two sentences ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The key to the approach is to carefully handle missing words that are not in the sentence , and thus rendering it superior to Latent Semantic Analysis ( LSA ) and Latent Dirichlet Allocation ( LDA ) .", "Our system ranks 20 out of 89 systems according to the official evaluation metric for the task , Pearson correlation , and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers .", "The Semantic Textual Similarity ( STS ) shared task ( Agirre et al , 2012 ) computes the degree of semantic equivalence between two sentences.1 We show that a simple unsupervised latent semantics based approach , Weighted Textual Matrix Factorization that only exploits bag-of-words features , can outperform most systems for this task ."]}
{"orig_sents": ["1", "2", "3", "5", "7", "6", "4", "0"], "shuf_sents": ["aligned , corpus .", "This paper presents the UNITOR system that participated to the SemEval 2012 Task 6 : Semantic Textual Similarity ( STS ) .", "The task is here modeled as a Support Vector ( SV ) regression problem , where a similarity scoring function between text pairs is acquired from examples .", "The semantic relatedness between sentences is modeled in an unsupervised fashion through different similarity functions , each capturing a specific semantic aspect of the STS , e.g .", "WordNet ) nor controlled , e.g .", "syntactic vs. lexical or topical vs. paradigmatic similarity .", "It provides a highly portable method as it does not depend on any manually built resource ( e.g .", "The SV regressor effectively combines the different models , learning a scoring function that weights individual scores in a unique resulting STS ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["The system is based on a combination of few simple vector space-based methods for word meaning similarity .", "The simple vector space components achieve high performance on short sentences ; on longer , more complex sentences , they are outperformed by a surprisingly competitive word overlap baseline , but they still bring improvements over this baseline when incorporated into a mixture model .", "This paper describes our system for the Semeval 2012 Sentence Textual Similarity task .", "Evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful ."]}
{"orig_sents": ["7", "3", "1", "2", "5", "4", "0", "6"], "shuf_sents": ["It used a multinomial classifier to predict which dataset regressor would be most appropriate to score a given pair , and used it to score that pair .", "We also incorporate precision focused scores over lexical and POS information derived from the BLEU measure , and lexical and POS features computed over split-bigrams from the ROUGE-S measure .", "These were used to train support vector regressors over the pairs in the training data .", "Our systems focused on using a simple set of features , featuring a mix of semantic similarity resources , lexical match heuristics , and part of speech ( POS ) information .", "Our third system maintained three separate regressors , each trained specifically for the STS dataset they were drawn from .", "From the three systems we submitted , two performed well in the overall ranking , with splitbigrams improving performance over pairs drawn from the MSR Research Video Description Corpus .", "This system underperformed , primarily due to errors in the dataset predictor .", "We describe the systems submitted by SRI International and the University of the Basque Country for the Semantic Textual Similarity ( STS ) SemEval-2012 task ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Our approach explores lexical , syntactic and semantic machine translation evaluation metrics combined with distributional and knowledgebased word similarity metrics .", "This paper describes the participation of FBK in the Semantic Textual Similarity ( STS ) task organized within Semeval 2012 .", "Our best model achieves 60.77 % correlation with human judgements ( Mean score ) and ranked 20 out of 88 submitted runs in the Mean ranking , where the average correlation across all the sub-portions of the test set is considered ."]}
{"orig_sents": ["2", "5", "4", "0", "3", "1"], "shuf_sents": ["The third approach employs Random Indexing and Bag of Concepts based on context vectors .", "The best ALL result was obtained with the third approach , with a Pearson correlation equal to 0.663 .", "In this paper we describe the three approaches we submitted to the Semantic Textual Similarity task of SemEval 2012 .", "We consider that the first and third approaches obtained a comparable performance , meanwhile the second approach got a very poor behavior .", "The second approach uses the semantic similarity reported by Mihalcea in ( Mihalcea et al , 2006 ) .", "The first approach considers to calculate the semantic similarity by using the Jaccard coefficient with term expansion using synonyms ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Our evaluations show that corpus-based methods display a more robust behavior on the training data , yet combining a variety of methods allows a learning algorithm to achieve a superior decision than that achievable by any of the individual parts .", "Based on prior research in semantic similarity and relatedness , we combine various methods in a machine learning framework .", "The three variations submitted during the task evaluation period ranked number 5 , 9 and 14 among the 89 participating systems .", "This paper presents the systems that we participated with in the Semantic Text Similarity task at SEMEVAL 2012 ."]}
{"orig_sents": ["3", "7", "5", "8", "0", "4", "2", "6", "1"], "shuf_sents": ["Linear Regression and Bagging models were used for this purpose .", "This paper shows a significant improvement in calculating the semantic similarity between sentences by the fusion of the knowledge-based similarity measure and the corpus-based relatedness measure against corpus based measure taken alone .", "For the knowledgebased semantic similarity between words , a modified WordNet based Lin measure was used .", "In this paper , we describe our system submitted for the semantic textual similarity ( STS ) task at SemEval 2012 .", "We used Explicit Semantic Analysis ( ESA ) as the corpus-based semantic relatedness measure .", "First approach combines corpus-based semantic relatedness measure over the whole sentence with the knowledge-based semantic similarity scores obtained for the words falling under the same syntactic roles in both the sentences .", "Second approach uses a bipartite based method over the WordNet based Lin measure , without any modification .", "We implemented two approaches to calculate the degree of similarity between two sentences .", "We fed all these scores as features to machine learning models to obtain a single score giving the degree of similarity of the sentences ."]}
{"orig_sents": ["1", "3", "4", "6", "2", "0", "5"], "shuf_sents": ["The performance of our edit distance based models is contrasted with an adaptation of the Stanford textual entailment system to the STS task .", "This paper describes Stanford University ? s submission to SemEval 2012 Semantic Textual Similarity ( STS ) shared evaluation task .", "Our models are trained in a regression framework , and can easily incorporate a rich set of linguistic features .", "Our proposed metric computes probabilistic edit distance as predictions of semantic similarity .", "We learn weighted edit distance in a probabilistic finite state machine ( pFSM ) model , where state transitions correspond to edit operations .", "Our results show that the most advanced edit distance model , pPDA , outperforms our entailment system on all but one of the genres included in the STS task .", "While standard edit distance models can not capture long-distance word swapping or cross alignments , we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model ."]}
{"orig_sents": ["3", "0", "2", "5", "4", "1", "6"], "shuf_sents": ["Two approaches were developed .", "Results from the formal evaluation show that both approaches are useful for determining the similarity in meaning between pairs of sentences with the best performance being obtained by the supervised approach .", "The first is an unsupervised technique based on the widely used vector space model and information from WordNet .", "This paper describes the University of Sheffield ? s submission to SemEval-2012 Task 6 : Semantic Text Similarity .", "This approach also makes use of information from WordNet .", "The second method relies on supervised machine learning and represents each sentence as a set of n-grams .", "Incorporating information from WordNet alo improves performance for both approaches ."]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["This paper presents the UNL graph matching method for the Semantic Textual Similarity ( STS ) task .", "The SemEval 2012 task of Semantic Textual Similarity aims at finding the semantic similarity between two sentences .", "Thus , comparing the UNL graphs of two sentences can give an insight into how semantically similar the two sentences are .", "Sentences that are syntactically quite different can often have similar or same meaning .", "The semantic representation of Universal Networking Language ( UNL ) , represents only the inherent meaning in a sentence without any syntactic details ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In this paper we report the results obtained in the Semantic Textual Similarity ( STS ) task , with a system primarily developed for textual entailment .", "Our results are quite promising , getting a run ranked 39 in the official results with overall Pearson , and ranking 29 with the Mean metric ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["The UOW submissions to the Semantic Textual Similarity task at SemEval-2012 use a supervised machine learning algorithm along with features based on lexical , syntactic and semantic similarity metrics to predict the semantic equivalence between a pair of sentences .", "The lexical metrics are based on wordoverlap .", "Our submissions outperformed the official baseline , with our best system ranked above average , but the contribution of the semantic metrics was not conclusive .", "A shallow syntactic metric is based on the overlap of base-phrase labels .", "The semantically informed metrics are based on the preservation of named entities and on the alignment of verb predicates and the overlap of argument roles using inexact matching ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We used regression to combine the different similarity measures , and found that each provides partially independent predictive signal above baseline models .", "We explore the contributions of different vector models for computing sentence and word similarity : Collobert and Weston embeddings as well as two novel approaches , namely eigenwords and selectors .", "These embeddings provide different measures of distributional similarity between words , and their contexts .", "We present the Penn system for SemEval2012 Task 6 , computing the degree of semantic equivalence between two sentences ."]}
{"orig_sents": ["2", "4", "1", "5", "6", "0", "3"], "shuf_sents": ["We have the third best result among the 29 systems submitted by 10 teams .", "This approach allows the machine-learning algorithm to obtain an asymmetric similarity function suitable for directional judgments .", "This paper presents a novel approach for building adaptive similarity functions based on cardinality using machine learning .", "Additionally , this paper presents better results compared with the best official score .", "Unlike current approaches that build feature sets using similarity scores , we have developed these feature sets with the cardinalities of the commonalities and differences between pairs of objects being compared .", "Besides using the classic set cardinality , we used soft cardinality to allow flexibility in the comparison between words .", "Our approach used only the information from the surface of the text , a stop-word remover and a stemmer to address the cross-lingual textual entailment task 8 at SEMEVAL 2012 ."]}
{"orig_sents": ["9", "7", "3", "5", "2", "8", "6", "4", "0", "1"], "shuf_sents": ["The best run is submitted for Italian ?", "English language with accuracy 0.326 .", "The entailment system considers Named Entity , Noun Chunks , Part of speech , N-Gram and some text similarity measures of the text pair to decide the entailment judgments .", "We set up different heuristics and measures for evaluating the entailment between two texts based on lexical relations .", "Four different rules have been developed for the four different classes of entailment .", "Experiments have been carried out with both the text and hypothesis converted to the same language using the Microsoft Bing translation system .", "Our system decides on the entailment judgment after comparing the entailment scores for the text pairs .", "The work explores cross-lingual textual entailment as a relation between two texts in different languages and proposes different measures for entailment decision in a four way classification tasks ( forward , backward , bidirectional and no-entailment ) .", "Rules have been developed to encounter the multi way entailment issue .", "This article presents the experiments carried out at Jadavpur University as part of the participation in Cross-lingual Textual Entailment for Content Synchronization ( CLTE ) of task 8 @ Semantic Evaluation Exercises ( SemEval-2012 ) ."]}
{"orig_sents": ["0"], "shuf_sents": ["This paper presents CELI ? s participation in the SemEval Cross-lingual Textual Entailment for Content Synchronization task ."]}
{"orig_sents": ["2", "3", "0", "5", "1", "4"], "shuf_sents": ["The features are used for multi-class and binary classification using SVMs .", "Our best run achieved an accuracy of 50.4 % on the Spanish-English dataset ( with the average score and the median system respectively achieving 40.7 % and 34.6 % ) , demonstrating the effectiveness of a ? pure ?", "This paper overviews FBK ? s participation in the Cross-Lingual Textual Entailment for Content Synchronization task organized within SemEval-2012 .", "Our participation is characterized by using cross-lingual matching features extracted from lexical and semantic phrase tables and dependency relations .", "cross-lingual approach that avoids intermediate translations .", "Using a combination of lexical , syntactic , and semantic features to create a cross-lingual textual entailment system , we report on experiments over the provided dataset ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["In this paper we present a report of the two different runs submitted to the task 8 of Semeval 2012 for the evaluation of Cross-lingual Textual Entailment in the framework of Content Synchronization .", "The first approach uses textual similarity on the translated and original versions of the texts , whereas the second approach expands the terms by means of synonyms .", "The evaluation of both approaches show a similar behavior which is still close to the average and median .", "Both approaches are based on textual similarity , and the entailment judgment ( bidirectional , forward , backward or no entailment ) is given based on a set of decision rules ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["We show the results obtained by our implementation of this simple and fast approach at the CLTE task from the SemEval2012 challenge .", "Cross-Lingual Text Entailment ( CLTE ) , besides introducing the extra dimension of cross-linguality , also requires to determine the exact direction of the entailment relation , to provide content synchronization ( Negri et al , 2012 ) .", "There are relatively few entailment heuristics that exploit the directional nature of the entailment relation .", "The key members of the conditions were derived from ( Corley and Mihalcea , 2005 ) formula initially for text similarity , while the entailment condition used as a starting point was that from ( Tatar et al , 2009 ) .", "Our system uses simple dictionary lookup combined with heuristic conditions to determine the possible directions of entailment between the two texts written in different languages ."]}
{"orig_sents": ["0", "2", "5", "4", "1", "3"], "shuf_sents": ["In this paper , we present our system description in task of Cross-lingual Textual Entailment .", "Since EDITS only draws monodirectional relations while the task requires bidirectional prediction , thus we exchange the hypothesis and test to detect entailment in another direction .", "The goal of this task is to detect entailment relations between two sentences written in different languages .", "Experimental results show that our method achieves promising results but not perfect results compared to other participants .", "Then , we use EDITS1 , an open source package , to recognize entailment relations .", "To accomplish this goal , we first translate sentences written in foreign languages into English ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Results are very promising always achieving results above mean score .", "Our system resides on machine learning and in the use of WordNet as semantic source knowledge .", "We represent an approach to CLTE using machine translation to tackle the problem of multilinguality .", "This paper describes our participation in the task denominated Cross-Lingual Textual Entailment ( CLTE ) for content synchronization ."]}
{"orig_sents": ["1", "0", "4", "3", "2", "5"], "shuf_sents": ["This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models , such as tensors and matrices , can be used to simulate different aspects of predicate logic .", "The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature .", "It suggests a variant of these tensor calculi capable of modelling quantifiers , using few non-linear operations .", "It provides tensor interpretations of the set of logical connectives required to model propositional calculi .", "This paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors .", "It finally discusses the relation between these variants , and how this relation should constitute the subject of future work ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives .", "We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks ( MLNs ) .", "We also show that distributional phrase similarity , used as textual inference rules created on the fly , improves its performance ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["Wikipedia articles are annotated by volunteer contributors with numerous links that connect words and phrases to relevant titles .", "Links to general senses of a word are used concurrently with links to more specific senses , without being distinguished explicitly .", "Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation .", "We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies ."]}
{"orig_sents": ["0", "3", "2", "5", "6", "4", "1"], "shuf_sents": ["In Semantic Textual Similarity ( STS ) , systems rate the degree of semantic equivalence , on a graded scale from 0 to 5 , with 5 being the most similar .", "The CORE task attracted 34 participants with 89 runs , and the TYPED task attracted 6 teams with 14 runs .", "CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012 , yet different in genre from the 2012 set , namely , this year we included newswire headlines , machine translation evaluation datasets and multiple lexical resource glossed sets .", "This year we set up two tasks : ( i ) a core task ( CORE ) , and ( ii ) a typed-similarity task ( TYPED ) .", "The annotation for both tasks leverages crowdsourcing , with relative high interannotator correlation , ranging from 62 % to 87 % .", "TYPED , on the other hand , is novel and tries to characterize why two items are deemed similar , using cultural heritage items which are described with metadata such as title , author or description .", "Several types of similarity have been defined , including similar author , similar time period or similar location ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The first , which achieved the best mean score of the 89 submitted runs , used a simple term alignment algorithm augmented with penalty terms .", "We describe three semantic text similarity systems developed for the *SEM 2013 STS shared task and the results of the corresponding three runs .", "The other two runs , ranked second and fourth , used support vector regression models to combine larger sets of features .", "All of them shared a word similarity feature that combined LSA word similarity and WordNet knowledge ."]}
{"orig_sents": ["5", "1", "0", "3", "4", "2"], "shuf_sents": ["Our systems rely on tree kernels to automatically extract a rich set of syntactic patterns to learn a similarity score correlated with human judgements .", "Different from the majority of approaches , where a large number of pairwise similarity features are used to learn a regression model , our model directly encodes the input texts into syntactic/semantic structures .", "Nevertheless , a slight refinement to our model makes it rank 4th .", "We experiment with different structural representations derived from constituency and dependency trees .", "While showing large improvements over the top results from the previous year task ( STS-2012 ) , our best system ranks 21st out of total 88 participated in the STS2013 task .", "This paper describes the participation of iKernels system in the Semantic Textual Similarity ( STS ) shared task at *SEM 2013 ."]}
{"orig_sents": ["1", "2", "3", "0", "5", "4"], "shuf_sents": ["Moreover , the approach does not require any manually coded resource ( e.g .", "This paper presents the UNITOR system that participated in the *SEM 2013 shared task on Semantic Textual Similarity ( STS ) .", "The task is modeled as a Support Vector ( SV ) regression problem , where a similarity scoring function between text pairs is acquired from examples .", "The proposed approach has been implemented in a system that aims at providing high applicability and robustness , in order to reduce the risk of over-fitting over a specific datasets .", "A good level of accuracy is achieved over the shared task : in the Typed STS task the proposed system ranks in 1st and 2nd position .", "WordNet ) , but mainly exploits distributional analysis of unlabeled corpora ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["The paper outlines the work carried out at NTNU as part of the *SEM ? 13 shared task on Semantic Textual Similarity , using an approach which combines shallow textual , distributional and knowledge-based features by a support vector regression model .", "; ( 3 ) deeper semantic relations based on the RelEx semantic dependency relationship extraction system ; ( 4 ) graph edit-distance on dependency trees ; ( 5 ) reused features of the TakeLab and DKPro systems from the STS ? 12 shared task .", "Feature sets include ( 1 ) aggregated similarity based on named entity recognition with WordNet and Levenshtein distance through the calculation of maximum weighted bipartite graphs ; ( 2 ) higher order word co-occurrence similarity using a novel method called ? Multisense Random Indexing ?", "The NTNU systems obtained 9th place overall ( 5th best team ) and 1st place on the SMT data set ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["In this shared task , we propose an interpolation STS model named Model_LIM integrating FrameNet parsing information , which has a good performance with low time complexity compared with former submissions .", "This paper describes our system submitted to *SEM 2013 Semantic Textual Similarity ( STS ) core task which aims to measure semantic similarity of two given text snippets ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We present a direct evaluation on the British National Corpus , and application based evaluations on Twitter messages and on automatic speech recognition ( where the system could be employed to restore case ) .", "We describe a number of techniques for automatically deriving lists of common and proper nouns , and show that the distinction between the two can be made automatically using a vector space model learning algorithm ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Multiple kernels provide different views of syntactic structure , from both tree and dependency parses .", "The kernels are then combined with simple lexical features using Gaussian process regression , which is trained on different subsets of training data for each run .", "We found that the simplest combination has the highest consistency across the different data sets , while introduction of more training data and models requires training and test data with matching qualities .", "This paper describes methods that were submitted as part of the *SEM shared task on Semantic Textual Similarity ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["For the STS typed task , the string kernel ( Lodhi et al , 2002 ) is used to compute similarity between two entities to avoid string variations in entities .", "The Semantic Textual Similarity ( STS ) task aims to exam the degree of semantic equivalence between sentences ( Agirre et al. , 2012 ) .", "For the STS core task , the PolyUCOMP system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses they shared .", "This paper presents the work of the Hong Kong Polytechnic University ( PolyUCOMP ) team which has participated in the STS core and typed tasks of SemEval2013 ."]}
{"orig_sents": ["3", "5", "4", "0", "2", "1"], "shuf_sents": ["et al , 2012 ) , and the 2013 STS task focused more on predicting similarity for text pairs from new domains .", "Our submissions performed well at most subtasks , particularly at measuring the similarity of news headlines , where one of our submissions ranked 2nd among 89 from 34 teams , but there is still room for improvement .", "Therefore , for the three variations of our system that we were allowed to submit , we used stacking ( Wolpert , 1992 ) to combine PERP with word and ngram features and applied the domain adaptation approach outlined by Daume III ( 2007 ) to facilitate generalization to new domains .", "This paper describes a system for automatically measuring the semantic similarity between two texts , which was the aim of the 2013 Semantic Textual Similarity ( STS ) task ( Agirre et al , 2013 ) .", "However , approaches including word and n-gram features also performed well ( Ba ? r et al , 2012 ; S ? aric ?", "For the 2012 STS task , Heilman and Madnani ( 2012 ) submitted the PERP system , which performed competitively in relation to other submissions ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We estimate the semantic similarity between two sentences using regression models with features : 1 ) n-gram hit rates ( lexical matches ) between sentences , 2 ) lexical semantic similarity between non-matching words , 3 ) string similarity metrics , 4 ) affective content similarity and 5 ) sentence length .", "Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47 .", "This paper describes our submission for the *SEM shared task of Semantic Textual Similarity ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["In order to establish which features are the most appropriate to improve STS results we participated with three runs using different set of features .", "This paper describes the specifications and results of UMCC_DLSI system , which participated in the Semantic Textual Similarity task ( STS ) of SemEval-2013 .", "Our supervised system uses different types of lexical and semantic features to train a Bagging classifier used to decide the correct option .", "Our best run reached the position 44 in the official ranking , obtaining a general correlation coefficient of 0.61 .", "Related to the different features we can highlight the resource ISR-WN used to extract semantic relations among words and the use of different algorithms to establish semantic and lexical similarities ."]}
{"orig_sents": ["7", "9", "4", "1", "5", "0", "6", "8", "2", "3"], "shuf_sents": ["a collection of records on European cultural heritage objects ?", "The goal is to evaluate the degree of semantic similarity between semi-structured records .", "Support Vector Regression is then used to combine the features and to provide a final similarity score .", "The system ranked third on the attribute author among 15 submitted runs in the typed-similarity task .", "The work described in this paper particularly shows effects of the mentioned processes in the context of the *SEM 2013 pilot task on typed-similarity , a part of the Semantic Textual Similarity shared task .", "As the evaluation dataset has been taken from Europeana ?", "we focus on computing a semantic distance on field author which has the highest potential to benefit from the domain knowledge .", "This paper deals with knowledge-based text processing which aims at an intuitive notion of textual similarity .", "Specific features that are employed in our system BUT-TYPED are briefly introduced together with a discussion on their efficient acquisition .", "Entities and relations relevant for a particular domain are identified and disambiguated by means of semi-supervised machine learning techniques and resulting annotations are applied for computing typedsimilarity of individual texts ."]}
{"orig_sents": ["0", "3", "2", "4", "1"], "shuf_sents": ["This paper reports our submissions to the Semantic Textual Similarity ( STS ) task in ? SEM Shared Task 2013 .", "Our best system ranks 5 out of 15 runs .", "Our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs .", "We submitted three Support Vector Regression ( SVR ) systems in core task , using 6 types of similarity measures , i.e. , string similarity , number similarity , knowledge-based similarity , corpus-based similarity , syntactic dependency similarity and machine translation similarity .", "We also submitted two systems in typed task using string based measure and Named Entity based measure ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The results indicate that the linear regression is key for good performance .", "We approach the typed-similarity task using a range of heuristics that rely on information from the appropriate metadata fields for each type of similarity .", "Our best system was ranked third in the task .", "In addition we train a linear regressor for each type of similarity ."]}
{"orig_sents": ["1", "5", "0", "2", "3", "4"], "shuf_sents": ["The similarity scores derived from these features are then fed into several multilayer perceptron neuronal networks .", "In this paper we describe KnCe2013-CORE , a system to compute the semantic similarity of two short text snippets .", "Depending on the size of the text snippets different parameters for the neural networks are used .", "The final output of the neural networks is compared to human judged data .", "In the evaluation our system performed sufficiently well for text snippets of equal length , but the performance dropped considerably once the pairs of text snippets differ in size .", "The system computes a number of features which are gathered from different knowledge bases , namely WordNet , Wikipedia and Wiktionary ."]}
{"orig_sents": ["3", "2", "1", "4", "5", "0"], "shuf_sents": ["normalization issues caused our correlation values to decrease .", "Our similarity estimator relies on a support vector regressor with RBF kernel .", "Our core features include ( i ) a set of metrics borrowed from automatic machine translation , originally intended to evaluate automatic against reference translations and ( ii ) an instance of explicit semantic analysis , built upon opening paragraphs of Wikipedia 2010 articles .", "In this paper we discuss our participation to the 2013 Semeval Semantic Textual Similarity task .", "Our best approach required 13 machine translation metrics + explicit semantic analysis and ranked 65 in the competition .", "Our postcompetition analysis shows that the features have a good expression level , but overfitting and ? mainly ?"]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["We explored three representations of semantics ( implicit or explicit ) : named entities , semantic vectors , and structured vectorial semantics .", "The Semantic Textual Similarity ( STS ) task examines semantic similarity at a sentencelevel .", "From a DKPro baseline , we also performed feature selection and used sourcespecific linear regression models to combine our features .", "Our systems placed 5th , 6th , and 8th among 90 submitted systems ."]}
{"orig_sents": ["3", "1", "2", "5", "4", "0"], "shuf_sents": ["A simple analysis of the soft similarity resources over two word phrases is provided , and future areas of improvement are described .", "To this end , we explored the use of neural probabilistic language models and a TF-IDF weighted variant of Explicit Semantic Analysis .", "The neural language model systems used vector representations of individual words , where these vectors were derived by training them against the context of words encountered , and thus reflect the distributional characteristics of their usage .", "In this year ? s Semantic Textual Similarity evaluation , we explore the contribution of models that provide soft similarity scores across spans of multiple words , over the previous year ? s system .", "We find that these soft similarity methods generally outperformed our previous year ? s systems , albeit they did not perform as well in the overall rankings .", "To generate a similarity score between spans , we experimented with using tiled vectors and Restricted Boltzmann Machines to identify similar encodings ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes the system used by the LIPN team in the Semantic Textual Similarity task at *SEM 2013 .", "It uses a support vector regression model , combining different text similarity measures that constitute the features .", "These measures include simple distances like Levenshtein edit distance , cosine , Named Entities overlap and more complex distances like Explicit Semantic Analysis , WordNet-based similarity , IR-based similarity , and a similarity measure based on syntactic dependencies ."]}
{"orig_sents": ["1", "0", "4", "2", "3", "6", "5"], "shuf_sents": ["We exploited three different systems for computing the similarity between two texts .", "This paper describes the UNIBA participation in the Semantic Textual Similarity ( STS ) core task 2013 .", "Such system is based on a distributional model of semantics capable of taking into account also syntactic structures that glue words together .", "In addition , we investigated the use of two different learning strategies exploiting both syntactic and semantic features .", "A system is used as baseline , which represents the best model emerged from our previous participation in STS 2012 .", "The latter tries to overcome the limit of working with different datasets with varying characteristics by selecting only the more suitable dataset for the training purpose .", "The former uses ensemble learning in order to combine the best machine learning techniques trained on 2012 training and test sets ."]}
{"orig_sents": ["3", "2", "4", "5", "1", "0"], "shuf_sents": ["Two different selections of training data result in very different performance levels : while a correlation of 0.4135 with gold standards was observed in the official evaluation ( ranked 63rd among all systems ) for one selection , the other resulted in a correlation of 0.5352 ( that would rank 21st ) .", "This model is then applied on the STS 2013 data to compute textual similarities .", "Given two short text fragments , the goal of the system is to determine their semantic similarity .", "We present a system submitted in the Semantic Textual Similarity ( STS ) task at the Second Joint Conference on Lexical and Computational Semantics ( *SEM 2013 ) .", "Our system makes use of three different measures of text similarity : word n-gram overlap , character n-gram overlap and semantic overlap .", "Using these measures as features , it trains a support vector regression model on SemEval STS 2012 data ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Given the simple nature of our approach , which uses only WordNet and unannotated corpus data as external resources , we consider this a remarkably good result , making the system an interesting tool for a wide range of practical applications .", "This paper describes our system entered for the *SEM 2013 shared task on Semantic Textual Similarity ( STS ) .", "We focus on the core task of predicting the semantic textual similarity of sentence pairs .", "The current system utilizes machine learning techniques trained on semantic similarity ratings from the *SEM 2012 shared task ; it achieved rank 20 out of 90 submissions from 35 different teams ."]}
{"orig_sents": ["2", "1", "3", "5", "0", "4"], "shuf_sents": ["Our team submitted three runs during the task evaluation period and they ranked number 11 , 15 and 19 among the 90 participating systems according to the official Mean Pearson correlation metric for the task .", "The Semantic Textual Similarity Core task ( STS ) computes the degree of semantic equivalence between two sentences where the participant systems will be compared to the manual scores , which range from 5 ( semantic equivalence ) to 0 ( no relation ) .", "We present in this paper the systems we participated with in the Semantic Textual Similarity task at SEM 2013 .", "We combined multiple text similarity measures of varying complexity .", "We also report an unofficial run with mean Pearson correlation of 0.59221 on STS2013 test dataset , ranking as the 3rd best system among the 90 participating systems .", "The experiments illustrate the different effect of four feature types including direct lexical matching , idf-weighted lexical matching , modified BLEU N-gram matching and named entities matching ."]}
{"orig_sents": ["2", "3", "1", "0", "4"], "shuf_sents": ["Already after the release of the gold standard annotations of the test data , we observed that using only the similarity measures without combining them with other features would have obtained positions 6th , 7th and 8th ; moreover , an arithmetic average of these similarity measures would have been 4th ( mean=0.5747 ) .", "Unfortunately , we combined these measures with other features using regression , obtaining positions 18th , 22nd and 23rd among the 90 participants systems in the official ranking .", "Soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity ( STS ) , obtaining 3rd place in SemEval-2012 .", "At *SEM-2013 shared task , beside the plain textoverlapping approach , we tested within soft cardinality two distributional word-similarity functions derived from the ukWack corpus .", "This paper describes both the 3 systems as they were submitted and the similarity measures that would obtained those better results ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["Using a core set of 11 lexical features of the most basic kind , it uses a support vector regressor which uses a combination of these lexical features to train a model for predicting similarity between sentences in a two phase method , which in turn uses all combinations of the features in the feature space and trains separate models based on each combination .", "CLaC-CORE , an exhaustive feature combination system ranked 4th among 34 teams in the Semantic Textual Similarity shared task STS 2013 .", "This two step process improves the results achieved by singlelayer standard learning methodology over the same simple features .", "Then it creates a meta-feature space and trains a final model based on that .", "We analyze the correlation of feature combinations with the data sets over which they are effective ."]}
{"orig_sents": ["0", "3", "1", "2", "4"], "shuf_sents": ["In this paper we present our systems for calculating the degree of semantic similarity between two texts that we submitted to the Semantic Textual Similarity task at SemEval2013 .", "We also explore methods for integrating predictions using different training datasets and feature sets .", "Our best system was ranked 17th out of 89 participating systems .", "Our systems predict similarity using a regression over features based on the following sources of information : string similarity , topic distributions of the texts based on latent Dirichlet alocation , and similarity between the documents returned by an information retrieval engine when the target texts are used as queries .", "In our post-task analysis , we identify simple changes to our system that further improve our results ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Our approach combines syntactic and word level similarity measures along with the UNL based semantic similarity measures for finding similarity scores between sentences .", "We describe a Universal Networking Language ( UNL ) based semantic extraction system for measuring the semantic similarity .", "The task aims to find the similarity score between a pair of sentences .", "This paper describes the system that was submitted in the *SEM 2013 Semantic Textual Similarity shared task ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["While the evaluation datasets are not designed to test the similarity of opinions , as a component of textual similarity , nonetheless , our system variations ranked number 38 , 39 and 45 among the 88 participating systems .", "In addition to more traditional components , such as knowledge-based and corpus-based metrics leveraged in a machine learning framework , we also use opinion analysis features to achieve a stronger semantic representation of textual units .", "This article provides a detailed overview of the CPN text-to-text similarity system that we participated with in the Semantic Textual Similarity task evaluations hosted at *SEM 2013 ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["The first two methods do not require labeled training data ; instead , they automatically extract semantic knowledge in the form of word associations from a given reference corpus .", "The third method was done in collaboration with groups from the Universities of Paris 13 , Matanzas and Alicante .", "This paper presents three methods to evaluate the Semantic Textual Similarity ( STS ) .", "It uses several word similarity measures as features in order to construct an accurate prediction model for the STS .", "Two kinds of word associations are considered : cooccurrence statistics and the similarity of word contexts ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["This paper describes the dataset , the syntactic representation , and the kinds of information provided .", "We created a dataset of syntactic-ngrams ( counted dependency-tree fragments ) based on a corpus of 3.5 million English books .", "The dataset includes over 10 billion distinct items covering a wide range of syntactic configurations .", "It also includes temporal information , facilitating new kinds of research into lexical semantics over time ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The results show that our topic modelling approach outperforms the other two methods .", "We evaluate these methods against a novel dataset made up of human ratings over 550 Twitter message pairs annotated for usage similarity for a set of 10 nouns .", "We propose an unsupervised method for automatically calculating word usage similarity in social media data based on topic modelling , which we contrast with a baseline distributional method and Weighted Textual Matrix Factorization ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Our overall best result is state-of-the-art , reaching Spearman ? s ?", "This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds : ( 1 ) Against our intuition , we demonstrate that window-based rather than syntax-based distributional features perform better predictions , and that not adjectives or verbs but nouns represent the most salient part-of-speech .", "( 2 ) While there are no significant differences in predicting compound ? modifier vs. compound ? head ratings on compositionality , we show that the modifier ( rather than the head ) properties predominantly influence the degree of compositionality of the compound .", "= 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["It outperforms the previous approaches to metaphor identification both in terms of accuracy and coverage , as well as providing an interpretation for each identified expression .", "However , cognitive evidence suggests that humans are likely to perform these two tasks simultaneously , as part of a holistic metaphor comprehension process .", "Automatic metaphor identification and interpretation in text have been traditionally considered as two separate tasks in natural language processing ( NLP ) and addressed individually within computational frameworks .", "We present a novel method that performs metaphor identification through its interpretation , being the first one in NLP to combine the two tasks in one step ."]}
{"orig_sents": ["2", "0", "5", "7", "3", "6", "4", "1"], "shuf_sents": ["Automatic short answer scoring is the task of automatically assessing the semantic content of a student ? s answer , marking it e.g .", "While the other two models do not improve classification accuracy , they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors .", "Short answer questions for reading comprehension are a common task in foreign language learning .", "First , we conduct a corpus study targeting the links between sentences in reading texts for learners of German and answers to reading comprehension questions based on those texts .", "The most promising approach is the first one , results for which show that textual features improve classification accuracy .", "as correct or incorrect .", "Second , we use the reading text directly for classification , considering three different models : an answer-based classifier extended with textual features , a simple text-based classifier , and a model that combines the two according to confidence of the text-based classification .", "While previous approaches mainly focused on comparing a learner answer to some reference answer provided by the teacher , we explore the use of the underlying reading texts as additional evidence for the classification ."]}
{"orig_sents": ["2", "4", "3", "0", "5", "1"], "shuf_sents": ["We characterize the types of errors that occur using the word count approach , and find lexical ambiguity to be the most prevalent .", "The resulting refined lexica improve precision as measured by human judgments of word occurrences in Facebook posts .", "Social scientists are increasingly using the vast amount of text available on social media to measure variation in happiness and other psychological states .", "This word count approach is simple and scalable , yet often picks up false signals , as words can appear in different contexts and take on different meanings .", "Such studies count words deemed to be indicators of happiness and track how the word frequencies change across locations or time .", "We then show that one can reduce error with a simple refinement to such lexica by automatically eliminating highly ambiguous words ."]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["Implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing .", "One reason for this lies in the scarce amount of annotated data sets available .", "Our contributions are threefold : we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts ; we show how the induced data can be used as training data for improving existing argument linking models ; finally , we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references .", "In this paper , we present a range of studies that empirically validate this claim .", "We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and to enable more in-depth studies of the phenomenon itself ."]}
{"orig_sents": ["4", "0", "1", "3", "2"], "shuf_sents": ["The setting is similar to co-training , except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages .", "Our approach can facilitate the construction of SRL models for resource-poor languages , while preserving the annotation schemes designed for the target language and making use of the limited resources available for it .", "Consistent improvements are observed over the self-training baseline .", "We evaluate the model on four language pairs , English vs German , Spanish , Czech and Chinese .", "We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling ( SRL ) models ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["In cross-domain experiments involving 23 domains , FreeParser can parse sentences for which it has seen comparable unannotated sentences with an F1 of 0.71 .", "FreeParser uses a domain-independent architecture to automatically identify sentences relevant to each new database symbol , which it uses to supplement its manually-annotated training data from the training domain .", "This paper introduces FreeParser , a system that trains on one domain and one set of predicate and constant symbols , and then can parse sentences for any new domain , including sentences that refer to symbols never seen during training .", "Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding databases ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["Within the SemEval-2013 evaluation exercise , the TempEval-3 shared task aims to advance research on temporal information processing .", "approaches , results , and the observations from the results , which may guide future research in this area .", "In this paper , we describe the participants ?", "It follows on from TempEval-1 and -2 , with : a three-part structure covering temporal expression , event , and temporal relation extraction ; a larger dataset ; and new single measures to rank systems ?", "in each task and in general ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The system is a pipeline of machine-learning models , each with a small set of features from a simple morpho-syntactic annotation pipeline , and where temporal relations are only predicted for a small set of syntactic constructions and relation types .", "ClearTKTimeML ranked 1st for temporal relation F1 , time extent strict F1 and event tense accuracy .", "The ClearTK-TimeML submission to TempEval 2013 competed in all English tasks : identifying events , identifying times , and identifying temporal relations ."]}
{"orig_sents": ["3", "4", "0", "2", "1"], "shuf_sents": ["Exploiting HeidelTime ? s strict separation between source code and languagedependent parts , we tuned HeidelTime ? s existing English resources and developed new Spanish resources .", "Both the improved English and the new Spanish resources are publicly available with HeidelTime .", "For both languages , we achieved the best results among all participants for task A , the combination of extraction and normalization .", "In this paper , we describe our participation in the TempEval-3 challenge .", "With our multilingual temporal tagger HeidelTime , we addressed task A , the extraction and normalization of temporal expressions for English and Spanish ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We show that it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets .", "In this paper we present the results of experiments comparing ( a ) rich syntactic and semantic feature sets and ( b ) big context windows , for the TempEval time expression and event segmentation and classification tasks ."]}
{"orig_sents": ["3", "1", "0", "4", "2"], "shuf_sents": ["Participants were presented with datasets for different language pairs , where multi-directional entailment relations ( ? forward ? , ? backward ? , ? bidirectional ? , ? no entailment ? )", "The task was designed to promote research on semantic inference over texts written in different languages , targeting at the same time a real application scenario .", "We report on the training and test data used for evaluation , the process of their creation , the participating systems ( six teams , 61 runs ) , the approaches adopted and the results achieved .", "This paper presents the second round of the task on Cross-lingual Textual Entailment for Content Synchronization , organized within SemEval-2013 .", "had to be identified ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["This system consists of a SVM classifier with features extracted from texts ( and their translations SMT ) based on a cardinality function .", "We also evaluated the use of additional circular-pivoting translations achieving results 6.14 % above the best official results .", "In this paper we describe our system submitted for evaluation in the CLTE-SemEval-2013 task , which achieved the best results in two of the four data sets , and finished third in average .", "Furthermore , this system was simplified by providing a single model for the 4 pairs of languages obtaining better ( unofficial ) results than separate models for each language pair .", "Such function was the soft cardinality ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["In succession , it introduces the systems that participated and discusses evaluation results .", "The paper discusses the importance and background of these subtasks and their structure .", "This paper describes the SemEval-2013 Task 5 : ? Evaluating Phrasal Semantics ? .", "The second one addresses deciding the compositionality of phrases in a given context .", "Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The classification is done by a support vector machine that uses all similarities as features .", "In order to compare a single word with a two word phrase we compute various distributional similarities , among which a new similarity measure , based on Jensen-Shannon Divergence with a correction for frequency effects .", "The approach turned out to be the most successful one in the task .", "This paper describes the approach of the Hochschule Hannover to the SemEval 2013 Task Evaluating Phrasal Semantics ."]}
{"orig_sents": ["6", "5", "1", "3", "4", "2", "0"], "shuf_sents": ["Surprisingly , the use of the silver data ( alone or in addition to the gold annotated ones ) does not improve the performance .", "We investigate the performance variation with respect to different feature types .", "Normalization accuracies are 0.84 ( type attribute ) and 0.77 ( value attribute ) .", "Specifically , we show that the use of WordNet-based features in the identification task negatively affects the overall performance , and that there is no statistically significant difference in using gazetteers , shallow parsing and propositional noun phrases labels on top of the morphological features .", "On the test data , the best run achieved 0.95 ( P ) , 0.85 ( R ) and 0.90 ( F1 ) in the identification phase .", "The identification phase combines the use of conditional random fields along with a post-processing identification pipeline , whereas the normalization phase is carried out using NorMA , an open-source rule-based temporal normalizer .", "This paper describes a temporal expression identification and normalization system , ManTIME , developed for the TempEval-3 challenge ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["FSS-TimEx was developed as part of a multilingual event extraction system , Nexus , which runs on top of the EMM news processing engine .", "We describe FSS-TimEx , a module for the recognition and normalization of temporal expressions we submitted to Task A and B of the TempEval-3 challenge .", "It consists of finite-state rule cascades , using minimalistic text processing stages and simple heuristics to model the relations between events and temporal expressions .", "Although FSS-TimEx is already deployed within an IE application in the medical domain , we found it useful to customize its output to the TimeML standard in order to have an independent performance measure and guide further developments ."]}
{"orig_sents": ["3", "5", "2", "1", "4", "0"], "shuf_sents": ["We have incorporated various features based on different lexical , syntactic and semantic information , using Stanford CoreNLP and Wordnet based tools .", "Our system seems to perform quite competitively in Task A and Task B .", "We have participated in all the tasks of TempEval-3 , namely Task A , Task B & Task C. We have primarily utilized the Conditional Random Field ( CRF ) based machine learning technique , for all the above tasks .", "In this paper , we present the JUCSE system , designed for the TempEval-3 shared task .", "In Task C , the system ? s performance is comparatively modest at the initial stages of system development .", "The system extracts events and temporal information from natural text in English ."]}
{"orig_sents": ["3", "0", "5", "4", "2", "1"], "shuf_sents": ["Task 1 is a unique challenge because it starts from raw text , rather than pre-annotated text with known events and times .", "The NavyTime system ranked second both overall and in most subtasks like event extraction and relation labeling .", "Experiments show that more specialized classifiers perform better than few joint classifiers .", "This paper describes a complete event/time ordering system that annotates raw text with events , times , and the ordering relations between them at the SemEval-2013 Task 1 .", "We present a split classifier approach that breaks the ordering tasks into smaller decision points .", "A working system first identifies events and times , then identifies which events and times should be ordered , and finally labels the ordering relation between them ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["SUTIME is available as part of the Stanford CoreNLP pipeline and can be used to annotate documents with temporal information .", "We analyze the performance of SUTIME , a temporal tagger for recognizing and normalizing temporal expressions , on TempEval-3 Task A for English .", "Testing on the TempEval-3 evaluation corpus showed that this system is competitive with state-of-the-art techniques ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["This paper describes a system for temporal processing of text , which participated in the Temporal Evaluations 2013 campaign .", "an approach which identifies temporal relation arguments ( eventevent and event-timex pairs ) and the semantic label of the relation as a single decision .", "The central feature of the proposed system is temporal parsing ?", "The system employs a number of machine learning classifiers to perform the core tasks of : identification of time expressions and events , recognition of their attributes , and estimation of temporal links between recognized events and times ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The system uses logistic regression classifiers and exploits features extracted from a deep syntactic parser , including paths between event words in phrase structure trees and their path lengths , and paths between event words in predicateargument structures and their subgraphs .", "UTTime achieved an F1 score of 34.9 based on the graphed-based evaluation for Task C ( ranked 2nd ) and 56.45 for Task C-relationonly ( ranked 1st ) in the TempEval-3 evaluation .", "In this paper , we present a system , UTTime , which we submitted to TempEval-3 for Task C : Annotating temporal relations ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Related to the different features we can highlight the resource WordNet used to extract semantic relations among words and the use of different algorithms to establish semantic similarities .", "Our system obtains promising results with a precision value around 78 % for the English corpus and 71.84 % for the Italian corpus .", "This paper describes the specifications and results of UMCC_DLSI- ( EPS ) system , which participated in the first Evaluating Phrasal Semantics of SemEval-2013 .", "Our supervised system uses different kinds of semantic features to train a bagging classifier used to select the correct similarity option ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The system computes the similarity between a particular noun instance and the head noun of a particular noun phrase , which was weighted according to the semantics of the modifier .", "Our system uses a dependency-based vector space model , in combination with a technique called latent vector weighting .", "The system is entirely unsupervised ; one single parameter , the similarity threshold , was tuned using the training data .", "In this paper we present our system for the SemEval 2013 Task 5a on semantic similarity of words and compositional phrases ."]}
{"orig_sents": ["0", "2", "1", "4", "3", "5"], "shuf_sents": ["This paper describes the IIRG 1 system entered in SemEval-2013 , the 7th International Workshop on Semantic Evaluation .", "We have adopted a token-based approach to solve this task using 1 ) Na ?", "We participated in Task 5 Evaluating Phrasal Semantics .", "We found that the word overlap method significantly out-performs the Na ?", "? ve Bayes methods and 2 ) Word Overlap methods , both of which rely on the extraction of syntactic features .", "? ve Bayes methods , achieving our highest overall score with an accuracy of approximately 78 % ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["The measurement of phrasal semantic relatedness is an important metric for many natural language processing applications .", "Our hybrid approach achieved an Fmeasure of 77.4 % on the task of evaluating the semantic similarity of words and compositional phrases .", "In this paper , we present three approaches for measuring phrasal semantics , one based on a semantic network model , another on a distributional similarity model , and a hybrid between the two ."]}
{"orig_sents": ["3", "5", "4", "0", "2", "1"], "shuf_sents": ["The system obtained a relative improvement in accuracy against the most-frequentclass baseline of 49.8 % in the ? unseen contexts ?", "( AllWords ) .", "( LexSample ) setting and 8.5 % in ? unseen phrases ?", "In this paper we describe the system used to participate in the sub task 5b in the Phrasal Semantics challenge ( task 5 ) in SemEval 2013 .", "The proposed approach is based on part-of-speech tags , stylistic features and distributional statistics gathered from the same development-training-test text collection .", "This sub task consists in discriminating literal and figurative usage of phrases with compositional and non-compositional meanings in context ."]}
{"orig_sents": ["0", "6", "7", "1", "4", "3", "5", "2"], "shuf_sents": ["This paper presents our approach used for cross-lingual textual entailment task ( task 8 ) organized within SemEval 2013 .", "Firstly , we use a off-the-shelf machine translation ( MT ) tool to convert the two input texts into the same language .", "The results on the cross-lingual data collections provided by SemEval 2013 show that ( 1 ) we can build portable and effective systems across languages using MT and multiple effective features ; ( 2 ) our systems achieve the best results among the participants on two test datasets , i.e. , FRA-ENG and DEU-ENG .", "We also propose novel feature types regarding to sentence difference and semantic similarity based on our observations in the preliminary experiments .", "Then after performing a text preprocessing , we extract multiple feature types with respect to surface text and grammar .", "Finally , we adopt a multiclass SVM algorithm for classification .", "Crosslingual textual entailment ( CLTE ) tries to detect the entailment relationship between two text fragments in different languages .", "We solved this problem in three steps ."]}
{"orig_sents": ["3", "2", "4", "1", "0"], "shuf_sents": ["The results obtained show a performance below the median of six teams that have participated in the competition .", "Five different runs were submitted , one of them considering voting system of the previous four approaches .", "We have counted the number of N grams for three types of textual entities ( character , word and PoS tags ) that exist in the pair of sentences from which we are interested in determining the judgment of textual entailment .", "This paper describes the evaluation of different kinds of textual features for the CrossLingual Textual Entailment Task of SemEval 2013 .", "Difference , intersection and distance ( Euclidian , Manhattan and Jaccard ) of N -grams were considered for constructing a feature vector which is further introduced in a support vector machine classifier which allows to construct a classification model ."]}
{"orig_sents": ["5", "2", "3", "4", "1", "0"], "shuf_sents": ["On the Italian/English and Spanish/English test sets our systems ranked second among five participants , close to the top results ( respectively 43.4 % and 45.4 % ) .", "In terms of accuracy , performance ranges from 38.8 % ( for German/English ) to 43.2 % ( for Italian/English ) .", "Our approach is language independent , and was used to participate in the CLTE task ( Task # 8 ) organized within Semeval 2013 ( Negri et al , 2013 ) .", "The four runs submitted , one for each language combination covered by the test data ( i.e .", "Spanish/English , German/English , French/English and Italian/English ) , achieved encouraging results .", "We present a supervised learning approach to cross-lingual textual entailment that explores statistical word alignment models to predict entailment relations between sentences written in different languages ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper describes The University of Melbourne NLP group submission to the Crosslingual Textual Entailment shared task , our first tentative attempt at the task .", "The approach involves using parallel corpora and automatic word alignment to align text fragment pairs , and statistics based on unaligned words as features to classify items as forward and backward before a compositional combination into the final four classes , as well as experiments with additional string similarity features ."]}
{"orig_sents": ["9", "5", "10", "1", "6", "4", "8", "7", "3", "0", "2"], "shuf_sents": ["They all beat a simple baseline on one of the two evaluation measures , but not on both measures .", "The list is automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators , recruited and managed through Amazon ? s Mechanical Turk .", "This shows that the task is difficult .", "Three systems participated in the task .", "The ? gold ?", "The task is to capture some of the meaning of English noun compounds via paraphrasing .", "The comparison of raw paraphrases is sensitive to syntactic and morphological variation .", "To make the ranking more reliable , highly similar paraphrases are grouped , so as to downplay superficial differences in syntax and morphology .", "ranking is based on the relative popularity of paraphrases among annotators .", "In this paper , we describe SemEval-2013 Task 4 : the definition , the data , the evaluation and the results .", "Given a two-word noun compound , the participating system is asked to produce an explicitly ranked list of its free-form paraphrases ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper describes the system submitted by the MELODI team for the SemEval-2013 Task 4 : Free Paraphrases of Noun Compounds ( Hendrickx et al , 2013 ) .", "Our approach combines the strength of an unsupervised distributional word space model with a supervised maximum-entropy classification model ; the distributional model yields a feature representation for a particular compound noun , which is subsequently used by the classifier to induce a number of appropriate paraphrases ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents an approach for generating free paraphrases of compounds ( task 4 at SemEval 2013 ) by decomposing the training data into a collection of templates and fillers and recombining/scoring these based on a generative language model and discriminative MaxEnt reranking .", "The system described in this paper achieved the highest score ( with a very small margin ) in the ( default ) isomorphic setting of the scorer , for which it was optimized , at a disadvantage to the non-isomorphic score ."]}
{"orig_sents": ["3", "0", "2", "1", "4"], "shuf_sents": ["Our system implements a corpusdriven probabilistic co-occurrence based model for predicting the paraphrases , that uses a seed list of paraphrases extracted from corpus to predict other paraphrases based on their co-occurrences .", "Therefore , to predict other paraphrases , we adopt a two-fold approach : ( i ) Prediction based on Verb-Verb cooccurrences , in case the seed paraphrases are greater than threshold ; and ( ii ) Prediction based on Semantic Relation of NC , otherwise .", "The corpus study reveals that the prepositional paraphrases for the noun compounds are quite frequent and well covered but the verb paraphrases , on the other hand , are scarce , revealing the unsuitability of the model for standalone corpus-driven approach .", "This paper presents a system for automatically generating a set of plausible paraphrases for a given noun compound and rank them in decreasing order of their usage represented by the confidence value provided by the human annotators .", "The system achieves a comparabale score of 0.23 for the isomorphic system while maintaining a score of 0.26 for the non-isomorphic system ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["French , Italian , English , German and Dutch .", "The goal of the Cross-lingual Word Sense Disambiguation task is to evaluate the viability of multilingual WSD on a benchmark lexical sample data set .", "We report results for the 12 official submissions from 5 different research teams , as well as for the ParaSense system that was developed by the task organizers .", "The traditional WSD task is transformed into a multilingual WSD task , where participants are asked to provide contextually correct translations of English ambiguous nouns into five target languages , viz ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The XLING system introduces a novel approach to skip the sense disambiguation step by matching query sentences to sentences in a parallel corpus using topic models ; it returns the word alignments as the translation for the target polysemous words .", "This paper describes the XLING system participation in SemEval-2013 Crosslingual Word Sense Disambiguation task .", "Although , the topic-model base matching underperformed , the matching approach showed potential in the simple cosine-based surface similarity matching ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Our three systems , in increasing order of complexity , were : maximum entropy classifiers trained to predict the desired targetlanguage phrase using only monolingual features ( we called this system L1 ) ; similar classifiers , but with the desired target-language phrase for the other four languages as features ( L2 ) ; and lastly , networks of five classifiers , over which we do loopy belief propagation to solve the classification tasks jointly ( MRF ) .", "We present our entries for the SemEval2013 cross-language word-sense disambiguation task ( Lefever and Hoste , 2013 ) .", "We submitted three systems based on classifiers trained on local context features , with some elaborations ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["We present the design of the system and the obtained results .", "We describe the LIMSI system for the SemEval-2013 Cross-lingual Word Sense Disambiguation ( CLWSD ) task .", "Word senses are represented by means of translation clusters in different languages built by a cross-lingual Word Sense Induction ( WSI ) method .", "Our CLWSD classifier exploits the WSI output for selecting appropriate translations for target words in context ."]}
{"orig_sents": ["4", "0", "6", "7", "1", "3", "2", "5"], "shuf_sents": ["The system closely resembles our winning system for the same task in SemEval 2010 .", "The system participated in the task for all five languages and obtained winning scores for four of them when asked to predict the best translation ( s ) .", "Our final results indicate that hyperparameter optimisation did not lead to the best results , indicating overfitting by our optimisation method in this aspect .", "We tested various configurations of our system , focusing on various levels of hyperparameter optimisation and feature selection .", "We present our system WSD2 which participated in the Cross-Lingual Word-Sense Disambiguation task for SemEval 2013 ( Lefever and Hoste , 2013 ) .", "Feature selection does have a modest positive impact .", "It is based on k-nearest neighbour classifiers which map words with local and global context features onto their translation , i.e .", "their cross-lingual sense ."]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["This paper describes the NRC submission to the Spanish Cross-Lingual Word Sense Disambiguation task at SemEval-2013 .", "Since this word sense disambiguation task uses Spanish translations of English words as gold annotation , it can be cast as a machine translation problem .", "However , its top 5 predictions are weaker than those from other systems .", "Using only local context information and no linguistic analysis beyond lemmatization , our machine translation system surprisingly yields top precision score based on the best predictions .", "We therefore submitted the output of a standard phrase-based system as a baseline , and investigated ways to improve its sense disambiguation performance ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The task enables the end-to-end evaluation and comparison of systems .", "Given a target query , induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query .", "In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application , namely Web search result clustering and diversification ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["These systems were all implemented using SenseClusters , a freely available open source software package .", "They relied on an approach that represented Web snippets using second ? order co ?", "The Duluth systems that participated in task 11 of SemEval ? 2013 carried out word sense induction ( WSI ) in order to cluster Web search results .", "occurrences ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["It describes the WSI system developed for Task 11 of SemEval - 2013 .", "This paper implements the idea of monotone submodular function optimization using greedy algorithm .", "The aim of this paper is to perform Word Sense induction ( WSI ) ; which clusters web search results and produces a diversified list of search results ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We developed a configurable pipeline which can be used to integrate and evaluate other components for the various steps of the complex task .", "In this paper , we describe the UKP Lab system participating in the Semeval-2013 task ? Word Sense Induction and Disambiguation within an End-User Application ? .", "Our approach uses preprocessing , co-occurrence extraction , graph clustering , and a state-of-theart word sense disambiguation system ."]}
{"orig_sents": ["4", "2", "3", "0", "1"], "shuf_sents": ["Our system adopts a preexisting Word Sense Induction ( WSI ) methodology based on Hierarchical Dirichlet Process ( HDP ) , a non-parametric topic model .", "Our system is trained over extracts from the full text of English Wikipedia , and is shown to perform well in the shared task .", "In the task , participants are provided with a set of ambiguous search queries and the snippets returned by a search engine , and are asked to associate senses with the snippets .", "The snippets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters .", "This paper describes our system for Task 11 of SemEval-2013 ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We present and analyze the results of participating systems , and discuss future directions .", "We describe our experience in producing a multilingual sense-annotated corpus for the task .", "This paper presents the SemEval-2013 task on multilingual Word Sense Disambiguation .", "The corpus is tagged with BabelNet 1.1.1 , a freely-available multilingual encyclopedic dictionary and , as a byproduct , WordNet 3.0 and the Wikipedia sense inventory ."]}
{"orig_sents": ["0", "5", "3", "4", "1", "2"], "shuf_sents": ["This article presents the GETALP system for the participation to SemEval-2013 Task 12 , based on an adaptation of the Lesk measure propagated through an Ant Colony Algorithm , that yielded good results on the corpus of Semeval 2007 Task 7 ( WordNet 2.1 ) as well as the trial data for Task 12 SemEval 2013 ( BabelNet 1.0 ) .", "Our system arrived third on this task and a more fine grained analysis of our results reveals that the algorithms performs best on general domain texts with as little named entities as possible .", "The presence of many named entities leads the performance of the system to plummet greatly .", "We proposed three runs of out system , exogenous estimation with BabelNet 1.1.1 synset id annotations , endogenous estimation with BabelNet 1.1.1 synset id annotations and endogenous estimation with WordNet 3.1 sense keys .", "A bug in our implementation led to incorrect results and here , we present an amended version thereof .", "We approach the parameter estimation to our algorithm from two perspectives : edogenous estimation where we maximised the sum the local Lesk scores ; exogenous estimation where we maximised the F1 score on trial data ."]}
{"orig_sents": ["5", "4", "1", "2", "3", "7", "6", "0"], "shuf_sents": ["Our system , recognized as the best in the competition , obtains results around 69 % of Recall .", "It does so by selecting the correct Babel synset for the word and the various Wiki Page titles that mention the word .", "BabelNet contains all the output information that our system needs , in its Babel synset .", "Through Babel synset , we find all the possible Synsets for the word in WordNet .", "Its main purpose is to automatically choose the intended sense ( meaning ) of a word in a particular context for different languages .", "This work introduces a new unsupervised approach to multilingual word sense disambiguation .", "To facilitate the work with WordNet , we use the ISR-WN which offers the integration of different resources to WordNet .", "Using these Synsets , we apply the disambiguation method Ppr+Freq to find what we need ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["PD exploits the frequency and diverse use of word senses in semantic subgraphs derived from larger sense inventories such as BabelNet , Wikipedia , and WordNet in order to achieve WSD .", "PD ? s f -measure scores for SemEval 2013 Task 12 outperform the Most Frequent Sense ( MFS ) baseline for two of the five languages : English , French , German , Italian , and Spanish .", "Despite PD remaining under-developed and under-explored , it demonstrates that it is robust , competitive , and encourages development .", "We introduce Peripheral Diversity ( PD ) as a knowledge-based approach to achieve multilingual Word Sense Disambiguation ( WSD ) ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Many NLP applications require information about locations of objects referenced in text , or relations between them in space .", "Spatial Role Labeling ( SpRL ) is an evaluation task in the information extraction domain which sets a goal to automatically process text and identify objects of spatial scenes and relations between them .", "For example , the phrase a book on the desk contains information about the location of the object book , as trajector , with respect to another object desk , as landmark .", "This paper describes the task in Semantic Evaluations 2013 , annotation schema , corpora , participants , methods and results obtained by the participants ."]}
{"orig_sents": ["3", "1", "0", "2", "4"], "shuf_sents": ["Thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way RTE-style tasks on educational data .", "The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment .", "In addition , a partial entailment task was piloted .", "We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge , aiming to bring together researchers in educational NLP technology and textual entailment .", "We present and compare results from 9 participating teams , and discuss future directions ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["As such , it seems desirable to integrate various approaches , making use of model answers from experts ( e.g. , to give higher scores to responses that are similar ) , prescored student responses ( e.g. , to learn direct associations between particular phrases and scores ) , etc .", "Automatic scoring of short text responses to educational assessment items is a challenging task , particularly because large amounts of labeled data ( i.e. , human-scored responses ) may or may not be available due to the variety of possible questions and topics .", "Here , we describe a system that uses stacking ( Wolpert , 1992 ) and domain adaptation ( Daume III , 2007 ) to achieve this aim , allowing us to integrate item-specific n-gram features and more general text similarity measures ( Heilman and Madnani , 2012 ) .", "We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge ."]}
{"orig_sents": ["4", "5", "6", "2", "0", "1", "3"], "shuf_sents": ["instances , which was the more challenging test set .", "This paper also describes another system that integrates this method with the lexical-overlap baseline provided by the task organizers obtaining better results than the best official results .", "Furthermore , our system performs particularly well with ? unseendomains ?", "We concluded that the soft cardinality method is a very competitive baseline for the automatic evaluation of student responses .", "In this paper we describe our system used to participate in the Student-Response-Analysis task-7 at SemEval 2013 .", "This system is based on text overlap through the soft cardinality and a new mechanism for weight propagation .", "Although there are several official performance measures , taking into account the overall accuracy throughout the two availabe data sets ( Beetle and SciEntsBank ) , our system ranked first in the 2 way classification task and second in the others ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Our system combines text similarity measures with a textual entailment system .", "In the main task , we focused on the influence of lexicalized versus unlexicalized features , and how they affect performance on unseen questions and domains .", "We also participated in the pilot partial entailment task , where our system significantly outperforms a strong baseline ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["Four teams submitted nine systems , which were evaluated in two settings .", "However , contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage .", "Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense .", "We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses , weighted by their applicability ."]}
{"orig_sents": ["3", "0", "2", "5", "1", "6", "4"], "shuf_sents": ["Once a sense inventory is obtained for an ambiguous word , word sense discrimination approaches choose the best-fitting single sense for a given context from the induced sense inventory .", "In contrast to the most common approach which is to apply clustering or graph partitioning on a representation of first or second order co-occurrences of a word , we propose a system that creates a substitute vector for each target word from the most likely substitutes suggested by a statistical language model .", "However , there may not be a clear distinction between one sense and another , although for a context , more than one induced sense can be suitable .", "Word sense induction aims to discover different senses of a word from a corpus by using unsupervised learning approaches .", "This approach outperforms the other systems on graded word sense induction task in SemEval-2013 .", "Graded word sense method allows for labeling a word in more than one sense .", "Word samples are then taken according to probabilities of these substitutes and the results of the co-occurrence model are clustered ."]}
{"orig_sents": ["5", "1", "2", "0", "3", "4"], "shuf_sents": ["The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance .", "of SemEval-2013 .", "The task is on word sense induction ( WSI ) , and builds on earlier SemEval WSI tasks in exploring the possibility of multiple senses being compatible to varying degrees with a single contextual instance : participants are asked to grade senses rather than selecting a single sense like most word sense disambiguation ( WSD ) settings .", "We adopt a previously-proposed WSI methodology for the task , which is based on a Hierarchical Dirichlet Process ( HDP ) , a nonparametric topic model .", "Our system requires no parameter tuning , uses the English ukWaC as an external resource , and achieves encouraging results over the shared task .", "This paper describes our system for shared task 13 ? Word Sense Induction for Graded and Non-Graded Senses ?"]}
{"orig_sents": ["4", "5", "2", "3", "0", "6", "1"], "shuf_sents": ["All datasets used in the evaluation are released to the research community .", "The bestperforming team achieved an F1 of 88.9 % and 69 % for subtasks A and B , respectively .", "To address this issue , we have proposed SemEval-2013 Task 2 : Sentiment Analysis in Twitter , which included two subtasks : A , an expression-level subtask , and B , a messagelevel subtask .", "We used crowdsourcing on Amazon Mechanical Turk to label a large Twitter training dataset alng with additional test sets of Twitter and SMS messages for both subtasks .", "In recent years , sentiment analysis in social media has attracted a lot of research interest and has been used for a number of applications .", "Unfortunately , research has been hindered by the lack of suitable datasets , complicating the comparison between approaches .", "The task attracted significant interest and a total of 149 submissions from 44 teams ."]}
{"orig_sents": ["0", "4", "5", "2", "3", "1"], "shuf_sents": ["In this paper , we describe how we created two state-of-the-art SVM classifiers , one to detect the sentiment of messages such as tweets and SMS ( message-level task ) and one to detect the sentiment of a term within a message ( term-level task ) .", "Both of our systems can be replicated using freely available resources.1", "We also generated two large word ? sentiment association lexicons , one from tweets with sentiment-word hashtags , and one from tweets with emoticons .", "In the message-level task , the lexicon-based features provided a gain of 5 F-score points over all others .", "Among submissions from 44 teams in a competition , our submissions stood first in both tasks on tweets , obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task .", "We implemented a variety of surface-form , semantic , and sentiment features ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We used a linear classifier trained by stochastic gradient descent with hinge loss and elastic net regularization to make our predictions , which were ranked first or second in three of the four experimental conditions of the shared task .", "Furthermore , our system makes use of social media specific text preprocessing and linguistically motivated features , such as word stems , word clusters and negation handling .", "This paper describes the details of our system submitted to the SemEval-2013 shared task on sentiment analysis in Twitter .", "Our approach to predicting the sentiment of Tweets and SMS is based on supervised machine learning techniques and task-specific feature engineering ."]}
{"orig_sents": ["4", "3", "5", "2", "0", "1"], "shuf_sents": ["Our systems performed competitively , placing in the top five for all subtasks and data conditions .", "More importantly , these results show that expanding the polarity lexicon and augmenting the training data with unlabeled tweets can yield improvements in precision and recall in classifying the polarity of non-neutral messages and contexts .", "These automatically labeled data are used for two purposes : 1 ) to discover prior polarities of words and 2 ) to provide additional training examples for self-training .", "For the constrained conditions of both the message polarity classification and contextual polarity disambiguation subtasks , our approach centers on training high-dimensional , linear classifiers with a combination of lexical and syntactic features .", "This paper describes the systems submitted by Avaya Labs ( AVAYA ) to SemEval-2013 Task 2 - Sentiment Analysis in Twitter .", "The constrained message polarity model is then used to tag nearly half a million unlabeled tweets ."]}
{"orig_sents": ["1", "2", "0", "3", "4"], "shuf_sents": ["Both subtasks have been very successful in participation and results .", "The DDIExtraction 2013 task concerns the recognition of drugs and extraction of drugdrug interactions that appear in biomedical literature .", "We propose two subtasks for the DDIExtraction 2013 Shared Task challenge : 1 ) the recognition and classification of drug names and 2 ) the extraction and classification of their interactions .", "There were 14 teams who submitted a total of 38 runs .", "The best result reported for the first subtask was F1 of 71.5 % and 65.1 % for the second one ."]}
{"orig_sents": ["1", "2", "3", "4", "0"], "shuf_sents": ["Our system obtained significantly higher results than all the other participating teams in this shared task and has been ranked 1st .", "This paper presents the multi-phase relation extraction ( RE ) approach which was used for the DDI Extraction task of SemEval 2013 .", "As a preliminary step , the proposed approach indirectly ( and automatically ) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data .", "Then , a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not filtered out by the previous step .", "The official results of the task show that our approach yields an F-score of 0.80 for DDI detection and an F-score of 0.65 for DDI detection and classification ."]}
{"orig_sents": ["6", "4", "0", "3", "2", "1", "5"], "shuf_sents": ["In this study , we investigate the impact of such domain-specific features on the performance of recognizing and classifying mentions of pharmacological substances .", "Still , our experiments show that using domain-specific features outperforms this general approach .", "We demonstrate that acceptable results can be achieved with the former system .", "We compare the performance of a system based on general features , which have been successfully applied to a wide range of NER tasks , with a system that additionally uses features generated from the output of an existing chemical NER tool and a collection of domain-specific resources .", "Nevertheless , time-consuming feature engineering is often needed to achieve state-of-the-art performance .", "Our system ranked first in the SemEval-2013 Task 9.1 : Recognition and classification of pharmacological substances .", "Named entity recognition ( NER ) systems are often based on machine learning techniques to reduce the labor-intensive development of hand-crafted extraction rules and domain-dependent dictionaries ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["We propose two different methods to efficiently incorporate prior knowledge .", "Our approach consists of adapting Naive Bayes probabilities in order to take into account prior knowledge ( represented in the form of a sentiment lexicon ) .", "In this paper , we describe our system that participated in SemEval-2013 , Task 2.B ( sentiment analysis in Twitter ) .", "We show that our approach outperforms the classical Naive Bayes method and shows competitive results with SVM while having less computational complexity ."]}
{"orig_sents": ["2", "4", "0", "1", "3"], "shuf_sents": ["It allows to combine the contribution of complex kernel functions , such as the Latent Semantic Kernel and Smoothed Partial Tree Kernel , to implicitly integrate syntactic and lexical information of annotated examples .", "In the challenge , UNITOR system achieves good results , even considering that no manual feature engineering is performed and no manually coded resources are employed .", "In this paper , the UNITOR system participating in the SemEval-2013 Sentiment Analysis in Twitter task is presented .", "These kernels in-fact embed distributional models of lexical semantics to determine expressive generalization of tweets .", "The polarity detection of a tweet is modeled as a classification task , tackled through a Multiple Kernel approach ."]}
{"orig_sents": ["4", "3", "5", "0", "1", "2"], "shuf_sents": ["We chose to work as ? constrained ? , which used only the provided training and development data without additional sentiment annotated resources .", "Our approach considered unigram , bigram and trigram using Na ? ve Bayes training model with the objective of establishing a simpleapproach baseline .", "Our system achieved Fscore 81.23 % and F-score 78.16 % in the results for SMS messages and Tweets respectively .", "The goal of this task is to predict whether marked contexts are positive , neutral or negative .", "This paper presents our system , TJP , which participated in SemEval 2013 Task 2 part A : Contextual Polarity Disambiguation .", "However , only the scores of positive and negative class will be used to calculate the evaluation result using F-score ."]}
{"orig_sents": ["4", "0", "2", "3", "1"], "shuf_sents": ["The first system ( for Task A ) classifies the polarity / sentiment orientation of one target word in a Twitter message .", "We present a few additional results , besides results of the submitted runs .", "The second system ( for Task B ) classifies the polarity of whole Twitter messages .", "Our two systems are very simple , based on supervised classifiers with bag-ofwords feature representation , enriched with information from several sources .", "We present two systems developed at the University of Ottawa for the SemEval 2013 Task 2 ."]}
{"orig_sents": ["3", "7", "4", "6", "5", "1", "0", "2"], "shuf_sents": ["Results suggest that unigrams are the most important features , bigrams and POS tags seem not helpful , and stopwords should be retained to achieve the best results .", "To see the contribution of each type of features , we do experimental study on features by leaving one type of features out each time .", "The overall results of our system are promising regarding the constrained features and data we use .", "This paper describes our system for participating SemEval2013 Task2-B ( Kozareva et al , 2013 ) : Sentiment Analysis in Twitter .", "It uses a co-occurrence rate model .", "We consider 9 types of features and use a subset of them in our submitted system .", "The training data are constrained to the data provided by the task organizers ( No other tweet data are used ) .", "Given a message , our system classifies whether the message is positive , negative or neutral sentiment ."]}
{"orig_sents": ["0", "1", "6", "2", "4", "5", "3"], "shuf_sents": ["This paper describes a dual-classifier approach to contextual sentiment analysis at the SemEval-2013 Task 2 .", "Contextual analysis of polarity focuses on a word or phrase , rather than the broader task of identifying the sentiment of an entire text .", "However , the context of a single word is dependent on the word ? s surrounding syntax , while a phrase contains most of the polarity within itself .", "We also show a surprising result that a very small amount of word context is needed for high-performance polarity extraction .", "We thus describe separate treatment with two independent classifiers , outperforming the accuracy of a single classifier .", "Our system ranked 6th out of 19 teams on SMS message classification , and 8th of 23 on twitter data .", "The Task 2 definition includes target word spans that range in size from a single word to entire sentences ."]}
{"orig_sents": ["0", "2", "3", "1", "4"], "shuf_sents": ["This paper describes our approach to the SemEval-2013 task on ? Sentiment Analysis in Twitter ? .", "Despite its simplicity , the system achieves competitive accuracies of 0.70 ? 0.72 in detecting the sentiment of text messages .", "We use simple bag-of-words models , a freely available sentiment dictionary automatically extended with distributionally similar terms , as well as lists of emoticons and internet slang abbreviations in conjunction with fast and robust machine learning algorithms .", "The resulting system is resource-lean , making it relatively independent of a specific language .", "We also apply our approach to the task of detecting the contextdependent sentiment of individual words and phrases within a message ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper describes the participation of the SINAI research group in the 2013 edition of the International Workshop SemEval .", "The SINAI research group has submitted two systems , which cover the two main approaches in the field of sentiment analysis : supervised and unsupervised ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We extract features from surface information of tweets , i.e. , content features , Microblogging features , emoticons , punctuation and sentiment lexicon , and adopt SVM to build classifier .", "This paper briefly reports our submissions to the two subtasks of Semantic Analysis in Twitter task in SemEval 2013 ( Task 2 ) , i.e. , the Contextual Polarity Disambiguation task ( an expression-level task ) and the Message Polarity Classification task ( a message-level task ) .", "For subtask A , our system on twitter data ranks 2 on unconstrained rank and on SMS data ranks 1 on unconstrained rank ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["In the Contextual Polarity Disambiguation subtask , we use a sentiment lexicon approach combined with polarity shift detection and tree kernel based classifiers .", "This paper presents the contribution of our team at task 2 of SemEval 2013 : Sentiment Analysis in Twitter .", "We submitted a constrained run for each of the two subtasks .", "In the Message Polarity Classification subtask , we focus on the influence of domain information on sentiment classification ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper is an overview of the SwatCS system submitted to SemEval-2013 Task 2A : Contextual Polarity Disambiguation .", "The sentiment of individual phrases within a tweet are labeled using a combination of classifiers trained on a range of lexical features .", "Performance is measured when using only the provided training data , and separately when including external data .", "The classifiers are combined by estimating the accuracy of the classifiers on each tweet ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["The paper describes experiments using grid searches over various combinations of machine learning algorithms , features and preprocessing strategies in order to produce the optimal systems for sentiment classification of microblog messages .", "The approach is fairly domain independent , as demonstrated by the systems achieving quite competitive results when applied to short text message data , i.e. , input they were not originally trained on ."]}
{"orig_sents": ["1", "4", "0", "5", "3", "2", "6"], "shuf_sents": ["The model uses an affective lexicon automatically generated from a very large corpus of raw web data .", "This paper describes our submission for SemEval2013 Task 2 : Sentiment Analysis in Twitter .", "The two models are fused at the posterior level to produce a final output .", "For the unconstrained data scenario we combine the lexicon-based model with a classifier built on maximum entropy language models and trained on a large external dataset .", "For the limited data condition we use a lexicon-based model .", "Statistics are calculated over the word and bigram affective ratings and used as features of a Naive Bayes tree model .", "The approach proved successful , reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively , despite using only lexical features ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["As a result , new sentiment inventories are obtained and applied in conjunction with detected informal patterns , to tackle the challenges posted in Task 2b of the Semeval2013 competition .", "Assessing the effectiveness of our application in sentiment classification , we obtained a 69 % F-Measure for neutral and an average of 43 % F-Measure for positive and negative using Tweets and SMS messages .", "This system uses corpora where phrases are annotated as Positive , Negative , Objective , and Neutral , to achieve new sentiment resources involving word dictionaries with their associated polarity .", "In this paper , we describe the development and performance of the supervised system UMCC_DLSI- ( SA ) ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["Our method ranks 23 out of all 48 participating systems , achieving an averaged ( positive , negative ) F-Score of 0.5456 and an averaged ( positive , negative , neutral ) F-Score of 0.595 , which is above median and average .", "Our system is designed to function as a baseline , to see what we can accomplish with well-understood and purely data-driven lexical features , simple generalizations as well as standard machine learning techniques : We use one-against-one Support Vector Machines with asymmetric cost factors and linear ? kernels ?", "This paper describes University of Leipzig ? s approach to SemEval-2013 task 2B on Sentiment Analysis in Twitter : message polarity classification .", "We consider generalizations of URLs , user names , hash tags , repeated characters and expressions of laughter .", "as classifiers , word uni- and bigrams as features and additionally model negation of word uni- and bigrams in word n-gram feature space ."]}
{"orig_sents": ["4", "2", "0", "3", "1"], "shuf_sents": ["We propose to use many features in order to improve a trained classifier of Twitter messages ; these features extend the feature vector of uni-gram model by the concepts extracted from DBpedia , the verb groups and the similar adjectives extracted from WordNet , the Sentifeatures extracted using SentiWordNet and some useful domain specific features .", "Adding these features has improved the f-measure accuracy 2 % with SVM and 4 % with NaiveBayes .", "Such analysis could be useful for many domains such as Marketing , Finance , Politics , and Social .", "We also built a dictionary for emotion icons , abbreviation and slang words in tweets which is useful before extending the tweets with different features .", "Sentiment Analysis in Twitter has become an important task due to the huge user-generated content published over such media ."]}
{"orig_sents": ["3", "0", "5", "4", "2", "1"], "shuf_sents": ["This phenomenon is of high importance to news monitoring systems , whose aim is to obtain an informative snapshot of media events and related comments .", "The results regarding tweet classification are promising and show that sentiment generalization can be an effective approach for tweets and that SMS language is difficult to tackle , even when specific normalization resources are employed .", "We describe the approaches used in the competition and the additional experiments performed combining different datasets for training , using or not slang replacement and generalizing sentiment-bearing terms by replacing them with unique labels .", "The fast development of Social Media made it possible for people to no loger remain mere spectators to the events that happen in the world , but become part of them , commenting on their developments and the entities involved , sharing their opinions and distributing related content .", "The main goal was to evaluate the best settings for a sentiment analysis component to be added to the online news monitoring system .", "This paper presents the strategies employed in the OPTWIMA participation to SemEval 2013 Task 2-Sentiment Analysis in Twitter ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The approach is based on the exploitation of various resources such as SentiWordNet and LIWC .", "This paper presents the Tweetsted system implemented for the SemEval 2013 task on Sentiment Analysis in Twitter .", "Official results show that our approach yields a F-score of 0.5976 for Twitter messages ( 11th out of 35 ) and a F-score of 0.5487 for SMS messages ( 8th out of 28 participants ) .", "In particular , we participated in Task B on Message Polarity Classification in the Constrained setting ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["Sentiment analysis refers to automatically extracting the sentiment present in a given natural language text .", "We present our participation to the SemEval2013 competition , in the sentiment analysis of Twitter and SMS messages .", "Our approach for this task is the combination of two sentiment analysis subsystems which are combined together to build the final system .", "Both subsystems use supervised learning using features based on various polarity lexicons ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We present a supervised sentiment detection system that classifies the polarity of subjective phrases as positive , negative , or neutral .", "We show how to incorporate these new features within a state of the art system and evaluate it on subtask A in SemEval-2013 Task 2 : Sentiment Analysis in Twitter .", "It is tailored towards online genres , specifically Twitter , through the inclusion of dictionaries developed to capture vocabulary used in online conversations ( e.g. , slang and emoticons ) as well as stylistic features common to social media ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper describes the system implemented by Fundacio ?", "Barcelona Media ( FBM ) for classifying the polarity of opinion expressions in tweets and SMSs , and which is supported by a UIMA pipeline for rich linguistic and sentiment annotations .", "FBM participated in the SEMEVAL 2013 Task 2 on polarity classification .", "It ranked 5th in Task A ( constrained track ) using an ensemble system combining ML algorithms with dictionary-based heuristics , and 7th ( Task B , constrained ) using an SVM classifier with features derived from the linguistic annotations and some heuristics ."]}
{"orig_sents": ["1", "4", "5", "2", "0", "3", "6"], "shuf_sents": ["The average F-score of both positive and negative classes was 45.01 % .", "We evaluate a naive machine learning approach to sentiment classification focused on Twitter in the context of the sentiment analysis task of SemEval-2013 .", "Our average F-score for all three classes on the Twitter evaluation dataset was 51.55 % .", "For the optional SMS evaluation dataset our overall average F-score was 58.82 % .", "We employ a classifier based on the Random Forests algorithm to determine whether a tweet expresses overall positive , negative or neutral sentiment .", "The classifier was trained only with the provided dataset and uses as main features word vectors and lexicon word counts .", "The average between positive and negative Fscores was 50.11 % ."]}
{"orig_sents": ["1", "5", "6", "3", "4", "0", "2"], "shuf_sents": ["Finally , the overall sentiment is determined using a rule-based classifier .", "This paper describes the specifications and results of SSA-UO , unsupervised system , presented in SemEval 2013 for Sentiment Analysis in Twitter ( Task 2 ) ( Wilson et al , 2013 ) .", "As it may be observed , the results obtained for Twitter and SMS sentiment classification are good considering that our proposal is unsupervised .", "Word polarity detection is carried out taking into account the sentiment associated with the context in which it appears .", "For this , we use a new contextual sentiment classification method based on coarse-grained word sense disambiguation , using WordNet ( Miller , 1995 ) and a coarse-grained sense inventory ( sentiment inventory ) built up from SentiWordNet ( Baccianella et al , 2010 ) .", "The proposal system includes three phases : data preprocessing , contextual word polarity detection and message classification .", "The preprocessing phase comprises treatment of emoticon , slang terms , lemmatization and POS-tagging ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["This article describes a Sentiment Analysis ( SA ) system named senti.ue-en , built for participation in SemEval-2013 Task 2 , a Twitter SA challenge .", "In the first subtask , there is a better result for SMS than the obtained for the more trained type of data , the tweets .", "In both challenge subtasks we used the same supervised machine learning approach , including two classifiers in pipeline , with 22 semantic oriented features , such as polarized term presence and index , and negation presence .", "Our system achieved a better score on Task A ( 0.7413 ) than in the Task B ( 0.4785 ) ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["For SemEval-2013 Task 2 , A and B ( Sentiment Analysis in Twitter ) , we use a rulebased pattern matching system that is based on an existing ? Domain Independent ?", "We have made some modifications to our set of rules , based on what we found in the annotated training data that was made available for the task .", "The resulting system scores competitively , especially on task B .", "sentiment taxonomy for English , essentially a highly phrasal sentiment lexicon ."]}
{"orig_sents": ["2", "1", "4", "0", "3"], "shuf_sents": ["We also used character n-gram language models to address the problem of high lexical variation in Twitter text and combined the two approaches to obtain the final results .", "We first used an SVM classifier with a wide range of features , including bag of word features ( unigram , bigram ) , POS features , stylistic features , readability scores and other statistics of the tweet being analyzed , domain names , abbreviations , emoticons in the Twitter text .", "This paper briefly reports our system for the SemEval-2013 Task 2 : sentiment analysis in Twitter .", "Our system is robust and achieves good performance on the Twitter test data as well as the SMS test data .", "Then we investigated the effectiveness of these features ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["We formed features that take into account the context of the expression and take a supervised approach towards subjectivity and polarity classification .", "In this paper , we describe our system for the SemEval-2013 Task 2 , Sentiment Analysis in Twitter .", "We tested our model for sentiment polarity classification on Twitter as well as SMS chat expressions , analyzed their F-measure scores and drew some interesting conclusions from them .", "Experiments were performed on the features to find out whether they were more suited for subjectivity or polarity Classification ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes an expression-level sentiment detection system that participated in the subtask A of SemEval-2013 Task 2 : Sentiment Analysis in Twitter .", "Our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive , negative or neutral .", "The proposed approach helps to understand the relevant features that contribute most in this classification task ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We implemented a combination of Explicit Semantic Analysis ( ESA ) with Naive Bayes classifier .", "ESA represents text as a high dimensional vector of explicitly defined topics , following the distributional semantic model .", "In this paper , we describe our system submitted for the Sentiment Analysis task at SemEval 2013 ( Task 2 ) .", "This approach is novel in the sense that ESA has not been used for Sentiment Analysis in the literature , to the best of our knowledge ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The resulting system is capable of classifying a document as neutral , positive , or negative with an overall accuracy of 61.2 % using our hierarchical Naive Bayes classifier.1", "Using labeled Twitter training data from SemEval-2013 , we train both a subjectivity classifier and a polarity classifier separately , and then combine the two into a single hierarchical classifier .", "Using additional unlabeled data that is believed to contain sentiment , we allow the polarity classifier to continue learning using self-training ."]}
{"orig_sents": ["0", "2", "1", "7", "5", "6", "4", "3"], "shuf_sents": ["This paper describes the system developed by the Serendio team for the SemEval-2013 Task 2 competition ( Task A ) .", "Our lexicon is built from the Serendio taxonomy .", "We use a lexicon based approach for discovering sentiments .", "Our system yields an F-score of 0.8004 on the test dataset .", "After the preprocessing , the lexicon-based system classifies the tweets as positive or negative based on the contextual sentiment orientation of the words .", "A typical tweet contains word variations , emoticons , hashtags etc .", "We use preprocessing steps such as stemming , emoticon detection and normalization , exaggerated word shortening and hashtag detection .", "The Serendio taxonomy consists of positive , negative , negation , stop words and phrases ."]}
{"orig_sents": ["0", "1", "2", "4", "3"], "shuf_sents": ["In this paper we introduce our contribution to the SemEval-2013 Task 2 on ? Sentiment Analysis in Twitter ? .", "We participated in ? task B ? , where the objective was to build models which classify tweets into three classes ( positive , negative or neutral ) by their contents .", "To solve this problem we basically followed the supervised learning approach and proposed several domain ( i.e .", "Beyond the supervised setting we also introduce some early results employing a huge , automatically annotated tweet dataset .", "microblog ) specific improvements including text preprocessing and feature engineering ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["The shortness of the length and the highly informal nature of tweets render it very difficult to automatically detect such information .", "Two systems are explained : System A for determining the sentiment of a phrase within a tweet and System B for determining the sentiment of a tweet .", "The widespread use of Twitter makes it very interesting to determine the opinions and the sentiments expressed by its users .", "Both approaches rely on rich feature sets , which are explained in detail .", "This paper reports the results to a challenge , set forth by SemEval-2013 Task 2 , to determine the positive , neutral , or negative sentiments of tweets ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We have also experimented with Naive Bayes classifiers trained with BOW features .", "We used a 2-stage pipeline approach employing a linear SVM classifier at each stage and several features including BOW features , POS based features and lexicon based features .", "This paper describes the systems with which we participated in the task Sentiment Analysis in Twitter of SEMEVAL 2013 and specifically the Message Polarity Classification ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We suggest a pipeline architecture that extracts the best characteristics from each classifier .", "Our system achieved an Fscore of 56.31 % in the Twitter message-level subtask .", "This paper describes the NILC USP system that participated in SemEval-2013 Task 2 : Sentiment Analysis in Twitter .", "Our system adopts a hybrid classification process that uses three classification approaches : rulebased , lexicon-based and machine learning approaches ."]}
{"orig_sents": ["1", "2", "0", "4", "5", "3"], "shuf_sents": ["In the identification of spatial relations , roles are combined to generate candidate relations , later verified by a SVM classifier .", "In this paper the UNITOR-HMM-TK system participating in the Spatial Role Labeling task at SemEval 2013 is presented .", "The spatial roles classification is addressed as a sequence-based word classification problem : the SVMhmm learning algorithm is applied , based on a simple feature modeling and a robust lexical generalization achieved through a Distributional Model of Lexical Semantics .", "Finally , results on three of the five tasks of the challenge are reported .", "The Smoothed Partial Tree Kernel is applied , i.e .", "a convolution kernel that enhances both syntactic and lexical properties of the examples , avoiding the need of a manual feature engineering phase ."]}
{"orig_sents": ["2", "5", "0", "1", "4", "3"], "shuf_sents": ["These measures are applied to question , reference answer and student answer triplets .", "We take into account the negation in the syntactic and predicateargument overlap measures .", "We present a 5-way supervised system based on syntactic-semantic similarity features .", "The results show that our system is above the median and mean on all the evaluation scenarios of the SemEval2013 task # 7 .", "Our system uses the domain-specific data as one dataset to build a robust system .", "The model deploys : Text overlap measures , WordNet-based lexical similarities , graphbased similarities , corpus-based similarities , syntactic structure overlap and predicateargument overlap measures ."]}
{"orig_sents": ["0"], "shuf_sents": ["This paper presents CELI ? s participation in the SemEval The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge ( Task7 ) and Cross-lingual Textual Entailment for Content Synchronization task ( Task 8 ) ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Substitutions are applied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary .", "Basic English paraphrases are acquired from the Simple English Wiktionary .", "The evaluation of our approach on the SemEval 2013 Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge data shows promising results , and this work is a first step toward an opendomain system able to exhibit deep text understanding capabilities .", "In this paper , we describe a method for assessing student answers , modeled as a paraphrase identification problem , based on substitution by Basic English variants ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["For four out of the five test sets , our system achieved an overall accuracy above the median and mean .", "Assessing student understanding by evaluating their free text answers to posed questions is a very important task .", "However , manually , it is time-consuming and computationally , it is difficult .", "This paper details our shallow NLP approach to computationally assessing student free text answers when a reference answer is provided ."]}
{"orig_sents": ["2", "1", "3", "0", "4"], "shuf_sents": ["CoMeT obtained the best result ( 73.1 % accuracy ) for the 3-way unseen answers in Beetle among all challenge participants .", "CoMeT is based on a meta-classifier that uses the outputs of the sub-systems we developed : CoMiC , CoSeC , and three shallower bag approaches .", "This paper describes the CoMeT system , our contribution to the SemEval 2013 Task 7 challenge , focusing on the task of automatically assessing student answers to factual questions .", "We sketch the functionality of all sub-systems and evaluate their performance against the official test set of the challenge .", "We also discuss possible improvements and directions for future research ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["The approach outlines a first step in the usage of semantic information for DDI identification .", "In this paper we purpose a new approach based on shallow linguistic kernel methods to identify DDIs in biomedical manuscripts .", "The domain of DDI identification is constantly showing a rise of interest from scientific community since it represents a decrease of time and healthcare cost .", "The system obtained an F1 measure of 0.534 ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["This paper describes a system based on ontologies for identifying the chemical substances in biomedical text .", "The system achieves an F-1 measure of 0.529 in the task .", "Drug name entity recognition focuses on identifying concepts appearing in the text that correspond to a chemical substance used in pharmacology for treatment , cure , prevention or diagnosis of diseases ."]}
{"orig_sents": ["1", "3", "7", "5", "4", "0", "2", "6"], "shuf_sents": ["For interaction detection we achieved F1 measures ranging from 73 % to almost 76 % depending on the run .", "This work describes the participation of the WBI-DDI team on the SemEval 2013 ?", "These results are on par or even higher than the performance estimation on the training dataset .", "Task 9.2 DDI extraction challenge .", "Our approach achieved the second rank in the DDI competition .", "We developed a two-step approach in which pairs are initially extracted using ensembles of up to five different classifiers and then relabeled to one of the four categories .", "When considering the four interaction subtypes we achieved an F1 measure of 60.9 % .", "The task consisted of extracting interactions between pairs of drugs from two collections of documents ( DrugBank and MEDLINE ) and their classification into four subtypes : advise , effect , mechanism , and int ."]}
{"orig_sents": ["4", "2", "1", "0", "3"], "shuf_sents": ["And secondly we will explain our SVM based approaches that use lexical , morphosyntactic and parse tree features .", "In this paper we firstly review some of the existing approaches in relation extraction generally and biomedical relations especially .", "Semeval 2013 DDI Extraction challenge is going to be held with the aim of identifying the state of the art relation extraction algorithms .", "Our combination of sequence and tree kernels have shown promising performance with a best result of 0.54 F1 macroaverage on the test dataset .", "A drug-drug interaction ( DDI ) occurs when one drug affects the level or activity of another drug ."]}
{"orig_sents": ["1", "2", "0", "3", "4"], "shuf_sents": ["We apply the machine learning based Turku Event Extraction System to both tasks .", "The DDIExtraction 2013 task in the SemEval conference concerns the detection of drug names and statements of drug-drug interactions ( DDI ) from text .", "Extraction of DDIs is important for providing up-to-date knowledge on adverse interactions between coadministered drugs .", "We evaluate three feature sets , syntactic features derived from deep parsing , enhanced optionally with features derived from DrugBank or from both DrugBank and MetaMap .", "TEES achieves F-scores of 60 % for the drug name recognition task and 59 % for the DDI extraction task ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["Using only Conditional Random Fields the results are slightly lower , achieving still a good result in terms of Fmeasure .", "Using the ChEBI ontology allowed a significant improvement in precision ( best precision of 0.93 in partial matching task ) , which indicates that taking advantage of an ontology can be extremely useful for enhancing chemical entity recognition .", "We obtained promising results , with a best F-measure of 0.81 for the partial matching task when using post-processing .", "For participating in the SemEval 2013 challenge of recognition and classification of drug names , we adapted our chemical entity recognition approach consisting in Conditional Random Fields for recognizing chemical terms and lexical similarity for entity resolution to the ChEBI ontology ."]}
{"orig_sents": ["4", "2", "0", "1", "3", "5"], "shuf_sents": ["Our approach begins with the use of a two-stage weighted SVM classifier to handle the highly unbalanced class distribution : the first stage for a binary classification of drug pairs as interacting or non-interacting , and the second stage for further classification of interacting pairs from the first stage into one of the four interacting types .", "Our SVM features exploit stemmed words , lemmas , bigrams , part of speech tags , verb lists , and similarity measures , among others .", "The challenge called for a five-way classification of all drug pairs in each sentence : a drug pair is either non-interacting , or interacting as one of four types .", "For each stage , we also developed a set of post-processing rules based on observations in the training data .", "S : Classifying Drug-Drug Interactions with Two-Stage SVM and Post-Processing Majid Rastegar-Mojarad Richard D. Boyce Rashmi Prasad University of Wisconsin-Milwaukee University of Pittsburgh University of Wisconsin-Milwaukee Milwaukee , WI , USA Pittsburgh , PA , USA Milwaukee , WI , USA Rastega3 @ uwm.edu rdb20 @ pitt.edu prasadr @ uwm.edu Abstract We describe our system for the DDIExtraction-2013 shared task of classifying Drug-Drug interactions ( DDIs ) given labeled drug mentions .", "Our best system achieved 0.472 F-measure ."]}
{"orig_sents": ["4", "2", "1", "3", "0"], "shuf_sents": ["For general drugdrug relation extraction , the system achieves 70.4 % in F1 score .", "We present our machine learning system which utilizes lexical , syntactical and semantic based feature sets .", "The DDIExtraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes .", "Resampling , balancing and ensemble learning experiments are performed to infer the best configuration .", "Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data ."]}
{"orig_sents": ["2", "1", "3", "0", "5", "4"], "shuf_sents": ["Tools like openDMAP and TEES are used to extract semantic concepts from the corpus .", "It is a feature rich classification using LIBSVM for Drug-Drug Interactions detection in the BioMedical domain .", "In this paper , we present our approach to SemEval-2013 Task 9.2 .", "The features are extracted considering morphosyntactic , lexical and semantic concepts .", "Keywords : text mining , event extraction , machine learning , feature extraction .", "The best F-score that we got for DrugDrug Interaction ( DDI ) detection is 50 % and 61 % and the best F-score for DDI detection and classification is 34 % and 48 % for test and development data respectively ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Senses of a target word are induced through use of a non-parameterised , linear-time clustering algorithm that returns maximal quasi-strongly connected components of a target word graph in which vertex pairs are assigned to the same cluster if either vertex has the highest edge weight to the other .", "This paper presents UoS , a graph-based Word Sense Induction system which attempts to find all applicable senses of a target word given its context , grading each sense according to its suitability to the context .", "Two system were submitted ; both systems returned results comparable with those of the best performing systems .", "UoS participated in SemEval-2013 Task 13 : Word Sense Induction for Graded and Non-Graded Senses ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["This paper describes the experimental combination of traditional Natural Language Processing ( NLP ) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation ( MT ) task .", "We conclude with a critical view on the developed approach .", "In the following , we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem .", "Therefore , we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We develop a task-oriented definition of comparability , based on the performance of automatic extraction of translation equivalents from the documents aligned by the proposed metrics , which formalises intuitive definitions of comparability for machine translation research .", "We demonstrate application of our metrics for the task of automatic extraction of parallel and semiparallel translation equivalents and discuss how these resources can be used in the frameworks of statistical and rule-based machine translation .", "In this paper we present and evaluate three approaches to measure comparability of documents in non-parallel corpora ."]}
{"orig_sents": ["0", "2", "1", "3", "5", "4"], "shuf_sents": ["In this paper we present an SMT-based approach to Question Answering ( QA ) .", "In our approach , the answer is a translation of the question obtained with an SMT system .", "QA is the task of extracting exact answers in response to natural language questions .", "We use the n-best translations of a given question to find similar sentences in the document collection that contain the real answer .", "Our approach is validated with the datasets of the TREC QA evaluation .", "Although it is not the first time that SMT inspires a QA system , it is the first approach that uses a full Machine Translation system for generating answers ."]}
{"orig_sents": ["0", "4", "3", "2", "1"], "shuf_sents": ["In this paper we evaluate the possibility of improving the performance of a statistical machine translation system by relaxing the complexity of the translation task by removing the most frequent and predictable terms from the target language vocabulary .", "While the word prediction results exhibits 77 % accuracy in predicting 40 % of the most frequent words in the text , the perplexity reduction did not help to produce better translations .", "We conducted some machine translation experiments to see if this perplexity reduction produced a better translation output .", "Empirically , we have found that when these words are omitted from the text , the perplexity of the text decreases , which may imply the reduction of confusion in the text .", "Afterwards , the removed terms are inserted back in the relaxed output by using an n-gram based word predictor ."]}
{"orig_sents": ["4", "3", "6", "0", "5", "1", "2"], "shuf_sents": ["We describe our initial experiences with a corpus consisting of descriptions for video segments crafted from TREC video data .", "interests and thoughts on videos .", "Such resource can also be used to evaluate automatic natural language generation systems for video .", "While the idea of annotating images with keywords is relatively well explored , work is still needed for annotating videos with natural language to improve the quality of video search .", "As video contents continue to expand , it is increasingly important to properly annotate videos for effective search , mining and retrieval purposes .", "Analysis of the descriptions created by 13 annotators presents insights into humans ?", "The focus of this work is to present a video dataset with natural language descriptions which is a step ahead of keywords based tagging ."]}
{"orig_sents": ["6", "2", "7", "0", "9", "5", "3", "4", "1", "8"], "shuf_sents": ["The hybrid system shows significant improvement in translation quality ( 0.82 and 2.75 absolute BLEU points ) for two different language pairs ( English ? Turkish ( En ? Tr ) and English ?", "Secondly , we use an IR-based indexing technique to speed up the time-consuming matching procedure of the EBMT system .", "First we present a runtime EBMT system using a subsentential translation memory ( TM ) .", "We explore two methods to make the system scalable at runtime .", "First , we use an heuristic-based approach .", "However , the EBMT approach suffers from significant time complexity issues for a runtime approach .", "In this paper we present a hybrid statistical machine translation ( SMT ) -example-based MT ( EBMT ) system that shows significant improvement over both SMT and EBMT baseline systems .", "The EBMT system is further combined with an SMT system for effective hybridization of the pair of systems .", "The index-based matching procedure substantially improves run-time speed without affecting translation quality .", "French ( En ? Fr ) ) over the baseline SMT system ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Von-Mell Park 6 , 20146 Hamburg , germany cristina.vertan @ uni-hamburg.de ?", "Abstract In this paper we present two approaches for integrating translation into cross-lingual search engines : the first approach relies on term translation via a language ontology , the other one is based on machine translation of specific information .", "Vertan University of Hamburg Research Group ? Computerphilology ?"]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Its main innovation is the integration of language technologies wi th in a web content management sys tem .", "A machine translation approach , as well as methods for obtaining and constructing training data for machine translation are under development .", "The main purpose of the project ATLAS ( Applied Technology for Language-Aided CMS ) is to facilitate multilingual web content development and management .", "The language processing framework , integrated with web content management , provides automatic annotation of important words , phrases and named entities , suggestions for categorisation o f documen t s , au toma t i c summary generation , and machine translation of summaries of documents ."]}
{"orig_sents": ["3", "2", "0", "1", "5", "4"], "shuf_sents": ["Structural similarity computation aligns subtrees and based on this alignment , subtrees are substituted to create more accurate translations .", "Two different techniques have been implemented to compute structural similarity : leaves and tree-edit distance .", "The nodes in the generation tree and targetside SCFG tree are aligned and form the basis for computing structural similarity .", "I present an automatic post-editing approach that combines translation systems which produce syntactic trees as output .", "The approach shows significant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl .", "I report on the translation quality of a machine translation ( MT ) system where both techniques are implemented ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Results are evaluated in three ways : a manual evaluation of WSD performance from MT perspective , an analysis of agreement between the WSD-proposed equivalent and those suggested by the three systems , and finally by computing BLEU , NIST and METEOR scores for all translation versions .", "Our results show that WSD performs with a MT-relevant precision of 71 % and that 21 % of sense-related MT er-rors could be prevented by using unsuper-vised WSD .", "We report on a series of experiments aimed at improving the machine translation of ambig-uous lexical items by using wordnet-based unsupervised Word Sense Disambiguation ( WSD ) and comparing its results to three MT systems .", "Our experiments are performed for the English-Slovene language pair using UKB , a freely available graph-based word sense disambiguation system ."]}
{"orig_sents": ["6", "5", "1", "2", "3", "4", "0", "7"], "shuf_sents": ["The processes are run in a bootstrapping manner until all the source chunks have been aligned with the target chunks or no new chunk alignment is identified by the bootstrapping process .", "Single-tokenization of Noun-noun MWEs , phrasal preposition ( source side only ) and reduplicated phrases ( target side only ) and the alignment of named entities and complex predicates provide the best SMT model for bootstrapping .", "Automatic bootstrapping on the alignment of various chunks makes significant gains over the previous best English-Bengali PB-SMT system .", "The source chunks are translated into the target language using the PB-SMT system and the translated chunks are compared with the original target chunk .", "The aligned chunks increase the size of the parallel corpus .", "In this paper the automatic alignments of different kind of chunks have been studied that boosts up the word alignment as well as the machine translation quality .", "The processing of parallel corpus plays very crucial role for improving the overall performance in Phrase Based Statistical Machine Translation systems ( PBSMT ) .", "The proposed system achieves significant improvements ( 2.25 BLEU over the best System and 8.63 BLEU points absolute over the baseline system , 98.74 % relative improvement over the baseline system ) on an English- Bengali translation task ."]}
{"orig_sents": ["1", "4", "5", "3", "0", "2"], "shuf_sents": ["The substitution process is controlled by a binary classifier trained on feature vectors from the different MT engines .", "We describe a substitution-based , hybrid machine translation ( MT ) system that has been extended with a machine learning component controlling its phrase selection .", "Using a set of manually annotated training data , we are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system .", "from one or more translation engines which could be substituted into our translation templates .", "Our approach is based on a rule-based MT ( RBMT ) system which creates template translations .", "Based on the generation parse tree of the RBMT system and standard word alignment computation , we identify potential ? translation snippets ?"]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The preliminary evaluation has shown very promising results in terms of BLEU scores ( 38.85 ) and the manual analysis also confirms the high quality of the translation the system delivers .", "In this paper , we present our linguisticallyaugmented statistical machine translation model from Bulgarian to English , which combines a statistical machine translation ( SMT ) system ( as backbone ) with deep linguistic features ( as factors ) .", "The motivation is to take advantages of the robustness of the SMT system and the linguistic knowledge of morphological analysis and the hand-crafted grammar through system combination approach ."]}
{"orig_sents": ["3", "5", "0", "1", "4", "2"], "shuf_sents": ["Several classifiers trained using syntactic and semantic features reach stateof-the-art performance , with F1 scores of 0.6 to 0.8 over thirteen ambiguous English connectives .", "Labeled connectives are then used into SMT systems either by modifying their phrase table , or by training them on labeled corpora .", "A threshold-based SMT system using only high-confidence labels improves BLEU scores by 0.2 ? 0.4 points .", "This article shows how the automatic disambiguation of discourse connectives can improve Statistical Machine Translation ( SMT ) from English to French .", "The best modified SMT systems improve the translation of connectives without degrading BLEU scores .", "Connectives are firstly disambiguated in terms of the discourse relation they signal between segments ."]}
{"orig_sents": ["4", "1", "0", "3", "2"], "shuf_sents": ["Using data from newspapers , we look at the distribution and lexical semantic usage of these morphemes not only within English , but across several languages and also across time , with a time-depth of 20 years .", "In particular , we examine the lexical semantic content expressed by three suffixes originating in English : -gate , -geddon and -athon .", "Processing and understanding the huge amounts of data is accomplished via visualization methods that allow the presentation of an overall distributional picture , with further details and different types of perspectives available on demand .", "The occurrence of these suffixes in available corpora are comparatively rare , however , by investigating huge amounts of data , we are able to arrive at interesting insights into the distribution , meaning and spread of the suffixes .", "We present a quantitative investigation of the cross-linguistic usage of some ( relatively ) newly minted derivational morphemes ."]}
{"orig_sents": ["1", "4", "2", "3", "0"], "shuf_sents": ["As a case study , we look at the occurrences of 476 Dutch nouns grouped in 214 synsets .", "In statistical NLP , Semantic Vector Spaces ( SVS ) are the standard technique for the automatic modeling of lexical semantics .", "To explore the way an SVS structures the individual occurrences of words , we use a non-parametric MDS solution of a token-by-token similarity matrix .", "The MDS solution is visualized in an interactive plot with the Google Chart Tools .", "However , it is largely unclear how these black-box techniques exactly capture word meaning ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["The visualization method is described and the aim is to find and to point to possible problems of synsets and their semantic relations .", "Princeton WordNet ( PrWN ) and Estonian Wordnet ( EstWN ) - are being examined from the visualization point of view .", "In this paper two lexicalsemantic databases ?", "Each expanding and developing system requires some feedback to evaluate the normal trends of the system and also the unsystematic steps ."]}
{"orig_sents": ["3", "4", "1", "5", "2", "6", "0"], "shuf_sents": ["The images so produced allow a multi-faceted perspective on the data which we hope will facilitate the interpretation of results and perhaps illuminate new areas of research in linguistic typology .", "The data presented here is extracted from the World Atlas of Language Structures ( WALS ) ( Dryer and Haspelmath , 2011 ) .", "The data are displayed in heat maps which reflect the strength of similarity between languages for different linguistic features .", "This paper presents a novel way of visualising relationships between languages .", "The key feature of the visualisation is that it brings geographic , phylogenetic , and linguistic data together into a single image , allowing a new visual perspective on linguistic typology .", "After pruning due to low coverage of WALS , we filter the typological data by geographical proximity in order to ascertain areal typological effects .", "Finally , the heat maps are annotated for language family membership ."]}
{"orig_sents": ["3", "2", "0", "1", "4"], "shuf_sents": ["exam scripts .", "The system displays directed graphs to model interactions between features and supports exploratory search over a set of learner scripts .", "We present a visual user interface supporting the investigation of a set of linguistic features discriminating between pass and fail ? English as a Second or Other Language ?", "We demonstrate how data-driven approaches to learner corpora can support Second Language Acquisition research when integrated with visualisation tools .", "We illustrate how the interface can support the investigation of the co-occurrence of many individual features , and discuss how such investigations can shed light on understanding the linguistic abilities that characterise different levels of attainment and , more generally , developmental aspects of learner grammars ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["These changes are reflected in the distribution of different lexico-grammatical features according to register .", "The present paper describes procedures to visualise diachronic language changes in academic discourse to support analysis .", "Findings about register differences are relevant for both linguistic applications ( e.g. , discourse analysis and translation studies ) and NLP tasks ( notably automatic text classification ) ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Words are important both in historical linguistics and natural language processing .", "In this presentation , I attempt to sketch the similarity patterns among a number of diverse research projects in which I participated .", "They are not indivisible abstract atoms ; much can be gained by considering smaller units such as morphemes , phonemes , syllables , and letters ."]}
{"orig_sents": ["4", "5", "2", "1", "6", "3", "0"], "shuf_sents": ["Our preliminary experiments show that this approach can supplement both the historical and the typological comparison of languages .", "The idea is that a simultaneous multilingual alignment yields a more adequate clustering of words across different languages than the successive analysis of bilingual alignments .", "To this end , we introduce a new method to quickly infer a multilingual alignment of words , using the co-occurrence of words in a massively parallel text ( MPT ) to simultaneously align a large number of languages .", "The usefulness of the approach is tested on an MPT that has been extracted from pamphlets of the Jehova ? s Witnesses .", "In this paper , we propose a novel approach to compare languages on the basis of parallel texts .", "Instead of using word lists or abstract grammatical characteristics to infer ( phylogenetic ) relationships , we use multilingual alignments of words in sentences to establish measures of language similarity .", "Since the method is computationally demanding for a larger number of languages , we reformulate the problem using sparse matrix calculations ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Different variations of this metric are tested on a corpus containing comparable texts from different Swiss German dialects and evaluated on the basis of spatial autocorrelation measures .", "The visualization of the results as cluster dendrograms shows that closely related dialects are reliably clustered together , while multidimensional scaling produces graphs that show high agreement with the geographic localization of the original texts .", "This paper proposes a simple metric of dialect distance , based on the ratio between identical word pairs and cognate word pairs occurring in two texts ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We also compare our proposal to Fisher ? s linear discriminant , and we demonstrate its effectiveness on Dutch and German dialect data .", "It is a general method that can be applied both in synchronic and diachronic linguistics that involve automatic classification of linguistic entities .", "We propose a generalization of the well-known precision and recall scores to deal with the case of detecting distinctive , characteristic variants when the analysis is based on numerical difference scores .", "A SHIBBOLETH is a pronunciation , or , more generally , a variant of speech that betrays where a speaker is from ( Judges 12:6 ) ."]}
{"orig_sents": ["4", "1", "3", "0", "2"], "shuf_sents": ["This software projects large amounts of data points to a two- or three-dimensional structure in such a way that groups of mutually similar items form spatial clusters .", "It turns out that weighted alignment according to the Needleman-Wunsch algorithm yields best results .", "The exploratory studies conducted along these ways lead to suggestive results that provide evidence for historical relationships beyond the traditionally recognized language families .", "For visualization and data exploration purposes , we used an implementation of the Fruchterman-Reingold algorithm , a version of force directed graph layout .", "The paper reports several studies about quantifying language similarity via phonetic alignment of core vocabulary items ( taken from Wichman ? s Automated Similarity Judgement Program data base ) ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We present evaluations to demonstrate that the new model yields improvements in performance , compared to previously reported models .", "The goal is , for a given corpus of cognate sets , to find the best alignment at the sound level .", "We introduce an imputation procedure to compare the goodness of the resulting models , as well as the goodness of the data sets .", "This paper presents a novel method for aligning etymological data , which models context-sensitive rules governing sound change , and utilizes phonetic features of the sounds ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["The main idea behind the method is to combine different approaches to sequence comparison in historical linguistics and evolutionary biology into a new framework which closely models the most important aspects of the comparative method .", "The method is implemented as a Python program and provides a convenient tool which is publicly available , easily applicable , and open for further testing and improvement .", "In this paper , a new method for automatic cognate detection in multilingual wordlists will be presented .", "Testing the method on a large gold standard of IPAencoded wordlists showed that its results are highly consistent and outperform previous methods ."]}
{"orig_sents": ["5", "9", "1", "2", "0", "7", "3", "6", "8", "4"], "shuf_sents": ["In this project the output data of Inputlog are segmented on the sentence level and then tokenized .", "The current research project explores the possibilities of aggregating the logged process data from the letter level ( keystroke ) to the word level by merging them with existing lexica and using NLP tools .", "Linking writing process data to lexica and using NLP tools enables researchers to analyze the data on a higher , more complex level .", "Coping with this problem was one of the main challenges in the current project .", "At this stage the Inputlog process data are enriched with the following linguistic information : part-of-speech tags , lemmas , chunks , syllable boundaries and word frequencies .", "Keystroke-logging tools are widely used in writing process research .", "Therefore , a parser has been developed that extracts three types of data from the S-notation : word-level revisions , deleted fragments , and the final writing product .", "However , by definition writing process data do not always represent clean and grammatical text .", "The within-word typing errors are identified and excluded from further analyses .", "These applications are designed to capture each character and mouse movement as isolated events as an indicator of cognitive processes ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The project focuses on German-language legislative drafting in Switzerland .", "To this aim , the approach of error modelling employed in automated style checkers for technical writing is enhanced to meet the requirements of legislative editing .", "The paper identifies and discusses the two main sets of challenges that have to be tackled in this process : ( i ) the provision of domain-specific NLP methods for legislative drafts , and ( ii ) the concretisation of guidelines for legislative drafting so that they can be assessed by machine .", "This paper reports on the development of methods for the automated detection of violations of style guidelines for legislative texts , and their implementation in a prototypical tool ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The essay concludes with speculation about ongoing development efforts , including a social helpfulness algorithm , a badging system , and Natural Language Processing ( NLP ) features .", "This essay provides a summary of research related to My Reviewers , a web-based application that can be used for teaching and assessment purposes ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We conduct several experiments in Spanish , although our conclusions also reach other languages since the procedure is corpus-driven .", "The paper reports on experiments involving different types of grammar errors , which are conducted to test different grammar-checking procedures , namely , spotting possible errors , deciding between different lexical possibilities and filling-in the blanks in a text .", "In this research we explore the possibility of using a large n-gram corpus ( Google Books ) to derive lexical transition probabilities from the frequency of word n-grams and then use them to check and suggest corrections in a target text without the need for grammar rules ."]}
{"orig_sents": ["5", "3", "4", "1", "6", "7", "8", "2", "0"], "shuf_sents": ["needs .", "First , we report on a focus group with professional writers , during which they discussed their experience using computer tools to write documents .", "We comment on a preliminary evaluation that we conducted to determine if this new platform meets professional writers ?", "Computer writing tools include language technologies , for example electronic dictionaries and text correction software , as well as information and communication technologies , for example collaborative platforms and search engines .", "As we will see , professional writing has become an entirely computerised activity .", "This paper focuses on computer writing tools used during the production of documents in a professional setting .", "We will describe their practices , point out the most important problems they encounter , and analyse their needs .", "Second , we describe LinguisTech , a reference web site for language professionals ( translators , writers , language instructors , etc . )", "that was launched in Canada in September , 2011 ."]}
{"orig_sents": ["1", "0", "2", "4", "3", "5"], "shuf_sents": ["Specifically , the main aim of this piece of research is the exploration of deceit in Spanish written communication .", "The present paper addresses the question of the nature of deception language .", "We have designed an automatic classifier based on Support Vector Machines ( SVM ) for the identification of deception in an ad hoc opinion corpus .", "The results indicate that the classification of the texts is more successful by means of our initial set of variables than with the latter system .", "In order to test the effectiveness of the LIWC2001 categories in Spanish , we have drawn a comparison with a Bag-of-Words ( BoW ) model .", "These findings are potentially applicable to areas such as forensic linguistics and opinion mining , where extensive research on languages other than English is needed ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Through an indepth case study of online hotel reviews , we demonstrate the implementation of this crowdsourcing technique and illustrate its applicability to a broad array of online reviews .", "In this study , we explore several popular techniques for obtaining corpora for deception research .", "Through a survey of traditional as well as non-gold standard creation approaches , we identify advantages and limitations of these techniques for webbased deception detection and offer crowdsourcing as a novel avenue toward achieving a gold standard corpus ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We describe a set of guidelines for acquiring and developing corpora that will enable researchers to build and test models of deceptive narrative while avoiding the problem of sanctioned lying that is typically required in a controlled experiment .", "Research in high stakes deception has been held back by the sparsity of ground truth verification for data collected from real world sources .", "Our proposals are drawn from our experience in obtaining data from court cases and other testimony , and uncovering the background information that enabled us to annotate claims made in the narratives as true or false ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["The results suggest that the models are effective in recognizing false statements , and their performance can be improved if subsets of homogeneous data are provided .", "Recent studies on deceptive language suggest that machine learning algorithms can be employed with good results for classification of texts as truthful or untruthful .", "In this paper , models have been trained in order to classify statements issued in Court as false or not-false , not only taking into consideration the whole corpus , but also by identifying more homogenous subsets of producers of deceptive language .", "However , the models presented so far do not attempt to take advantage of the differences between subjects ."]}
{"orig_sents": ["6", "2", "3", "1", "0", "5", "7", "4"], "shuf_sents": ["An automated system used an embodied conversational agent ( ECA ) to conduct interviews .", "document with incorrect data and asked to pass through a checkpoint .", "We conducted a field experiment with border guards from the European Union in order to demonstrate that deception detection can be done robustly using context specific computational models .", "In the study , some of the participants were given a ? fraudulent ?", "The overall accuracy was 94.47 % .", "Based on the participants ?", "Contextual differences present significant challenges when developing computational methods for detecting deception .", "vocalic and ocular behavior our specific model classified 100 % of the imposters while limiting false positive errors ."]}
{"orig_sents": ["5", "0", "4", "6", "3", "1", "2"], "shuf_sents": ["This study looks at the extent to which such physical co-presence effects have an impact on a child ? s ability to deceive .", "A second perception study presented participants with videos from children of the couples condition that were edited so that only one child was visible .", "The study revealed that the deceptive utterances could more often be detected correctly in the more talkative children than in the more passive ones .", "A first perception study in which minimal pairs of truthful and deceptive utterances were shown ( vision-only ) to adult observers revealed that the correct detection of deceptive utterances is dependent on whether the stimuli were produced by a child alone or together with another child ( both being visible ) .", "Using an experimental digitized puppet show , truthful and deceptive utterances were elicited from children who were interacting with two story characters .", "A person ? s expressive behavior is different in situations where he or she is alone , or where an additional person is present .", "The children were sitting alone , or as a couple together with another child ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Their pastiches were consistently clustered opposite to the original work , thereby confirming the performance of the method and proposing an extension of the method from simple authorship attribution to the more complicated problem of pastiche detection .", "We applied hierarchical clustering using Rank distance , previously used in computational stylometry , on literary texts written by Mateiu Caragiale and a number of different authors who attempted to impersonate Caragiale after his death , or simply to mimic his style .", "The novelty of our work is the use of frequency rankings of stopwords as features , showing that this idea yields good results for pastiche detection ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["Our study shows a considerable variation in the length of sex-related lexical chains according to the nature of the corpus , which supports our belief that this could be a valuable feature in an automated pedophile detection system .", "Due to the intention of a pedophile of hiding his/her true identity ( name , age , gender and location ) its detection is a challenge .", "The ability to detect deceptive statements in predatory communications can help in the identification of sexual predators , a type of deception that is recently attracting the attention of the research community .", "In this paper we approach this problem by computing sexrelated lexical chains spanning over the conversation .", "According to previous research , fixated discourse is one of the main characteristics inherent to the language of online sexual predation ."]}
{"orig_sents": ["4", "3", "2", "6", "1", "0", "5"], "shuf_sents": ["In this paper , we show that although the specific author may not be identifiable , the intent to deceive and to hide his identity can be .", "attempting to deceive ) , the results are much less promising .", "have made it possible to identify the author with high reliability in a non-confrontational setting .", "Recent advances in the technology of stylometry ( the study of authorial style ) or ? authorship attribution ?", "Whistleblowers and activists need the ability to communicate without disclosing their identity , as of course do kidnappers and terrorists .", "We show this by a reanalysis of the Brennan and Greenstadt ( 2009 ) deception corpus and discuss some of the implications of this surprising finding .", "In a confrontational setting , where the author is deliberately masking their identity ( i.e ."]}
{"orig_sents": ["2", "5", "1", "4", "3", "7", "6", "0"], "shuf_sents": ["This is satisfactory , given the absence of explicit semantic relations between words in the corpus .", "The approach is corpus-based and thus suitable for languages lacking general dictionarybased resources .", "Numerous sentiment analysis applications make usage of a sentiment lexicon .", "We evaluate the performance on three tasks that capture different aspects of a sentiment lexicon : polarity ranking task , polarity regression task , and sentiment classification task .", "The approach is a hybrid two-step process that combines semisupervised graph-based algorithms and supervised models .", "In this paper we present experiments on hybrid sentiment lexicon acquisition .", "On the sentiment classification task , the results are also comparable to SentiWordNet when restricted to monosentimous ( all senses carry the same sentiment ) words .", "Extensive evaluation shows that the results are comparable to those of a well-known sentiment lexicon SentiWordNet on the polarity ranking task ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper describes several novel hybrid semantic similarity measures .", "We study various combinations of 16 baseline measures based on WordNet , Web as a corpus , corpora , dictionaries , and encyclopedia .", "Our results show that hybrid measures outperform single measures by a wide margin , achieving a correlation up to 0.890 and MAP ( 20 ) up to 0.995 .", "The hybrid measures rely on 8 combination methods and 3 measure selection techniques and are evaluated on ( a ) the task of predicting semantic similarity scores and ( b ) the task of predicting semantic relation between two terms ."]}
{"orig_sents": ["9", "2", "0", "4", "8", "3", "1", "7", "6", "5"], "shuf_sents": ["This paper examines creating a new dependency structure through ensemble learning using a hybrid of the outputs of various parsers .", "We examine the new dependency structure in terms of accuracy and errors on individual part-of-speech values .", "There are a few dependency parsers that achieve comparable accuracy scores with each other but with very different types of errors .", "From this graph we take a maximum spanning tree .", "We combine all tree outputs into a weighted edge graph , using 4 weighting mechanisms .", "Additionally , the ensemble system reduces the average relative error on selected POS tags by 9.82 % .", "The combined ensemble system , using 5 parsers based on 3 different parsing techniques , achieves an accuracy score of 92.58 % , beating all single parsers on the Wall Street Journal section 23 test set .", "The results indicate that using a greater number of more varied parsers will improve accuracy results .", "The weighted edge graph is the input into our ensemble system and is a hybrid of very different parsing techniques ( constituent parsers , transitionbased dependency parsers , and a graphbased parser ) .", "Dependency parsing has made many advancements in recent years , in particular for English ."]}
{"orig_sents": ["2", "3", "5", "4", "0", "1"], "shuf_sents": ["Evaluation is made by calculating ROUGE scores between human annotated and machine generated descriptions .", "Further we introduce a task based evaluation by human subjects which provides qualitative evaluation of generated descriptions .", "This contribution addresses generation of natural language descriptions for human actions , behaviour and their relations with other objects observed in video streams .", "The work starts with implementation of conventional image processing techniques to extract high level features from video .", "Although feature extraction processes are erroneous at various levels , we explore approaches to putting them together to produce a coherent description .", "These features are converted into natural language descriptions using context free grammar ."]}
{"orig_sents": ["5", "3", "1", "4", "6", "0", "2"], "shuf_sents": ["Based on the evaluation on real dataset in Vietnamese language , our approach gives an acceptable performance ( detection accuracy 86 % , correction accuracy 71 % ) .", "First , we conduct a detailed analysis on characteristics of spelling errors given by an OCR scanner .", "In addition , we also give a result analysis to show how accurate our approach can achieve .", "This paper presents an investigation for the task of spell checking for OCR-scanned text documents .", "Then , we propose a fully automatic approach combining both error detection and correction phases within a unique scheme .", "OCR ( Optical Character Recognition ) scanners do not always produce 100 % accuracy in recognizing text documents , leading to spelling errors that make the texts hard to process further .", "The scheme is designed in an unsupervised & data-driven manner , suitable for resource-poor languages ."]}
{"orig_sents": ["0", "5", "2", "7", "1", "3", "4", "6"], "shuf_sents": ["This paper contrasts the content and form of objective versus subjective texts .", "As resources for subjective classification are still limited for Portuguese , we use a parallel corpus and tools developed for English to build our subjective spoken corpus , through annotations produced for English projected onto a parallel corpus in Portuguese .", "The aim is to provide general linguistic patterns as used in objective written media and subjective speeches and blog posts , to help construct domainindependent templates for information extraction and opinion mining .", "A measure for the saliency of n-grams is used to extract relevant linguistic patterns deemed ? objective ?", "and ? subjective ? .", "A collection of on-line newspaper news items serve as objective texts , while parliamentary speeches ( debates ) and blog posts form the basis of our subjective texts , all in Portuguese .", "Perhaps unsurprisingly , our contrastive approach shows that , in Portuguese at least , subjective texts are characterized by markers such as descriptive , reactive and opinionated terms , while objective texts are characterized mainly by the absence of subjective markers .", "Our hybrid approach combines statistical data along with linguistic knowledge to filter out irrelevant patterns ."]}
{"orig_sents": ["1", "4", "0", "5", "3", "2"], "shuf_sents": ["Hybridation is a main feature of our system , as we have performed experiments combining two types of NER , based respectively on symbolic and statistical techniques .", "We present a joint system for named entity recognition ( NER ) and entity linking ( EL ) , allowing for named entities mentions extracted from textual data to be matched to uniquely identifiable entities .", "Linking accuracy reaches up to 87 % , and the NER Fscore up to 83 % .", "An implementation of our system is described , along with experiments and evaluation results on French news wires .", "Our approach relies on combined NER modules which transfer the disambiguation step to the EL component , where referential knowledge about entities can be used to select a correct entity reading .", "Furthermore , the statistical EL module relies on entity knowledge acquired over a large news corpus using a simple rule-base disambiguation tool ."]}
{"orig_sents": ["3", "9", "10", "5", "0", "7", "6", "4", "1", "8", "2"], "shuf_sents": ["It has the undisputable advantage of being modular .", "Instead of considering Named Entity Recognition as a labeling task , it relies on complex context-aware features provided by lower-level systems and considers the tagging task as a markovian process .", "We report experiments using this system and compare it to other hybridization strategies along with a baseline CRF model .", "Within Information Extraction tasks , Named Entity Recognition has received much attention over latest decades .", "To assess the accuracy of mined patterns , we designed a module that recognizes Named Entities in texts by determining their most probable boundaries .", "We use a knowledge-based system , based on manually implemented transducers , that reaches satisfactory performances .", "In this context , we implemented a pattern extractor that extracts symbolic knowledge , using hierarchical sequential pattern mining over annotated corpora .", "However , such a hand-crafted system requires substantial efforts to cope with dedicated tasks .", "Using thos systems , coupling knowledge-based system with extracted patterns is straightforward and leads to a competitive hybrid NE-tagger .", "From symbolic / knowledge-based to data-driven / machine-learning systems , many approaches have been experimented .", "Our work may be viewed as an attempt to bridge the gap from the data-driven perspective back to the knowledge-based one ."]}
{"orig_sents": ["1", "3", "2", "4", "0", "5"], "shuf_sents": ["Random forests typically require training data so we investigate how we can apply random forests to combine individual base methods that are themselves unsupervised without requiring large amounts of training data .", "When digitizing a print bilingual dictionary , whether via optical character recognition or manual entry , it is inevitable that errors are introduced into the electronic version that is created .", "We investigate combining methods and show that using random forests is a promising approach .", "We investigate automating the process of detecting errors in an XML representation of a digitized print dictionary using a hybrid approach that combines rulebased , feature-based , and language modelbased methods .", "We find that in isolation , unsupervised methods rival the performance of supervised methods .", "Experiments reveal empirically that a relatively small amount of data is sufficient and can potentially be further reduced through specific selection criteria ."]}
{"orig_sents": ["4", "0", "2", "1", "5", "6", "3"], "shuf_sents": ["In order to increase the number of questions which can be answered by a single process , we propose solutions to combine two question answering systems , QAVAL and RITEL .", "RITEL develops a multi-level analysis of questions and documents .", "QAVAL proceeds by selecting short passages , annotates them by question terms , and then extracts from them answers which are ordered by a machine learning validation process .", "The fusion of endresults is realized by voting , merging , and by a machine learning process on answer characteristics , which lead to an improvement of the best system results of 19 % .", "Question answering systems answer correctly to different questions because they are based on different strategies .", "Answers are extracted and ordered according to two strategies : by exploiting the redundancy of candidates and a Bayesian model .", "In order to merge the system results , we developed different methods either by merging passages before answer ordering , or by merging end-results ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["By generalising the results of systems for sentiment analysis and ambiguity recognition , we argue that if correctly combined , multiple techniques classify better than single techniques .", "Many tasks in natural language processing require that sentences be classified from a set of discrete interpretations .", "In these cases , there appear to be great benefits in using hybrid systems which apply multiple analyses to the test cases .", "In this paper , we examine a general principle for building hybrid systems , based on combining the results of several , high precision heuristics .", "More importantly , the combined techniques can be used in tasks where no single classification is appropriate ."]}
{"orig_sents": ["2", "3", "1", "0", "4", "5"], "shuf_sents": ["In this work we use monolingual language resources to determine the set of prepositions that are most likely to occur with each verb .", "In the context of statistical machine translation , this difficulty is enhanced due to the possible long distance between the preposition and the head it modifies , as opposed to the local nature of standard language models .", "Prepositions are hard to translate , because their meaning is often vague , and the choice of the correct preposition is often arbitrary .", "At the same time , making the correct choice is often critical to the coherence of the output text .", "We use this information in a transfer-based Arabic-to-Hebrew statistical machine translation system .", "We show that incorporating linguistic knowledge on the distribution of prepositions significantly improves the translation quality ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Such techniques tend to apply differently in different contexts , so in this paper we describe a hybrid approach in which a number of different summarization techniques are combined in a rule-based system using manual knowledge acquisition , where human intuition , supported by data , specifies not only attributes and algorithms , but the contexts where these are best used .", "We show how a preliminary knowledge base , composed of only 23 rules , already outperforms competitive baselines .", "We apply this approach to automatic summarization of legal case reports .", "Summarization , like other natural language processing tasks , is tackled with a range of different techniques - particularly machine learning approaches , where human intuition goes into attribute selection and the choice and tuning of the learning algorithm ."]}
{"orig_sents": ["0", "4", "5", "3", "1", "2"], "shuf_sents": ["Unsupervised part-of-speech ( POS ) tagging has recently been shown to greatly benefit from Bayesian approaches where HMM parameters are integrated out , leading to significant increases in tagging accuracy .", "This model discovers POS specific topics from an unlabelled corpus .", "We show that this model consistently achieves improvements in unsupervised POS tagging and language modeling over the Bayesian HMM approach with varying amounts of side information in the noisy and esoteric domain of Twitter .", "Specifically , we present Part-of-Speech LDA ( POSLDA ) , a syntactically and semantically consistent generative probabilistic model .", "These improvements in unsupervised methods are important especially in specialized social media domains such as Twitter where little training data is available .", "Here , we take the Bayesian approach one step further by integrating semantic information from an LDA-like topic model with an HMM ."]}
{"orig_sents": ["2", "1", "8", "5", "3", "6", "4", "0", "7"], "shuf_sents": ["Network analysis shows that neurotic users post more than secure ones and have the tendency to build longer chains of interacting users .", "In particular we study users ?", "In this paper , we address the issue of how different personalities interact in Twitter .", "We collected a corpus of about 200000 Twitter posts and we annotated it with an unsupervised personality recognition system .", "We tested the system on a dataset annotated with personality models produced from human judgements .", ": emotional stability .", "This system exploits linguistic features , such as punctuation and emoticons , and statistical features , such as followers count and retweeted posts .", "Secure users instead have more mutual connections and simpler networks .", "interactions using one trait of the standard model known as the ? Big Five ?"]}
{"orig_sents": ["1", "3", "2", "5", "4", "0"], "shuf_sents": ["We test our classifier on a sample of blog posts , and report up to 0.69 accuracy for multi-class labelling and 0.9 for binary classification .", "Classifying blog posts by topics is useful for applications such as search and marketing .", "The state-of-the-art relies on supervised methods , requiring considerable training effort , that use the whole corpus vocabulary as features , demanding considerable memory to process .", "However , topic classification is time consuming and error prone , especially in an open domain such as the blogosphere .", "We address the memory requirements by using only named entities as features .", "We show an effective alternative whereby distant supervision is used to obtain training data : we use Wikipedia articles labelled with Freebase domains ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["Accordingly , it proposes a distinctive workflow with well-defined strategies for an ontology-aware user and NLP-assisted flexible and multidimensional approach for the management of the abundantly available Social data .", "Originating from a multidisciplinary research project that gathers , around the Semantic Web standards and principles , Social Networking and Natural Language Processing along with some Bioinformatics notions , this paper sheds the light on some of the most critical aspects of the correspondingly adopted framework and realtime knowledge architecture and modeling platform .", "It recognizes the considerable profits of an appropriate fusion between the aforementioned disciplines , especially via the proper exploitation of OWL 2 ( Web Ontology Language ) features and novelties , typically OWL 2 language profiles .", "Application scenarios related to awareness and orientation recommender systems based on biomedical domain ontologies for childhood obesity prevention and surveillance are explored as typical proof of concept application areas ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["In this paper we present a modular and scalable framework to Social Media Opinion Mining that combines stochastic and symbolic techniques to structure a semantic space to exploit and interpret efficiently .", "We describe the use of this framework for the discovery and clustering of opinion targets and topics in user-generated comments for the Telecom and Automotive domains .", "Text mining of massive Social Media postings presents interesting challenges for NLP applications due to sparse interpretation contexts , grammatical and orthographical variability as well as its very fragmentary nature .", "No single methodological approach can be expected to work across such diverse typologies as twitter micro-blogging , customer reviews , carefully edited blogs , etc ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["To what extend can one use Twitter in opinion polls for political elections ?", "Merely counting Twitter messages mentioning political party names is no guarantee for obtaining good election predictions .", "By improving the quality of the document collection and by performing sentiment analysis , predictions based on entity counts in tweets can be considerably improved , and become nearly as good as traditionally obtained opinion polls ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["In this paper , we show how we applied an opinion extraction system to extract opinions but also suggestions from the content of the reviews , use the results to compare other products with the reviewed one , and eventually recommend a better product to the user .", "In this paper , we propose the use of finegrained information such as opinions and suggestions extracted from users ?", "reviews about products , in order to improve a recommendation system .", "While typical recommender systems compare a user profile with some reference characteristics to rate unseen items , they rarely make use of the content of reviews users have done on a given product ."]}
{"orig_sents": ["1", "5", "4", "6", "2", "0", "3"], "shuf_sents": ["We also test our model on a part of the ongoing Persian dependency treebank .", "Unsupervised dependency parsing is one of the most challenging tasks in natural languages processing .", "By employing two simple universal linguistic rules inspired from the classical dependency grammar , we improve the results in some languages and get the state of the art results .", "This work is the first work done on the Persian language .", "In this paper , we illustrate that by applying a supervised incremental parsing model to unsupervised parsing ; parsing with a linear time complexity will be faster than the other methods .", "The task involves finding the best possible dependency trees from raw sentences without getting any aid from annotated data .", "With only 15 training iterations with linear time complexity , we gain results comparable to those of other state of the art methods ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["Building shallow semantic representations from text corpora is the first step to perform more complex tasks such as text entailment , enrichment of knowledge bases , or question answering .", "Open Information Extraction ( OIE ) is a recent unsupervised strategy to extract billions of basic assertions from massive corpora , which can be considered as being a shallow semantic representation of those corpora .", "In this paper , we propose a new multilingual OIE system based on robust and fast rule-based dependency parsing .", "It permits to extract more precise assertions ( verb-based triples ) from text than state of the art OIE systems , keeping a crucial property of those systems : scaling to Web-size document collections ."]}
{"orig_sents": ["4", "2", "1", "7", "5", "3", "6", "0", "8"], "shuf_sents": ["Similar improvements are achieved with the mode method : We store all assigned topic IDs during each inference iteration step and select the most frequent topic ID assigned to each word .", "usually , the recommendations from the original papers are adopted .", "At this , the model parameters and the influence of randomized sampling and inference are rarely examined ?", "We show substantial variance in the results for different runs of model estimation and inference , and give recommendations for increasing the robustness and stability of topic models .", "Topic Models ( TM ) such as Latent Dirichlet Allocation ( LDA ) are increasingly used in Natural Language Processing applications .", "We find that the recommended settings result in error rates far from optimal for our application .", "Running the inference step several times and selecting the last topic ID assigned per token , shows considerable improvements .", "In this paper , we examine the parameter space of LDA topic models with respect to the application of Text Segmentation ( TS ) , specifically targeting error rates and their variance across different runs .", "These recommendations do not only apply to TS , but are generic enough to transfer to other applications ."]}
{"orig_sents": ["4", "2", "1", "0", "3"], "shuf_sents": ["We show that we can use word clusters for learning rules , and significantly improve on a baseline with only slightly worse performance than for standard POS-tags on an English ? German translation task .", "Part-of-speech tagging has previously been successfully used for learning reordering rules that can be applied before training and translation .", "In this work we investigate if clustered word classes can be used in a preordering strategy , where the source language is reordered prior to training and translation .", "We also show the usefulness of the approach for the less-resourced language Haitian Creole , for translation into English , where the suggested approach is significantly better than the baseline .", "Clustered word classes have been used in connection with statistical machine translation , for instance for improving word alignments ."]}
{"orig_sents": ["1", "7", "6", "0", "8", "4", "9", "5", "2", "3"], "shuf_sents": ["Recent work in the biomedical domain has applied distant supervision for proteinprotein interaction ( PPI ) with reasonable results making use of the IntAct database .", "Relation extraction is frequently and successfully addressed by machine learning methods .", "We demonstrate the broad applicability of our approach by using the same workflow for the analysis of drug-drug interactions , utilizing relationships available from the drug database DrugBank .", "We achieve 37.31 % in F1 measure without manually annotated training data on an independent test set .", "We propose a constraint to increase the quality of data used for training based on the assumption that no self-interaction of realworld objects are described in sentences .", "These two steps show an increase of 7 percentage points ( pp ) for the PPI corpus AIMed .", "Distantly supervised approaches make use of weakly annotated data , like automatically annotated corpora .", "The downside of this approach is the need for annotated training data , typically generated in tedious manual , cost intensive work .", "Such data is typically noisy and heuristics to filter the data are commonly applied .", "In addition , we make use of the University of Kansas Proteomics Service ( KUPS ) database ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We show that the categorization performance of the algorithm is comparable with the coclustering algorithm of Leibbrandt and Powers ( 2008 ) , but out-performs that algorithm in robustly pruning less-useful clusters and merging them into categories strongly corresponding to the three main open classes of English .", "We introduce Conflict-Driven Co-Clustering , a novel algorithm for data co-clustering , and apply it to the problem of inducing parts-ofspeech in a corpus of child-directed spoken English .", "Co-clustering is preferable to unidimensional clustering as it takes into account both item and context ambiguity ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Dependency Parsing domain adaptation involves adapting a dependency parser , trained on an annotated corpus from a given domain ( e.g. , newspaper articles ) , to work on a different target domain ( e.g. , legal documents ) , given only an unannotated corpus from the target domain .", "We illustrate the the experiments we performed with this parser on a domain adaptation task for the Italian language .", "We present a shift/reduce dependency parser that can handle unlabeled sentences in its training set using a transductive SVM as its action selection classifier ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["An open question in [ FU ? LO ? P , MALETTI , VOGLER : Weighted extended tree transducers .", "Fundamenta Informaticae 111 ( 2 ) , 2011 ] asks whether weighted linear extended tree transducers preserve recognizability in countably complete commutative semirings .", "Due to the completeness of the semiring , the inside weights always exist , but the construction is only effective if they can be effectively determined .", "In this contribution , the question is answered positively , which is achieved with a construction that utilizes inside weights .", "It is demonstrated how to achieve this in a number of important cases ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We show that the twins property for weighted tree automata over extremal semifields is decidable .", "It has remained an open question whether the twins property for weighted tree automata is decidable .", "This property is crucial for determinizing such an automaton , and it has been argued that determinization improves the output of parsers and translation systems ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["We motivate the overall ? template-to-template ?", "In this paper we present the tree to tree transduction language , TTT .", "approach to the design of the language , and outline its constructs , also providing some examples .", "We then show that TTT allows transparent formalization of rules for parse tree refinement and correction , logical form refinement and predicate disambiguation , inference , and verbalization of logical forms ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["The simultaneously phonological and syntactic grammar of second position clitics is an instance of the broader problem of applying constraints across multiple levels of linguistic analysis .", "Syntax frameworks extended with simple tree transductions can make efficient use of these necessary additional forms of structure .", "An analysis of Sahidic Coptic second position clitics in a context-free grammar extended by a monadic second-order transduction exemplifies this approach ."]}
{"orig_sents": ["0", "3", "9", "2", "1", "4", "7", "5", "8", "6"], "shuf_sents": ["Languages evolve , undergoing repeated small changes , some with permanent effect and some not .", "Contactinduced changes can happen when languages share speakers , or when their speakers are in contact .", "En masse , such changes cause isolated languages to drift apart in lexical form and grammatical structure .", "Changes affecting a language may be independent or contactinduced .", "Frequently , languages in contact are related , having a common ancestor from which they still retain visible structure .", "In this paper , we present a simulation of contact-induced change .", "For a particular model , we determine how much data is enough to distinguish these two cases at p < 0.05 .", "This relatedness makes it difficult to distinguish contact-induced change from inherited similarities .", "We show that it is possible to distinguish contact-induced change from independent change given ( a ) enough data , and ( b ) that the contactinduced change is strong enough .", "Independent changes arise internally or , if externally , from non-linguistic causes ."]}
{"orig_sents": ["0"], "shuf_sents": ["In this demonstration we present our web services to perform Bayesian learning for classification tasks ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["English phonotactic learning is modeled by means of the PHACTS algorithm , a topological neuronal receptive field implementing a phonotactic activation function aimed at capturing both local ( i.e. , phonemic ) and global ( i.e. , word-level ) similarities among strings .", "Limits and merits of the model are presented ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper we present a profile of verb usage across ages in child-produced sentences in English and Portuguese .", "We examine in particular lexical and syntactic characteristics of verbs and find common trends in these languages as children ? s ages increase , such as the prominence of general and polysemic verbs , as well as divergences such as the proportion of subject dropping .", "We also find a correlation between the age of acquisition and the number of complements of a verb for English ."]}
{"orig_sents": ["4", "0", "2", "1", "5", "3"], "shuf_sents": ["Nonetheless , children successfully learn to use them and eventually acquire a number of Multiword Expressions comparable to that of simplex words .", "Given their potentially higher complexity in relation to simplex verbs , we examine whether they appear less prominently in child-produced than in childdirected speech , and whether the VPCs that children produce are more conservative than adults , displaying proportionally reduced lexical repertoire of VPCs or of verbs in these combinations .", "In this paper we report a wide-coverage investigation of a particular type of MWE : verb-particle constructions ( VPCs ) in English and their usage in child-produced and child-directed sentences .", "Studies like these can inform the development of computational models for language acquisition .", "Much has been discussed about the challenges posed by Multiword Expressions ( MWEs ) given their idiosyncratic , flexible and heterogeneous nature .", "The results obtained indicate that regardless of any additional complexity VPCs feature widely in children data following closely adult usage ."]}
{"orig_sents": ["4", "1", "0", "3", "2"], "shuf_sents": ["Results support the high level of predictability of Brazilian Portuguese phonemes distribution , the consonantvowel syllabic pattern as the most common , as well as the stress pattern distribution 'CV.CV # .", "The software Nhenh ? m ( Vasil ? vski , 2008 ) was used for treating data : written texts which were decoded into phonologic symbols , forming a corpus , and subjected to a statistical analysis .", "These results are displayed and discussed , as well as some aspects of Nhe-nh ? m building .", "The efficiency of a phoneme-grapheme converter based entirely on rules is also proven .", "This paper presents Brazilian Portuguese phoneme patterns of distribution , according to an automatic grammar rulesbased grapheme to phoneme converter ."]}
{"orig_sents": ["3", "5", "1", "4", "6", "2", "0"], "shuf_sents": ["The tool is implemented using the LAMP architecture and is freely available for research purposes .", "The annotations used in our project are modern-day word form equivalent , lemma , part-of-speech tag and optional gloss .", "The tool accepts preannotated corpora in TEI P5 format and is able to export the corpus and lexicon in TEI P5 as well .", "This paper describes a Web-based editor called CoBaLT ( Corpus-Based Lexicon Tool ) , developed to construct corpusbased computational lexica and to correct word-level annotations and transcription errors in corpora .", "The CoBaLT interface is word form oriented and compact .", "The paper describes the tool as well as our experience in using it to annotate a reference corpus and compile a large lexicon of historical Slovene .", "It enables wildcard word searching and sorting according to several criteria , which makes the editing process flexible and efficient ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We survey the basic transcription workflow of some commonly used tools ( Transcriber , BlitzScribe , and ELAN ) and describe how the new transcription interface improves on these existing implementations .", "We present a new transcription mode for the annotation tool ELAN .", "This mode is designed to speed up the process of creating transcriptions of primary linguistic data ( video and/or audio recordings of linguistic behaviour ) .", "We describe the design of the transcription interface and explore some further possibilities for improvement in the areas of segmentation and computational enrichment of annotations ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["To help the aspiring bertsolari , we provide a tool that includes a web interface that is able to analyze , correct , provide suggestions and synonyms , and tentatively also sing ( using text-to-speech synthesis ) verses composed by the user .", "A performing bertsolari ? a verse singer in the Basque Country ? must adhere to strict rules that dictate the format and content of the verses sung .", "We present work on a verse-composition assistant for composing , checking correctness of , and singing traditional Basque bertsoak ? impromptu verses on particular themes ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Today museums and other cultural heritage institutions are increasingly storing object descriptions using semantic web domain ontologies .", "This paper describes how semantic and syntactic information such as that provided in a framenet can contribute to solving this task .", "To make this content accessible in a multilingual world , it will need to be conveyed in many languages , a language generation task which is domain specific and language dependent .", "It is argued that the kind of information offered by such lexical resources enhances the output quality of a multilingual language generation application , in particular when generating domain specific content ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The controlled vocabulary is intended to support editors in assigning descriptors to new documents and to support users in retrieving documents of interest regardless of the spelling or language variety used in the documents .", "We describe ongoing work aiming at deriving a multilingual controlled vocabulary ( German , French , Italian ) from the combined subject indices from 22 volumes of a large-scale critical edition of historical documents ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We present on-going work on the automated ontology-based detection and recognition of characters in folktales , restricting ourselves for the time being to the analysis of referential nominal phrases occurring in such texts .", "Focus of the presently reported work was to investigate the interaction between an ontology and linguistic analysis of indefinite and indefinite nominal phrase for both the incremental annotation of characters in folktales text , including some inference based co-reference resolution , and the incremental population of the ontology .", "This in depth study was done at this early stage using only a very small textual base , but the demonstrated feasibility and the promising results of our smallscale experiment are encouraging us to deploy the strategy on a larger text base , covering more linguistic phenomena in a multilingual fashion ."]}
{"orig_sents": ["4", "3", "6", "7", "0", "1", "2", "5"], "shuf_sents": ["names , their gender and their normalized , linked form , including mentions of theistic beings ( e.g. , Gods ?", "names and mythological figures ) , and examined their appearance over the course of the novel .", "A case study based on 13 novels , from the aforementioned collection , shows a number of interesting applications of visual analytics methods to literature problems , where named entities can play a prominent role , demonstrating the advantage of visual literature analysis .", "This paper combines robust text analysis with advanced visual analytics and brings a new set of tools to literature analysis .", "The volumes of digitized literary collections in various languages increase at a rapid pace , which results also in a growing demand for computational support to analyze such linguistic data .", "Our work is inspired by the notion of distant reading or macroanalysis for the analyses of large literature collections .", "Visual analytics techniques can offer new and unexpected insights and knowledge to the literary scholar .", "We analyzed a small subset of a large literary collection , the Swedish Literature Bank , by focusing on the extraction of persons ?"]}
{"orig_sents": ["0", "4", "2", "3", "1"], "shuf_sents": ["This paper illustrates the use of distributional techniques , as investigated in computational semantics , for supplying data from large-scale corpora to areas of the humanities which focus on the analysis of concepts .", "Further , we highlight that different models of phrasal distributions can be compared to support the claim of intersectionality theory that ? there is more to a phrase than the intersection of its parts ? .", "can be seen as evidence for some representative tendencies of general discourse .", "We present a case study where distributional data is used by philosophers working in the areas of gender studies and intersectionality as confirmation of certain trends described in previous work .", "We suggest that the distributional notion of ? characteristic context ?"]}
{"orig_sents": ["3", "6", "0", "1", "4", "5", "2"], "shuf_sents": ["In this paper we propose a method for extracting verbs and their complements from historical Swedish text , using NLP tools and dictionaries developed for contemporary Swedish and a set of normalisation rules that are applied before tagging and parsing the text .", "When evaluated on a sample of texts from the period 1550 ?", "Moreover , the exact match rate for complete verb constructions is in fact higher for historical texts than for contemporary texts ( 38.7 % vs. 30.8 % ) .", "Even though NLP tools are widely used for contemporary text today , there is a lack of tools that can handle historical documents .", "1880 , this method identifies verbs with an F-score of 77.2 % and finds a partially or completely correct set of complements for 55.6 % of the verbs .", "Although these results are in general lower than for contemporary Swedish , they are strong enough to make the approach useful for information extraction in historical research .", "Such tools could greatly facilitate the work of researchers dealing with large volumes of historical texts ."]}
{"orig_sents": ["3", "1", "0", "2", "4"], "shuf_sents": ["in Chinese , previous Chinese corpora suffer from a lack of standardization in word segmentation , resulting in inconsistencies in POS tags , therefore hindering interoperability among corpora .", "Due to the ill-defined concept of a ? word ?", "We address this problem with nested POS tags , which accommodates different theories of wordhood and facilitates research objectives requiring annotations of the ? word ?", "We introduce a corpus of classical Chinese poems that has been word segmented and tagged with parts-ofspeech ( POS ) .", "at different levels of granularity ."]}
{"orig_sents": ["3", "1", "2", "0", "4"], "shuf_sents": ["This paper explores the use of information from these various media for computing similarity between Cultural Heritage artefacts .", "Being able to identify items that are similar would be useful for search and navigation through these data sets .", "Information about items in these repositories is often multimodal , such as pictures of the artefact and an accompanying textual description .", "A significant amount of information about Cultural Heritage artefacts is now available in digital format and has been made available in digital libraries .", "Results show that combining information from images and text produces better estimates of similarity than when only a single medium is considered ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["While these provide adequate search functionality for the expert user , this may not offer the best support for non-expert or novice users .", "In this paper we propose a novel mechanism for introducing new users to the items in a collection by allowing them to browse Wikipedia articles , which are augmented with items from the cultural heritage collection .", "Using Europeana as a case-study we demonstrate the effectiveness of our approach for encouraging users to spend longer exploring items in Europeana compared with the existing search provision .", "Over the past years large digital cultural heritage collections have become increasingly available ."]}
{"orig_sents": ["5", "2", "0", "1", "4", "3"], "shuf_sents": ["Techniques have been developed for automatically adding links to Wikipedia to text but the methods are general and not designed for use with cultural heritage data .", "This paper explores a range of methods for adapting a system for adding links to Wikipedia to cultural heritage items .", "This information could be enriched by adding links to resources that provide background information about the items .", "It is found that an approach that makes use of Wikipedia ? s link structure can be used to improve the quality of the Wikipedia links that are added .", "The approaches make use of the structure of Wikipedia , including the category hierarchy .", "Large numbers of cultural heritage items are now archived digitally along with accompanying metadata and are available to anyone with internet access ."]}
{"orig_sents": ["3", "2", "0", "4", "1", "5"], "shuf_sents": ["An approach for handwritten text line detection is presented which uses machinelearning techniques and methods widely used in natural language processing .", "Experimental results show the impact of using increasingly constrained ? vertical layout language models ?", "Text layout commonly found in handwritten legacy documents is in the form of one or more paragraphs composed of parallel text lines .", "Document layout analysis is an important task needed for handwritten text recognition among other applications .", "It is shown that text line detection can be accurately solved using a formal methodology , as opposed to most of the proposed heuristic approaches found in the literature .", "in text line detection accuracy ."]}
{"orig_sents": ["5", "3", "2", "1", "4", "0"], "shuf_sents": ["The results show that depending on the algorithm and the complexity of the domain , training on a handful of descriptions can already lead to a performance that is not significantly different from training on a much larger data set .", "Both rely on a notion of preferred attributes that can be learned from human descriptions .", "We compare two REG algorithms in terms of their performance : the classic Incremental Algorithm and the more recent Graph algorithm .", "In this paper , we study how much training data is required for algorithms to do this properly .", "In our experiments , preferences are learned from training sets that vary in size , in two domains and languages .", "One important subtask of Referring Expression Generation ( REG ) algorithms is to select the attributes in a definite description for a given object ."]}
{"orig_sents": ["4", "3", "1", "2", "0"], "shuf_sents": ["We discuss what new challenges this creates for NLG systems .", "We present a corpus of human ? human interactions in the GIVE-2 setting in which instructions are spoken .", "A first study of object descriptions in this corpus shows that references in installments are quite common in this scenario and suggests that contextual factors partly determine their use .", "In interactive settings with a shared workspace , however , human dialog partners often split referring expressions into installments that adapt to changes in the context and to actions of their partners .", "Commonly , the result of referring expression generation algorithms is a single noun phrase ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["This Natural Language Generation ( NLG ) differs from other ways of describing spatio-temporal data , in that it deals with abstractions on data across large geographical spaces ( total projected area 20,600 km2 ) , as well as temporal trends across longer time frames ( ranging from one week up to a year ) .", "We identify challenges at all stages of the classical NLG pipeline .", "We describe preliminary work on generating contextualized text for nature conservation volunteers ."]}
{"orig_sents": ["1", "2", "4", "7", "3", "0", "6", "5"], "shuf_sents": ["Our goal is to initiate a debate on how a generation suitable annotation schema should be defined .", "Until recently , deep stochastic surface realization has been hindered by the lack of semantically annotated corpora .", "This is about to change .", "The attempts to adapt them for generation resulted so far in a better performance of the realizers , but not yet in a genuinely semantic generationoriented annotation schema .", "Such corpora are increasingly available , e.g. , in the context of CoNLL shared tasks .", "Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them .", "We define some general principles of a semantic generation-oriented annotation and propose an annotation schema that is based on these principles .", "However , recent experiments with CoNLL 2009 corpora show that these popular resources , which serve well for other applications , may not do so for generation ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We argue that generation can be exploited to address other issues that are relevant to grammar engineering such as in particular , detecting grammar incompleteness , identifying sources of overgeneration and analysing the linguistic coverage of the grammar .", "We present an algorithm that implements these functionalities and we report on experiments using this algorithm to analyse a Feature-Based Lexicalised Tree Adjoining Grammar consisting of roughly 1500 elementary trees .", "While in Computer Science , grammar engineering has led to the development of various tools for checking grammar coherence , completion , under- and over-generation , in Natural Langage Processing , most approaches developed to improve a grammar have focused on detecting under-generation and to a much lesser extent , over-generation ."]}
{"orig_sents": ["3", "6", "1", "0", "4", "5", "2"], "shuf_sents": ["Whilst personality perception of our dialogues is consistent with perceptions of human behaviour , we find that the introduction of alignment leads to negative perceptions of the dialogues and the interlocutors ?", "relationship .", "We discuss our findings in relation to the literature and in the context of dialogue systems .", "Variation in language style can lead to different perceptions of the interaction , and different behaviour outcomes .", "relationship .", "A follow up evaluation study of the perceptions of different forms of alignment in the dialogues reveals that while similarity at polarity , topic and construction levels is viewed positively , similarity at the word level is regarded negatively .", "Using the CRAG 2 language generation system we examine how accurately judges can perceive character personality from short , automatically generated dialogues , and how alignment ( similarity between speakers ) alters judge perceptions of the characters ?"]}
{"orig_sents": ["0", "1", "3", "5", "2", "4"], "shuf_sents": ["Recent studies have shown that incremental systems are perceived as more reactive , natural , and easier to use than non-incremental systems .", "However , previous work on incremental NLG has not employed recent advances in statistical optimisation using machine learning .", "We present a proof-of-concept study in the domain of Information Presentation ( IP ) , where a learning agent faces the trade-off of whether to present information as soon as it is available ( for high reactiveness ) or else to wait until input ASR hypotheses are more reliable .", "This paper combines the two approaches , showing how the update , revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a Markov Decision Process .", "Results show that the agent learns to avoid long waiting times , fillers and self-corrections , by re-ordering content based on its confidence .", "We design a model of incremental NLG that generates output based on micro-turn interpretations of the user ? s utterances and is able to optimise its decisions using statistical machine learning ."]}
{"orig_sents": ["3", "0", "4", "1", "2"], "shuf_sents": ["The system incorporates extensive typological , semantic , syntactic , and discourse research into its semantic representational system and its transfer and synthesizing grammars .", "This paper will summarize the major components of the NLG system , and then present the results of experiments that were performed to determine the quality of the generated texts .", "The experiments indicate that when experienced mothertongue translators use the drafts generated by LA , their productivity is typically quadrupled without any loss of quality .", "Linguist ? s Assistant ( LA ) is a large scale semantic analyzer and multi-lingual natural language generator designed and developed entirely from a linguist ? s perspective .", "LA has been tested with English , Korean , Kewa ( Papua New Guinea ) , Jula ( Cote d ? Ivoure ) , and North Tanna ( Vanuatu ) , and proof-of-concept lexicons and grammars have been developed for Spanish , Urdu , Tagalog , Chinantec ( Mexico ) , and Angas ( Nigeria ) ."]}
{"orig_sents": ["1", "0", "2", "5", "4", "3"], "shuf_sents": ["We argue that by taking advantage of this information density , NLG systems applied to ontologies can guide the choice and construction of sentences to express useful ontological information , solely through the verbalisations of identifier names , and that by doing so , they can replace the extremely fussy and repetitive texts produced by ontology verbalisers with shorter and simpler texts which are clearer and easier for human readers to understand .", "Despite their flat , semantics-free structure , ontology identifiers are often given names or labels corresponding to natural language words or phrases which are very dense with information as to their intended referents .", "We specify which axioms in an ontology are ? defining axioms ?", "information explicit are preferred by readers and yet communicate the same information as the longer texts in which such information is spelled out explicitly .", "By generating texts from ontologies , and selectively including or omitting these defining axioms , we show by surveys that human readers are typically capable of inferring information implicitly encoded in identifier phrases , and that texts which do not make such ? obvious ?", "for linguistically-complex identifiers and analyse a large corpus of OWL ontologies to identify common patterns among all defining axioms ."]}
{"orig_sents": ["11", "8", "9", "10", "0", "2", "6", "3", "5", "7", "1", "4"], "shuf_sents": ["To determine the principles governing referential chains , we gathered data from three languages : English , Swedish and Hebrew , and studied how coreference is expressed in a discourse .", "Guernica has as color White , Gray and Black .", "As a result of the study , a set of language specific coreference strategies were identified .", "A preliminary evaluation of our method shows languagedependent coreference strategies lead to better generation results .", "Figure 1 : A natural language description generated from a set of ontology statements .", "createdBy ( Guernica , PabloPicasso ) currentLocation ( Guernica , MuseoReinaSof ? a ) hasColor ( Guernica , White ) hasColor ( Guernica , Gray ) hasColor ( Guernica , Black ) Guernica is created by Pablo Picasso .", "Using these strategies , an ontology-based multilingual grammar for generating written natural language descriptions about paintings was implemented in the Grammatical Framework .", "Guernica has as current location the Museo Reina Sof ? a .", "To make these descriptions coherent and accessible in different languages , a methodology is needed for identifying the general principles that would determine the distribution of referential forms .", "Previous work has proved through crosslinguistic investigations that strategies for building coreference are language dependent .", "However , to our knowledge , there is no language generation methodology that makes a distinction between languages about the generation of referential chains .", "During the last decade , there has been a shift from developing natural language generation systems to developing generic systems that are capable of producing natural language descriptions directly from Web ontologies ."]}
{"orig_sents": ["5", "2", "0", "6", "4", "7", "3", "1"], "shuf_sents": ["We analyzed 4000+ high quality summaries for a high traffic mailing list and manually assembled 39 quotation-introducing verb classes that cover the majority of the verb occurrences .", "We used this fact to highlight the trade-offs of risk taking in NLG , where interesting prose might come at the cost of unsettling some of the readers .", "In this work , we focused on the lexical choice for verbs introducing quoted text .", "verbs that most likely will be beyond the state of the art for longer time .", "However , we found that one third of the ? tail ?", "Human-written , good quality extractive summaries pay great attention to the text intermixing the extracts .", "A significant amount of the data is covered by on-going work on e-mail ? speech acts . ?", "is composed by ? risky ?"]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Given a sequence of lemmas and any subset of morphological features , we produce the inflected word forms .", "We present an approach for generation of morphologically rich languages using statistical machine translation .", "Testing on Arabic , a morphologically rich language , our models can reach 92.1 % accuracy starting only with lemmas , and 98.9 % accuracy if all the gold features are provided ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["In this paper we take an initial look at tutor reformulations of student contributions in naturalistic tutorial dialogue in order to characterize the range of pedagogical intentions that may be associated with these reformulations .", "While some recent work in tutorial dialogue has touched upon tutor reformulations of student contributions , there has not yet been an attempt to characterize the intentions of reformulations in this educational context nor an attempt to determine which types of reformulation actually contribute to student learning .", "By implementing reformulations in a tutorial dialogue system we can begin to test their impact on student learning in a more controlled way in addition to testing whether our approximation of reformulation is adequate .", "We further outline our plans for implementing reformulation in our tutorial dialogue system , Rimac , which engages high school physics students in post problem solving reflective discussions ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In this paper , we describe how NLG developers worked with clinicians ( nurses ) to improve an NLG system which generates information for parents of babies in a neonatal intensive care unit , using a structured revision process .", "NLG developers must work closely with domain experts in order to build good NLG systems , but relatively little has been published about this process .", "We believe that such a process can significantly enhance the quality of many NLG systems , in medicine and elsewhere ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["In particular we describe a microplanner based on an expert-system and a combinatory categorial grammar used in realization .", "This paper concerns the architecture of a generator for Italian Sign Language ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["A useful enhancement of an NLG system for verbalising ontologies would be a module capable of explaining undesired entailments of the axioms encoded by the developer .", "This task raises interesting issues of content planning .", "We suggest an approach in which further statements are added in order to construct a proof tree , with every step based on a relatively simple deduction rule of known difficulty ; we also describe an empirical study through which the difficulty of these simple deduction patterns has been measured .", "One approach , useful as a baseline , is simply to list the subset of axioms relevant to inferring the entailment ; however , in many cases it will still not be obvious , even to OWL experts , why the entailment follows ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["We describe and evaluate a question answering system that provides a point-and-click , webbased interface in conjunction with a semantic grammar to support user-controlled natural language question generation .", "Question answering is an age old AI challenge .", "How we approach this challenge is determined by decisions regarding the linguistic and domain knowledge our system will need , the technical and business acumen of our users , the interface used to input questions , and the form in which we should present answers to a user ? s questions .", "Our approach to question answering involves the interactive construction of natural language queries .", "A preliminary evaluation is performed using a selection of 12 questions based on the Adventure Works sample database ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We then discuss the implications for NLG systems that generate blogs from satellite-tag data .", "This paper proposes the use of NLG to enhance public engagement during the course of species reintroductions .", "minds , a requirement for successful reintroduction .", "We examine whether ecological insights can be effectively communicated through blogs about satellite-tagged individuals , and whether such blogs can help create a positive perception of the species in readers ?"]}
{"orig_sents": ["1", "5", "4", "3", "2", "0"], "shuf_sents": ["In this demo paper , we describe the design of the system and present example outputs generated by the video summarization system .", "g Natural Language Summaries for Multimedia Duo Ding , Florian Metze , Shourabh Rawat , Peter F. Schulam , Susanne Burger School of Computer Science , Carnegie Mellon University Pittsburgh , PA , USA 15213 { dding , fmetze , srawat , pschulam , sburger } @ cs.cmu.edu Abstract In this paper we introduce an automatic sys-tem that generates textual summaries of Inter-net-style video clips by first identifying suitable high-level descriptive features that have been detected in the video ( e.g .", "To reduce the complexity of the task , we restrict ourselves to work with videos that show a limited number of ? events ? .", "The generated summary contains information from both visual and acoustic sources , intend-ing to give a general review and summary of the video .", "Then a natural language genera-tor is constructed using SimpleNLG to com-pile the high-level features into a textual form .", "visual concepts , recognized speech , actions , objects , persons , etc . ) ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We demonstrate a novel , robust vision-tolanguage generation system called Midge .", "We explain how to connect vision detections to trees in Penn Treebank syntax , which provides the scaffolding necessary to further refine data-driven statistical generation approaches for a variety of end goals .", "Midge is a prototype system that connects computer vision to syntactic structures with semantic constraints , allowing for the automatic generation of detailed image descriptions ."]}
{"orig_sents": ["3", "0", "1", "4", "2"], "shuf_sents": ["Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs .", "However , the input representations had several shortcomings which we have been aiming to address in the time since .", "We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas may be usefully brought together in the future .", "The Surface Realisation Shared Task was first run in 2011 .", "This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We describe a new shared task on syntactic paraphrase ranking that is intended to run in conjunction with the main surface realization shared task .", "Taking advantage of the human judgments collected to evaluate the surface realizations produced by competing systems , the task is to automatically rank these realizations ? viewed as syntactic paraphrases ? in a way that agrees with the human judgments as often as possible .", "The task is designed to appeal to developers of surface realization systems as well as machine translation evaluation metrics : for surface realization systems , the task sidesteps the thorny issue of converting inputs to a common representation ; for MT evaluation metrics , the task provides a challenging framework for advancing automatic evaluation , as many of the paraphrases are expected to be of high quality , differing only in subtle syntactic choices ."]}
{"orig_sents": ["5", "0", "3", "4", "2", "1"], "shuf_sents": ["This article gives an overview of our project on multi-modal sensing , analysis and ? understanding ?", "Based on these analyses , we design a smart posterboard which can sense human behaviors and annotate interactions and interest level during poster sessions .", "We investigate whether we can predict when and who will ask what kind of questions , and also interest level of the audience .", "of poster conversations .", "We focus on the audience ? s feedback behaviors such as non-lexical backchannels ( reactive tokens ) and noddings as well as joint eye-gaze events by the presenter and the audience .", "Conversations in poster sessions in academic events , referred to as poster conversations , pose interesting and challenging topics on multi-modal analysis of multi-party dialogue ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We present and evaluate two state-of-the art dialogue systems developed to support dialog with French speaking virtual characters in the context of a serious game : one hybrid statistical/symbolic and one purely statistical .", "We conducted a quantitative evaluation where we compare the accuracy of the interpreter and of the dialog manager used by each system ; a user based evaluation based on 22 subjects using both the statistical and the hybrid system ; and a corpus based evaluation where we examine such criteria as dialog coherence , dialog success , interpretation and generation errors in the corpus of Human-System interactions collected during the user-based evaluation .", "We show that although the statistical approach is slightly more robust , the hybrid strategy seems to be better at guiding the player through the game ."]}
{"orig_sents": ["6", "2", "0", "1", "7", "3", "4", "5"], "shuf_sents": ["For tutoring systems , this may additionally require knowing how relationships are signaled among non-adult users .", "We therefore investigate conversational strategies used by teenagers in peer tutoring dialogues , and how these strategies function differently among friends or strangers .", "We believe this challenge includes evoking and signaling aspects of long-term relationships such as rapport .", "To take into account the sparse nature of these features in real data we use models including Lasso , ridge estimator , and elastic net .", "We evaluate the predictive power of our models under various settings , and compare our sparse models with standard non-sparse solutions .", "Our experiments demonstrate that our models are more accurate than non-sparse models quantitatively , and that teens use unexpected kinds of language to do relationship work such as signaling rapport , but friends and strangers , tutors and tutees , carry out this work in quite different ways from one another .", "One challenge of implementing spoken dialogue systems for long-term interaction is how to adapt the dialogue as user and system become more familiar .", "In particular , we use annotated and automatically extracted linguistic devices to predict impoliteness and positivity in the next turn ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We show that in situated communication , eyetracking can be used to reliably and efficiently monitor the hearer ? s reference resolution process .", "An interactive system that draws on hearer gaze to provide positive or negative feedback after referring to objects outperforms baseline systems on metrics of referential success and user confusion .", "The ability to monitor the communicative success of its utterances and , if necessary , provide feedback and repair is useful for a dialog system ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["words .", "Moreover , our proposed token-level summarization approach , which is able to remove redundancies within utterances , outperforms existing utterance ranking based summarization methods .", "We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify ? summaryworthy ?", "Finally , context information is also investigated to add additional relevant information to the summary .", "Concretely , a series of unsupervised topic models is explored and experimental results show that fine-grained topic models , which discover topics at the utterance-level rather than the document-level , can better identify the gist of the decisionmaking process ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["To verify the quality of the simulation , the proposed method was applied to the Let ? s Go domain ( Raux et al , 2005 ) and a set of measures was used to analyze the simulated data at several levels .", "The proposed method adopts a dynamic Bayesian network to infer the unobservable true user action from which the parameters of other components are naturally derived .", "This paper proposes an unsupervised approach to user simulation in order to automatically furnish updates and assessments of a deployed spoken dialog system .", "The results showed a very close correspondence between the real and simulated data , implying that it is possible to create a realistic user simulator that does not necessitate human intervention ."]}
{"orig_sents": ["5", "6", "4", "0", "1", "3", "2"], "shuf_sents": ["First , we mark sentence-level behavior using an information sharing annotation scheme .", "By taking advantage of Integer Linear Programming and a sociolinguistic framework , we enforce structural relationships between sentence-level annotations and sequences of interaction .", "This model is highly accurate , performing near human accuracy , and performs analysis on-line , opening the door to real-time analysis of the discourse of conversation .", "Then , we show that clustering these sequences can effectively disentangle the threads of conversation .", "In this work we present a machine learning model for multi-party chat which predicts conversation structure across differing units of analysis .", "Conversational practices do not occur at a single unit of analysis .", "To understand the interplay between social positioning , information sharing , and rhetorical strategy in language , various granularities are necessary ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We demonstrate that a semantic web oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations in the rapid development process of spoken dialogue systems .", "We herein propose a method for the rapid development of a spoken dialogue system based on collaboratively constructed semantic resources and compare the proposed method with a conventional method that is based on a relational database .", "Previous development frameworks of spoken dialogue systems , which presuppose a relational database management system as a background application , require complex data definition , such as making entries in a task-dependent language dictionary , templates of semantic frames , and conversion rules from user utterances to the query language of the database ."]}
{"orig_sents": ["1", "4", "3", "2", "0"], "shuf_sents": ["In particular , it was found that users chose to respond to different system questions and use different speaking styles , which indicate the need for an incremental dialogue approach .", "In recent years statistical dialogue systems have gained significant attention due to their potential to be more robust to speech recognition errors .", "The influences of cognitive loading were investigated and some clear differences in behaviour were discovered .", "In this paper , a statistical dialogue system providing restaurant information is evaluated in a set-up where the subjects used a driving simulator whilst talking to the system .", "However , these systems must also be robust to changes in user behaviour caused by cognitive loading ."]}
{"orig_sents": ["4", "2", "1", "5", "0", "6", "3"], "shuf_sents": ["The results indicate that these features are not predictive of a patient ? s adherence to treatment or satisfaction with the communication , although they do have some association with symptoms .", "patient clarification .", "specifically , the pro-activity of the patient in checking their understanding , i.e .", "These preliminary experiments indicate that patient adherence is predictable from dialogue transcripts , but further work is necessary to develop a meaningful , general and reliable feature set .", "Recent work on consultations between outpatients with schizophrenia and psychiatrists has shown that adherence to treatment can be predicted by patterns of repair ?", "Using machine learning techniques , we investigate whether this tendency can be predicted from high-level dialogue features , such as backchannels , overlap and each participant ? s proportion of talk .", "However , all these can be predicted if we allow features at the word level ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["A simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using RL .", "We analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the Museum of Science in Boston , in order to build a realistic model of user behavior when interacting with these characters .", "Our learned policy outperforms two baselines ( including the original dialogue policy that was used for collecting the corpus ) in a simulation setting .", "We use Reinforcement Learning ( RL ) to learn question-answering dialogue policies for a real-world application ."]}
{"orig_sents": ["0", "2", "7", "1", "8", "3", "5", "4", "6"], "shuf_sents": ["A robust system that understands route instructions should be able to process instructions generated naturally by humans .", "We found that instructions could be classified into four categories , depending on their intent such as imperative , feedback , or meta comment .", "Also desirable would be the ability to handle repairs and other modifications to existing instructions .", "Finally , we constructed a semantic grammar and evaluated its coverage .", "Our work suggests that predictable sub-languages may exist for well-defined tasks .", "To determine whether instructiongiving forms a predictable sub-language , we tested the grammar on three corpora collected by others and determined that this was largely the case .", "Index Terms : Robot Navigation , Spoken Instructions", "To this end , we collected a corpus of spoken instructions ( and modified instructions ) produced by subjects provided with an origin and a destination .", "We asked a different set of subjects to follow these instructions to determine the usefulness and comprehensibility of individual instructions ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Our results constitute a new set of baselines for future studies of implicit discourse relation identification .", "We provide a systematic study of previously proposed features for implicit discourse relation identification , identifying new feature combinations that optimize F1-score .", "The resulting classifiers achieve the best F1-scores to date for the four top-level discourse relation classes of the Penn Discourse Tree Bank : COMPARISON , CONTINGENCY , EXPANSION , and TEMPORAL .", "We further identify factors for feature extraction that can have a major impact on performance and determine that some features originally proposed for the task no longer provide performance gains in light of more powerful , recently discovered features ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["However , incorporating real users into the development cycle is expensive and current simulation techniques are inadequate .", "Developing sophisticated turn-taking behavior is necessary for next-generation dialogue systems .", "As a foundation for advancing turn-taking behavior , we present a temporal simulator that models the interaction between the user and the system , including speech , voice activity detection , and incremental speech recognition .", "We describe the details of the simulator and demonstrate it on a sample domain ."]}
{"orig_sents": ["3", "4", "0", "5", "2", "1"], "shuf_sents": ["Based on these observations , we propose a new approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers ?", "To our knowledge , this work is the first 1 to tackle this promising research direction of speaker adaptation for dialogue act recogntion .", "Our experiments on a realworld meeting dataset show that with even only 200 speaker-specific annotated dialogue acts , the performances on dialogue act recognition are significantly improved when compared to several baseline algorithms .", "In this work we study the effectiveness of speaker adaptation for dialogue act recognition in multiparty meetings .", "First , we analyze idiosyncracy in dialogue verbal acts by qualitatively studying the differences and conflicts among speakers and by quantitively comparing speaker-specific models .", "data ."]}
{"orig_sents": ["4", "2", "5", "0", "6", "1", "3"], "shuf_sents": ["and ? We ?", "It was shown that lower verbal intelligence speakers repeated more nouns and adjectives from the other and used the same linguistic categories more often than higher verbal intelligence speakers .", "The work is based on a corpus consisting of 100 descriptions of a short film ( monologues ) , 56 discussions about the same topic ( dialogues ) , and verbal intelligence scores of the test participants .", "In dialogues between strangers , participants with higher verbal intelligence showed a greater level of adaptation .", "This work investigates to what degree speakers with different verbal intelligence may adapt to each other .", "Adaptation between two dialogue partners was measured using cross-referencing , proportion of ? I ? , ? You ?", "words , between-subject correlation and similarity of texts ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["It is supported by a new information-state based dialogue manager , FLoReS ( Forward-Looking , Reward Seeking dialogue manager ) , that allows both advanced , flexible , mixed initiative interaction , and efficient policy creation by domain experts .", "To easily reach its target population this dialogue system is accessible as a web application .", "We present a mixed initiative conversational dialogue system designed to address primarily mental health care concerns related to military deployment ."]}
{"orig_sents": ["0", "1", "4", "3", "2"], "shuf_sents": ["To enable effective referential grounding in situated human robot dialogue , we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities .", "In particular , we have developed a graph-based representation to capture linguistic discourse and visual discourse , and applied inexact graph matching to ground references .", "These results demonstrate that , due to its error-tolerance nature , inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction .", "84.7 % of the objects in the environment are mis-recognized ) , our approach can still achieve 66 % accuracy in referential grounding .", "Our empirical results have shown that , even when computer vision algorithms produce many errors ( e.g ."]}
{"orig_sents": ["1", "5", "6", "2", "3", "0", "7", "4"], "shuf_sents": ["The parser adopts a twostage approach where first the local constraints are applied and then global constraints are used on a reduced weighted search space ( n-best ) .", "A coherently related group of sentences may be referred to as a discourse .", "We present techniques on using inter-sentential or sentence-level ( global ) , data-driven , nongrammatical features in the task of parsing discourse .", "The parser model follows up previous approach based on using tokenlevel ( local ) features with conditional random fields for shallow discourse parsing , which is lacking in structural knowledge of discourse .", "The two-stage parser yields significant improvements over the best performing model of discourse parser on the PDTB corpus .", "In this paper we address the problem of parsing coherence relations as defined in the Penn Discourse Tree Bank ( PDTB ) .", "A good model for discourse structure analysis needs to account both for local dependencies at the token-level and for global dependencies and statistics .", "In the latter stage we experiment with different rerankers trained on the first stage n-best parses , which are generated using lexico-syntactic local features ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Experimental results on the RST Discourse Treebank corpus show that our model outperforms existing discourse segmenters in both settings that use gold standard Penn Treebank parse trees and Stanford parse trees .", "This paper presents a discriminative reranking model for the discourse segmentation task , the first step in a discourse parsing system .", "Our model exploits subtree features to rerank Nbest outputs of a base segmenter , which uses syntactic and lexical features in a CRF framework ."]}
{"orig_sents": ["5", "4", "2", "3", "1", "0"], "shuf_sents": ["We evaluate our approach on a corpus of destination setting dialogs and show that it significantly outperforms a deterministic baseline .", "In this paper , we describe a belief tracking system for a location identification task that combines a semantic belief tracker for categorical concepts based on the DPOT framework ( Raux and Ma , 2011 ) with a kernel density estimator that incorporates landmark evidence from multiple turns and landmark hypotheses , into a posterior probability over candidate locations .", "In particular , a car navigation system that infers users ?", "intended destination using nearby landmarks as descriptions must be able to use distance measures as a factor in inference .", "However , such models are ill-suited to probabilistic reasoning about spatial relationships between entities .", "Many modern spoken dialog systems use probabilistic graphical models to update their belief over the concepts under discussion , increasing robustness in the face of noisy input ."]}
{"orig_sents": ["1", "2", "4", "0", "3", "5"], "shuf_sents": ["The probabilistic rules are associated with a small , compact set of parameters that can be directly estimated from data .", "Probabilistic models such as Bayesian Networks are now in widespread use in spoken dialogue systems , but their scalability to complex interaction domains remains a challenge .", "One central limitation is that the state space of such models grows exponentially with the problem size , which makes parameter estimation increasingly difficult , especially for domains where only limited training data is available .", "We argue that the introduction of this abstraction mechanism yields probabilistic models that are easier to learn and generalise better than their unstructured counterparts .", "In this paper , we show how to capture the underlying structure of a dialogue domain in terms of probabilistic rules operating on the dialogue state .", "We empirically demonstrate the benefits of such an approach learning a dialogue policy for a human-robot interaction domain based on a Wizard-of-Oz data set ."]}
{"orig_sents": ["0", "1", "2", "6", "5", "4", "3"], "shuf_sents": ["This paper proposes the use of unsupervised approaches to improve components of partition-based belief tracking systems .", "The proposed method adopts a dynamic Bayesian network to learn the user action model directly from a machine-transcribed dialog corpus .", "It also addresses confidence score calibration to improve the observation model in a unsupervised manner using dialog-level grounding information .", "In addition , the calibrated confidence score was verified by demonstrating the positive influence on the user action model learning process and on overall system performance .", "The results show that the proposed method can learn an effective user action model without human intervention .", "Overall system performance for several comparative models were measured .", "To verify the effectiveness of the proposed method , we applied it to the Let ? s Go domain ( Raux et al , 2005 ) ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The top 20 out of 76 dimensions accounted for 81 % of the variance , and each of these dimensions clearly related to dialog states and activities , including turn taking , topic structure , grounding , empathy , cognitive processes , attitude and rhetorical structure .", "This paper presents a new way to identify the important dimensions of dialog state , more bottomup and empirical than previous approaches .", "Specifically , we applied Principal Component Analysis to a large number of low-level prosodic features to find the most important dimensions of variation .", "Models of dialog state are important , both scientifically and practically , but today ? s best build strongly on tradition ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["The best model performs identification about 20 % better than the model that uses the audio-visual features of the child alone .", "Addressee identification is an element of all language-based interactions , and is critical for turn-taking .", "We examine the particular problem of identifying when each child playing an interactive game in a small group is speaking to an animated character .", "After analyzing child and adult behavior , we explore a family of machine learning models to integrate audio and visual features with temporal group interactions and limited , task-independent language ."]}
{"orig_sents": ["3", "0", "4", "2", "1"], "shuf_sents": ["We compare this version with the prior version of our system , which only adapts to user uncertainty .", "Moreover , responding to disengagement breaks its negative correlations with task success and user satisfaction , reduces uncertainty levels , and reduces the likelihood of continued disengagement .", "We find a significant increase in motivation for users who most frequently received the disengagement adaptation .", "We evaluate a wizard-of-oz spoken dialogue system that adapts to multiple user affective states in real-time : user disengagement and uncertainty .", "Our analysis investigates how iteratively adding new affect adaptation to an existing affect-adaptive system impacts global and local performance ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We also propose a realtime crowdsourcing framework for handling the case in which there is no adequate response in the database .", "Instead of using complex dialog management , our system replies with the utterance from the database that is most similar to the user input .", "We propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from Twitter and real-time crowdsourcing ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We train a CRF on manually annotated phrases , and predict a fine-grained set of labels .", "We achieve an accuracy score of 69.56 % on our most detailed label set , 76.62 % when gold standard coreference is available .", "We present a model for automatically predicting information status labels for German referring expressions ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["In this approach , the notion of reference domains serves an important role to handle context-dependent attributes of entities and references to sets .", "This paper proposes a probabilistic approach to the resolution of referring expressions for task-oriented dialogue systems .", "The evaluation with the REX-J corpus shows promising results .", "The approach resolves descriptions , anaphora , and deixis in a unified manner ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We present a semantic-specificity metric to gauge this complexity for dialogue systems that access a relational database .", "Ambiguous or open-ended requests to a dialogue system result in more complex dialogues .", "An experiment where a simulated user makes requests to a dialogue system shows that semantic specificity correlates with dialogue length ."]}
{"orig_sents": ["0", "4", "2", "1", "5", "3"], "shuf_sents": ["Huang Hsin-Hsi Chen Department of Computer Science and Department of Computer Science and Information Engineering , Information Engineering , National Taiwan University , Taipei , Taiwan National Taiwan University , Taipei , Taiwan hhhuang @ nlg.csie.ntu.edu.tw hhchen @ csie.ntu.edu.tw Abstract Unlike in English , the sentence boundaries in Chinese are fuzzy and not well-defined .", "We construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure .", "In this paper , we focus on two important relations , Contingency and Comparison , which occur often inside a sentence .", "Experimental results show our model achieves accuracies of 81.63 % in the task of relation labeling and 74.8 % in the task of relation structure prediction .", "As a result , Chinese sentences tend to be long and consist of complex discourse relations .", "A learning based model is evaluated with various features ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["We conclude with a discussion of the potential for a hybrid architecture incorporating the strengths of both approaches .", "This paper presents an analysis of how the level of performance achievable by an NLU module can affect the optimal modular design of a dialogue system .", "We present an evaluation that shows how NLU accuracy levels impact the overall performance of a system that includes an NLU module and a rule-based dialogue policy .", "We contrast these performance levels with the performance of a direct classification design that omits a separate NLU module ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We present an Incremental Interaction Manager that supports the use of ISR with strictly turn-based dialogue managers .", "We then show that using a POMDP-based dialogue manager with ISR substantially improves the semantic accuracy of the incremental results .", "The goal of this paper is to present a first step toward integrating Incremental Speech Recognition ( ISR ) and Partially-Observable Markov Decision Process ( POMDP ) based dialogue systems .", "The former provides support for advanced turn-taking behavior while the other increases the semantic accuracy of speech recognition results ."]}
{"orig_sents": ["0", "2", "1", "3", "4"], "shuf_sents": ["During conversations , addressees produce conversational acts ? verbal and nonverbal backchannels ? that facilitate turn-taking , acknowledge speakership , and communicate common ground without disrupting the speaker ? s speech .", "Therefore , gaining a deeper understanding of how these acts interact with speaker behaviors in shaping conversations might offer key insights into the design of technologies such as computer-mediated communication systems and embodied conversational agents .", "These acts play a key role in achieving fluent conversations .", "In this paper , we explore how a regression-based approach might offer such insights into modeling predictive relationships between speaker behaviors and addressee backchannels in a storytelling scenario .", "Our results reveal speaker eye contact as a significant predictor of verbal , nonverbal , and bimodal backchannels and utterance boundaries as predictors of nonverbal and bimodal backchannels ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["With the aim of investigating how humans understand each other through language and gestures , this paper focuses on how people understand incomplete sentences .", "Our promising results are based on multi-modal features .", "We trained a system based on interrupted but resumed sentences , in order to find plausible completions for incomplete sentences ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Participants in a conversation are normally receptive to their surroundings and their interlocutors , even while they are speaking and can , if necessary , adapt their ongoing utterance .", "We present combinable components for incremental natural language generation and incremental speech synthesis and demonstrate the flexibility they can achieve with an example system that adapts to a listener ? s acoustic understanding problems by pausing , repeating and possibly rephrasing problematic parts of an utterance .", "In an evaluation , this system was rated as significantly more natural than two systems representing the current state of the art that either ignore the interrupting event or just pause ; it also has a lower response time .", "Typical dialogue systems are not receptive and can not adapt while uttering ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We evaluate the approach on a decision summarization task and show that it outperforms unsupervised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method .", "Moreover , our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score .", "We adapt an existing in-domain relation learner ( Chen et al , 2011 ) by exploiting a set of task-specific constraints and features .", "We present a novel unsupervised framework for focused meeting summarization that views the problem as an instance of relation extraction ."]}
{"orig_sents": ["4", "3", "1", "2", "0"], "shuf_sents": ["We conclude that MLNs offer a promising framework for specifying such models in a general , possibly domain-independent way .", "We explore a set of models specified as Markov Logic Networks , and show that a model that has access to information about the visual context of an utterance , its discourse context , as well as the linguistic structure of the utterance performs best .", "We explore its incremental properties , and also its use in a joint parsing and understanding module .", "Such type of understanding is required in conversational systems that need to act immediately on language input , such as multi-modal systems or dialogue systems for robots .", "We present work on understanding natural language in a situated domain , that is , language that possibly refers to visually present entities , in an incremental , word-by-word fashion ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We develop a novel computational approach that enables us to explore the role of such cues , and show that our model can replicate aspects of the developmental trajectory of MSV acquisition .", "Children acquire mental state verbs ( MSVs ) much later than other , lower-frequency , words .", "One factor proposed to contribute to this delay is that children must learn various semantic and syntactic cues that draw attention to the difficult-to-observe mental content of a scene ."]}
{"orig_sents": ["2", "3", "1", "4", "0"], "shuf_sents": ["Our technique performs particularly well on extracting features relevant to a given concept , and suggests a number of promising areas for future focus .", "This paper investigates the use of semi-supervised learning and support vector machines to automatically extract concept-relation-feature triples from two large corpora ( Wikipedia and UKWAC ) for concrete noun concepts .", "For a given concrete noun concept , humans are usually able to cite properties ( e.g. , elephant is animal , car has wheels ) of that concept ; cognitive psychologists have theorised that such properties are fundamental to understanding the abstract mental representation of concepts in the brain .", "Consequently , the ability to automatically extract such properties would be of enormous benefit to the field of experimental psychology .", "Previous approaches have relied on manually-generated rules and hand-crafted resources such as WordNet ; our method requires neither yet achieves better performance than these prior approaches , measured both by comparison with a property norm-derived gold standard as well as direct human evaluation ."]}
{"orig_sents": ["1", "0", "3", "4", "2"], "shuf_sents": ["Their leading explanation states that they are caused by visual acuity limitations on word recognition .", "Some of the most robust effects of linguistic variables on eye movements in reading are those of word length .", "We present an extension of Bicknell and Levy ? s model that incorporates word length uncertainty , and show that it produces more humanlike word length effects .", "However , Bicknell ( 2011 ) presented data showing that a model of eye movement control in reading that includes visual acuity limitations and models the process of word identification from visual input ( Bicknell & Levy , 2010 ) does not produce humanlike word length effects , providing evidence against the visual acuity account .", "Here , we argue that uncertainty about word length in early word identification can drive word length effects ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We explore the predictions of the framework in an artificial grammar task in which humans and recurrent neural networks are trained on a language with recursive structure .", "The results provide evidence for the claim of the dynamical systems models that grammatical systems continuously metamorphose during learning .", "We describe a computational framework for language learning and parsing in which dynamical systems navigate on fractal sets .", "The present perspective permits structural comparison between the recursive representations in symbolic and neural network models ."]}
{"orig_sents": ["2", "3", "0", "1", "4"], "shuf_sents": ["One side-effect of the transform is that it guarantees at most a single expansion ( push ) and at most a single reduction ( pop ) during a syntactic parse .", "The primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser .", "Probabilistic context-free grammars ( PCFGs ) are a popular cognitive model of syntax ( Jurafsky , 1996 ) .", "These can be formulated to be sensitive to human working memory constraints by application of a right-corner transform ( Schuler , 2009 ) .", "This yields a simpler structure that more closely resembles existing simple recurrent network models of sentence comprehension ."]}
{"orig_sents": ["0", "1", "4", "7", "2", "5", "6", "8", "3"], "shuf_sents": ["Experimental evidence demonstrates that syntactic structure influences human online sentence processing behavior .", "Despite this evidence , open questions remain : which type of syntactic structure best explains observed behavior ? hierarchical or sequential , and lexicalized or unlexicalized ?", "We investigate these claims and find a picture more complicated than the one they present .", "Our results demonstrate that the claim of Frank and Bod ( 2011 ) that sequential models predict reading times better than hierarchical models is premature , and also that lexicalization matters for prediction accuracy .", "Recently , Frank and Bod ( 2011 ) find that unlexicalized sequential models predict reading times better than unlexicalized hierarchical models , relative to a baseline prediction model that takes wordlevel factors into account .", "First , we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of Frank and Bod ( 2011 ) eliminates all differences in accuracy between those unlexicalized sequential and hierarchical models .", "Second , we show that lexicalizing the hierarchical models used in Frank and Bod ( 2011 ) significantly improves prediction accuracy relative to the unlexicalized versions .", "They conclude that the human parser is insensitive to hierarchical syntactic structure .", "Third , we show that using stateof-the-art lexicalized hierarchical models further improves prediction accuracy ."]}
{"orig_sents": ["0", "4", "1", "2", "3"], "shuf_sents": ["Logical metonymies ( The student finished the beer ) represent a challenge to compositionality since they involve semantic content not overtly realized in the sentence ( covert events ?", "We present a contrastive study of two classes of computational models for logical metonymy in German , namely a probabilistic and a distributional , similarity-based model .", "These are built using the SDEWAC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context .", "The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models .", "drinking the beer ) ."]}
{"orig_sents": ["0", "6", "1", "5", "4", "3", "2"], "shuf_sents": ["In the last two decades , information-seeking spoken dialog systems ( SDS ) have moved from research prototypes to real-life commercial applications .", "Future spoken interaction are required to be multilingual , understand and act on large scale knowledge bases in all its forms ( from structured to unstructured ) .", "We argue that a ) it is crucial to leverage this massive amount of Web lightly structured knowledge and b ) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community .", "Yago ) .", "Wikipedia ) and knowledge bases ( e.g .", "The Web research community have striven to build large scale and open multilingual resources ( e.g .", "Still , dialog systems are limited by the scale , complexity of the task and coverage of knowledge required by problemsolving machines or mobile personal assistants ."]}
{"orig_sents": ["0"], "shuf_sents": ["We argue that standardized metrics and automatic evaluation tools are necessary for speeding up knowledge generation and development processes for dialog systems ."]}
{"orig_sents": ["3", "4", "1", "0", "2"], "shuf_sents": ["In this position paper , we focus on statistical methods for user simulation , their main advantages and drawbacks .", "Since the late 90 ? s , user simulation is also used for dialogue management optimisation .", "We initiate a reflection about the utility of such methods and give some insights of what their future should be .", "There has been a lot of interest for user simulation in the field of spoken dialogue systems during the last decades .", "User simulation was first proposed to assess the performance of SDS before a public release ."]}
{"orig_sents": ["0"], "shuf_sents": ["A sketch of dialogue systems as long-term adaptive , conversational agents ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["We discuss the challenges for existing systems in meeting these needs and propose strategies to overcome them .", "However , different user groups have different requirements and expectations for such systems .", "Spoken dialog systems frameworks fill a crucial role in the spoken dialog systems community by providing resources to lower barriers to entry .", "Here , we consider the particular needs for spoken dialog systems toolkits within an instructional setting ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["Belief tracking is a promising technique for adding robustness to spoken dialog systems , but current research is fractured across different teams , techniques , and domains .", "This paper amplifies past informal discussions ( Raux , 2011 ) to call for a belief tracking challenge task , based on the Spoken dialog challenge corpus ( Black et al , 2011 ) .", "Benefits , limitations , evaluation design issues , and next steps are presented ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We demonstrate that a semantic Web-oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations , which are caused by the previous relational database-based approach , in the development process of spoken dialogue systems .", "We herein introduce our project of realizing a framework for the development of a spoken dialogue system based on collaboratively constructed semantic resources .", "In addition , we show that the proposed framework enables multilingual spoken dialogue system development due to clear separation of model , view and controller components ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["These components work fairly domainindependently ; we also provide example implementations of higher-level components such as natural language understanding and dialogue management that are somewhat more tied to a particular domain .", "We offer this release of the toolkit to foster research in this new and exciting area , which promises to help increase the naturalness of behaviours that can be modelled in such systems .", "We describe the 2012 release of our ? Incremental Processing Toolkit ?", "( INPROTK ) 1 , which combines a powerful and extensible architecture for incremental processing with components for incremental speech recognition and , new to this release , incremental speech synthesis ."]}
{"orig_sents": ["2", "3", "0", "4", "1"], "shuf_sents": ["The framework proposed in this paper leverages models of the environ-ment , user and system to predict possible fu-ture world states via simulation .", "In this paper we introduce this frame-work and demonstrate its effectiveness for in-car navigation .", "ion-based Framework for Spoken Language Understanding and Action Selection in Situated Interaction David Cohen Ian Lane Carnegie Mellon University Carnegie Mellon University Nasa Research Park Nasa Research Park Moffett Field , CA Moffett Field , CA david.cohen @ sv.cmu.edu lane @ cs.cmu.edu Abstract This paper introduces a simulation-based framework for performing action selection and understanding for interactive agents .", "By simulating the objects and actions relevant to an interaction , an agent can semantically ground natural language and interact consid-erately and on its own initiative in situated environments .", "It leverages understanding of spoken language and multi-modal input to estimate the state of the ongo-ing interaction and select actions based on the utility of future outcomes in the simulated world ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["In this paper , we survey recent research on bootstrapping or improving SLU systems by using information mined or extracted from web search query logs , which include ( natural language ) queries entered by users as well as the links ( web sites ) they click on .", "In a spoken dialog system that can handle natural conversation between a human and a machine , spoken language understanding ( SLU ) is a crucial component aiming at capturing the key semantic components of utterances .", "Building a robust SLU system is a challenging task due to variability in the usage of language , need for labeled data , and requirements to expand to new domains ( movies , travel , finance , etc . ) .", "We focus on learning methods that help unveiling hidden information in search query logs via implicit crowd-sourcing ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["Developing interactive robots is an extremely challenging task which requires a broad range of expertise across diverse disciplines , including , robotic planning , spoken language understanding , belief tracking and action management .", "While there has been a boom in recent years in the development of reusable components for robotic systems within common architectures , such as the Robot Operating System ( ROS ) , little emphasis has been placed on developing components for HumanRobot-Interaction .", "In this paper we introduce HRItk ( the Human-Robot-Interaction toolkit ) , a framework , consisting of messaging protocols , core-components , and development tools for rapidly building speech-centric interactive systems within the ROS environment .", "The proposed toolkit was specifically designed for extensibility , ease of use , and rapid development , allowing developers to quickly incorporate speech interaction into existing projects ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["This information , however , provides a means for inherently improving the dialogue performance by adapting the Dialogue Manager during the interaction accordingly .", "Therefore , we address requirements for the quality metric and , additionally , present approaches for quality-adaptive dialogue management .", "Information about the quality of a Spoken Dialogue System ( SDS ) is usually used only for comparing SDSs with each other or manually improving the dialogue strategy .", "For a quality metric to be suitable , it must suffice certain conditions ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["An existing state-of-the-art Bayesian model for PropBank-style unsupervised semantic role induction ( Titov and Klementiev , 2012 ) is extended to jointly induce semantic frames and their roles .", "We evaluate the model performance both quantitatively and qualitatively by comparing the induced representation against FrameNet annotations .", "The frame-semantic parsing task is challenging for supervised techniques , even for those few languages where relatively large amounts of labeled data are available .", "In this preliminary work , we consider unsupervised induction of frame-semantic representations ."]}
{"orig_sents": ["4", "3", "5", "1", "2", "6", "0"], "shuf_sents": ["How well can the frame information of running text be transferred from one language to another ?", "We focus on evaluating the results to get an estimation on how often the parallel sentences can be said to express the same frame .", "This sheds light on the questions : Are the same situations in the two languages expressed using different frames , i.e .", "We evaluate both the theoretical possibility of transferring frames and the possibility of performing it using available lexical resources .", "In our experiment , we evaluate the transferability of frames from Swedish to Finnish in parallel corpora .", "We add the frame information to an extract of the Swedish side of the Kotus and JRC-Acquis corpora using an automatic frame labeler and copy it to the Finnish side .", "are the frames transferable even in theory ?"]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["In the Penn Treebank , transitions between upper- and lowercase tokens tend to align with the boundaries of base ( English ) noun phrases .", "We show that orthographic cues can be helpful for unsupervised parsing .", "Combining capitalization with punctuation-induced constraints in inference further improved parsing performance , attaining state-of-the-art levels for many languages .", "Such signals can be used as partial bracketing constraints to train a grammar inducer : in our experiments , directed dependency accuracy increased by 2.2 % ( average over 14 languages having case information ) ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We then conduct a variety of experiments with this model , first inducing grammars on a portion of the Penn Treebank and the Korean Treebank 2.0 , and next experimenting with grammar refinement from a single nonterminal and from the Universal Part of Speech tagset .", "We provide a model that extends the splitmerge framework of Petrov et al ( 2006 ) to jointly learn latent annotations and Tree Substitution Grammars ( TSGs ) .", "We present qualitative analysis showing promising signs across all experiments that our combined approach successfully provides for greater flexibility in grammar induction within the structured guidance provided by the treebank , leveraging the complementary natures of these two approaches ."]}
{"orig_sents": ["3", "7", "0", "4", "2", "5", "1", "6"], "shuf_sents": ["Researchers suggested repeated random restarts or constraints that guide the model evolution .", "We vary the amount and distribution of initial partial annotations , and compare the results to unsupervised and supervised approaches .", "Restarts are time-intensive , and most constraint-based approaches require serious re-engineering or external solvers .", "For many NLP tasks , EM-trained HMMs are the common models .", "Neither approach is ideal .", "In this paper we measure the effectiveness of very limited initial constraints : specifically , annotations of a small number of words in the training data .", "We find that partial annotations improve accuracy and can reduce the need for random restarts , which speeds up training time considerably .", "However , in order to escape local maxima and find the best model , we need to start with a good initial model ."]}
{"orig_sents": ["3", "4", "2", "1", "0"], "shuf_sents": ["We compare different ways of generating senses and assess the quality of the alignments relative to the IBM HMM model , as well as the generated sense probabilities , in order to gauge the usefulness in Word Sense Disambiguation .", "We test a preliminary version of this model on English-French data .", "We try to address this issue by extending the IBM HMM model with an extra hidden layer which represents the senses a word can take , allowing similar words to share similar output distributions .", "Some of the most used models for statistical word alignment are the IBM models .", "Although these models generate acceptable alignments , they do not exploit the rich information found in lexical resources , and as such have no reasonable means to choose better translations for specific senses ."]}
{"orig_sents": ["3", "7", "4", "0", "8", "1", "2", "5", "6"], "shuf_sents": ["particle filters , are well suited to approximating such models , resolving their multi-modal nature at the cost of generating additional samples .", "We analyze the behavior of the particle filters , and compare them to a block sentence sampler , a local token sampler , and a heuristic sampler , which constrains inference to a single PoS per word type .", "Our findings show that particle filters can closely approximate a difficult or even intractable sampler quickly .", "As linguistic models incorporate more subtle nuances of language and its structure , standard inference techniques can fall behind .", "However , Sequential Monte Carlo ( SMC ) approaches , i.e .", "However , we found that high posterior likelihood do not necessarily correspond to better Many-to-One accuracy .", "The results suggest that the approach has potential and more advanced particle filters are likely to lead to stronger performance .", "Often , such models are tightly coupled such that they defy clever dynamic programming tricks .", "We implement two particle filters , which jointly sample either sentences or word types , and incorporate them into a Gibbs sampler for part-of-speech ( PoS ) inference ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["Specifically , we extend the method recently proposed by Ta ? ckstro ? m et al ( 2012 ) , which is based on cross-lingual word cluster features .", "Finally , we show that we can significantly improve target language performance , even after annotating up to 64,000 tokens in the target language , by simply concatenating source and target language annotations .", "In this paper , we study direct transfer methods for multilingual named entity recognition .", "Second , we investigate how the direct transfer system fares against a supervised target language system and conclude that between 8,000 and 16,000 word tokens need to be annotated in each target language to match the best direct transfer system .", "First , we show that by using multiple source languages , combined with self-training for target language adaptation , we can achieve significant improvements compared to using only single source direct transfer ."]}
{"orig_sents": ["2", "0", "1", "3", "4"], "shuf_sents": ["Although many previous competitions have featured dependency grammars or partsof-speech , these were invariably framed as supervised learning and/or domain adaption .", "This is the first challenge to evaluate unsupervised induction systems , a sub-field of syntax which is rapidly becoming very popular .", "This paper presents the results of the PASCAL Challenge on Grammar Induction , a competition in which competitors sought to predict part-of-speech and dependency syntax from text .", "Our challenge made use of a 10 different treebanks annotated in a range of different linguistic formalisms and covering 9 languages .", "We provide an overview of the approaches taken by the participants , and evaluate their results on each dataset using a range of different evaluation metrics ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The novel approach introduces a fertility model and reducibility model , which assumes that dependent words can be removed from a sentence without violating its syntactic correctness .", "This paper describes a system for unsupervised dependency parsing based on Gibbs sampling algorithm ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["that verbs may be the roots of sentences and can take nouns as arguments .", "Our system consists of a simple , EM-based induction algorithm ( Bisk and Hockenmaier , 2012 ) , which induces a language-specific Combinatory Categorial grammar ( CCG ) and lexicon based on a small number of linguistic principles , e.g ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["When assigning POS tags , we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag .", "This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets .", "We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation .", "Then we create a hierarchical clustering of the word types : we use an agglomerative clustering algorithm where the distance between clusters is defined as the JensenShannon divergence between the probability distributions over classes associated with each word-type ."]}
{"orig_sents": ["3", "4", "1", "0", "5", "2"], "shuf_sents": ["The unambiguity bias favors a grammar that leads to unambiguous parses , which is motivated by the observation that natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is very small .", "The sparsity bias favors a grammar with fewer grammar rules .", "Our experiments show that both types of inductive biases are beneficial to grammar induction .", "In this paper we describe our participating system for the dependency induction track of the PASCAL Challenge on Grammar Induction .", "Our system incorporates two types of inductive biases : the sparsity bias and the unambiguity bias .", "We introduce our approach to combining these two types of biases and discuss the system implementation ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["The experimental results highlight the important factors in modeling the questioning process .", "A key challenge for dialogue-based intelligent tutoring systems lies in selecting follow-up questions that are not only context relevant but also encourage self-expression and stimulate learning .", "This paper presents an approach to ranking candidate questions for a given dialogue context and introduces an evaluation framework for this task .", "This work provides a framework for future work in automatic question generation and it represents a step toward the larger goal of directly learning tutorial dialogue policies directly from human examples .", "We learn to rank using judgments collected from expert human tutors , and we show that adding features derived from a rich , multi-layer dialogue act representation improves system performance over baseline lexical and syntactic features to a level in agreement with the judges ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We provide initial models and evaluations of the models for each component .", "We present initial steps towards an interactive essay writing tutor that improves science knowledge by analyzing student essays for misconceptions and recommending science webpages that help correct those misconceptions .", "We describe the five components in this system : identifying core science concepts , determining appropriate pedagogical sequences for the science concepts , identifying student misconceptions in essays , aligning student misconceptions to science concepts , and recommending webpages to address misconceptions ."]}
{"orig_sents": ["3", "0", "5", "4", "6", "2", "1"], "shuf_sents": ["The project has developed two virtual worlds that each have a mystery or natural phenomenon requiring scientific explanation ; by recording students ?", "In experiments on over 300 middle school students , our best automated grader improves by over 50 % relative to the closest system from previous work in predicting grades supplied by human judges .", "This paper presents an automated grader that can combine with SAVE Science ? s virtual worlds to provide a cheap mechanism for assessments of the ability to apply scientific methodology .", "The SAVE Science project is an attempt to address the shortcomings of current assessments of science .", "Currently , however , the scoring of the assessment depends either on manual grading of students ?", "behavior as they investigate the mystery , these worlds can be used to assess their understanding of the scientific method .", "written responses , or , on multiple choice questions ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We examine the predictive power of different coherence models by measuring the effect on performance when combined with an AA system that achieves competitive results , but does not use discourse coherence features , which are also strong indicators of a learner ? s level of attainment .", "We present the first systematic analysis of several methods for assessing coherence under the framework of automated assessment ( AA ) of learner free-text responses .", "Additionally , we identify new techniques that outperform previously developed ones and improve on the best published result for AA on a publically-available dataset of English learner free-text examination scripts .", "To date , few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts ."]}
{"orig_sents": ["2", "5", "1", "0", "4", "3"], "shuf_sents": ["We further implement six different methods for extracting whole-sentence corrections from the lattice .", "We develop a novel alignment algorithm for combining multiple round-trip translations into a lattice using the TERp machine translation metric .", "To date , most work in grammatical error correction has focused on targeting specific error types .", "Most importantly , though , they make it clear the methods we propose have strong potential and require further study .", "Our preliminary experiments yield fairly satisfactory results but leave significant room for improvement .", "We present a probe study into whether we can use round-trip translations obtained from Google Translate via 8 different pivot languages for whole-sentence grammatical error correction ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["Incorrect usage of prepositions and determiners constitute the most common types of errors made by non-native speakers of English .", "It is not surprising , then , that there has been a significant amount of work directed towards the automated detection and correction of such errors .", "This paper reports on the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners , where systems developed by 14 teams from around the world were evaluated on the same previously unseen errorful text .", "However , to date , the use of different data sets and different task definitions has made it difficult to compare work on the topic ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["We found medium correlations with the proposed measures , that remained significant after the effect of essay length was factored out .", "The correlations did not differ substantionally between a simple , relatively robust measure vs a more sophisticated measure with better construct validity .", "Implications for development of automated essay scoring systems are discussed .", "We describe a study aimed at measuring the use of factual information in test-taker essays and assessing its effectiveness for predicting essay scores ."]}
{"orig_sents": ["5", "1", "0", "2", "6", "3", "4", "7"], "shuf_sents": ["On a large set of responses to an English proficiency test , we systematically compare the CLM with two other scoring models that have been widely used , i.e. , linear regression and decision trees .", "First , we introduce the Cumulative Logit Model ( CLM ) , which has been widely used in modeling categorical outcomes in statistics .", "Our experiments suggest that the CLM has advantages in its scoring performance and its robustness to limited-sized training data .", "Applying accurate human ratings on a small set of responses can improve the whole scoring system ? s performance while meeting cost and score-reporting time requirements .", "We find that the scoring difficulty of each speech response , which could be modeled by the degree to which it challenged human raters , could provide a way to select an optimal set of responses for the application of human scoring .", "We report two new approaches for building scoring models used by automated speech scoring systems .", "Second , we propose a novel way to utilize human rating processes in automated speech scoring .", "In a simulation , we show that focusing on challenging responses can achieve a larger scoring performance improvement than simply applying human scoring on the same number of randomly selected responses ."]}
{"orig_sents": ["1", "0", "2", "3", "4"], "shuf_sents": ["We use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two co-sine-similarity-based features , previously used in the context of automated essay scoring .", "Ontology for Improved Automated Content Scoring of Spontaneous Non-Native Speech Miao Chen Klaus Zechner School of Information Studies Educational Testing Service Syracuse University 660 Rosedale Road Syracuse , NY 13244 , USA Princeton , NJ 08541 , USA mchen14 @ syr.edu kzechner @ ets.org Abstract This paper presents an exploration into auto-mated content scoring of non-native sponta-neous speech using ontology-based information to enhance a vector space ap-proach .", "We use two ontology-facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in WordNet : ( 1 ) extending word vectors with semantic con-cepts from the WordNet ontology ( synsets ) ; and ( 2 ) using a reasoning approach for esti-mating the concept weights of concepts not present in the set of training responses by ex-ploiting the hierarchical structure of WordNet .", "Furthermore , we compare features computed from human transcriptions of spoken respons-es with features based on output from an au-tomatic speech recognizer .", "We find that ( 1 ) for one of the two features , both ontologically based approaches improve average feature correlations with human scores , and that ( 2 ) the correlations for both features decrease on-ly marginally when moving from human speech transcriptions to speech recognizer output ."]}
{"orig_sents": ["1", "4", "0", "2", "3"], "shuf_sents": ["As with many language teaching situations , a major problem is data sparsity , which we account for in our feature selection , learning algorithm , and in the setup .", "We develop a system for predicting the level of language learners , using only a small amount of targeted language data .", "Specifically , we define a two-phase classification process , isolating individual errors and linguistic constructions which are then aggregated into a second phase ; such a two-step process allows for easy integration of other exercises and features in the future .", "The aggregation of information also allows us to smooth over sparse features .", "In particular , we focus on learners of Hebrew and predict level based on restricted placement exam exercises ."]}
{"orig_sents": ["2", "3", "1", "0", "4"], "shuf_sents": ["We also present comparative evaluations with Aspell and the speller from Microsoft Office 2007 .", "and GRE ? ) .", "In this paper we present a new spell-checking system that utilizes contextual information for automatic correction of non-word misspellings .", "The system is evaluated with a large corpus of essays written by native and nonnative speakers of English to the writing prompts of high-stakes standardized tests ( TOEFL ?", "Using context-informed re-ranking of candidate suggestions , our system exhibits superior errorcorrection results overall and also corrects errors generated by non-native English writers with almost same rate of success as it does for writers who are native English speakers ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["This represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain .", "Prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text .", "Evaluating on discriminative coarse and fine grammaticality classification tasks , we show that a simple , deterministic , count-based approach to fragment identification performs on par with the more complicated grammars of Post ( 2011 ) .", "To date such fragments have been extracted from derivations of Bayesianinduced Tree Substitution Grammars ( TSGs ) ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["To handle these challenges for doing content-scoring in speaking tests , we propose two new methods based on information extraction ( IE ) and machine learning .", "Compared to writing tests , responses in speaking tests are noisy ( due to recognition errors ) , full of incomplete sentences , and short .", "Accuracy of content have not been fully utilized in the previous studies on automated speaking assessment .", "Compared to using an ordinary content-scoring method based on vector analysis , which is widely used for scoring written essays , our proposed methods provided content features with higher correlations to human holistic scores ."]}
{"orig_sents": ["0", "2", "1", "4", "3"], "shuf_sents": ["This paper describes a novel Arabic Reading Enhancement Tool ( ARET ) for classroom use , which has been built using corpus-based Natural Language Processing in combination with expert linguistic annotation .", "ARET also makes use of a commercial Arabic text-to-speech ( TTS ) system to add a speech layer ( with male and female voices ) to the Al-Kitaab language textbook resources .", "The NLP techniques include a widely used morphological analyzer for Modern Standard Arabic to provide word-level grammatical details , and a relational database index of corpus texts to provide word concordances .", "We describe the background and the motivation behind the building of ARET , presenting the various components and the method used to build the tools .", "The system generates test questions and distractors , offering teachers and students an interesting computer-aided language learning tool ."]}
{"orig_sents": ["2", "1", "3", "4", "0"], "shuf_sents": ["comments reveal limitations and suggest how to address some of them .", "Unlike previous methods , it generates different types of distracters designed to diagnose different types of comprehension failure , and tests comprehension not only of an individual sentence but of the context that precedes it .", "This paper describes and evaluates DQGen , which automatically generates multiple choice cloze questions to test a child ? s comprehension while reading a given text .", "We evaluate the quality of the overall questions and the individual distracters , according to 8 human judges blind to the correct answers and intended distracter types .", "The results , errors , and judges ?"]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["extracted from existing documents or from the web ; and those that seek to facilitate language acquisition by presenting the learner with exercises whose syntax is as simple as possible and whose vocabulary is restricted to that contained in the textbook being used .", "Using generation techniques , we show that a grammar can be used to semi-automatically generate grammar exercises which target a specific learning goal ; are made of short , simple sentences ; and whose vocabulary is restricted to that used in a given textbook .", "In this paper , we introduce a framework ( called GramEx ) which permits generating the second type of grammar exercises .", "Grammar exercises for language learning fall into two distinct classes : those that are based on ? real life sentences ?"]}
{"orig_sents": ["3", "4", "2", "1", "0"], "shuf_sents": ["The resulting classifiers significantly outperform the previous approaches on readability classification , reaching a classification accuracy of 93.3 % .", "We show that the developmental measures from Second Language Acquisition ( SLA ) research when combined with traditional readability features such as word length and sentence length provide a good indication of text readability across different grades .", "On the conceptual side , we explore the use of lexical and syntactic measures originally designed to measure language development in the production of second language learners .", "We investigate the problem of readability assessment using a range of lexical and syntactic features and study their impact on predicting the grade level of texts .", "As empirical basis , we combined two web-based text sources , Weekly Reader and BBC Bitesize , targeting different age groups , to cover a broad range of school grades ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["This tool helps instructors discover interesting patterns in writing performance that are reflected through peer reviews .", "It employs data visualization at multiple levels of granularity , and provides automated analytic support using clustering and natural language processing .", "This paper presents an interactive analytic tool for educational peer-review analysis ."]}
{"orig_sents": ["5", "3", "2", "0", "4", "6", "1"], "shuf_sents": ["Three different classes of features were generated based on the words in a spoken response : coverage-related , average word rank and the average word frequency and the extent to which they influence human-assigned language proficiency scores was studied .", "The contribution of the current study lies in the use of vocabulary profile as a measure of lexical sophistication for spoken language assessment , an aspect heretofore unexplored in the context of automated speech scoring .", "Focusing on vocabulary sophistication , we estimate the difficulty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difficulty level of the vocabulary usage across the responses ( vocabulary profile ) .", "vocabulary usage to improve an automated scoring system of spontaneous speech responses by non-native English speakers .", "Among these three types of features , the average word frequency showed the most predictive power .", "This study presents a method that assesses ESL learners ?", "We then explored the impact of vocabulary profile features in an automated speech scoring context , with particular focus on the impact of two factors : genre of reference corpora and the characteristics of item-types ."]}
{"orig_sents": ["1", "0", "5", "2", "3", "4"], "shuf_sents": ["They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria , among others .", "A number of different research subfields are concerned with the automatic assessment of student answers to comprehension questions , from language learning contexts to computer science exams .", "It discusses the different research strands , details the crucial differences , and explores under which circumstances systems can be compared given publicly available data .", "To that end , we present results with the CoMiC-EN Content Assessment system ( Meurers et al , 2011a ) on the dataset published by Mohler et al ( 2011 ) and outline what was necessary to perform this comparison .", "We conclude with a general discussion on comparability and evaluation of short answer assessment systems .", "This paper has the intention of fostering synergy between the different research strands ."]}
{"orig_sents": ["0", "5", "2", "1", "4", "3"], "shuf_sents": ["and Correction of Preposition and Determiner Errors in English : HOO 2012 Pinaki Bhaskar Aniruddha Ghosh Santanu Pal Sivaji Bandyopadhyay Department of Computer Science and Engineering , Jadavpur University 188 , Raja S. C. Mallick Road Kolkata ?", "For that , we have developed a hybrid system of an n-gram statistical model along with some rule-based techniques .", "The task is to automatically detect , recognize and correct the errors in the use of prepositions and determiners in a set of given test documents in English .", "We have submitted one run , which has demonstrated an F-score of 7.1 , 6.46 and 2.58 for detection , recognition and correction respectively before revision and F-score of 8.22 , 7.59 and 3.16 for detec-tion , recognition and correction respectively after revision .", "The system has been trained on the HOO shared task ? s training datasets and run on the test set given .", "700032 , India pinaki.bhaskar @ gmail.com arghyaonline @ gnail.com santanu.pal.ju @ gmail.com sivaji_cse_ju @ yahoo.com Abstract This paper reports on our work in the HOO 2012 shared task ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["We extend our n-gram-based data-driven prediction approach from the Helping Our Own ( HOO ) 2011 Shared Task ( Boyd and Meurers , 2011 ) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset ( Yannakoudakis et al , 2011 ) as part of the HOO 2012 Shared Task .", "Our system focuses on three error categories : missing determiner , incorrect determiner , and incorrect preposition .", "Approximately two-thirds of the errors annotated in HOO 2012 training and test data fall into these three categories .", "To improve our approach , we developed a missing determiner detector and incorporated word clustering ( Brown et al , 1992 ) into the n-gram prediction approach ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes the submission of the National University of Singapore ( NUS ) to the HOO 2012 shared task .", "Our system uses a pipeline of confidence-weighted linear classifiers to correct determiner and preposition errors .", "Our system achieves the highest correction F1 score on the official test set among all 14 participating teams , based on gold-standard edits both before and after revision ."]}
{"orig_sents": ["3", "2", "5", "0", "1", "4"], "shuf_sents": ["Furthermore , I employ error correction ranking based on the ratio of the sentence probabilities using original and corrected language models .", "Our system has been ranked for the ninth position out of thirteen teams .", "The task was to correct determiner and preposition errors .", "This paper describes the system has been developed for the HOO 2012 Shared Task .", "The best result was achieved in correcting missing prepositions , which was ranked for the sixth position .", "I explore the possibility of learning error correcting rules from the given manually annotated data using features such as word length and word endings only ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["Some grammatical error detection methods , including the ones currently used by the Educational Testing Service ? s e-rater system ( Attali and Burstein , 2006 ) , are tuned for precision because of the perceived high cost of false positives ( i.e. , marking fluent English as ungrammatical ) .", "On the HOO 2012 Shared Task , the hybrid method performed better than its component methods in terms of F-score , and it was competitive with submissions from other HOO 2012 participants .", "Precision , however , is not optimal for all tasks , particularly the HOO 2012 Shared Task on grammatical errors , which uses F-score for evaluation .", "In this paper , we extend e-rater ? s preposition and determiner error detection modules with a largescale n-gram method ( Bergsma et al , 2009 ) that complements the existing rule-based and classifier-based methods ."]}
{"orig_sents": ["3", "0", "2", "1", "4"], "shuf_sents": ["Nevertheless , most extant models have poor precision , particularly when attempting error correction , and this limits their usefulness in practical applications requiring feedback .", "? ve Bayes ( NB ) classifiers and develop one model which maximizes precision .", "We experiment with various feature types , varying quantities of error-corrected data , and generic versus L1-specific adaptation to typical errors using Na ?", "Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error-annotated ESL text generally outperforms training on native text alone and that adaptation of error correction models to the native language ( L1 ) of the writer improves performance .", "We report and discuss the results for 8 models , 5 trained on the HOO data and 3 ( partly ) on the full error-coded Cambridge Learner Corpus , from which the HOO data is drawn ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We focus our work on training the system on a large collection of error-tagged texts provided by the HOO 2012 Shared Task organizers and incrementally applying several methods to achieve better performance .", "In this paper , we describe the Korea University system that participated in the HOO 2012 Shared Task on the correction of preposition and determiner errors in non-native speaker texts ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The features we use include n-grams of words and POS tags together with features based on the external Google NGrams corpus .", "This is the report for the CNGL ILT team entry to the HOO 2012 shared task .", "Our system placed 11th out of 14 teams for the detection and recognition tasks and 11th out of 13 teams for the correction task based on F-score for both preposition and determiner errors .", "A NaiveBayes-based classifier was used in the task which involved error detection and correction in ESL exam scripts ."]}
{"orig_sents": ["3", "0", "5", "1", "2", "4"], "shuf_sents": ["The system employs a number of preprocessing steps and machine learning classifiers for correction of determiner and preposition errors in non-native English texts .", "The system proposes a number of highlyprobable corrections , which are evaluated by a language model and compared with the original text .", "A number of deterministic rules are used to increase the precision and recall of the system .", "In this paper we describe the technical implementation of our system that participated in the Helping Our Own 2012 Shared Task ( HOO-2012 ) .", "Our system is ranked among the three best performing HOO-2012 systems with a precision of 31.15 % , recall of 22.08 % and F1score of 25.84 % for correction of determiner and preposition errors combined .", "We use maximum entropy classifiers trained on the provided HOO-2012 development data and a large high-quality English text collection ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["The task consisted of three metrics : Detection , Recognition , and Correction , and measured performance before and after additional revisions to the test data were made .", "We describe our underlying approach , which relates to our previous work in this area , and propose an improvement to the earlier method , error inflation , which results in significant gains in performance .", "We describe the University of Illinois ( UI ) system that participated in the Helping Our Own ( HOO ) 2012 shared task , which focuses on correcting preposition and determiner errors made by non-native English speakers .", "Out of 14 teams that participated , our system scored first in Detection and Recognition and second in Correction before the revisions ; and first in Detection and second in the other metrics after revisions ."]}
{"orig_sents": ["5", "0", "2", "1", "3", "4"], "shuf_sents": ["Our system targets preposition and determiner errors with spelling correction as a pre-processing step .", "With regard to preposition error correction , F-scores were not improved when using the training set with correction of all but preposition errors .", "The result shows that spelling correction improves the Detection , Correction , and Recognition Fscores for preposition errors .", "As for determiner error correction , there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an NP were removed .", "Our system ranked third in preposition and fourth in determiner error corrections .", "This paper describes the Nara Institute of Science and Technology ( NAIST ) error correction system in the Helping Our Own ( HOO ) 2012 Shared Task ."]}
{"orig_sents": ["5", "0", "4", "1", "2", "3"], "shuf_sents": ["Our systems consists of four memory-based classifiers that generate correction suggestions for middle positions in small text windows of two words to the left and to the right .", "The second pair of classifiers determines which is the most likely correction given a masked determiner or preposition .", "The hyperparameters that govern the classifiers are optimized on the shared task training data .", "We point out a number of obvious improvements to boost the medium-level scores attained by the system .", "Trained on the Google 1TB 5gram corpus , the first two classifiers determine the presence of a determiner or a preposition between all words in a text in which the actual determiners and prepositions are masked .", "We describe the Valkuil.net team entry for the HOO 2012 Shared Task ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Our focus was to implement a highly flexible and modular system which can be easily augmented by other researchers .", "The system might be used to provide a level playground for subsequent shared tasks and enable further progress in this important research field on top of the state of the art identified by the shared task .", "In this paper , we describe the UKP Lab system participating in the HOO 2012 Shared Task on preposition and determiner error correction ."]}
{"orig_sents": ["5", "1", "3", "0", "4", "2"], "shuf_sents": ["i.e. , the semantic impact of documents on an individual reader ? s state of knowledge .", "We propose to address the task of helping readers comprehend complex technical material , by using statistical methods to model the ? prerequisite structure ?", "The features that we consider relate pairs of pages by analyzing not only textual features of the pages , but also how the containing corpora is connected and created .", "of a corpus ?", "Experimental results using Wikipedia as the corpus suggest that this task can be approached by crowdsourcing the production of ground-truth labels regarding prerequisite structure , and then generalizing these labels using a learned classifier which combines signals of various sorts .", "The growth of open-access technical publications and other open-domain textual information sources means that there is an increasing amount of online technical material that is in principle available to all , but in practice , incomprehensible to most ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["To support vocabulary acquisition and reading comprehension in a second language , we have developed a system to display senseappropriate examples to learners for difficult words .", "We also show that it helps learners , to some extent , with their reading comprehension .", "We show that sensespecific information in an intelligent reading system helps learners in their vocabulary acquisition , even if the sense information contains some noise from automatic processing .", "We describe the construction of the system , incorporating word sense disambiguation , and an experiment we conducted testing it on a group of 60 learners of English as a second language ( ESL ) ."]}
{"orig_sents": ["3", "0", "4", "2", "1"], "shuf_sents": ["The approaches typically use a combination of shallow and deep representations , but little use is made of the semantic formalisms created by theoretical linguists to represent meaning .", "The use of such a formalism also readily supports the integration of notions building on semantic distinctions , such as the information structuring in discourse , which we show to be useful for content assessment .", "We show that a content-assessment approach built on LRS outperforms a previous approach on the CREG data set , a freely available corpus of answers to reading comprehension exercises by learners of German .", "There is a rise in interest in the evaluation of meaning in real-life applications , e.g. , for assessing the content of short answers .", "In this paper , we explore the use of the underspecified semantic formalism LRS , which combines the capability of precisely representing semantic distinctions with the robustness and modularity needed to represent meaning in real-life applications ."]}
{"orig_sents": ["1", "2", "8", "4", "5", "3", "7", "0", "6"], "shuf_sents": ["In order to identify strong candidates for translation , we propose a novel part-of-speech tagging method that helps select words based on POS categories that strongly reflect code-mixing behavior .", "The main aim of this work is to perform sentiment analysis on Urdu blog data .", "We use the method of structural correspondence learning ( SCL ) to transfer sentiment analysis learning from Urdu newswire data to Urdu blog data .", "Transliteration oracle , to accommodate script variation and spelling variation and 2 .", "We consider two oracles to generate the pivots .", "1 .", "We validate our approach against a supervised learning method and show that the performance of our proposed approach is comparable .", "Translation oracle , to accommodate code-switching and code-mixing behavior .", "The pivots needed to transfer learning from newswire domain to blog domain is not trivial as Urdu blog data , unlike newswire data is written in Latin script and exhibits codemixing and code-switching behavior ."]}
{"orig_sents": ["3", "7", "5", "8", "0", "6", "4", "2", "1"], "shuf_sents": ["This paper reports on classification of distressed and non-distressed short , written excerpts from relevant web forums , using features automatically extracted from input text .", "Excerpt length contributed to interaction effects .", "Analyzing the importance of different linguistic features for this task indicates main effects of affect word list matches , pronouns , and parts of speech in the predictive model .", "haul Lehrman Cecilia Ovesdotter Alm Rub ? n A. Proa ? o Rochester Institute of Technology michael.lehrman @ alum.rit.edu coagla @ rit.edu rpmeie @ rit.edu Abstract Improving mental wellness with preventive measures can help people at risk of experiencing mental health conditions such as depression or post-traumatic stress disorder .", "The study also compares the importance of bundled linguistic super-factors with a 2k factorial model .", "Such a computational model can be useful in developing an early warning system able to analyze writing samples for signs of mental distress .", "Varying the value of k in k-fold cross-validation shows that both coarse-grained and fine-grained automatic classification of affect states are generally 20 % more accurate in detecting affect state than randomly assigning a distress label to a text .", "We describe an encouraging study on how automatic analysis of short written texts based on relevant linguistic text features can be used to identify whether the authors of such texts are experiencing distress .", "This could serve as a red flag , signaling when someone might need a professional assessment by a clinician ."]}
{"orig_sents": ["2", "3", "0", "1", "4"], "shuf_sents": ["In this paper we describe our definition of hate speech , the collection and annotation of our hate speech corpus , and a mechanism for detecting some commonly used methods of evading common ? dirty word ?", "filters .", "We present an approach to detecting hate speech in online text , where hate speech is defined as abusive speech targeting specific group characteristics , such as ethnic origin , religion , gender , or sexual orientation .", "While hate speech against any group may exhibit some common characteristics , we have observed that hatred against each different group is typically characterized by the use of a small set of high frequency stereotypical words ; however , such words may be used in either a positive or a negative sense , making our task similar to that of words sense disambiguation .", "We describe pilot classification experiments in which we classify anti-semitic speech reaching an accuracy 94 % , precision of 68 % and recall at 60 % , for an F1 measure of .6375 ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["It has long been established that there is a correlation between the dialog behavior of a participant and how influential he or she is perceived to be by other discourse participants .", "In this paper we explore the characteristics of communication that make someone an opinion leader and develop a machine learning based approach for the automatic identification of discourse participants that are likely to be influencers in online communication .", "Our approach relies on identification of three types of conversational behavior : persuasion , agreement/disagreement , and dialog patterns ."]}
{"orig_sents": ["3", "1", "5", "2", "4", "0"], "shuf_sents": ["Judicious use of # hashtags also helps to encourage retweeting .", "We study the content of tweets to uncover linguistic tendencies of shared microblog posts ( retweets ) , by examining surface linguistic features , deeper parse-based features and Twitterspecific conventions in tweet content .", "We find that both linguistic features and functional classification contribute to re-tweeting .", "What makes a tweet worth sharing ?", "Our work shows that opinion tweets favor originality and pithiness and that update tweets favor direct statements of a tweeter ? s current activity .", "We show how these features correlate with a functional classification of tweets , thereby categorizing people ? s writing styles based on their different intentions on Twitter ."]}
{"orig_sents": ["4", "0", "2", "1", "3"], "shuf_sents": ["We demonstrate the frequency and productivity of these sequences in social media such as Twitter .", "We find that these emoticons occur broadly in many languages , hence our approach is language agnostic .", "Previous approaches to detection and analysis of kaomoji have placed limits on the range of phenomena that could be detected with their method , and have looked at largely monolingual evaluation sets ( e.g. , Japanese blogs ) .", "Rather than relying on regular expressions over a predefined set of likely tokens , we build weighted context-free grammars that reward graphical affinity and symmetry within whatever symbols are used to construct the emoticon .", "In this paper , we look at the problem of robust detection of a very productive class of Asian style emoticons , known as facemarks or kaomoji ."]}
{"orig_sents": ["2", "0", "1", "4", "6", "3", "7", "5"], "shuf_sents": ["We explore the use of Twitter to obtain authentic user-generated text in low-resource languages such as Nepali , Urdu , and Ukrainian .", "Automatic language identification ( LID ) can be used to extract language-specific data from Twitter , but it is unclear how well LID performs on short , informal texts in low-resource languages .", "Social media services such as Twitter offer an immense volume of real-world linguistic data .", "We also advance the state-of-the-art by evaluating new , highly-accurate LID systems , trained both on our new corpus and on standard materials only .", "We address this question by annotating and releasing a large collection of tweets in nine languages , focusing on confusable languages using the Cyrillic , Arabic , and Devanagari scripts .", "We provide a detailed analysis showing how the accuracy of our systems vary along certain dimensions , such as the tweet-length and the amount of in- and out-of-domain training data .", "This is the first publiclyavailable collection of LID-annotated tweets in non-Latin scripts , and should become a standard evaluation set for LID systems .", "Both types of systems achieve a huge performance improvement over the existing state-of-the-art , correctly classifying around 98 % of our gold standard tweets ."]}
{"orig_sents": ["3", "1", "5", "4", "0", "2"], "shuf_sents": ["Our model combines information at the word and character levels , allowing it to handle out-of-vocabulary items .", "There are romanization conventions for some character sets , but they are used inconsistently in informal text , such as SMS .", "Compared with a baseline deterministic approach , our system reduces both word and character error rate by over 50 % .", "Regardless of language , the standard character set for text messages ( SMS ) and many other social media platforms is the Roman alphabet .", "Doing so prepares the messages for existing downstream processing tools , such as machine translation , which are typically trained on well-formed , native script text .", "In this work , we convert informal , romanized Urdu messages into the native Arabic script and normalize non-standard SMS language ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["In this paper we present the results of the analysis of a parallel corpus of original and simplified texts in Spanish , gathered for the purpose of developing an automatic simplification system for this language .", "We here concentrate on lexical simplification operations applied by human editors on the basis of which we derive a set of rules to be implemented automatically .", "The system is intended for individuals with cognitive disabilities who experience difficulties reading and interpreting informative texts .", "We have so far addressed the issue of lexical units substitution , with special attention to reporting verbs and adjectives of nationality ; insertion of definitions ; simplification of numerical expressions ; and simplification of named entities ."]}
{"orig_sents": ["2", "5", "4", "0", "1", "3"], "shuf_sents": ["We find , most importantly , that the two correlate .", "Magnitude estimation can be run on the web without supervision , and the results can be analysed automatically .", "While there has been much work on computational models to predict readability based on the lexical , syntactic and discourse properties of a text , there are also interesting open questions about how computer generated text should be evaluated with target populations .", "The sentence recall methodology is more resource intensive , but allows us to tease apart the fluency and comprehension issues that arise .", "These methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues .", "In this paper , we compare two offline methods for evaluating sentence quality , magnitude estimation of acceptability judgements and sentence recall ."]}
{"orig_sents": ["2", "5", "0", "1", "3", "4"], "shuf_sents": ["This study explores the relation between text readability and the visual conceptual schemes which aim to make the text more clear for these specific target readers .", "Our results are based on a user study for Spanish native speakers through a group of twenty three dyslexic users and a control group of similar size .", "Generally , people with dyslexia are poor readers but strong visual thinkers .", "The data collected from our study combines qualitative data from questionnaires and quantitative data from tests carried out using eye tracking .", "The findings suggest that graphical schemes may help to improve readability for dyslexics but are , unexpectedly , counterproductive for understandability .", "The use of graphical schemes for helping text comprehension is recommended in education manuals ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["In this work , we explore automatic creation of these lexicons using methods which go beyond simple term frequency , but without relying on age-graded texts .", "We show the efficacy of this approach by comparing our lexicon with an existing coarse-grained , low-coverage resource and a new crowdsourced annotation .", "Lexicons of word difficulty are useful for various educational applications , including readability classification and text simplification .", "In particular , we derive information for each word type from the readability of the web documents they appear in and the words they co-occur with , linearly combining these various features ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["We address this issue by presenting a system that , for a given document in Italian , provides not only a list of readability indices inspired by CohMetrix , but also a graphical representation of the difficulty of the text compared to the three levels of Italian compulsory education , namely elementary , middle and high-school level .", "Although many approaches have been presented to compute and predict readability of documents in different languages , the information provided by readability systems often fail to show in a clear and understandable way how difficult a document is and which aspects contribute to content readability .", "In addition , we present the first available system for readability assessment of Italian inspired by Coh-Metrix .", "We believe that this kind of representation makes readability assessment more intuitive , especially for educators who may not be familiar with readability predictions via supervised classification ."]}
{"orig_sents": ["4", "1", "2", "5", "8", "7", "11", "9", "10", "3", "6", "0"], "shuf_sents": ["features resulted in a significant gain in performance .", "reading level .", "Several methodological paradigms have previously been investigated in the field .", "features ; ( 3 ) modern machine learning algorithms did not improve the explanatory power of our readability model , but allowed to better classify new observations ; and ( 4 ) combining ? classic ?", "Readability formulas are methods used to match texts with the readers ?", "The most popular paradigm dates several decades back and gave rise to well known readability formulas such as the Flesch formula ( among several others ) .", "and ? non-classic ?", "with an emerging paradigm which uses sophisticated NLPenabled features and machine learning techniques .", "This paper compares this approach ( henceforth ? classic ? )", "formula ; ( 2 ) ? non-classic ?", "features were slightly more informative than ? classic ?", "Our experiments , carried on a corpus of texts for French as a foreign language , yield four main results : ( 1 ) the new readability formula performed better than the ? classic ?"]}
{"orig_sents": ["3", "2", "0", "5", "1", "4"], "shuf_sents": ["However , the previous methodology for using ? found ?", "In this work , we ask whether the annotation process can be automated , and also experiment with richer syntactic features found in the literature that can be automatically derived from either the humancorrected or raw OCR text .", "In previous work ( Ma et al , 2012 ) , we suggested that the fine-grained assessment task can be approached using a ranking methodology , and incorporating features that correspond to the visual layout of the page improves performance .", "Early primary children ? s literature poses some interesting challenges for automated readability assessment : for example , teachers often use fine-grained reading leveling systems for determining appropriate books for children to read ( many current systems approach readability assessment at a coarser whole grade level ) .", "We find that automated visual and text feature extraction work reasonably well and can allow for scaling to larger datasets , but that in our particular experiments the use of syntactic features adds little to the performance of the system , contrary to previous findings .", "text ( e.g. , scanning in a book from the library ) requires human annotation of the text regions and correction of the OCR text ."]}
{"orig_sents": ["0", "4", "2", "3", "1"], "shuf_sents": ["Most tools and resources developed for natural language processing of Arabic are designed for Modern Standard Arabic ( MSA ) and perform terribly on Arabic dialects , such as Egyptian Arabic .", "It accepts multiple orthographic variants and normalizes them to a conventional orthography .", "We present a linguistically accurate , large-scale morphological analyzer for Egyptian Arabic .", "The analyzer extends an existing resource , the Egyptian Colloquial Arabic Lexicon , and follows the part-of-speech guidelines used by the Linguistic Data Consortium for Egyptian Arabic .", "Egyptian Arabic differs from MSA phonologically , morphologically and lexically and has no standardized orthography ."]}
{"orig_sents": ["6", "5", "4", "3", "1", "0", "2"], "shuf_sents": ["First , it successfully incorporates derivational analysis in the inflectional analyzer .", "Our algorithm upgrades an existing inflectional analyzer to a derivational analyzer and primarily achieves two goals .", "Second , it also increases the coverage of the inflectional analysis of the existing inflectional analyzer .", "In this paper , we present our Hindi derivational morphological analyzer .", "However , they give only inflectional analysis of the language .", "A few morphological analyzers of this language have been developed .", "Hindi is an Indian language which is relatively rich in morphology ."]}
{"orig_sents": ["6", "3", "5", "2", "0", "1", "4"], "shuf_sents": ["This paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations , the dependencies of which are also taken into account .", "The model has a good OOV recognition ability , which improves the overall performance significantly .", "The traditional tokenization model is efficient but inaccurate .", "19 , T ? bingen , 72074 , Germany Wilhelmstr .", "The training is a linear time phrase extraction and MLE procedure , while the decoding is via dynamic programming based algorithms .", "19 , T ? bingen , 72074 , Germany jma @ sfs.uni-tuebingen.de dg @ sfs.uni-tuebingen.de Abstract Fast re-training of word segmentation models is required for adapting to new resources or domains in NLP of many Asian languages without word delimiters .", "sed Approach for Adaptive Tokenization Jianqiang Ma Dale Gerdemann Department of Linguistics University of T ? bingen Department of Linguistics University of T ? bingen Wilhelmstr ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["Preliminary test results on the Bernstein-Ratner corpus and Bakeoff-2005 show that the our method is comparable to the state-of-the-art in terms of effectiveness and efficiency .", "We reduce this process to an optimization problem , and propose a greedy inclusion solution .", "Languages are constantly evolving through their users due to the need to communicate more efficiently .", "Under this hypothesis , we formulate unsupervised word segmentation as a regularized compression process ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["The performance of the approach is tested on the NEMLAR Written Arabic Corpus .", "This paper describes a small experiment to test a rule-based approach to unknown word recognition in Arabic .", "The morphological complexity of Arabic presents its challenges to a variety of NLP applications , but it can also be viewed as an advantage , if we can tap into the complex linguistic knowledge associated with these complex forms .", "In particular , the derived forms of verbs can be analysed and an educated guess at the likely meaning of a derived form can be predicted , based on the meaning of a known form and the relationship between the known form and the unknown one ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper first defines the conditions under which copying and deletion processes are subsequential : specifically this is the case when the process is bounded in the right ways .", "Then , if we analyze metathesis as the composition of copying and deletion , it can be shown that the set of attested metathesis patterns fall into the subsequential or reverse subsequential classes .", "The implications of bounded copying are extended to partial reduplication , which is also shown to be either subsequential or reverse subsequential ."]}
{"orig_sents": ["1", "3", "0", "4", "5", "2"], "shuf_sents": ["Our objective function incorporates a term aimed to ensure generalization , independently required for phonotactic learning in Optimality Theory , and does not have a bias for single URs for morphemes .", "We show that a class of cases that has been previously studied in terms of learning of abstract phonological underlying representations ( URs ) can be handled by a learner that chooses URs from a contextually conditioned distribution over observed surface representations .", "This case includes lexically conditioned variation , an aspect of the data that can not be handled by abstract URs , showing that in this respect our approach is more general .", "We implement such a learner in a Maximum Entropy version of Optimality Theory , in which UR learning is an instance of semisupervised learning .", "This learner is successful on a test language provided by Tesar ( 2006 ) as a challenge for UR learning .", "We also provide successful results on learning of a toy case modeled on French vowel alternations , which have also been previously analyzed in terms of abstract URs ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["It also models the effect of labeling ( positive and negative labels vs. positive labels of two different kinds ) on category complexity .", "In particular , this learner predicts the order of difficulty of learning simple Boolean categories , including the advantage of conjunctive categories over the disjunctive ones ( an advantage that is not typically modeled by the statistical approaches ) .", "This effect has implications for the differences between learning a single category ( e.g. , a phonological class of segments ) vs. a set of non-overlapping categories ( e.g. , affixes in a morphological paradigm ) .", "This paper presents a memoryless categorization learner that predicts differences in category complexity found in several psycholinguistic and psychological experiments ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["Narrative recall tasks are widely used in neuropsychological evaluation protocols in order to detect symptoms of disorders such as autism , language impairment , and dementia .", "The significant reduction in alignment error rate ( AER ) afforded by the graph-based method results in improved automatic scoring and diagnostic classification .", "The approach described here is general enough to be applied to almost any narrative recall scenario , and the reductions in AER achieved in this work attest to the potential utility of this graph-based method for enhancing multilingual word alignment and alignment of comparable corpora for more standard NLP tasks .", "From these alignments , we automatically extract narrative recall scores which can then be used for diagnostic screening .", "In this paper , we propose a graph-based method commonly used in information retrieval to improve word-level alignments in order to align a source narrative to narrative retellings elicited in a clinical setting ."]}
{"orig_sents": ["7", "1", "2", "6", "4", "3", "5", "8", "0"], "shuf_sents": ["Experimental results show significant improvements over NELL ? s original bootstrapping algorithm on two types of tasks : learning terms from biomedical categories , and named-entity recognition for biomedical entities using a learned lexicon .", "NELL uses a coupled semi-supervised bootstrapping approach to learn new facts from text , given an initial ontology and a small number of ? seeds ?", "for each ontology category .", "Using NELL to extract facts from biomedical text quickly leads to semantic drift .", "We show that NELL ? s bootstrapping algorithm is susceptible to ambiguous seeds , which are frequent in the biomedical domain .", "To address this problem , we introduce a method for assessing seed quality , based on a larger corpus of data derived from the Web .", "In contrast to previous applications of NELL , in our task the initial ontology and seeds are automatically derived from existing resources .", "We describe an open information extraction system for biomedical text based on NELL ( the Never-Ending Language Learner ) ( Carlson et al , 2010 ) , a system designed for extraction from Web text .", "In our method , seed quality is assessed at each iteration of the bootstrapping process ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["Two approaches are exploited : semantic distance within structured resources and terminology structuring methods applied to a raw list of terms .", "The identification of semantically similar linguistic expressions despite their formal difference is an important task within NLP applications ( information retrieval and extraction , terminology structuring ... ) We propose to detect the semantic relatedness between biomedical terms from the pharmacovigilance area .", "We compare these methods and study their complementarity .", "The results are evaluated against the reference pharmacovigilance data and manually by an expert ."]}
{"orig_sents": ["0", "1", "4", "6", "3", "5", "2"], "shuf_sents": ["We investigate the task of assigning medical events in clinical narratives to discrete time-bins .", "The time-bins are defined to capture when a medical event occurs relative to the hospital admission date in each clinical narrative .", "We observe over 8 % improvement in overall tagging accuracy with the inclusion of sequence information .", "The sequence tagging system outperforms a system that does not utilize any sequence information modeled using a Maximum Entropy classifier .", "We model the problem as a sequence tagging task using Conditional Random Fields .", "We present results with both handtagged as well as automatically extracted features .", "We extract a combination of lexical , section-based and temporal features from medical events in each clinical narrative ."]}
{"orig_sents": ["3", "1", "9", "0", "2", "5", "6", "4", "8", "7"], "shuf_sents": ["One aspect of this understanding is knowing if a medical condition outlined in a patient record is recent , or if it occurred in the past .", "Promising application areas include Information Retrieval and Question-Answering systems .", "As well as this , patient records often discuss other individuals such as family members .", "The growth of digital clinical data has raised questions as to how best to leverage this data to aid the world of healthcare .", "Our results show that our novel Score-based feature approach outperforms the standard Linguistic and Contextual features described in the related literature .", "This presents a second problem - determining if a medical condition is experienced by the patient described in the report or some other individual .", "In this paper , we investigate the suitability of a machine learning ( ML ) based system for resolving these tasks on a previously unexplored collection of Patient History and Physical Examination reports .", "While for the task of establishing when a patient experienced a condition , our ML system significantly outperforms the ConText system ( 87 % versus 69 % f-score , respectively ) .", "Specifically , near-perfect performance is achieved in resolving if a patient experienced a condition .", "Such systems require an in-depth understanding of the texts that are processed ."]}
{"orig_sents": ["2", "0", "3", "5", "4", "1"], "shuf_sents": ["Our approach is based on an alignment HMM , matching abbreviations and their definitions .", "Using the abbreviation alignment model we were able to extract over 1.4 million abbreviations from a corpus of 200K full-text PubMed papers , including 455,844 unique definitions .", "We present an algorithm for extracting abbreviation definitions from biomedical text .", "We report 98 % precision and 93 % recall on a standard data set , and 95 % precision and 91 % recall on an additional test set .", "Our model : ( 1 ) is simpler and faster than a comparable alignment-based abbreviation extractor ; ( 2 ) is naturally generalizable to specific types of abbreviations , e.g. , abbreviations of chemical formulas ; ( 3 ) is trained on a set of unlabeled examples ; and ( 4 ) associates a probability with each predicted definition .", "Our results show an improvement over previously reported methods and our model has several advantages ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We define a clinical uncertainty and negation taxonomy and a translation map for converting annotation labels between two schemas and report observed similarities and differences between the two data sets .", "However , few studies have made an in-depth characterization of uncertainties expressed in a clinical setting , and compared this between different annotation efforts .", "This preliminary , qualitative study attempts to 1 ) create a clinical uncertainty and negation taxonomy , 2 ) develop a translation map to convert annotation labels from an English schema into a Swedish schema , and 3 ) characterize and compare two data sets using this taxonomy .", "In the English clinical and biomedical text domains , negation and certainty usage are two well-studied phenomena ."]}
{"orig_sents": ["2", "4", "0", "3", "1"], "shuf_sents": ["To overcome this task , our system comprises several components designed to accomplish two separate goals : 1 ) achieve the highest re-call ( no patient data can be exposed ) ; and 2 ) create methods to filter out false positives .", "de-identification or named entity recognition systems .", "Stepwise Approach for De-identifying Person Names in Clinical Documents Oscar Ferr ? ndez1,2 , Brett R. South1,2 , Shuying Shen1,2 , St ? phane M. Meystre1,2 1 Department of Biomedical Informatics , University of Utah , Salt Lake City , Utah , USA 2 IDEAS Center SLCVA Healthcare System , Salt Lake City , Utah , USA oscar.ferrandez @ utah.edu , { brett.south , shuying.shen , stephane.meystre } @ hsc.utah.edu Abstract As Electronic Health Records are growing ex-ponentially along with large quantities of un-structured clinical information that could be used for research purposes , protecting patient privacy becomes a challenge that needs to be met .", "As a result , our system reached 92.6 % F2-measure when de-identifying person names in Veteran ? s Health Administration clinical notes , and considerably outperformed other existing ? out-of-the-box ?", "In this paper , we present a novel hybrid system designed to improve the current strate-gies used for person names de-identification ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["Active learning can lower the cost of annotation for some natural language processing tasks by using a classifier to select informative instances to send to human annotators .", "However , coreference annotations often require some context and the traditional active learning approach may not be feasible .", "In this work we explore various active learning methods for coreference resolution that fit more realistically into coreference annotation workflows .", "It has worked well in cases where the training instances are selected one at a time and require minimal context for annotation ."]}
{"orig_sents": ["6", "0", "3", "4", "2", "1", "5"], "shuf_sents": ["The BioNLP ? 11 Shared Task extended the event extraction approach to sub-protein events and relations in the Epigenetics and Post-translational Modifications ( EPI ) and Protein Relations ( REL ) tasks .", "The extraction of such detailed protein information provides a unique text mining dataset , offering the opportunity to further deepen the information provided by existing PubMed-scale event extraction efforts .", "This normalization effort allows direct mapping of the extracted events and relations with posttranslational modifications from UniProt , epigenetics from PubMeth , functional domains from InterPro and macromolecular structures from PDB .", "In this study , we apply the Turku Event Extraction System , the best-performing system for these tasks , to all PubMed abstracts and all available PMC full-text articles , extracting 1.4M EPI events and 2.2M REL relations from 21M abstracts and 372K articles .", "We introduce several entity normalization algorithms for genes , proteins , protein complexes and protein components , aiming to uniquely identify these biological entities .", "The methods and data introduced in this study are freely available from bionlp.utu.fi .", "Recent efforts in biomolecular event extraction have mainly focused on core event types involving genes and proteins , such as gene expression , protein-protein interactions , and protein catabolism ."]}
{"orig_sents": ["8", "1", "9", "5", "3", "6", "0", "7", "4", "2"], "shuf_sents": ["Disease mentions are categorized into Specific Disease , Disease Class , Composite Mention and Modifier categories .", "However , with the rapid growth of the biomedical literature and a high level of variation and ambiguity in disease names , the task of retrieving disease-related articles becomes increasingly challenging using the traditional keywordbased approach .", "The NCBI corpus is available for download at http : //www.ncbi.nlm.nih.gov/CBBresearch/Fe llows/Dogan/disease.html .", "Towards this aim , we created a large-scale disease corpus consisting of 6900 disease mentions in 793 PubMed citations , derived from an earlier corpus .", "Such characteristics make this disease name corpus a valuable resource for mining disease-related information from biomedical text .", "However , despite the strong interest , there has not been enough work done on disease name identification , perhaps because of the difficulty in obtaining adequate corpora .", "Our corpus contains rich annotations , was developed by a team of 12 annotators ( two people per annotation ) and covers all sentences in a PubMed abstract .", "When used as the gold standard data for a state-of-the-art machine-learning approach , significantly higher performance can be found on our corpus than the previous one .", "The latest discoveries on diseases and their diagnosis/treatment are mostly disseminated in the form of scientific publications .", "An important first step for any disease-related information extraction task in the biomedical literature is the disease mention recognition task ."]}
{"orig_sents": ["1", "3", "5", "2", "6", "4", "0"], "shuf_sents": ["representation and evaluation to complement the mentionlevel evaluations pursued in most recent work .", "Event extraction is a major focus of recent work in biomedical information extraction .", "In providing for the first time the outputs of a broad set of state-ofthe-art event extraction systems , this resource opens many new opportunities for studying aspects of event extraction , from the identification of common errors to the study of effective approaches to combining the strengths of systems .", "Despite substantial advances , many challenges still remain for reliable automatic extraction of events from text .", "We further argue for new perspectives to the performance evaluation of domain event extraction systems , considering a document-level , ? off-the-page ?", "We introduce a new biomedical event extraction resource consisting of analyses automatically created by systems participating in the recent BioNLP Shared Task ( ST ) 2011 .", "We demonstrate these opportunities through a multi-system analysis on three BioNLP ST 2011 main tasks , focusing on events that none of the systems can successfully extract ."]}
{"orig_sents": ["2", "0", "4", "1", "3"], "shuf_sents": ["However , their validity should also be computed and indicated , especially for automatic systems and applications .", "We then apply pagerank-derived algorithm to the obtained semantic graph in order to filter out the acquired synonyms .", "The acquisition of semantic resources and relations is an important task for several applications , such as query expansion , information retrieval and extraction , machine translation .", "Evaluation performed with two independent experts indicates that the quality of synonyms is systematically improved by 10 to 15 % after their filtering .", "We exploit the compositionality based methods for the acquisition of synonymy relations and of indicators of these synonyms ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We reformulated the task and developed an unsupervised algorithm based on heuristics for coreference resolution in radiology reports .", "In this paper we explore the applicability of existing coreference resolution systems to a biomedical genre : radiology reports .", "The algorithm is shown to perform well on a test dataset of 150 manually annotated radiology reports .", "Analysis revealed that , due to the idiosyncrasies of the domain , both the formulation of the problem of coreference resolution and its solution need significant domain adaptation work ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The data we analyzed was the proceedings from last decade of BioNLP workshops .", "Our findings reveal the prominent research problems and techniques in the field , their progression over time , the approaches that researchers are using to solve those problems , insightful ways to categorize works in the field , and the prominent researchers and groups whose works are influencing the field .", "The goal of this work is to apply NLP techniques to the field of BioNLP in order to gain a better insight into the field and show connections and trends that might not otherwise be apparent ."]}
{"orig_sents": ["5", "0", "2", "1", "3", "4"], "shuf_sents": ["Although a variety of open source annota-tion tools currently exist , there is a clear opportunity to develop new tools and assess functionalities that introduce efficiencies into the process of generating reference standards .", "The goals of reducing annotator workload and improving the quality of reference standards are important considerations for development of new tools .", "These features include : man-agement of document corpora and batch as-signment , integration of machine-assisted verification functions , semi-automated cu-ration of annotated information , and sup-port of machine-assisted pre-annotation .", "An infrastruc-ture is also needed that will support large-scale but secure annotation of sensitive clinical data as well as crowdsourcing which has proven successful for a variety of annotation tasks .", "We introduce the Ex-tensible Human Oracle Suite of Tools ( eHOST ) http : //code.google.com/p/ehost that provides such functionalities that when coupled with server integration offer an end-to-end solution to carry out small or large scale as well as crowd sourced anno-tation projects .", "Manually annotating clinical document corpora to generate reference standards for Natural Language Processing ( NLP ) sys-tems or Machine Learning ( ML ) is a time-consuming and labor-intensive endeavor ."]}
{"orig_sents": ["4", "2", "1", "3", "5", "0"], "shuf_sents": ["The goal for this paper is to introduce MedLingMap to the community and show how it can be a power-ful tool for research and exploration in the field .", "( www.medlingmap.org ) which is a compila-tion of references with a multi-faceted index .", "In order to make sense of how these fields relate and intersect , we have created ? MedLingMap ?", "The initial focus has been creating the infra-structure and populating it with references an-notated with facets such as topic , resources used ( ontologies , tools , corpora ) , and organi-zations .", "eer , Bensiin Borukhov , Michael Crivaro , Michael Shafir , Attapol Thamrongrattanarit { mmeteer , bborukhov , mcrivaro , mshafir , tet } @ brandeis.edu Department of Computer Science Brandeis University Waltham , MA 02453 , USA Abstract The application of natural language process-ing ( NLP ) in the biology and medical domain crosses many fields from Healthcare Informa-tion to Bioinformatics to NLP itself .", "Simultaneously we are applying NLP techniques to the text to find clusters , key terms and other relationships ."]}
{"orig_sents": ["0", "2", "3", "1", "4", "6", "5"], "shuf_sents": ["Many genetic epidemiological studies of human diseases have multiple variables related to any given phenotype , resulting from different definitions and multiple measurements or subsets of data .", "That algorithm learns to determine whether a pair of phenotypes is in the same class .", "Manually mapping and harmonizing these phenotypes is a timeconsuming process that may still miss the most appropriate variables .", "Previously , a supervised learning algorithm was proposed for this problem .", "Though that algorithm accomplished satisfying F-scores , the need to manually label training examples becomes a bottleneck to improve its coverage .", "Active learning will make phenotype mapping more efficient and improve its accuracy .", "Herein we present a novel active learning solution to solve this challenging phenotype-mapping problem ."]}
{"orig_sents": ["4", "2", "1", "0", "3"], "shuf_sents": ["The topic coherence of the emergent topics and the ability of the model to retrieve relevant scientific articles and proteins related to the topic are compared to that of a text-only approach that does not make use of the protein-protein interaction matrix .", "articles and yeast protein-protein interaction networks .", "We evaluate Block-LDA in the yeast biology domain by jointly modeling PubMed R ?", "Evaluation of the results by biologists show that the joint modeling results in better topic coherence and improves retrieval performance in the task of identifying top related papers and proteins .", "Block-LDA is a topic modeling approach to perform data fusion between entity-annotated text documents and graphs with entity-entity links ."]}
{"orig_sents": ["3", "5", "1", "2", "0", "4"], "shuf_sents": ["We show that such simplification improves the results by up to 13 % .", "Features to capture how the relationship is described textually , as well as how central the relationship is in the sentence , are used in the learning process .", "Simplification of complex sentences into simple structures is also applied for the extraction of the features .", "This paper presents a machine learning approach that selects and , more generally , ranks sentences containing clear relations between genes and terms that are related to them .", "We conducted three different evaluations and we found that the system significantly outperforms the baselines .", "This is treated as a binary classification task , where preference judgments are used to learn how to choose a sentence from a pair of sentences ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["The relationship between small molecules and proteins has attracted attention from the biomedical research community .", "The results show the feasibility of the bootstrapping system , which will subsequently be further investigated and improved .", "The technique has been applied to the complete collection of MEDLINE abstracts and pairs were extracted and evaluated .", "In this paper a text mining method of extracting smallmolecule and protein pairs from natural text is presented , based on a semi-supervised machine learning approach ."]}
{"orig_sents": ["1", "2", "3", "4", "0"], "shuf_sents": ["With the feature sets extracted from publication types , Medical Subject Headings ( MeSH ) , title , and body of the abstracts , we obtain a 73.77 % grading accuracy with a stacking based approach , a considerable improvement over previous work .", "Evidence Based Medicine ( EBM ) is the practice of using the knowledge gained from the best medical evidence to make decisions in the effective care of patients .", "This medical evidence is extracted from medical documents such as research papers .", "The increasing number of available medical documents has imposed a challenge to identify the appropriate evidence and to access the quality of the evidence .", "In this paper , we present an approach for the automatic grading of evidence using the dataset provided by the 2011 Australian Language Technology Association ( ALTA ) shared task competition ."]}
{"orig_sents": ["1", "6", "3", "4", "2", "5", "0"], "shuf_sents": ["The experimental results show that the proposed approach outperforms BANNER as a stand-alone classifier for newly annotated sets as well as previous gold-standard sets .", "Gene name identification is a fundamental step to solve more complicated text mining problems such as gene normalization and protein-protein interactions .", "Wellknown named entity tools use similar goldstandard sets for training and testing , which results in relatively poor performance for unknown sets .", "In this regard , a relaxed task , gene/protein sentence identification , may serve more effectively for manually searching and browsing biomedical literature .", "In this paper , we set up a new task , gene/protein sentence classification and propose an ensemble approach for addressing this problem .", "We here explore how to combine diverse high-precision gene identifiers for more robust performance .", "However , state-ofthe-art name identification methods are not yet sufficient for use in a fully automated system ."]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["Datasets that answer difficult clinical questions are expensive in part due to the need for medical expertise and patient informed consent .", "We show that normal vectors of decision hyperplanes can be used for assessing reliability and internal cross-validation can be used for assessing stability of small sample data .", "We investigate the effect of small sample size on the performance of a text categorization algorithm .", "We show how to determine whether the dataset is large enough to train support vector machines .", "Since it is not possible to cover all aspects of sample size calculation in one manuscript , we focus on how certain types of data relate to certain properties of support vector machines ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["As those resources accumulate , a new issue arises about the reusability .", "There has been an active development of corpora and annotations in the BioNLP community .", "As a position paper , it explains the motivation and the core concepts of the repository and presents a prototype repository as a proof-of-concept .", "As a solution to improve the reusability of corpora and annotations , we present PubAnnotation , a persistent and sharable repository , where various corpora and annotations can be stored together in a stable and comparable way ."]}
{"orig_sents": ["4", "5", "0", "2", "1", "3"], "shuf_sents": ["To develop such an algorithm , we created a corpus of Federal Drug Administration approved drug package insert statements that have been manually annotated for pharmacokinetic DDIs by a pharmacist and a drug information expert .", "Experiments found that a support vector machine algorithm performed best on both tasks with an F-measure of 0.859 for pharmacokinetic DDI identification and 0.949 for modality assignment .", "We then evaluated three different machine learning algorithms for their ability to 1 ) identify pharmacokinetic DDIs in the package insert corpus and 2 ) classify pharmacokinetic DDI statements by their modality ( i.e. , whether they report a DDI or no interaction between drug pairs ) .", "We also found that the use of syntactic information is very helpful for addressing the problem of sentences containing both interacting and non-interacting pairs of drugs .", "The package insert ( aka drug product label ) is the only publicly-available source of information on drug-drug interactions ( DDIs ) for some drugs , especially newer ones .", "Thus , an automated method for identifying DDIs in drug package inserts would be a potentially important complement to methods for identifying DDIs from other sources such as the scientific literature ."]}
{"orig_sents": ["2", "0", "6", "8", "1", "7", "4", "3", "5"], "shuf_sents": ["Natural language processing ( NLP ) methods can be very useful for automatically extracting knowledge such as gene-drug interactions , offering researchers immediate access to published findings , and allowing curators a shortcut for their work .", "We evaluated basic approaches to automatic extraction , including gene and drug cooccurrence , co-occurrence plus interaction terms , and a linguistic pattern-based method .", "Publications that report genotype-drug interaction findings , as well as manually curated databases such as DrugBank and PharmGKB are essential to advancing pharmacogenomics , a relatively new area merging pharmacology and genomic research .", "Co-occurrence is a reasonable baseline method , with pattern-based being a promising approach if enough patterns can be generated to address recall .", "Basic co-occurrence yields 68.99 % precision , with the addition of an interaction term precision increases slightly ( 69.60 % ) , though not as much as could be expected .", "The corpus is available at http : //diego.asu.edu/index.php/projects", "We present a corpus of gene-drug interactions for evaluating and training systems to extract those interactions .", "The linguistic pattern method had the highest precision ( 96.61 % ) but lowest recall ( 7.30 % ) , for an f-score of 13.57 % .", "The corpus includes 551 sentences that have a mention of a drug and a gene from about 600 journals found to be relevant to pharmacogenomics through an analysis of gene-drug relationships in the PharmGKB knowledgebase ."]}
{"orig_sents": ["2", "1", "6", "3", "4", "7", "0", "5"], "shuf_sents": ["The feasibility and effectiveness of the methods and reasonable features are verified , and several interesting and helpful results are shown .", "This problem is viewed as labeling each character in FCRs of TCM with a pre-defined tag ( ? B-SYC ? , ? I-SYC ?", "A preliminary work on symptom name recognition from free-text clinical records ( FCRs ) of traditional Chinese medicine ( TCM ) is depicted in this paper .", "to indicate the character ? s role ( a beginning , inside or outside part of a symptom name ) .", "The task is handled by Conditional Random Fields ( CRFs ) based on two types of features .", "A detailed analysis for recognizing symptom names from FCRs of TCM is presented through analyzing labeling results of CRFs .", "or ? OSYC ? )", "The symptom name recognition FMeasure can reach up to 62.829 % with recognition rate 93.403 % and recognition error rate 52.665 % under our experiment settings ."]}
{"orig_sents": ["2", "0", "5", "1", "4", "3"], "shuf_sents": ["However , these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms .", "This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus .", "The most accurate approaches to Word Sense Disambiguation ( WSD ) for biomedical documents are based on supervised learning .", "The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus .", "The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised ( unlike some previous approaches ) .", "An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The results show that the performance changes signicantly according to the choice of text collection where the training samples to bootstrap , and that an improvement can be obtained only with a well chosen text collection .", "The paper reports a series of experimental results of bootstrapping for protein name recognition .", "When only a small amount of manually annotated data is available , application of a bootstrapping method is often considered to compensate for the lack of sufcient training material for a machine-learning method ."]}
{"orig_sents": ["2", "5", "3", "1", "0", "4"], "shuf_sents": ["Unfortunately , the less successful story indicates that state-of-the-art coreference resolution systems fail to achieve high accuracy for this genre of discourse .", "that the narrator ? s choice of referring expression depends on the amount of time that elapsed between events in a story .", "This paper discusses successes and failures of computational linguistics techniques in the study of how inter-event time intervals in a story affect the narrator ? s use of different types of referring expressions .", "based on manual coding ?", "Fine-grained analyses of these failures provide insight into the limitations of current coreference resolution systems , and ways of improving them .", "The success story shows that a conditional frequency distribution analysis of proper nouns and pronouns yields results that are consistent with our previous results ?"]}
{"orig_sents": ["5", "1", "2", "4", "0", "3"], "shuf_sents": ["This result highlights the influence of Imagism in contemporary professional poetry , and suggests that concreteness may be one of the most appealing features of poetry to the modern aesthetic .", "We use computational methods to compare the stylistic and content features employed by awardwinning poets and amateur poets .", "Building upon existing techniques designed to quantitatively analyze style and affect in texts , we examined elements of poetic craft such as diction , sound devices , emotive language , and imagery .", "We also report on other features that characterize high-quality poetry and argue that methods from computational linguistics may provide important insights into the analysis of beauty in verbal art .", "Results showed that the most important indicator of high-quality poetry we could detect was the frequency of references to concrete objects .", "What makes a poem beautiful ?"]}
{"orig_sents": ["1", "4", "0", "5", "2", "3"], "shuf_sents": ["Eliot , which is traditionally analyzed in terms of numerous voices which appear throughout the text .", "The identification of stylistic inconsistency is a challenging task relevant to a number of genres , including literature .", "We show that this extrinsic information is more useful than ( within-text ) distributional features .", "We achieve well above baseline performance on both artificial mixed-style texts and The Waste Land itself .", "In this work , we carry out stylistic segmentation of a well-known poem , The Waste Land by T.S .", "Our method , adapted from work in topic segmentation and plagiarism detection , predicts breaks based on a curve of stylistic change which combines information from a diverse set of features , most notably co-occurrence in larger corpora via reduced-dimensionality vectors ."]}
{"orig_sents": ["1", "0", "2", "3", "4"], "shuf_sents": ["It is often the case that literary works exist in more than one language , suggesting that , if properly aligned , they could be turned into useful resources for many practical applications , such as writing and language learning aids , translation studies , or data-based machine translation .", "Electronic versions of literary works abound on the Internet and the rapid dissemination of electronic readers will make electronic books more and more common .", "To be of any use , these bilingual works need to be aligned as precisely as possible , a notoriously difficult task .", "In this paper , we revisit the problem of sentence alignment for literary works and explore the performance of a new , multi-pass , approach based on a combination of systems .", "Experiments conducted on excerpts of ten masterpieces of the French and English literature show that our approach significantly outperforms two open source tools ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This study explores the use of function words for authorship attribution in modern Chinese ( C-FWAA ) .", "This study consists of three tasks : ( 1 ) examine the C-FWAA effectiveness in three genres : novel , essay , and blog ; ( 2 ) compare the strength of function words as both genre and authorship indicators , and explore the genre interference on C-FWAA ; ( 3 ) examine whether C-FWAA is sensitive to the time periods when the texts were written ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["Simple text classification algorithms perform remarkably well when used for detecting famous quotes in literary or philosophical text , with f-scores approaching 95 % .", "We compare the task to topic classification , polarity classification and authorship attribution ."]}
{"orig_sents": ["1", "5", "2", "3", "4", "0"], "shuf_sents": ["Our experiments show that the structural information from fragments provides complementary information to the baseline trigram model .", "We present a method of authorship attribution and stylometry that exploits hierarchical information in phrase-structures .", "Texts are parsed to obtain phrase-structures , and compared with texts to be analyzed .", "An efficient tree kernel method identifies common tree fragments among data of known authors and unknown texts .", "These fragments are then used to identify authors and characterize their styles .", "Contrary to much previous work in stylometry , we focus on content words rather than function words ."]}
{"orig_sents": ["0", "4", "1", "2", "3"], "shuf_sents": ["We compare four methods for transcribing early printed texts .", "? tiennes by Jose ? phine de Monbart .", "We provide a detailed error analysis of transcription by optical character recognition ( OCR ) , non-expert humans , and expert humans and weigh each technique based on accuracy , speed , cost and the need for scholarly overhead .", "Our findings are relevant to 18th-century French scholars as well as the entire community of scholars working to preserve , present , and revitalize interest in literature published before the digital age .", "Our comparison is through a case-study of digitizing an eighteenthcentury French novel for a new critical edition : the 1784 Lettres ta ?"]}
{"orig_sents": ["4", "2", "0", "3", "1"], "shuf_sents": ["These quotable phrases are memorable and succinct statements that people are likely to find useful outside of their original context .", "A study using a reddit community of quote enthusiasts as well as a simple corpus analysis further demonstrate the practical applications of our work .", "While research on largescale analysis of text reuse has found effective methods for detecting widely disseminated and famous quotations , this paper explores the complementary problem of detecting , from internal evidence alone , which phrases are quotable .", "We evaluate quotable phrase extraction using a large digital library and demonstrate that an integration of lexical and shallow syntactic features results in a reliable extraction process .", "Readers suffering from information overload have often turned to collections of pithy and famous quotations ."]}
{"orig_sents": ["5", "1", "2", "4", "0", "7", "6", "3"], "shuf_sents": ["The Quranic Arabic PropBank ( QAPB ) will be a unique new resource of its kind for the Arabic NLP research community as it will allow for interesting insights into the semantic use of classical Arabic , poetic literary Arabic , as well as significant religious texts .", "Accordingly it is significantly richer and more complex than the newswire style used in the previously released Arabic PropBank ( Zaghouani et al , 2010 ; Diab et al , 2008 ) .", "We present preliminary work on the creation of a unique Arabic proposition repository for Quranic Arabic .", "All the QAPB annotations will be made freely available for research purposes .", "We annotate the semantic roles for the 50 most frequent verbs in the Quranic Arabic Dependency Treebank ( QATB ) ( Dukes and Buckwalter 2010 ) .", "The Quran is a significant religious text written in a unique literary style , close to very poetic language in nature .", "In this pilot experiment , we leverage our knowledge and experience from our involvement in the APB project .", "Moreover , on a pragmatic level QAPB will add approximately 810 new verbs to the existing Arabic PropBank ( APB ) ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["Topic modeling of fashion trends were analyzed using the MALLET toolkit .", "Trends over time were analyzed in several different ways using 100-topics and 20-topics .", "Harper ? s Bazaar magazines from 1860-1899 were used ( freely available online ) .", "This resulted in 20 topics with 4 characterizing words each ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["We propose the use of dynamic network analysis to overcome these limitations .", "We build novel types of networks in which links between characters are different types of social events .", "We present a network analysis of a literary text , Alice in Wonderland .", "Also , static network analysis has limitations which become apparent from our analysis .", "We show that analyzing networks based on these social events gives us insight into the roles of characters in the story ."]}
{"orig_sents": ["0", "7", "2", "6", "5", "4", "3", "1"], "shuf_sents": ["Automatic evaluation has greatly facilitated system development in summarization .", "Finally , we present a case study of how new metrics should be compared to the reference evaluation , as we search for even more accurate automatic measures .", "In this paper we provide an assessment of the automatic evaluations used for multi-document summarization of news .", "We then demonstrate the accuracy of these metrics in reproducing human judgements about the relative content quality of pairs of systems and present an empirical assessment of the relationship between statistically significant differences between systems according to manual evaluations , and the difference according to automatic evaluations .", "ROUGE 1 and 2 ? that appear to best emulate human pyramid and responsiveness scores on four years of NIST evaluations .", "We identify the reference automatic evaluation metrics ?", "We outline our recommendations about how any evaluation , manual or automatic , should be used to find statistically significant differences between summarization systems .", "At the same time , the use of automatic evaluation has been viewed with mistrust by many , as its accuracy and correct application are not well understood ."]}
{"orig_sents": ["0", "7", "8", "1", "5", "4", "6", "3", "2"], "shuf_sents": ["Numerous NLP tasks rely on clustering or community detection algorithms .", "ACD is a sub-task of an abstractive summarization system and represents a twostep process .", "We use the Omega Index to compare and contrast several community detection algorithms .", "In this paper , we describe how the Omega Index , a metric for comparing non-disjoint clustering solutions , can be used as a summarization evaluation metric for this task .", "This results in an undirected graph with sentences as nodes and predicted abstractive links as edges .", "In the first step , we classify sentence pairs according to whether the sentences should be realized by a common abstractive sentence .", "The second step is to identify communities within the graph , where each community corresponds to an abstractive sentence to be generated .", "For many of these tasks , the solutions are disjoint , and the relevant evaluation metrics assume nonoverlapping clusters .", "In contrast , the relatively recent task of abstractive community detection ( ACD ) results in overlapping clusters of sentences ."]}
{"orig_sents": ["2", "5", "4", "0", "6", "1", "3"], "shuf_sents": ["We start with the discussion whether ROUGE can produce system rankings similar to those received from manual summary scoring by measuring their correlation .", "Building such summaries gives opportunity to run additional experiments and reinforce the evaluation .", "The multilingual summarization pilot task at TAC ? 11 opened a lot of problems we are facing when we try to evaluate summary quality in different languages .", "Later , we investigate whether an evaluation based on machine translated models can perform close to an evaluation based on original models .", "For the TAC pilot task English articles were first translated to other 6 languages , model summaries were written and submitted system summaries were evaluated .", "The additional language dimension greatly increases annotation costs .", "We study then three ways of projecting summaries to a different language : projection through sentence alignment in the case of parallel corpora , simple summary translation and summarizing machine translated articles ."]}
{"orig_sents": ["4", "2", "1", "0", "3", "5"], "shuf_sents": ["In this paper , we propose an unsupervised method called HeterogeneityBased Ranking ( HBR ) that combines summarization evaluation measures without requiring human assessments .", "However , the reliability of measures depends on the test collection in which the measure is meta-evaluated ; for this reason , it has not yet been possible to reliably establish which are the best evaluation measures for automatic summarization .", "A reliable measure should have correspondence with human judgements .", "Our empirical results indicate that HBR achieves a similar correspondence with human assessments than the best single measure for every observed corpus .", "The development of summarization systems requires reliable similarity ( evaluation ) measures that compare system outputs with human references .", "In addition , HBR results are more robust across topics than single measures ."]}
{"orig_sents": ["4", "7", "0", "6", "1", "5", "3", "2"], "shuf_sents": ["In this paper , we present our own experiments in comparing the results of manual evaluations versus automatic evaluations using our own text summarizer : BlogSum .", "The t-test results showed that there is no significant difference between BlogSum-generated summaries and OList summaries .", "These results agree with previous work and show the need for a better automated summary evaluation metric rather than the standard ROUGE metric .", "A manual evaluation of summary coherence also shows that BlogSum performs significantly better than OList .", "Today , automatic evaluation metrics such as ROUGE have become the de-facto mode of evaluating an automatic summarization system .", "However , two manual evaluations for content using two different datasets show that BlogSum performed significantly better than OList .", "We have evaluated BlogSum-generated summary content using ROUGE and compared the results with the original candidate list ( OList ) .", "However , based on the DUC and the TAC evaluation results , ( Conroy and Schlesinger , 2008 ; Dang and Owczarzak , 2008 ) showed that the performance gap between humangenerated summaries and system-generated summaries is clearly visible in manual evaluations but is often not reflected in automated evaluations using ROUGE scores ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["We experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n-gram assumption is statistically reasonable as long as n is sufficiently high , and that efforts should be focused on improving the estimation procedures for such large models .", "This study is made possible by the development of several novel Neural Network Language Model architectures , which can easily fare with such large context windows .", "In spite of their well known limitations , most notably their use of very local contexts , n-gram language models remain an essential component of many Natural Language Processing applications , such as Automatic Speech Recognition or Statistical Machine Translation .", "This paper investigates the potential of language models using larger context windows comprising up to the 9 previous words ."]}
{"orig_sents": ["3", "8", "4", "6", "0", "5", "7", "9", "2", "1"], "shuf_sents": ["Continuous space methods are a very competitive approach , but they have a high computational complexity and are not yet in widespread use .", "This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build .", "By these means , we are able to train an CSLM on more than 500 million words in 20 hours .", "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems .", "Some years ago , there was a clear tendency to build huge language models trained on hundreds of billions of words .", "This paper presents an experimental comparison of all these approaches on a large statistical machine translation task .", "Lately , this tendency has changed and recent works concentrate on data selection .", "We also describe an open-source implementation to train and use continuous space language models ( CSLM ) for such large tasks .", "The dominant approach since several decades are back-off language models .", "We describe an efficient implementation of the CSLM using graphical processing units from Nvidia ."]}
{"orig_sents": ["2", "1", "5", "0", "4", "3"], "shuf_sents": ["Motivated by the success of DNNs in acoustic modeling , we explore deep neural network language models ( DNN LMs ) in this paper .", "Most NNLMs are trained with one hidden layer .", "In recent years , neural network language models ( NNLMs ) have shown success in both peplexity and word error rate ( WER ) compared to conventional n-gram language models .", "Furthermore , our preliminary results are competitive with a model M language model , considered to be one of the current state-of-the-art techniques for language modeling .", "Results on a Wall Street Journal ( WSJ ) task demonstrate that DNN LMs offer improvements over a single hidden layer NNLM .", "Deep neural networks ( DNNs ) with more hidden layers have been shown to capture higher-level discriminative information about input features , and thus produce better networks ."]}
{"orig_sents": ["2", "6", "1", "7", "4", "3", "0", "8", "5"], "shuf_sents": ["These options were then hand-groomed , to identify four decoys which are globally incoherent , yet syntactically correct .", "The test set consists of 1040 sentences , each of which is missing a content word .", "In this paper , we describe a new , publicly available corpus intended to stimulate research into language modeling techniques which are sensitive to overall sentence coherence .", "The set was generated by using an N-gram language model to generate a long list of likely words , given the immediate context .", "In general , all of the options are syntactically valid , and reasonable with respect to local N-gram statistics .", "The test sentences were derived from five of Conan Doyle ? s Sherlock Holmes novels , and we provide a large set of Nineteenth and early Twentieth Century texts as training material .", "The task uses the Scholastic Aptitude Test ? s sentence completion format .", "The goal is to select the correct replacement from amongst five alternates .", "To ensure the right to public distribution , all the data is derived from out-of-copyright materials from Project Gutenberg ."]}
{"orig_sents": ["2", "3", "1", "4", "0", "5"], "shuf_sents": ["The adapted pronunciation rules are finally generated with a trainable grapheme-tophoneme converter .", "Foreign word candidates are detected automatically from in-domain text through the use of letter n-gram perplexity .", "Modeling of foreign entity names is an important unsolved problem in morpheme-based modeling that is common in morphologically rich languages .", "In this paper we present an unsupervised vocabulary adaptation method for morph-based speech recognition .", "Over-segmented foreign entity names are restored to their base forms in the morph-segmented in-domain text for easier and more reliable modeling and recognition .", "In ASR performance the unsupervised method almost matches the ability of supervised adaptation in correctly recognizing foreign entity names ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["We present a distributed framework for largescale discriminative language models that can be integrated within a large vocabulary continuous speech recognition ( LVCSR ) system using lattice rescoring .", "We also provide an analysis of the various parameters of our models including model size , types of features , size of partitions in the MapReduce framework with the help of supporting experiments .", "We report small but significant improvements in recognition accuracies on a standard voice-search data set using our discriminative reranking model .", "We intentionally use a weakened acoustic model in a baseline LVCSR system to generate candidate hypotheses for voice-search data ; this allows us to utilize large amounts of unsupervised data to train our models .", "We propose an efficient and scalable MapReduce framework that uses a perceptron-style distributed training strategy to handle these large amounts of data ."]}
{"orig_sents": ["5", "4", "3", "1", "2", "0"], "shuf_sents": ["It is demonstrated that n-grams are good word-predictors , even linguistically speaking , in a large majority of word-positions , and it is suggested that to improve over n-grams , one must explore syntax-aware ( or other ) language models that focus on positions where n-grams are weak .", "Furthermore , they have doggedly matched or outperformed numerous competing proposals for syntactically well-motivated models .", "This unusual resilience of n-grams , as well as their weaknesses , are examined here .", "? ve , but estimating them from any amount of text , large or small , is straightforward .", "They are regarded as linguistically na ?", "Statistical language models used in deployed systems for speech recognition , machine translation and other human language technologies are almost exclusively n-gram models ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["From this data , a mapping from instructions to tasks is learned , enabling the agent to carry out new instructions in novel environments .", "This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior ."]}
{"orig_sents": ["7", "0", "3", "5", "2", "1", "6", "4"], "shuf_sents": ["Previous approaches to learning these grounded meaning representations require detailed annotations at training time .", "We assume the action policy takes a parametric form that factors based on the structure of the language , based on the G3 framework and use stochastic gradient ascent to optimize policy parameters .", "and a specific object in the environment .", "In this paper , we present an approach which is capable of jointly learning a policy for following natural language commands such as ? Pick up the tire pallet , ?", "commands given to a robotic forklift by untrained users .", "as well as a mapping between specific phrases in the language and aspects of the external world ; for example the mapping between the words ? the tire pallet ?", "Our preliminary evaluation demonstrates the effectiveness of the model on a corpus of ? pick up ?", "In order for robots to effectively understand natural language commands , they must be able to acquire a large vocabulary of meaning representations that can be mapped to perceptual features in the external world ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Based on a personalised mapping between three expressive synthetic voices and the users facial expressions , the system selects a voice that matches their face at the moment of sending a message .", "With the help of a webcamera and facial expression analysis , the system allows the user to control the expressive features of the synthetic speech for a particular utterance with their facial expressions .", "This paper describes a demonstration of the WinkTalk system , which is a speech synthesis platform using expressive synthetic voices .", "The WinkTalk system is an early research prototype that aims to demonstrate that facial expressions can be used as a more intuitive control over expressive speech synthesis than manual selection of voice types , thereby contributing to an improved communication experience for users of speech generating devices ."]}
{"orig_sents": ["1", "2", "4", "3", "6", "5", "7", "0"], "shuf_sents": ["Also useful , we believe , is our estimate that about 3.5-4.0 % of utterances in dialogs are in principle predictable given previous context .", "This paper presents a method for an AAC system to predict a whole response given features of the previous utterance from the interlocutor .", "It uses a large corpus of scripted dialogs , computes a variety of lexical , syntactic and whole phrase features for the previous utterance , and predicts features that the response should have , using an entropy-based measure .", "We find that for about 3.5 % of cases in the held-out corpus , we are able to predict a response , and among those , over half are either exact or at least reasonable substitutes for the actual response .", "We evaluate the system on a held-out portion of the corpus .", "Finally we compare our approach to a state-of-the-art chatbot , and show ( not surprisingly ) that a system like ours , tuned for a particular style of conversation , outperforms one that is not .", "We also present some results on keystroke savings .", "Predicting possible responses automatically by mining a corpus of dialogues is a novel contribution to the literature on whole utterance-based methods in AAC ."]}
{"orig_sents": ["2", "4", "1", "5", "3", "0"], "shuf_sents": ["A compu-tational experiment showed that our prediction system led to 56.3 % average keystroke sav-ings .", "In this paper we investigate how prediction tech-niques can be applied to improve user perfor-mance of such systems .", "ristensson School of Computer Science University of St Andrews pok @ st-andrews.ac.uk Abstract It is well documented that people with severe speech and physical impairments ( SSPI ) often experience literacy difficulties , which hinder them from effectively using orthographic-based AAC systems for communication .", "We incorpo-rated our prediction system into a hypothetical 12-key reduced phoneme keyboard .", "To address this problem , phoneme-based AAC systems have been proposed , which enable users to access a set of spoken phonemes and combine phonemes into speech output .", "We have developed a phoneme-based prediction system , which sup-ports single phoneme prediction and pho-neme-based word prediction using statistical language models generated using a crowdsourced AAC-like corpus ."]}
{"orig_sents": ["4", "6", "5", "2", "7", "8", "0", "3", "1"], "shuf_sents": ["Performance of four word-level prediction algorithms , two based on sem-grams and two based on n-grams , were compared on a corpus of informal blogs .", "Hybrid methods that combine n-gram and sem-gram approaches may be viable for unordered prediction in AAC .", "Facilitating communication via unordered message formulation , however , requires new methods of prediction .", "Results showed that sem-grams yield accurate word prediction , but lack prediction coverage .", "Most icon-based augmentative and alternative communication ( AAC ) devices require users to formulate messages in syntactic order in order to produce syntactic utterances .", "Some of these users may benefit from unordered message formulation accompanied by automatic message expansion to generate syntactically correct messages .", "Reliance on syntactic ordering , however , may not be appropriate for individuals with limited or emerging literacy skills .", "This paper describes a novel approach to word prediction using semantic grams , or ? sem-grams , ?", "which provide relational information about message components regardless of word order ."]}
{"orig_sents": ["2", "0", "1", "3", "5", "7", "4", "6", "8"], "shuf_sents": ["One of the significant impacts of this disease is a decline in the ability to communicate using natural language .", "This decline in language facility often results in decreased social interaction and life satisfaction for persons with DAT and their caregivers .", "The number of people with dementia of the Alzheimer 's type ( DAT ) continues to grow .", "One possible strategy to lessen the effects of this loss of language facility is for the unaffected conversational partner ( Facilitator ) to `` co-construct '' short autobiographical stories from the life of the DATaffected conversational partner ( Storyteller ) .", "This technology could provide context-sensitive suggestions to an unskilled Facilitator to help maintain the flow of conversation .", "It has been observed that a skilled conversational partner can facilitate co-constructed narrative with individuals who have mild to moderate DAT .", "This paper describes a framework in which the necessary computational model of co-constructed narrative can be developed .", "Developing a computational model of this type of co-constructed narrative would enable assistive technology to be developed that can monitor a conversation between a Storyteller and Facilitator .", "An analysis of the fundamental elements of such a model will be presented ."]}
{"orig_sents": ["4", "1", "3", "2", "0"], "shuf_sents": ["We conclude by outlining current directions of research including specialized grammars and automatic detection of confusion .", "These costs , normally associated with human caregivers , can be mitigated to some extent given automated systems that mimic some of their functions .", "Here , we show how to improve rates of correct speech recognition by preprocessing acoustic noise and by modifying the vocabulary according to the task .", "In this paper , we present inaugural work towards producing a generic automated system that assists individuals with Alzheimer ? s to complete daily tasks using verbal communication .", "Currently , health care costs associated with aging at home can be prohibitive if individuals require continual or periodic supervision or assistance because of Alzheimer ? s disease ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["A worked example shows the plausibility of the solution and the output that the prototype generates given input derived from experimental data .", "Tactile maps are important substitutes for visual maps for blind and visually impaired people and the efficiency of tactile-map reading can largely be improved by giving assisting utterances that make use of spatial language .", "In this paper , we elaborate earlier ideas for a system that generates such utterances and present a prototype implementation based on a semantic conceptualization of the movements that the map user performs ."]}
{"orig_sents": ["4", "0", "1", "2", "3"], "shuf_sents": ["The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents .", "Using motion-capture data recorded from human signers , we model how the mo-tion-paths of verb signs vary based on the lo-cation of their subject and object .", "This model yields a lexicon for ASL verb signs that is pa-rameterized on the 3D locations of the verb ? s arguments ; such a lexicon enables more real-istic and understandable ASL animations .", "A new model presented in this paper , based on identifying the principal movement vector of the hands , shows improvement in modeling ASL verb signs , including when trained on movement data from a different human signer .", "erfauth Department of Computer Science Queens College and Graduate Center City University of New York ( CUNY ) 65-30 Kissena Blvd , Flushing , NY 11367 matt @ cs.qc.cuny.edu Abstract American Sign Language ( ASL ) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We describe an automatic text simplification system for Spanish which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts .", "Automatic text simplifications aims at reducing the reading difficulty for people with cognitive disability , among other target groups .", "Our system is integrated in a service architecture which includes a web service and mobile applications .", "This paper addresses the problem of automatic text simplification ."]}
{"orig_sents": ["1", "2", "0", "4", "3", "6", "5"], "shuf_sents": ["Recent analysis by Bojar et al .", "Human assessment is often considered the gold standard in evaluation of translation systems .", "But in order for the evaluation to be meaningful , the rankings obtained from human assessment must be consistent and repeatable .", "We extend their analysis to all of the ranking tasks from 2010 and 2011 , and show through an extension of their reasoning that the ranking is naturally cast as an instance of finding the minimum feedback arc set in a tournament , a wellknown NP-complete problem .", "( 2011 ) raised several concerns about the rankings derived from human assessments of English-Czech translation systems in the 2010 Workshop on Machine Translation .", "This leads to strong caveats and recommendations for both producers and consumers of these rankings .", "All instances of this problem in the workshop data are efficiently solvable , but in some cases the rankings it produces are surprisingly different from the ones previously published ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["We introduced a new quality estimation task this year , and evaluated submissions from 11 teams .", "This paper presents the results of the WMT12 shared tasks , which included a translation task , a task for machine translation evaluation metrics , and a task for run-time estimation of machine translation quality .", "We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams .", "We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics ."]}
{"orig_sents": ["0", "3", "2", "6", "7", "4", "1", "5"], "shuf_sents": ["Textual Similarity for MT evaluation Julio Castillo ? ?", "We described results for the Spanish-English , Czech-English and German-English language pairs according to our submission on the Eight Workshop on Statistical Machine Translation .", "? FaMAF , UNC , Argentina ?", "Paula Estrella ?", "This problem is addressed using a textual entail-ment engine entirely based on WordNet se-mantic features .", "Our first experiments reports a competitive score to system level .", "? UTN-FRC , Argentina jotacastillo @ gmail.com pestrella @ famaf.unc.edu.ar Abstract This paper describes the system used for our participation in the WMT12 Machine Transla-tion evaluation shared task .", "We also present a new approach to Machine Translation evaluation based on the recently defined task Semantic Textual Similarity ."]}
{"orig_sents": ["3", "2", "1", "5", "0", "4"], "shuf_sents": ["Each of the changes by itself improved the performance of AMBER , and the two together yielded even greater improvement , which in some cases was more than additive .", "The first one is incorporation of a new ordering penalty ; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER .", "This paper describes two changes to AMBER .", "A recent paper described a new machine translation evaluation metric , AMBER .", "The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment .", "We tested the impact of the two changes , using data from the WMT metrics task ."]}
{"orig_sents": ["0", "1", "3", "5", "2", "4"], "shuf_sents": ["We investigate the use of error classification results for automatic evaluation of machine translation output .", "Five basic error classes are taken into account : morphological errors , syntactic ( reordering ) errors , missing words , extra words and lexical errors .", "Machine translation outputs in five different European languages are used : English , Spanish , French , German and Czech .", "In addition , linear combinations of these categories are investigated .", "The results show that the following combinations are the most promising : the sum of all class error rates , the weighted sum optimised for translation into English and the weighted sum optimised for translation from English .", "Correlations between the class error rates and human judgments are calculated on the data of the third , fourth , fifth and sixth shared tasks of the Statistical Machine Translation Workshop ."]}
{"orig_sents": ["1", "5", "2", "4", "0", "3"], "shuf_sents": ["Our models are trained in a regression framework , and can easily incorporate a rich set of linguistic features .", "This paper describes Stanford University ? s submission to the Shared Evaluation Task of WMT 2012 .", "We learn weighted edit distance in a probabilistic finite state machine ( pFSM ) model , where state transitions correspond to edit operations .", "Evaluated on two different prediction tasks across a diverse set of datasets , our methods achieve state-of-the-art correlation with human judgments .", "While standard edit distance models can not capture long-distance word swapping or cross alignments , we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model .", "Our proposed metric ( SPEDE ) computes probabilistic edit distance as predictions of translation quality ."]}
{"orig_sents": ["5", "4", "2", "0", "3", "1"], "shuf_sents": ["The Quality Estimation problem was addressed both as a regression task and as a discretised classification task , but the latter did not generalise well on the unseen testset .", "Indications that RMSE is not always sufficient for measuring performance were observed .", "Several Feature Selection algorithms were employed .", "The most successful regression methods had an RMSE of 0.86 and were trained with a feature set given by Correlation-based Feature Selection .", "Data were augmented with features from linguistic analysis and statistical features from the SMT search graph .", "We describe a submission to the WMT12 Quality Estimation task , including an extensive Machine Learning experimentation ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In this paper we introduce a number of new features for quality estimation in machine translation that were developed for the WMT 2012 quality estimation shared task .", "We find that very simple features such as indicators of certain characters are able to outperform complex features that aim to model the connection between two languages ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper describes a study on the contribution of linguistically-informed features to the task of quality estimation for machine translation at sentence level .", "A standard regression algorithm is used to build models using a combination of linguistic and non-linguistic features extracted from the input text and its machine translation .", "Experiments with EnglishSpanish translations show that linguistic features , although informative on their own , are not yet able to outperform shallower features based on statistics from the input text , its translation and additional corpora .", "However , further analysis suggests that linguistic information is actually useful but needs to be carefully combined with other features in order to produce better results ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Results showed that an adequate selection of a subset of highly discriminative features can improve efficiency and performance of the quality estimation system .", "This is a description of the submissions made by the pattern recognition and human language technology group ( PRHLT ) of the Universitat Polite`cnica de Vale`ncia to the quality estimation task of the seventh workshop on statistical machine translation ( WMT12 ) .", "We focus on two different issues : how to effectively combine subsequence-level features into sentence-level features , and how to select the most adequate subset of features ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We present a QE system based on Support Vector Machine regression , using a number of explicitly defined features extracted from the Machine Translation input , output and models in combination with tree kernels over constituency and dependency parse trees for the input and output sentences .", "We confirm earlier results suggesting that tree kernels can be a useful tool for QE system construction especially in the early stages of system design .", "This paper describes Uppsala University ? s submissions to the Quality Estimation ( QE ) shared task at WMT 2012 ."]}
{"orig_sents": ["0", "5", "7", "6", "3", "2", "9", "8", "4", "1"], "shuf_sents": ["In this paper we present the system we submitted to the WMT12 shared task on Quality Estimation .", "Our results show that a lot of information is already present in baseline features , and that our feature selection algorithm discards features which are linearly correlated .", "We experiment with two kernels : linear and radial basis function .", "To this end , we use a Support Vector Machine .", "To deal with this large number of features , we propose an in-house feature selection algorithm .", "Each translated sentence is given a score between 1 and 5 .", "We perform a linear regression of the feature space against scores in the range .", "The score is obtained using several numerical or boolean features calculated according to the source and target sentences .", "This leads to 66 features .", "In our submission we use the features from the shared task baseline system and our own features ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We have used various similarity measures and also an external resource ( Google N -grams ) .", "We present the approach we took for our participation to the WMT12 Quality Estimation Shared Task : our main goal is to achieve reasonably good results without appeal to supervised learning .", "Details of results clarify the interest of such an approach ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We document the results obtained on the shared task dataset , obtained by combining the features that we designed with the baseline features provided by the task organizers .", "In this paper , we describe the UPC system that participated in the WMT 2012 shared task on Quality Estimation for Machine Translation .", "Based on the empirical evidence that fluencyrelated features have a very high correlation with post-editing effort , we present a set of features for the assessment of quality estimation for machine translation designed around different kinds of n-gram language models , plus another set of features that model the quality of dependency parses automatically projected from source sentences to translations ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["The IBM1 scores calculated on morphemes and POS-4grams of the source sentence and obtained translation output are shown to be competitive with the classic evaluation metrics for ranking of translation systems .", "Since these scores do not require any reference translations , they can be used as features for the quality estimation task presenting a connection between the source language and the obtained target language .", "In addition , target language model scores of morphemes and POS tags are investigated as estimates for the obtained target language quality .", "We present a method we used for the quality estimation shared task of WMT 2012 involving IBM1 and language model scores calculated on morphemes and POS tags ."]}
{"orig_sents": ["4", "2", "6", "3", "0", "5", "1"], "shuf_sents": ["In total , more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data .", "We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques .", "Two sets of features are proposed : one constrained , i.e .", "using data or tools trained on data that was not provided by the workshop organisers .", "This paper describes the features and the machine learning methods used by Dublin City University ( DCU ) and SYMANTEC for the WMT 2012 quality estimation task .", "In this paper , we focus on a subset of our feature set that we consider to be relatively novel : features based on a topic model built using the Latent Dirichlet Allocation approach , and features based on source and target language syntax extracted using part-of-speech ( POS ) taggers and parsers .", "respecting the data limitation suggested by the workshop organisers , and one unconstrained , i.e ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We present in this paper the system submissions of the SDL Language Weaver team in the WMT 2012 Quality Estimation shared-task .", "Our MT quality-prediction systems use machine learning techniques ( M5P regression-tree and SVM-regression models ) and a feature-selection algorithm that has been designed to directly optimize towards the official metrics used in this shared-task .", "The resulting submissions placed 1st ( the M5P model ) and 2nd ( the SVM model ) , respectively , on both the Ranking task and the Scoring task , out of 11 participating teams ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We train a SVM regression model for predicting the scores and numerical results show the effectiveness of our phrase indicators and method in both ranking and scoring tasks .", "This paper focuses on exploiting special phrases , or word sequences , to estimate translation quality .", "Several feature templates on this topic are put forward subsequently .", "We in this paper describe the regression system for our participation in the quality estimation task of WMT12 ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Our contribution is twofold : i ) we first present an analysis of the data which highlights the difficulty of the task and motivates our approach ; ii ) we show that using non-linear models , namely random forests , with a simple and limited feature set , succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task .", "This paper describes our work with the data distributed for the WMT ? 12 Confidence Estimation shared task ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In addition to helping with document-level quality estimation , sentencelevel predictions are used for system selection , improving the quality of the output translations .", "This paper presents techniques for referencefree , automatic prediction of Machine Translation output quality at both sentence- and document-level .", "We present three system selection techniques and perform evaluations that quantify the gains across multiple domains and language pairs ."]}
{"orig_sents": ["3", "4", "1", "2", "0"], "shuf_sents": ["Our method shows high correlation with human judgements and good results on all datasets without relying on reference translations .", "From a system evaluation perspective , pushing semantics into MT ( b ) is a necessity in order to complement the shallow methods currently used overcoming their limitations .", "Casting the problem as a cross-lingual textual entailment application , we experiment with different benchmarks and evaluation settings .", "We address two challenges for automatic machine translation evaluation : a ) avoiding the use of reference translations , and b ) focusing on adequacy estimation .", "From an economic perspective , getting rid of costly hand-crafted reference translations ( a ) permits to alleviate the main bottleneck in MT evaluation ."]}
{"orig_sents": ["1", "5", "2", "4", "0", "6", "3"], "shuf_sents": ["Cognitive effort is difficult to examine directly , but ways to reduce the cognitive effort in particular may prove valuable in reducing the frustration associated with postediting work .", "Post-editing performed by translators is an increasingly common use of machine translated texts .", "For this reason , estimating whether machine translations are of sufficient quality to be used for post-editing and finding means to reduce post-editing effort are an important field of study .", "We present results of an error analysis performed on such sentences and discuss the clues they may provide about edits requiring great cognitive effort compared to the technical effort , on one hand , or little cognitive effort , on the other .", "Post-editing effort consists of different aspects , of which temporal effort , or the time spent on post-editing , is the most visible and involves not only the technical effort needed to perform the editing , but also the cognitive effort required to detect and plan necessary corrections .", "While high quality MT may increase productivity , post-editing poor translations can be a frustrating task which requires more effort than translating from scratch .", "In this paper , we describe an experiment aimed at studying the relationship between technical post-editing effort and cognitive post-editing effort by comparing cases where the edit distance and a manual score reflecting perceived effort differ ."]}
{"orig_sents": ["1", "3", "4", "7", "0", "2", "6", "5"], "shuf_sents": ["Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences .", "Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination .", "All aligners yield significant BLEU score gains over the best individual system included in the combination .", "The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature .", "This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion network decoding .", "? The work reported in this paper was carried out while the authors were at Raytheon BBN Technologies and ? RWTH Aachen University .", "Incremental indirect hidden Markov model and a novel incremental inversion transduction grammar with flexible matching consistently yield the best translation quality , though keeping all things equal , the differences between aligners are relatively small .", "Controlled experiments using identical pre-processing , decoding , and weight tuning methods on standard system combination evaluation sets are presented ."]}
{"orig_sents": ["1", "2", "4", "0", "3"], "shuf_sents": ["In particular , we show that an existing ITG constraint ( Zens et al , 2004 ) does not prevent all non-ITG permutations , and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training .", "The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation ( PBSMT ) system .", "Permutation parsers have been used to implement hierarchical re-ordering models ( Galley and Manning , 2008 ) and to enforce inversion transduction grammar ( ITG ) constraints ( Feng et al. , 2010 ) .", "Experimentally , we verify the utility of hierarchical re-ordering , and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output .", "We present a number of theoretical results regarding the use of permutation parsers in PBSMT ."]}
{"orig_sents": ["2", "3", "0", "5", "4", "1"], "shuf_sents": ["Syntax , or sentence structure , could provide guidance to phrasebased systems , but the ? non-constituent ?", "Using CCG parse charts , we train a syntactic analogue of a lexicalized reordering model by labelling phrase table entries with multiword labels and demonstrate significant improvements in translating between Urdu and English , two language pairs with divergent sentence structure .", "Statistical phrase-based machine translation requires no linguistic information beyond word-aligned parallel corpora ( Zens et al , 2002 ; Koehn et al , 2003 ) .", "Unfortunately , this linguistic agnosticism often produces ungrammatical translations .", "We address these issues by using Combinatory Categorial Grammar , or CCG , ( Steedman , 2000 ) , which has a much more flexible notion of constituency , thereby providing more labels for putative nonconstituent multiword translation phrases .", "word strings that phrase-based decoders manipulate complicate the use of most recursive syntactic tools ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["We introduce a labeling scheme based on categorial grammar , which allows syntactic labeling of many rules with a minimal , well-motivated label set .", "SAMT ( Zollmann and Venugopal , 2006 ) introduces heuristics to create new non-constituent labels , but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar .", "Adding syntactic labels to synchronous context-free translation rules can improve performance , but labeling with phrase structure constituents , as in GHKM ( Galley et al , 2004 ) , excludes potentially useful translation rules .", "We show that our labeling scheme performs comparably to SAMT on an Urdu ? English translation task , yet the label set is an order of magnitude smaller , and translation is twice as fast ."]}
{"orig_sents": ["4", "6", "3", "0", "1", "2", "5"], "shuf_sents": ["On the other hand , limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations .", "This paper presents a simple but effective translation model , called the Head-Driven HPB ( HD-HPB ) model , which incorporates head information in translation rules to better capture syntax-driven information in a derivation .", "In addition , unlike the original glue rules , the HD-HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space .", "However , the original HPB model is prone to overgeneration due to lack of linguistic knowledge : the grammar may suggest more derivations than appropriate , many of which may lead to ungrammatical translations .", "Chiang ? s hierarchical phrase-based ( HPB ) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases ?", "An extensive set of experiments on Chinese-English translation on four NIST MT test sets , using both a small and a large training set , show that our HDHPB model consistently and statistically significantly outperforms Chiang ? s model as well as a source side SAMT-style model .", "phrases that contain sub-phrases ."]}
{"orig_sents": ["1", "3", "4", "5", "0", "6", "2"], "shuf_sents": ["Sentence level correlation analysis , following standard NIST MetricsMATR protocol , shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU , NIST , METEOR , PER , CDER , WER , or TER .", "We introduce the first fully automatic , fully semantic frame based MT evaluation metric , MEANT , that outperforms all other commonly used automatic metrics in correlating with human judgment on translation adequacy .", "Despite its high performance , fully automated MEANT is still able to preserve HMEANT ? s virtues of simplicity , representational transparency , and inexpensiveness .", "Recent work on HMEANT , which is a human metric , indicates that machine translation can be better evaluated via semantic frames than other evaluation paradigms , requiring only minimal effort from monolingual humans to annotate and align semantic frames in the reference and machine translations .", "We propose a surprisingly effective Occam ? s razor automation of HMEANT that combines standard shallow semantic parsing with a simple maximum weighted bipartite matching algorithm for aligning semantic frames .", "The matching criterion is based on lexical similarity scoring of the semantic role fillers through a simple context vector model which can readily be trained using any publicly available large monolingual corpus .", "Furthermore , we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We point out several common pitfalls when designing factored setups .", "The paper also describes our WMT12 submissions CU-BOJAR and CU-POOR-COMB .", "We introduce a taxonomy of factored phrasebased translation scenarios and conduct a range of experiments in this taxonomy ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We present a method for training data selection , a description of our hierarchical phrase-based translation system , and a discussion of the impact of data size on best practice for system building .", "This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation ( NAACL WMT12 ) ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["One of the most notable recent improvements of the TectoMT English-to-Czech translation is a systematic and theoretically supported revision of formemes ? the annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the Prague tectogrammatics theory .", "Formemes can be used not only in MT , but in various other NLP tasks .", "Our modifications aim at reducing data sparsity , increasing consistency across languages and widening the usage area of this markup ."]}
{"orig_sents": ["3", "5", "0", "4", "1", "2"], "shuf_sents": ["Variations adopted several improvement techniques such as morphology simplification and generation and domain adaptation .", "The domain adaptation approach improves the SMT system by adding new translation units learned from MT-output and reference alignment .", "Results depict an improvement on TER , METEOR , NIST and BLEU scores compared to our baseline system , obtaining on the official test set more benefits from the domain adaptation approach than from the morphological generalization method .", "rmiga , carlos.henriquez , adolfo.hernandezjose.marino , enric.monte , jose.fonollosa } @ upc.eduAbstract This paper describes the UPC participation in the WMT 12 evaluation campaign .", "The morphology simplification overcomes the data sparsity problem when translating into morphologicallyrich languages such as Spanish by translating first to a morphology-simplified language and secondly leave the morphology generation to an independent classification task .", "All systems presented are based on standard phrasebased Moses systems ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The main contributions in this release are the introduction of a compact grammar representation based on packed tries , and the integration of our implementation of pairwise ranking optimization , J-PRO .", "We present Joshua 4.0 , the newest version of our open-source decoder for parsing-based statistical machine translation .", "We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Various feature functions are combined in a log-linear fashion to evaluate paths through that lattice .", "We present a variant of phrase-based SMT that uses source-side parsing and a constituent reordering model based on word alignments in the word-aligned training corpus to predict hierarchical block-wise reordering of the input .", "Multiple possible translation orders are represented compactly in a source order lattice .", "This source order lattice is then annotated with phrase-level translations to form a lattice of tokens in the target language ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The evaluation results show that we rank second in BLEU and TER for Spanish-English , and in the top tier for German-English .", "We used a phrase-based statistical machine translation model with several non-standard settings , most notably tuning data selection and phrase table combination .", "We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task ."]}
{"orig_sents": ["2", "4", "0", "1", "3"], "shuf_sents": ["Both hierarchical and phrase-based SMT systems are applied .", "A number of different techniques are evaluated , including an insertion model , different lexical smoothing methods , a discriminative reordering extension for the hierarchical system , reverse translation , and system combination .", "This paper describes the statistical machine translation ( SMT ) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation ( WMT 2012 ) .", "By application of these methods we achieve considerable improvements over the respective baseline systems .", "We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions ."]}
{"orig_sents": ["3", "1", "4", "2", "5", "0"], "shuf_sents": ["We are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system .", "The approach is based on a rule-based MT ( RBMT ) system which creates template translations .", "translation candidates from one or more translation engines which could be substituted into our translation templates .", "We describe a substitution-based system for hybrid machine translation ( MT ) that has been extended with machine learning components controlling its phrase selection .", "Based on the rule-based generation parse tree and target-to-target algnments , we identify the set of ? interesting ?", "The substitution process is either controlled by the output from a binary classifier trained on feature vectors from the different MT engines , or it is depending on weights for the decision factors , which have been tuned using MERT ."]}
{"orig_sents": ["0"], "shuf_sents": ["We report on findings of exploiting large data sets for translation modeling , language modeling and tuning for the development of competitive machine translation systems for eight language pairs ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Four groups ( RWTH Aachen University , Karlsruhe Institute of Technology , LIMSI-CNRS , and SYSTRAN ) of the QUAERO project submitted a joint translation for the WMT German ? English task .", "Experimental results show improvements of up to 1.7 points in BLEU and 3.4 points in TER compared to the best single system .", "Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission .", "This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation ."]}
{"orig_sents": ["4", "6", "0", "1", "2", "5", "3"], "shuf_sents": ["Our submissions use n-code , an open source system based on bilingual n-grams .", "In this approach , both the translation and target language models are estimated as conventional smoothed n-gram models ; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks .", "Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions .", "translation model .", "This paper describes LIMSI ? s submissions to the shared translation task .", "We also report preliminary experiments using an ? on-the-fly ?", "We report results for French-English and German-English in both directions ."]}
{"orig_sents": ["0", "6", "3", "5", "2", "4", "1", "7"], "shuf_sents": ["This paper describes the UPM system for the Spanish-English translation task at the NAACL 2012 workshop on statistical machine translation .", "And as a result of the WMT12 challenge we have obtained a 31.80 % BLEU with the 2012 test set .", "This technique is based on the similarity with the sentences to translate .", "We have used all available free corpora , cleaning and deleting some repetitions .", "With our approach , we improve the BLEU score from 28.37 % to 28.57 % .", "In this paper , we also propose a technique for selecting the sentences for tuning the system .", "This system is based on Moses .", "Finally , we explain different experiments that we have carried out after the competition ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper describes the PROMT submission for the WMT12 shared translation task .", "We participated in two language pairs : EnglishFrench and English-Spanish .", "The translations were made using the PROMT DeepHybrid engine , which is the first hybrid version of the PROMT system .", "We report on improvements over our baseline RBMT output both in terms of automatic evaluation metrics and linguistic analysis ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper describes the phrase-based SMT systems developed for our participation in the WMT12 Shared Translation Task .", "Translations for English ? German and English ? French were generated using a phrase-based translation system which is extended by additional models such as bilingual , fine-grained part-of-speech ( POS ) and automatic cluster language models and discriminative word lexica .", "Furthermore , we extended the POS-based reordering approach to also use information from syntactic trees .", "In addition , we explicitly handle out-of-vocabulary ( OOV ) words in German , if we have translations for other morphological forms of the same stem ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We also modified the dependency parser of McDonald et al ( 2005 ) in two ways to adjust it for the parsing of MT outputs .", "We enhanced the rule set used by the original DEPFIX system and measured the performance of the individual rules .", "We show that our system is able to improve the quality of the state-of-the-art MT systems .", "We present an improved version of DEPFIX ( Marec ? ek et al , 2011 ) , a system for automatic rule-based post-processing of Englishto-Czech MT outputs designed to increase their fluency ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes the development of French ? English and English ? French statistical machine translation systems for the 2012 WMT shared task evaluation .", "We developed phrase-based systems based on the Moses decoder , trained on the provided data only .", "Additionally , new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection ."]}
{"orig_sents": ["3", "6", "2", "4", "0", "5", "1"], "shuf_sents": ["We introduce a novel approach to data selection by full-text indexing and search : we select sentences similar to the test set from a large monolingual corpus and explore several options of incorporating them in a machine translation system .", "Finally , we describe our submitted system CU-TAMCH-BOJ .", "We describe a simple technique for reducing out-of-vocabulary rate after phrase extraction .", "We provide a few insights on data selection for machine translation .", "We discuss the benefits of tuning towards multiple reference translations for English-Czech language pair .", "We show that this method can improve translation quality .", "We evaluate the quality of the new CzEng 1.0 , a parallel data source used in WMT12 ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Different setups have been tested and combined using a sentence selection method .", "The submission is based on the freely available machine translation toolkit Jane , which supports phrase-based and hierarchical phrase-based translation models .", "We describe DFKI ? s statistical based submission to the 2012 WMT evaluation ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We compare systems trained on the same data using different grammar extraction methods .", "We developed a string-to-tree system for English ? German , achieving competitive results against a hierarchical model baseline .", "We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We trained one system for 14 translation directions between English or Czech on one side and English , Czech , German , Spanish or French on the other side .", "We describe a set of results with different training data sizes and subsets .", "We describe our experiments with phrasebased machine translation for the WMT 2012 Shared Task ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Recent work has established the efficacy of Amazon ? s Mechanical Turk for constructing parallel corpora for machine translation research .", "We conduct a variety of baseline experiments and analysis , and release the data to the community .", "These languages are low-resource , under-studied , and exhibit linguistic phenomena that are difficult for machine translation .", "We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent : Bengali , Hindi , Malayalam , Tamil , Telugu , and Urdu ."]}
{"orig_sents": ["1", "2", "3", "4", "0"], "shuf_sents": ["Our method outperforms other approaches to domain adaptation for SMT such as language model adaptation , meta-parameter tuning , or self-translation .", "Microblogging services such as Twitter have become popular media for real-time usercreated news reporting .", "Such communication often happens in parallel in different languages , e.g. , microblog posts related to the same events of the Arab spring were written in Arabic and in English .", "The goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic Twitter translation , namely the lack of bilingual sentence pairs for training SMT systems .", "We show that translation-based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrasebased SMT pipeline ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["In this paper , we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity , then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline ( alignment , phrase extraction , and phrase scoring ) .", "In statistical machine translation ( SMT ) , it is known that performance declines when the training data is in a different domain from the test data .", "Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words , but may degrade the translation quality for more common words .", "Nevertheless , it is frequently necessary to supplement scarce in-domain training data with out-of-domain data ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["A still open problem is the evaluation of SMT systems that evolve over time .", "In this respect , the SMT ability of incrementally learning from the translations produced by users plays a central role .", "In this paper , we propose a new metric for assessing the quality of an adaptive MT component that is derived from the theory of learning curves : the percentage slope .", "The new frontier of computer assisted translation technology is the effective integration of statistical MT within the translation workflow ."]}
{"orig_sents": ["4", "5", "0", "1", "2", "3"], "shuf_sents": ["Using a phrase-based SMT system in various data conditions , we show that SMT translates documents remarkably consistently , even without document knowledge .", "Nevertheless , translation inconsistencies often indicate translation errors .", "However , unlike in human translation , these errors are rarely due to terminology inconsistency .", "They are more often symptoms of deeper issues with SMT models instead .", "SMT typically models translation at the sentence level , ignoring wider document context .", "Does this hurt the consistency of translated documents ?"]}
{"orig_sents": ["5", "2", "0", "1", "3", "4"], "shuf_sents": ["We introduce a novel lattice design , which explicitly distinguishes between different preprocessing alternatives for the source sentence .", "It allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model .", "Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters , or by applying indicator features .", "We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously .", "On the newscommentary portion of the German ? English WMT 2011 task we can show moderate improvements of up to 0.6 % BLEU over a stateof-the-art baseline system .", "In statistical machine translation , word lattices are used to represent the ambiguities in the preprocessing of the source sentence , such as word segmentation for Chinese or morphological analysis for German ."]}
{"orig_sents": ["2", "0", "6", "5", "4", "3", "1"], "shuf_sents": ["We apply this procedure to several large-scale tasks , with the primary goal of reducing model sizes without sacrificing translation quality .", "Index Terms : phrasal machine translation , phrase training , phrase table pruning", "Training the phrase table by force-aligning ( FA ) the training data with the reference translation has been shown to improve the phrasal translation quality while significantly reducing the phrase table size on medium sized tasks .", "We further introduce two global scaling factors for re-estimation of the phrase table via posterior phrase alignment probabilities and a modified absolute discounting method that can be applied to fractional counts .", "We are able to reduce already heavily pruned baseline phrase tables by more than 50 % with little to no degradation in quality and occasionally slight improvement , without any increase in OOVs .", "We also add heuristics to avoid any increase in OOV rates .", "To deal with the noise in the automatically crawled parallel training data , we introduce on-demand word deletions , insertions , and backoffs to achieve over 99 % successful alignment rate ."]}
{"orig_sents": ["4", "3", "0", "6", "2", "5", "1", "7"], "shuf_sents": ["In our work , we instead minimize error rate directly by integrating the decoder into the minimizer .", "Since integrating the decoder into the minimizer is often too slow to be practical , we also exploit statistical significance tests to accelerate the search by quickly discarding unpromising models .", "First , the function being optimized is the true error rate .", "MERT minimizes error rate by using a surrogate representation of the search space , such as N best lists or hypergraphs , which only offer an incomplete view of the search space .", "Minimum error rate training is often the preferred method for optimizing parameters of statistical machine translation systems .", "Second , it lets us optimize parameters of translations systems other than standard linear model features , such as distortion limit .", "This approach yields two benefits .", "Experiments with a phrasebased system show that our approach is scalable , and that optimizing the parameters that MERT can not handle brings improvements to translation results ."]}
{"orig_sents": ["0", "4", "2", "3", "1"], "shuf_sents": ["The introduction of large-margin based discriminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process .", "This papers aims to shed light on large-margin learning for MT , explicitly presenting the simple passive-aggressive algorithm which underlies many previous approaches , with direct application to MT , and empirically comparing several widespread optimization strategies .", "However , these methods have not yet met with wide-spread adoption .", "This may be partly due to the perceived complexity of implementation , and partly due to the lack of standard methodology for applying these methods to MT .", "By removing the limitation on the number of parameters which can be optimized , these methods have allowed integrating millions of sparse features ."]}
{"orig_sents": ["1", "3", "2", "0", "4"], "shuf_sents": ["In this paper , we shed the light on the usefulness of AAN citing sentences for understanding research trends and summarizing previous discoveries and contributions .", "The ACL Anthology Network ( AAN ) 1 is a comprehensive manually curated networked database of citations and collaborations in the field of Computational Linguistics .", "A citing sentence is one that appears in a scientific article and contains an explicit reference to another article .", "Each citation edge in AAN is associated with one or more citing sentences .", "We also propose and motivate several different uses and applications of citing sentences ."]}
{"orig_sents": ["0", "2", "4", "5", "3", "1", "6"], "shuf_sents": ["We develop a people-centered computational history of science that tracks authors over topics and apply it to the history of computational linguistics .", "We find that the government-sponsored bakeoffs brought new researchers to the field , and bridged early topics to modern probabilistic approaches .", "We present four findings in this paper .", "Third , we analyze the flow of authors across topics to discern how some subfields flow into the next , forming different stages of ACL research .", "First , we identify the topical subfields authors work on by assigning automatically generated topics to each paper in the ACL Anthology from 1980 to 2008 .", "Next , we identify four distinct research epochs where the pattern of topical overlaps are stable and different from other eras : an early NLP period from 1980 to 1988 , the period of US government-sponsored MUC and ATIS evaluations from 1989 to 1994 , a transitory period until 2001 , and a modern integration period from 2002 onwards .", "Last , we identify steep increases in author retention during the bakeoff era and the modern era , suggesting two points at which the field became more integrated ."]}
{"orig_sents": ["4", "0", "2", "3", "1"], "shuf_sents": ["The model reveals latent factions , or groups of individuals whom we expect to collaborate more closely within their faction , cite within the faction using language distinct from citation outside the faction , and be largely understandable through the language used when cited from without .", "faction memberships over time .", "We conduct an exploratory data analysis on the ACL Anthology .", "We extend the model to reveal changes in some authors ?", "We present a joint probabilistic model of who cites whom in computational linguistics , and also of the words they use to do the citing ."]}
{"orig_sents": ["5", "3", "0", "1", "2", "4"], "shuf_sents": ["We use topic models ( Latent Dirichlet Allocation ) to explore the research topics of men and women in the ACL Anthology Network .", "We find that women publish more on dialog , discourse , and sentiment , while men publish more than women in parsing , formal semantics , and finite state models .", "To conduct our study we labeled the gender of authors in the ACL Anthology mostly manually , creating a useful resource for other gender studies .", "Going beyond these coarse measures of gender participation , we conduct a fine-grained study of gender in the field of Natural Language Processing .", "Finally , our study of historical patterns in female participation shows that the proportion of women authors in computational linguistics has been continuously increasing , with approximately a 50 % increase in the three decades since 1980 .", "Studies of gender balance in academic computer science are typically based on statistics on enrollment and graduation ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Results show that both methods extract high-quality definition sentences intended for automated glossary construction .", "Definitional sentences extracted for a set of well-defined concepts were rated by domain experts .", "Computational Linguistics was used as the target domain and the ACL Anthology as the corpus .", "The paper reports on a comparative study of two approaches to extracting definitional sentences from a corpus of scholarly discourse : one based on bootstrapping lexico-syntactic patterns and another based on deep analysis ."]}
{"orig_sents": ["7", "2", "0", "9", "1", "6", "3", "8", "5", "4"], "shuf_sents": ["The results of the study show that until 1986 , the most significant terms were related to formal/rule based methods .", "For instance , language model , similarity measure , text classification .", "In this study I employ collocation segmentation to extract terms from the large and complex ACL Anthology Reference Corpus , and also briefly research and describe the history of the ACL .", "Although Penn Treebank was a significant term only temporarily in the early nineties , the corpus is still used by researchers today .", "This shows that some terms can be significant globally while remaining insignificant at a local level .", "While machine translation as a term is significant throughout the ACL ARC corpus , it is not significant for any particular time period .", "In 1990 , the terms Penn Treebank , Mutual Information , statistical parsing , bilingual corpus , and dependency tree became the most important , showing that newly released language resources appeared together with many new research areas in computational linguistics .", "Collocation is a well-known linguistic phenomenon which has a long history of research and use .", "The most recent significant terms are Bleu score and semantic role labeling .", "Starting in 1987 , terms related to statistical methods became more important ."]}
{"orig_sents": ["4", "2", "3", "0", "1"], "shuf_sents": ["In our study , we found some strong reuse cases which can be an indicator to establish a clear policy to handle text reuse for the upcoming editions of ACL .", "The results are anonymised .", "This study aims to investigate the trends of text reuse in the ACL submissions , if any .", "We carried a set of analyses on two spans of five years papers ( the past and the present ) of ACL using a publicly available text reuse detection application to notice the behaviour .", "With rapidly increasing community , a plethora of conferences related to Natural Language Processing and easy access to their proceedings make it essential to check the integrity and novelty of the new submissions ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["At the backend , the Anthology was updated to incorporate its publication records into a database .", "We describe the ACL Anthology ? s previous legacy , redesign and revamp process and technologies , and its resulting functionality .", "The ACL Anthology was revamped in 2012 to its second major version , encompassing faceted navigation , social media use , as well as author- and reader-generated content and comments on published work as part of the revised frontend user interface ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["The Contributed Task extends the current Anthology Reference Corpus ( ARC ) both in size , quality , and by aiming to provide tools that allow the corpus to be automatically extended with new content ? be they scanned or born-digital .", "The goal of the task is threefold : ( a ) to provide a shared resource for experimentation on scientific text ; ( b ) to serve as a basis for advanced search over the ACL Anthology , based on textual content and citations ; and , by combining the aforementioned goals , ( c ) to present a showcase of the benefits of natural language processing to a broader audience .", "a new resource dubbed the ACL Anthology Corpus ( AAC ) .", "The ACL 2012 Contributed Task is a community effort aiming to provide the full ACL Anthology as a high-quality corpus with rich markup , following the TEI P5 guidelines ?"]}
{"orig_sents": ["2", "4", "0", "3", "1"], "shuf_sents": ["those prepared electronically using typesetting systems like LATEX , OpenOffice , and the like .", "?", "Extracting textual content and document structure from PDF presents a surprisingly ( depressingly , to some , in fact ) difficult challenge , owing to the purely display-oriented design of the PDF document standard .", "This short paper summarizes a new tool for high-quality extraction of text and structure from PDFs , combining state-ofthe-art PDF parsing , font interpretation , layout analysis , and TEI-compliant output of text and logical document markup .", "While a variety of lower-level PDF extraction toolkits exist , none fully support the recovery of original text ( in reading order ) and relevant structural elements , even for so-called borndigital PDFs , i.e ."]}
{"orig_sents": ["5", "0", "4", "1", "3", "2"], "shuf_sents": ["PaperXML has been initially developed for the ACL Anthology Searchbench .", "PaperXML markup includes information on page and paragraph breaks , section headings , footnotes , tables , captions , boldface and italics character styles as well as bibliographic and publication metadata .", "We sketch transformation of paperXML into the ACL Contributed Task ? s TEI P5 XML .", "The role of paperXML in the ACL Contributed Task Rediscovering 50 Years of Discoveries is to serve as fall-back source ( 1 ) for older , scanned papers ( mostly published before the year 2000 ) , for which born-digital PDF sources are not available , ( 2 ) for borndigital PDF papers on which the PDFExtract method failed , ( 3 ) for document parts where PDFExtract does not output useful markup such as currently for tables .", "The main purpose was to robustly provide uniform access to sentences in ACL Anthology papers from the past 46 years , ranging from scanned , typewriter-written conference and workshop proceedings papers , up to recent high-quality typeset , born-digital journal articles , with varying layouts .", "We describe how paperXML , a logical document structure markup for scholarly articles , is generated on the basis of OCR tool outputs ."]}
{"orig_sents": ["3", "4", "2", "5", "1", "0"], "shuf_sents": ["We achieved 95 % precision and 80 % recall on this dataset .", "Furthermore , we prepared a small evaluation dataset , to test the efficiency of our method .", "We use Parscit , to process the Bibliography of each paper .", "In this paper we describe our participation in the contributed task at ACL Special workshop 2012 .", "We contribute to the goal of enriching the textual content of ACL Anthology by identifying the citation contexts in a paper and linking them to their corresponding references in the bibliography section .", "Pattern matching heuristics are then used to connect the citations with their references ."]}
{"orig_sents": ["5", "1", "0", "4", "3", "2", "6", "7"], "shuf_sents": ["To address this issue , we investigate these techniques analysing the following dimensions : expression type ( compound nouns , phrasal verbs ) , language ( English , French ) and corpus size .", "However , there is no agreement about which of them presents the best cost-benefit ratio , as they have been evaluated on distinct datasets and/or languages .", "70 % ) for verbals .", "80 % ) for nominals and high precision ( ?", "Results show that these techniques tend to extract similar candidate lists with high recall ( ?", "Several approaches have been proposed for the automatic acquisition of multiword expressions from corpora .", "The use of association measures for candidate filtering is useful but some of them are more onerous and not significantly better than raw counts .", "We finish with an evaluation of flexibility and an indication of which technique is recommended for each languagetype-size context ."]}
{"orig_sents": ["0", "2", "3", "1", "4"], "shuf_sents": ["In my thesis I propose a data-oriented study on how social power relations between participants manifest in the language and structure of online written dialogs .", "Using dialog and language features , I have built a system to predict participants possessing these types of power within email threads .", "I propose that there are different types of power relations and they are different in the ways they are expressed and revealed in dialog and across different languages , genres and domains .", "So far , I have defined four types of power and annotated them in corporate email threads in English and found support that they in fact manifest differently in the threads .", "I intend to extend this system to other languages , genres and domains and to improve it ? s performance using deeper linguistic analysis ."]}
{"orig_sents": ["2", "4", "0", "5", "6", "1", "3"], "shuf_sents": ["However , when the labeled instances are insufficient , the performance of active learning is limited .", "Thus , AVR gets benefits from source domain when it is helpful , and avoids the negative affects when it is harmful .", "In sentiment classification , unlabeled user reviews are often free to collect for new products , while sentiment labels are rare .", "Extensive experiments on toy data and review texts show our success , compared with other state-of-theart active learning approaches , as well as approaches with domain adaptation .", "In this case , active learning is often applied to build a high-quality classifier with as small amount of labeled instances as possible .", "In this paper , we aim at enhancing active learning by employing the labeled reviews from a different but related ( source ) domain .", "We propose a framework Active Vector Rotation ( AVR ) , which adaptively utilizes the source domain data in the active learning procedure ."]}
{"orig_sents": ["4", "2", "1", "0", "3"], "shuf_sents": ["Moreover , we show how Topic Model can be exploited to further enrich the query with hidden topics induced from the library meta-data .", "We show how click-through links , i.e. , the links that a user clicks after submitting a query , can be exploited for extracting information useful to enrich the query as well as for creating the training set for a machine learning based classifier .", "We take as a case study queries asked to a search engine of an art , cultural and history library and classify them against the library cataloguing categories .", "The experimental evaluations show that this system considerably outperforms a matching and ranking classification approach , where queries ( and categories ) were also enriched with similar information .", "This paper describes a query classification system for a specialized domain ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["We evaluate our system using standard shared tasks and also introduce new automated semantic evaluations and supervised baselines , both of which highlight the current limitations of existing Word Sense Induction evaluations .", "We propose evaluating various unsupervised ensembles when applied to the unsupervised task of Word Sense Induction with a framework for combining diverse feature spaces and clustering algorithms .", "Ensembles combine knowledge from distinct machine learning approaches into a general flexible system .", "While supervised ensembles frequently show great benefit , unsupervised ensembles prove to be more challenging ."]}
{"orig_sents": ["1", "0", "2", "4", "3"], "shuf_sents": ["This algorithm is based on the well-known TextTiling algorithm , and segments documents using the Latent Dirichlet Allocation ( LDA ) topic model .", "This work presents a Text Segmentation algorithm called TopicTiling .", "We show that using the mode topic ID assigned during the inference method of LDA , used to annotate unseen documents , improves performance by stabilizing the obtained topics .", "As an additional benefit , TopicTiling performs the segmentation in linear time and thus is computationally less expensive than other LDA-based segmentation methods .", "We show significant improvements over state of the art segmentation algorithms on two standard datasets ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Mean durations were compared with previous work by Gusev et al ( 2011 ) and it was found that there is a small positive correlation .", "Regular expressions were used to extract verbs and durations from each tweet in a corpus of more than 14 million tweets with 90.38 % precision covering 486 verb lemmas .", "Descriptive statistics for each verb lemma were found as well as the most typical fine-grained duration measure .", "This paper presents recent work on a new method to automatically extract finegrained duration information for common verbs using a large corpus of Twitter tweets ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["This paper reviews some of the major claims about the structure in discourse and proposes an investigation of discourse structure for simultaneous spoken Turkish by focusing on tree-violations and exploring ways to explain them away by non-structural means .", "The current debate regarding the data structure necessary to represent discourse structure , specifically whether tree-structure is sufficient to represent discourse structure or not , is mainly focused on written text ."]}
{"orig_sents": ["3", "4", "1", "0", "2"], "shuf_sents": ["Both applications can benefit from automatic MWE acquisition and the expressions acquired automatically from corpora can both speed up and improve their quality .", "After briefly presenting the modules of the framework , the paper reports extrinsic evaluation results considering two applications : computer-aided lexicography and statistical machine translation .", "The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into these and many other applications .", "This paper presents an open and flexible methodological framework for the automatic acquisition of multiword expressions ( MWEs ) from monolingual textual corpora .", "This research is motivated by the importance of MWEs for NLP applications ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Building knowledge bases for Chinese is of great importance on its own right .", "However , simply adapting existing tools from English to Chinese yields inferior results.In this paper , we propose to create Chinese knowledge bases from online resources with less human involvement.This project will be formulated in a self-supervised framework which requires little manual work to extract knowledge facts from online encyclopedia resources in a probabilistic view.In addition , this framework will be able to update the constructed knowledge base with knowledge facts extracted from up-to-date newswire.Currently , we have obtained encouraging results in our pilot experiments that extracting knowledge facts from infoboxes can achieve a high accuracy of around 95 % , which will be then used as training data for the extraction of plain webpages .", "Automatically constructing knowledge bases from online resources has become a crucial task in many research areas .", "Most existing knowledge bases are built from English resources , while few efforts have been made for other languages ."]}
{"orig_sents": ["4", "3", "2", "0", "1", "5"], "shuf_sents": ["We use a richer framework that allows for probabilistic generalization , with a word represented as a probability distribution over a space of generalized classes : lemmas , clusters , or synsets .", "Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features .", "The standard approach for lexical generalization in parsing is to map a word to a single generalized class , either replacing the word with the class or adding a new feature for the class .", "A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking .", "This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis .", "We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks , with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In the last decade , substantial progress has been made in the induction of semantic relations from raw text , especially of hypernymy and meronymy in the English language and in the classification of noun-noun relations in compounds or other contexts .", "We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German , by first introducing a hand-tagged dataset of associated noun-verb pairs for this task , and then provide classification results using a general framework for supervised classification of lexical relations ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["To the best of our knowledge , it is the first model that jointly clusters syntactic verbs arguments into semantic roles , and also creates verbs classes according to the syntactic frames accepted by the verbs .", "On English , it achieves results comparable to state-of-the-art unsupervised approaches to semantic role induction .", "The model is evaluated on French and English , outperforming , in both cases , a strong baseline .", "This paper introduces a novel unsupervised approach to semantic role induction that uses a generative Bayesian model ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["t of Linguisticsversley @ sfs.uni-tuebingen.de Verena HenrichUniversity of Tu ? bingenDepartment of Linguisticsverena.henrich @ uni-tuebingen.deAbstract In many morphologically rich languages , conceptually independent morphemes are glued together to form a new word ( a compound ) with a meaning that is often at least in part predictable from the meanings of the contributing morphemes .", "Assuming that most compounds express a subconcept of exactly one sense of its nominal head , we use compounds as a higher-quality alternative to simply using general second-order collocate terms in the task of word sense discrimination .", "We evaluate our approach using lexical entries from the German wordnet GermaNet ( Henrich and Hinrichs , 2010 ) ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["The paper presents a novel approach to extracting dependency information in morphologically rich languages using co-occurrence statistics based not only on lexical forms ( as in previously described collocation-based methods ) , but also on morphosyntactic and wordnet-derived semantic properties of words .", "Statistics generated from a corpus annotated only at the morphosyntactic level are used as features in a Machine Learning classifier which is able to detect which heads of groups found by a shallow parser are likely to be connected by an edge in the complete parse tree .", "The approach reaches the precision of 89 % and the recall of 65 % , with an extra 6 % recall , if only words present in the wordnet are considered ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Although parsing performances have greatly improved in the last years , grammar inference from treebanks for morphologically rich languages , especially from small treebanks , is still a challenging task .", "Providing state-of-the-art results on Spanish , our methodology is applicable to other languages with high level of inflection .", "We rely on accurate part-of-speech tagging and datadriven lemmatization to provide parsing models able to cope lexical data sparseness .", "In this paper we investigate how state-of-the-art parsing performances can be achieved on Spanish , a language with a rich verbal morphology , with a non-lexicalized parser trained on a treebank containing only around 2,800 trees ."]}
{"orig_sents": ["0", "1", "3", "4", "2", "5"], "shuf_sents": ["Deep linguistic grammars are able to provide rich and highly complex grammatical representations of sentences , capturing , for instance , long-distance dependencies and returning a semantic representation .", "These grammars lack robustness in the sense that they do not gracefully handle words missing from their lexicon .", "We investigate whether the use of features that encode discrete structures , namely grammatical dependencies , can improve the performance of a machine learning classifier that assigns deep lexical types .", "Several approaches have been explored to handle this problem , many of which consist in pre-annotating the input to the grammar with shallow processing machine-learning tools .", "Most of these tools , however , use features based on a fixed window of context , such as n-grams .", "In this paper we report on the design and evaluation of this classifier ."]}
{"orig_sents": ["5", "6", "4", "3", "2", "1", "0", "7"], "shuf_sents": ["Using only model agreements as features allows this method to remain language independent and applicable to a wide range of morphologically rich languages .", "We use this SVM classifier on an edge by edge decision to form an ensemble parse tree .", "Using these models we train an SVM classifier using only the model agreements as features .", "Using Tamil as our test language , we create 9 dependency parse models with a limited amount of training data .", "This paper examines a new approach for addressing morphologically rich languages with little training data to start .", "Dependency parsing has been shown to improve NLP systems in certain languages and in many cases helps achieve state of the art results in NLP applications , in particular applications for free word order languages .", "Morphologically rich languages are often short on training data or require much higher amounts of training data due to the increased size of their lexicon .", "We show a statistically significant 5.44 % improvement over the average dependency model and a statistically significant 0.52 % improvement over the best individual system ."]}
{"orig_sents": ["2", "6", "7", "4", "5", "3", "0", "1"], "shuf_sents": ["The methods are applied to Sejong treebank , which is the largest constituent treebank in Korean , and the transformed treebank is used to train and test various probabilistic CFG parsers .", "The experimental result shows that the proposed transformation methods reduce ambiguity in the training corpus , increasing the overall F1 score up to about 9 % .", "Korean is a morphologically rich language in which grammatical functions are marked by inflections and affixes , and they can indicate grammatical relations such as subject , object , predicate , etc .", "In this paper , we propose methods to transform eojeol-based Korean treebanks into entity-based Korean treebanks .", "Korean treebanks ( Choi et al , 1994 ; Han et al , 2002 ; Korean Language Institute , 2012 ) use eojeol as their fundamental unit of analysis , thus representing an eojeol as a prepreterminal phrase inside the constituent tree .", "This eojeol-based annotating schema introduces various complexity to train the parser , for example an entity represented by a sequence of nouns will be annotated as two or more different noun phrases , depending on the number of spaces used .", "A Korean sentence could be thought as a sequence of eojeols .", "An eojeol is a word or its variant word form agglutinated with grammatical affixes , and eojeols are separated by white space as in English written texts ."]}
{"orig_sents": ["3", "5", "4", "1", "0", "2"], "shuf_sents": ["We test our approach on the the Penn Treebank and the French Treebank .", "Our method is language agnostic and enables the incorporation of additional information which are useful for the choice of the best parse candidate .", "Evaluation shows a significative improvement on different parse metrics .", "We present an architecture for parsing in two steps .", "These dependency structures are then rescored by a discriminative reranker .", "A phrase-structure parser builds for each sentence an n-best list of analyses which are converted to dependency trees ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Feedback is given to the user by a graphical user interface and a speech synthesis system .", "By this , multimodal and natural communication with the robot system is possible .", "This contribution focuses on multimodal interaction techniques for a mobile communication and assistance system on a robot platform .", "The system comprises of acoustic , visual and haptic input modalities ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["experience of information can be considered to be supportive to this architecture .", "Also , we present an architecture and design of the integration mechanism with respect to information access in second language learning .", "This paper discusses the significance of the multimodal interaction in virtual environments ( VE ) and the criticalities involved in integration and coordination between modes during interaction .", "In this connection , we have conducted an experiential study on speech inputs to understand how far users ?"]}
{"orig_sents": ["4", "2", "0", "1", "3"], "shuf_sents": ["A variety of psychic , emotional as well as behavioral conditions can manifest at the same time .", "Systems that fail to take them into account might not only fail at joint tasks , but also risk damage to their interlocutors .", "Assistive systems intended for these user groups have to take their individual vulnerabilities into account .", "We identify important conditions and disorders and analyze their immediate consequences for the design of careful assistive systems .", "The VASA project develops a multimodal assistive system mediated by a virtual agent that is intended to foster autonomy of communication and activity management in older people and people with disabilities ."]}
{"orig_sents": ["5", "4", "2", "3", "1", "0"], "shuf_sents": ["This work focuses on noise reduction by a non-negative matrix factorization ( NMF ) approach to efficiently suppress non stationary noise produced by the sensors of an assisting robot system .", "However , noises produced during movement of robots can degrade the ASR performances .", "Natural and intuitive human-machine interfaces are an essential feature to achieve acceptance of the users .", "Therefore , automatic speech recognition ( ASR ) is a promising modality for such assistive devices .", "Robot systems are promising solutions but their value has to be acknowledged by the patients and the care personnel .", "Due to the demographic changes , support by means of assistive systems will become inevitable for home care and in nursing homes ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["In contrast to existing approaches , the vocal interface is self-learning , which means it can be used with any language , dialect , vocabulary and grammar .", "In addition , the paper describes encouraging results of early implementations of these vocabulary and grammar learning components , applied to recorded sessions of a vocally guided card game , Patience .", "This paper introduces research within the ALADIN project , which aims to develop an assistive vocal interface for people with a physical impairment .", "This paper describes the overall learning framework , and the two components that will provide vocabulary learning and grammar induction ."]}
{"orig_sents": ["2", "1", "3", "4", "0"], "shuf_sents": ["For speech generation we used Epoch Synchronous Non Overlap Add ( ESNOLA ) based concatenative speech synthesis technique which uses the partnemes as the smallest signal units for concatenations .", "However , porting these systems to a resource limited device such as a mobile phone is not an easy task .", "Different Bengali TTS systems are already available on a resourceful platform such as a personal computer .", "Practical aspects including application size and processing time have to be concerned .", "This paper describes the implementation of a Bengali speech synthesizer on a mobile device ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["These weights are used on SentiWordNet to compute a final estimation of the polarity .", "The evaluation over a generated corpus of tweets shows that this technique is promising .", "This paper presents a novel approach in Sentiment Polarity Detection on Twitter posts , by extracting a vector of weighted nodes from the graph of WordNet .", "Therefore , the method proposes a non-supervised solution that is domain-independent ."]}
{"orig_sents": ["2", "0", "5", "4", "3", "1", "6"], "shuf_sents": ["Tweets contain user opinion and sentiment towards an object or person .", "Using this scoring function we achieve classification accuracy of 87 % on Stanford Dataset and 88 % on Mejaj dataset .", "Twitter is a micro blogging website , where users can post messages in very short text called Tweets .", "We present a sentiment scoring function which uses prior information to classify ( binary classification ) and weight various sentiment bearing words/phrases in tweets .", "In this paper , we present a method which performs the task of tweet sentiment identification using a corpus of pre-annotated tweets .", "This sentiment information is very useful in various aspects for business and governments .", "Using supervised machine learning approach , we achieve classification accuracy of 88 % on Stanford dataset ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Our results suggest that we need individualized solutions for each domain and task , but that lemmatization is a feature in all the best approaches .", "We investigate : how to best represent lexical information ; whether standard features are useful ; how to treat Arabic dialects ; and , whether genre specific features have a measurable impact on performance .", "In this work , we present SAMAR , a system for Subjectivity and Sentiment Analysis ( SSA ) for Arabic social media genres ."]}
{"orig_sents": ["0", "3", "2", "4", "1", "5"], "shuf_sents": ["The classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach .", "In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards .", "It consists of building one probabilistic model for the positive and another one for the negative opinions .", "We propose an approach based on the order of the words without using any syntactic and semantic information .", "Then the test opinions are compared to both models and a decision and confidence measure are calculated .", "We present an accuracy above 81 % for Spanish opinions in the financial products domain ."]}
{"orig_sents": ["11", "8", "10", "4", "5", "0", "7", "1", "12", "9", "2", "3", "6"], "shuf_sents": ["We analyze the influential sentences from the following two points of view , 1 ) targets and evaluations and 2 ) personal tastes .", "and ? scenery ?", "are important evaluations which express sentiment or explain targets .", "Also we showed personal tastes appeared on ? meal ?", "In this paper , we conducted the analyses using the reader ? s point of view .", "We asked 20 subjects to read 500 sentences in the reviews of Rakuten travel and extracted the sentences that gave a big influence to the subjects .", "and ? service ? .", "We found that ? room ? , ? service ? , ? meal ?", "However the reviews are used by the readers .", "and ? human senses ?", "The reviews that give a big influence to the readers should have the highest value , rather than the reviews to which was assigned the highest score by the writer .", "In the NLP field , there have been a lot of works which focus on the reviewer ? s point of view conducted on sentiment analyses , which ranges from trying to estimate the reviewer ? s score .", "are important targets which are items included in the reviews , and that ? features ?"]}
{"orig_sents": ["1", "4", "3", "2", "0"], "shuf_sents": ["Our extensive evaluation scenarios show that SMT systems are mature enough to be reliably employed to obtain training data for languages other than English and that sentiment analysis systems can obtain comparable performances to the one obtained for English .", "The past years have shown a steady growth in interest in the Natural Language Processing task of sentiment analysis .", "To this aim , the present article deals with the problem of sentiment detection in three different languages - French , German and Spanish - using three distinct Machine Translation ( MT ) systems - Bing , Google and Moses .", "A less explored aspect has remained , however , the issue of dealing with sentiment expressed in texts in languages other than English .", "The research community in this field has actively proposed and improved methods to detect and classify the opinions and sentiments expressed in different types of text - from traditional press articles , to blogs , reviews , fora or tweets ."]}
{"orig_sents": ["7", "8", "5", "1", "4", "6", "3", "2", "0"], "shuf_sents": ["global positions on an issue .", "In this paper , we propose a new method for identifying participants ?", "Our experimental results have shown that aggregating local positions over posts yields better performance than nonaggregation baselines when identifying users ?", "We have explored the use of sentiment , emotional and durational features to improve the accuracy of automatic agreement and disagreement classification .", "agreement or disagreement on an issue by exploiting information contained in each of the posts .", "It is , however , a challenging task because of the informal language use and the dynamic nature of online conversations .", "Our proposed method first regards each post in its local context , then aggregates posts to estimate a participant ? s overall position .", "Online debate forums provide a powerful communication platform for individual users to share information , exchange ideas and express opinions on a variety of topics .", "Understanding people ? s opinions in such forums is an important task as its results can be used in many ways ."]}
{"orig_sents": ["1", "3", "2", "4", "0"], "shuf_sents": ["The goal of this work is to distinguish automatically between prior and contextual emotion , with a focus on exploring features important for this task .", "A set of words labelled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence , but it is clear that context must also be considered .", "We present a method which enables us to take the contextual emotion of a word and the syntactic structure of the sentence into account to classify sentences by emotion classes .", "No simple function of the labels on the individual words may capture the overall emotion of the sentence ; words are interrelated and they mutually influence their affectrelated interpretation .", "We show that this promising method outperforms both a method based on a Bag-of-Words representation and a system based only on the prior emotions of words ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["The results of this indicate that despite the difference in discourse function , the learned models perform favourably .", "Current approaches to sentiment analysis assume that the sole discourse function of sentiment-bearing texts is expressivity .", "In this work , we present the results of training supervised classifiers on a new corpus of clinical texts that contain documents with an expressive discourse function , and we test the learned models on a subset of the same corpus containing persuasive texts .", "However , the persuasive discourse function also utilises expressive language ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We show that annotators can distinguish these perspectives reliably and that correlation between the annotator ? s own perspective and that of a generic individual is higher than those with the narrator .", "Finally , as a sample application , we demonstrate that a simple compositional model built off of lexical resources outperforms a lexical baseline .", "The corpus , POLITICAL-ADS is drawn from 141 television ads from the 2008 U.S. presidential race and contains 3945 NPs and 1549 VPs annotated for scalar sentiment from three different perspectives : the narrator , the annotator , and general society .", "This paper presents a corpus targeting evaluative meaning as it pertains to descriptions of events ."]}
{"orig_sents": ["3", "5", "1", "6", "9", "4", "0", "2", "8", "7"], "shuf_sents": ["The annotations are evaluated in several ways .", "We choose the largest blog corpus for the language and annotate it with the use of two systems for affect analysis : ML-Ask for word- and sentence-level affect analysis and CAO for detailed analysis of emoticons .", "Firstly , on a test set of a thousand sentences extracted randomly and evaluated by over forty respondents .", "This paper presents our research on automatic annotation of a five-billion-word corpus of Japanese blogs with information on affect and sentiment .", "The annotations are also generalized on a 2-dimensional model of affect to obtain information on sentence valence/polarity ( positive/negative ) useful in sentiment analysis .", "We first perform a study in emotion blog corpora to discover that there has been no large scale emotion corpus available for the Japanese language .", "The annotated information includes affective features like sentence subjectivity ( emotive/non-emotive ) or emotion classes ( joy , sadness , etc .", "Finally , the corpus is applied in several tasks , such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions .", "Secondly , the statistics of annotations are compared to other existing emotion blog corpora .", ") , useful in affect analysis ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Here we discuss the difficulties of evaluating opinionated keyphrase extraction .", "We present our method to reduce the subjectivity of the task and to alleviate the evaluation process and we also compare the results of human and machine-based evaluation .", "Evaluation often denotes a key issue in semantics- or subjectivity-related tasks ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We argue that progress towards deep analysis depends on a ) enriching shallow representations with linguistically motivated , rich information , and b ) focussing different branches of research and combining ressources to create synergies with related work in NLP .", "Current work on sentiment analysis is characterized by approaches with a pragmatic focus , which use shallow techniques in the interest of robustness but often rely on ad-hoc creation of data sets and methods .", "In the paper , we propose SentiFrameNet , an extension to FrameNet , as a novel representation for sentiment analysis that is tailored to these aims ."]}
{"orig_sents": ["2", "4", "3", "0", "5", "1"], "shuf_sents": ["We have used a corpus of predators ?", "Naive Bayes classification based on the proposed features achieves accuracies of up to 94 % while baseline systems of word and character n-grams can only reach up to 72 % .", "According to previous work on pedophile psychology and cyberpedophilia , sentiments and emotions in texts could be a good clue to detect online sexual predation .", "In particular , since pedophiles are known to be emotionally unstable , we were interested in investigating if emotion-based features could help in their detection .", "In this paper , we have suggested a list of high-level features , including sentiment and emotion based ones , for detection of online sexual predation .", "chats with pseudo-victims downloaded from www.perverted-justice.com and two negative datasets of different nature : cybersex logs available online and the NPS chat corpus ."]}
{"orig_sents": ["2", "10", "4", "1", "9", "8", "3", "7", "0", "11", "6", "5"], "shuf_sents": ["Nasal filled pauses were more often followed by longer silent pauses .", "Attending physicians described more and used more filled pauses than residents .", "We explore filled pause usage in spontaneous medical narration .", "um ) and non-nasal ( e.g .", "The narratives were analyzed for differences in filled pauses used by attending ( experienced ) and resident ( in-training ) physicians and by male and female physicians .", "Also , we report on a computational model for predicting types of filled pause .", "The number of filled and silent pauses trends upward as correctness scores increase , indicating a tentative relationship between filled pause usage and expertise .", "uh ) .", "Acoustic speech features were examined for two types of filled pauses : nasal ( e.g .", "No difference was found by speaker gender .", "Expert physicians viewed images of dermatological conditions and provided a description while working toward a diagnosis .", "Scores capturing diagnostic correctness and diagnostic thoroughness for each narrative were compared against filled pauses ."]}
{"orig_sents": ["4", "0", "1", "3", "2"], "shuf_sents": ["Based on linguistic experiments informed by native speakers , we distill these effects according to the type of modality and negation .", "We show that each type has a specific effect on the opinion expression in its scope : both on the polarity and the strength for negation , and on the strength and/or the degree of certainty for modality .", "The methodology we used for deriving this basis was applied for French but it can be easily instantiated for other languages like English .", "The empirical results reported in this paper provide a basis for future opinion analysis systems that have to compute the sentiment orientation at the sentence or at the clause level .", "In this paper , we propose to study the effects of negation and modality on opinion expressions ."]}
{"orig_sents": ["1", "2", "4", "0", "3", "7", "5", "6"], "shuf_sents": ["We report on an unusual data set of spoken diagnostic narratives used to computationally model and predict diagnostic correctness based on automatically extracted and linguistically motivated features that capture physicians ?", "In the medical domain , misdiagnoses and diagnostic uncertainty put lives at risk and incur substantial financial costs .", "Clearly , medical reasoning and decision-making need to be better understood .", "uncertainty .", "We explore a possible link between linguistic expression and diagnostic correctness .", "We discuss experimentation and analysis in initial and secondary pilot studies .", "In both cases , we experimented with computational modeling using features from the acoustic-prosodic and lexical-structural linguistic modalities .", "A multimodal data set was collected as dermatologists viewed images of skin conditions and explained their diagnostic process and observations aloud ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In addition to evaluating factuality detection in isolation , we also evaluate its impact on a system for event detection .", "The two components for factuality detection and event detection form part of a system for identifying negative factual events , or counterfacts , with top-ranked results in the *SEM 2012 shared task .", "This paper describes a system for discriminating between factual and non-factual contexts , trained on weakly labeled data by taking advantage of information implicit in annotations of negated events ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In this paper we present an iterative methodology to improve classifier performance by incorporating linguistic knowledge , and propose a way to incorporate domain rules into the learning process .", "We applied the methodology to the tasks of hedge cue recognition and scope detection and obtained competitive results on a publicly available corpus ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["We study two approaches to the marking of extra-propositional aspects of statements in text : the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task , and the tagged-event representation applied in several recent event extraction tasks .", "Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research , we identify specific points of mismatch between the two perspectives and propose ways of addressing them .", "The system and resources introduced in this work are publicly available for research purposes at : https : //github.com/ninjin/eepura", "We demonstrate the feasibility of our approach by constructing a method that uses cue-and-scope analyses together with a small set of features motivated by data analysis to predict event negation and speculation .", "Evaluation on BioNLP Shared Task 2011 data indicates the method to outperform the negation/speculation components of state-of-theart event extraction systems ."]}
{"orig_sents": ["0", "4", "1", "3", "5", "2"], "shuf_sents": ["We explore training an automatic modality tagger .", "One of the main hurdles for training a linguistic tagger is gathering training data .", "We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance .", "This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences .", "Modality is the attitude that a speaker might have toward an event or state .", "We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Concentrating on their annotation process , this paper argues that the focusbased implicit positivity should be separated from concepts of scalar implicature and negraising , as well as the placement of stress .", "rates above .80 , but that it substantially deflates the rates of focus of negation in text .", "We show that a model making these distinctions clear and which incorporates the pragmatic notion of question under discussion yields ?", "Blanco & Moldovan ( Blanco and Moldovan , 2011 ) have empirically demonstrated that negated sentences often convey implicit positive inferences , or focus , and that these inferences are both human annotatable and machine learnable ."]}
{"orig_sents": ["2", "7", "1", "3", "6", "0", "4", "5"], "shuf_sents": ["We propose a detailed approach to studying whether hedge detection can be used to understanding scientific framing in the GMO debates , and provide corpora to facilitate this study .", "In the interests of furthering that goal , we propose the following specific , interesting and , we believe , relatively accessible question : In the controversy regarding the use of genetically-modified organisms ( GMOs ) in agriculture , do pro- and anti-GMO articles differ in whether they choose to adopt a more ? scientific ?", "Understanding the ways in which participants in public discussions frame their arguments is important in understanding how public opinion is formed .", "tone ?", "Some of our preliminary analyses suggest that hedges occur less frequently in scientific discourse than in popular text , a finding that contradicts prior assertions in the literature .", "We hope that our initial work and data will encourage others to pursue this promising line of inquiry .", "Prior work on the rhetoric and sociology of science suggests that hedging may distinguish popular-science text from text written by professional scientists for their colleagues .", "In this paper , we adopt the position that it is time for more computationallyoriented research on problems involving framing ."]}
{"orig_sents": ["0", "5", "6", "3", "2", "4", "7", "1"], "shuf_sents": ["In this paper we investigate two distinct tasks .", "In addition , we explore a two-phase approach to recognizing arguments , with promising results .", "We develop a new annotation scheme and assemble a new annotated corpus to support our learning efforts .", "We refer to these two tasks collectively as ? recognizing arguments ? .", "Through our machine learning experiments , we investigate the utility of a sentiment lexicon , discourse parser , and semantic similarity measures with respect to recognizing arguments .", "The first task involves detecting arguing subjectivity , a type of linguistic subjectivity on which relatively little work has yet to be done .", "The second task involves labeling instances of arguing subjectivity with argument tags reflecting the conceptual argument being made .", "By incorporating information gained from these resources , we outperform a unigram baseline by a significant margin ."]}
{"orig_sents": ["0", "3", "7", "6", "4", "5", "1", "2"], "shuf_sents": ["The current paper presents a languageindependent methodology , which facilitates the creation of machine translation ( MT ) systems for various language pairs .", "This paper proposes extracting information for disambiguation from the monolingual corpus .", "Experimental results indicate that such information substantially contributes in improving translation quality .", "This methodology is implemented in the PRESEMT hybrid MT system .", "The first one , Structure selection , determines the overall structure of a target language ( TL ) sentence , drawing on syntactic information from a small bilingual corpus .", "The second phase , Translation equivalent selection , relies on models extracted solely from monolingual corpora to implement translation disambiguation , determine intra-phrase word order and handle functional words .", "In PRESEMT , the main translation process comprises two phases .", "PRESEMT has the lowest possible requirements on specialised resources and tools , given that for many languages ( especially less widely used ones ) only limited linguistic resources are available ."]}
{"orig_sents": ["6", "8", "2", "1", "4", "7", "5", "0", "3"], "shuf_sents": ["Through this approach , we have availed the structured , semi-structured and multilingual content of the Wikipedia to a massive extent .", "The approach includes clustering of highly similar Wikipedia articles .", "We have substituted the required language specific resources by the richly structured multilingual content of Wikipedia .", "Experimental results suggest that the proposed approach yields promising results in rates of precision and recall .", "Then the NEs in an English article are mapped with other language terms in interlinked articles based on co-occurrence frequencies .", "Hence , the English Wikipedia is used to bootstrap the NEs for other languages .", "Recognition of Named Entities ( NEs ) is a difficult process in Indian languages like Hindi , Telugu , etc. , where sufficient gazetteers and annotated corpora are not available compared to English language .", "The cluster information and the term co-occurrences are considered in extracting the NEs from non-English languages .", "This paper details a novel clustering and co-occurrence based approach to map English NEs with their equivalent representations from different languages recognized in a language-independent way ."]}
{"orig_sents": ["4", "3", "1", "0", "2"], "shuf_sents": ["We experiment this model on two highly related and agglutinative languages namely Tamil and Telugu , and compare our results with the state of the art Morfessor system .", "The model is based on ( Goldwater et al. , 2006 ) unigram word segmentation model and assumes a simple prior distribution over morph length .", "We show that , knowledge of morph length has a positive impact and provides competitive results in terms of overall performance .", "In this paper , we introduce a simple unsupervised model for morphological segmentation and study how the knowledge of morph length affect the performance of the segmentation task under the Bayesian framework .", "Morph length is one of the indicative feature that helps learning the morphology of languages , in particular agglutinative languages ."]}
{"orig_sents": ["3", "2", "0", "4", "6", "1", "5"], "shuf_sents": ["The building resource process is exemplified by the construction of annotated comparable corpora in English , Portuguese , and French .", "The main goal of this paper is to describe the design methodology followed by the creation of the corpora .", "This resource can be exploited to foster research on multilingual corpus-based ontology learning , population and matching .", "In this paper we present a methodology for building comparable corpus , using multilingual ontologies of a scpecific domain .", "The corpora , from the conference organization domain , are built using the multilingual ontology concept labels as seeds for crawling relevant documents from the web through a search engine .", "We present a preliminary evaluation and discuss their characteristics and potential applications .", "Using ontologies allows a better coverage of the domain ."]}
{"orig_sents": ["4", "0", "2", "5", "3", "1"], "shuf_sents": ["Our game aims at annotating sentiments of a collection of text documents and simultaneously constructing a highly discriminative lexicon of positive and negative phrases .", "Obtained results are promising and show improvements over traditional approaches .", "Human computation games have been widely used in recent years to acquire human knowledge and use it to solve problems which are infeasible to solve by machine intelligence .", "We compare the results obtained by the game with that of other well-known sentiment detection approaches .", "In this paper , we propose a novel human computation game for sentiment analysis .", "We package the problems of lexicon construction and sentiment detection as a single human computation game ."]}
{"orig_sents": ["2", "4", "3", "0", "1"], "shuf_sents": ["As this is an ongoing project , we discuss challenges and long-term goals.We present the current network in terms a quantitative and qualitative analysis , comparing it to other resources .", "Finally , we describe our target applications .", "This paper presents a game with a purpose for the construction of a Portuguese lexicalsemantic network .", "We describe the principles and implementation of the platform .", "The network creation is implicit , as players collaboratively create links between words while they have fun ."]}
{"orig_sents": ["3", "2", "1", "4", "0", "5"], "shuf_sents": ["The approach also supports the building of resources such as parallel corpora ?", "The architectural approach chosen enables collaborative , in-context , and realtime localisation of web content supported by the crowd and high-quality language resources .", "of web content capturing and aligning source and target content produced by the ? power of the crowd ? .", "In this paper , we propose the collaborative construction of language resources ( translation memories ) using a novel browser extension-based client-server architecture that allows translation ( or ? localisation ? )", "To the best of our knowledge , this is the only practical web content localisation methodology currently being proposed that incorporates the collaborative construction and use of TMs .", "resources that are still not available for many , and especially not for underserved languages ."]}
{"orig_sents": ["5", "6", "1", "3", "2", "4", "0"], "shuf_sents": ["The user studies strongly suggest that the proposed approach successfully resolve task specification and path inconsistency in taxonomy construction .", "Most existing browsing taxonomies are manually constructed thus they could not easily adapt to arbitrary document collections .", "Particular , we focus on encoding user feedback in taxonomy construction process to handle task-specification rising from a given document collection .", "In this paper , we investigate both automatic and interactive techniques to derive taxonomies from scratch for arbitrary document collections .", "We also addresses the problem of path inconsistency due to local relation recognition in existing taxonomy construction algorithms .", "Taxonomies , such as Library of Congress Subject Headings and Open Directory Project , are widely used to support browsing-style information access in document collections .", "We call them browsing taxonomies ."]}
{"orig_sents": ["2", "1", "0", "3", "5", "4"], "shuf_sents": ["Like previous approaches , we value WIKIPEDIA as a huge , well-curated , and relatively unbiased source of entities .", "Exploiting current sources of structured information , we propose a novel method for extending minimal seed lists into complete gazetteers .", "Key to named entity recognition , the manual gazetteering of entity lists is a costly , errorprone process that often yields results that are incomplete and suffer from sampling bias .", "However , in contrast to previous work , we exploit not only its content , but also its structure , as exposed in DBPEDIA .", "The resulting gazetteers easily outperform previous approaches on named entity recognition .", "We extend gazetteers through Wikipedia categories , carefully limiting the impact of noisy categorizations ."]}
{"orig_sents": ["3", "4", "0", "2", "1"], "shuf_sents": ["We propose a methodology for the automatic acquisition of large scale context-rich entailment rules from Wikipedia revisions , taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable .", "acquired rules are not present in other available resources ) and good quality rule repository .", "We report on rule acquisition experiments on Wikipedia , showing that it enables the creation of an innovative ( i.e .", "Recent work on Textual Entailment has shown a crucial role of knowledge to support entailment inferences .", "However , it has also been demonstrated that currently available entailment rules are still far from being optimal ."]}
{"orig_sents": ["1", "0", "4", "3", "2"], "shuf_sents": ["In this paper we present a parametric method that employs a combination of both hard and soft clustering .", "Relational clustering has received much attention from researchers in the last decade .", "The effectiveness of the proposed approach is demonstrated on several real datasets against spectral clustering methods .", "This probabilistic model would enable us to use expectation maximization for parameter estimation .", "Based on the corresponding Markov chain of an affinity matrix , we simulate a probability distribution on the states by defining a conditional probability for each subpopulation of states ."]}
{"orig_sents": ["4", "0", "1", "6", "7", "2", "8", "3", "5", "10", "9"], "shuf_sents": ["There are much more insights that we may gain by generalizing social networks to the signed case where both positive and negative edges are considered .", "One of the reasons why signed social networks have received less attention that networks based on positive links only is the lack of an explicit notion of negative relations in most social network applications .", "In this work , we propose a new method to automatically construct a signed social network from text .", "Edge polarity is a means for indicating a positive or negative affinity between two individuals .", "Most of the research on social networks has almost exclusively focused on positive links between entities .", "We apply the proposed method to a larger amount of online discussion posts .", "However , most such applications have text embedded in the social network .", "Applying linguistic analysis techniques to this text enables us to identify both positive and negative interactions .", "The resulting networks have a polarity associated with every edge .", "We also connect out analysis to social psychology theories of signed network , namely the structural balance theory .", "Experiments show that the proposed method is capable of constructing networks from text with high accuracy ."]}
{"orig_sents": ["4", "1", "3", "2", "0"], "shuf_sents": ["Experimental results on real world data demonstrate that the proposed method can achieve better performance than several baseline methods .", "Considering the number of times that a message is retweeted across Twitter is a straightforward way to estimate how interesting it is .", "In this paper , we leverage retweets as implicit relationships between Twitter users and messages and address the problem of automatically finding messages in Twitter that may be of potential interest to a wide audience by using link analysis methods that look at more than just the sheer number of retweets .", "However , a considerable number of messages in Twitter with high retweet counts are actually mundane posts by celebrities that are of interest to themselves and possibly their followers .", "Twitter , a popular social networking service , enables its users to not only send messages but re-broadcast or retweet a message from another Twitter user to their own followers ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["A constrained graph walk variant that has been successfully applied in the past in similar settings is shown to outperform a state-of-the-art syntactic vectorbased approach on this task .", "Further , we show that learning specialized similarity measures for different word types is advantageous .", "We learn graph-based similarity measures for the task of extracting word synonyms from a corpus of parsed text ."]}
{"orig_sents": ["2", "4", "3", "1", "0"], "shuf_sents": ["The proposed method achieves competitive performance on a benchmark dataset .", "Word sense disambiguation is performed on a disambiguation graph via a vertex centrality measure .", "This paper presents a graph-based method for all-word word sense disambiguation of biomedical texts using semantic relatedness as edge weight .", "The sense inventory is generated by the MetaMap program .", "Semantic relatedness is derived from a term-topic co-occurrence matrix ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["In this paper we present the SDOIrmi text graph-based semi-supervised algorithm for the task for relation mention identification when the underlying concept mentions have already been identified and linked to an ontology .", "To overcome the lack of annotated data , we propose a labelling heuristic based on information extracted from the ontology .", "We evaluated the algorithm on the kdd09cma1 dataset using a leave-one-document-out framework and demonstrated an increase in F1 in performance over a co-occurrence based AllTrue baseline algorithm .", "An extrinsic evaluation of the predictions suggests a worthwhile precision on the more confidently predicted additions to the ontology ."]}
{"orig_sents": ["1", "0", "5", "3", "2", "4"], "shuf_sents": ["Many efforts have been payed on is-a and part-of relation leaning , however few have focused on cause-effect learning .", "To be able to answer the question What causes tumors to shrink ? , one would require a large cause-effect relation repository .", "To evaluate the performance of the acquired causeeffect terms , we conduct three evaluations : ( 1 ) human-based , ( 2 ) comparison with existing knowledge bases and ( 3 ) application driven ( SemEval-1 Task 4 ) in which the goal is to identify the relation between pairs of nominals .", "To filter out the erroneously extracted information , we incorporate graph-based methods .", "The results show that the extractions at rank 1500 are 89 % accurate , they comprise 61 % from the terms used in the SemEval-1 Task 4 dataset and can be used in the future to produce additional training examples for the same task .", "This paper describes an automated bootstrapping procedure which can learn and produce with minimal effort a causeeffect term repository ."]}
{"orig_sents": ["4", "5", "1", "0", "6", "2", "3"], "shuf_sents": ["In this paper , we proposed a novel approach for bringing the associative ability to model the social tagging behavior and then to enhance the performance of automatic tag recommendation .", "Modeling the social tagging behavior could better reflect the nature of this issue and improve the result of recommendation .", "The semantic relationships are learnt via a word alignment model in statistical machine translation on large datasets .", "Experiments on real world datasets demonstrate that our method is effective , robust and language-independent compared with the stateof-the-art methods .", "Social tagging systems , which allow users to freely annotate online resources with tags , become popular in the Web 2.0 era .", "In order to ease the annotation process , research on social tag recommendation has drawn much attention in recent years .", "To simulate human tagging process , our approach ranks the candidate tags on a weighted digraph built by the semantic relationships among meaningful words in the summary and the corresponding tags for a given resource ."]}
{"orig_sents": ["3", "2", "1", "4", "0"], "shuf_sents": ["Both approaches lead to significant improvements in translation performance , highlighting the usefulness of source side disambiguation for SMT .", "First , the probabilities serve to rerank a list of n-best translations produced by the system .", "A Word Sense Disambiguation ( WSD ) classifier produces a probability distribution over the translation candidates of source words which is exploited in two ways .", "We integrate semantic information at two stages of the translation process of a state-ofthe-art SMT system .", "Second , the WSD predictions are used to build a supplementary language model for each sentence , aimed to favor translations that seem more adequate in this specific sentential context ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["In this paper , we present our linguisticallyenriched Bulgarian-to-English statistical machine translation model , which takes a statistical machine translation ( SMT ) system as backbone various linguistic features as factors .", "The automatic evaluation has shown promising results and our extensive manual analysis confirms the high quality of the translation the system delivers .", "The whole framework is also extensible for incorporating information provided by different sources .", "The motivation is to take advantages of both the robustness of the SMT system and the rich linguistic knowledge from morphological analysis as well as the hand-crafted grammar resources ."]}
{"orig_sents": ["4", "3", "0", "2", "1"], "shuf_sents": ["The corpus expansion is achieved by high quality rephrasing of existing sentences to their negated counterparts making use of semantic transfer .", "Our results show an overall improvement of 0.16 BLEU points , with a statistically significant increase of 1.63 BLEU points when tested on only negated test data .", "The method is designed to work on both sides of the parallel corpus while preserving the alignment .", "In particular this contribution is targeted towards tackling the poor performance of a state-of-the-art system on negated sentences .", "This paper presents an approach to improving performance of statistical machine translation by automatically creating new training data for difficult to translate phenomena ."]}
{"orig_sents": ["3", "1", "2", "0", "4"], "shuf_sents": ["HMEANT proves to correlate with manual rankings at the sentence level better than a range of automatic metrics .", "We relate HMEANT to an established linguistic theory , highlighting the possibilities of reusing existing knowledge and resources for interpreting and automating HMEANT .", "We apply HMEANT to a new language , Czech in particular , by evaluating a set of Englishto-Czech MT systems .", "HMEANT ( Lo and Wu , 2011a ) is a manual MT evaluation technique that focuses on predicate-argument structure of the sentence .", "However , the main contribution of this paper is the identification of several issues of HMEANT annotation and our proposal on how to resolve them ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["We evaluate the modified parser on DEPFIX , a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input .", "In this paper , we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation ( SMT ) , which pose problems to standard parsers due to their frequent ungrammaticality .", "Both parser modifications led to improvements in BLEU score ; their combination was evaluated manually , showing a statistically significant improvement of the translation quality .", "We adapt the MST parser by exploiting additional features from the source language , and by introducing artificial grammatical errors in the parser training data , so that the training sentences resemble SMT output ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We present an unsupervised approach to estimate the appropriate degree of contribution of each semantic role type for semantic translation evaluation , yielding a semantic MT evaluation metric whose correlation with human adequacy judgments is comparable to that of recent supervised approaches but without the high cost of a human-ranked training corpus .", "Empirical results show that even without a training corpus of human adequacy rankings against which to optimize correlation , using instead our relative frequency weighting scheme to approximate the importance of each semantic role type leads to a semantic MT evaluation metric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus .", "As a result , the cost of semantic MT evaluation is greatly reduced .", "Our new unsupervised estimation approach is motivated by an analysis showing that the weights learned from supervised training are distributed in a similar fashion to the relative frequencies of the semantic roles ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["Here , we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair .", "In Statistical Machine Translation , reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different .", "Experimental results show substantially improvements ( from 20.70 to 23.17 BLEU ) when head-finalization rules based on HPSG parses are used , and further gains ( to 24.14 BLEU ) were obtained using more refined rules .", "Linguistically motivated rules have been incorporated into Chineseto-English ( Wang et al , 2007 ) and Englishto-Japanese ( Isozaki et al , 2010b ) translation with significant gains to the statistical translation system ."]}
{"orig_sents": ["3", "0", "4", "2", "5", "1"], "shuf_sents": ["First a ? shallow ?", "The procedures were employed on a 10 million word Japanese English parallel corpus and 190,000 semantic transfer rules were extracted .", "method where the parallel corpus is parsed by deep parsers before the resulting predicates are aligned by phrase aligners .", "This paper presents two procedures for extracting transfer rules from parallel corpora for use in a rule-based Japanese-English MT system .", "method where the parallel corpus is lemmatized before it is aligned by a phrase aligner , and then a ? deep ?", "In both procedures , the phrase tables produced by the phrase aligners are used to extract semantic transfer rules ."]}
{"orig_sents": ["6", "0", "3", "1", "4", "2", "5"], "shuf_sents": ["They flexibly capture many kinds of stateful left-toright substitution , simple transducers can be composed into more complex ones , and they are EM- trainable .", "Tree automata have been profitably used in syntaxbased MT systems .", "Feature structures provide an attractive , well-studied , standard format ( Shieber , 1986 ; Rounds and Kasper , 1986 ) , which we can view computationally as directed acyclic graphs .", "They are unable to handle long-range syntactic movement , but tree acceptors and transducers address this weakness ( Knight and Graehl , 2005 ) .", "Still , strings and trees are both weak at representing linguistic structure involving semantics and reference ( ? who did what to whom ? ) .", "In this paper , we develop probabilistic acceptors and transducers for feature structures , demonstrate them on linguistic problems , and lay down a foundation for semantics-based MT .", "Weighted finite-state acceptors and transducers ( Pereira and Riley , 1997 ) are a critical technology for NLP and speech systems ."]}
{"orig_sents": ["5", "6", "3", "0", "2", "1", "4"], "shuf_sents": ["The first finding of our research is that the vocabulary and the structure of the parallel corpus are important .", "the domain-specific resources , whereby some translation evaluation metrics outperformed the results of Google Translate .", "By integrating the multilingual knowledge base Wikipedia , we further improved the translation wrt .", "We learned that a domainspecific resource produces better results than a bigger , but more general one .", "This finding leads us to the conclusion that a hybrid translation system , a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-specific terms .", "In this article we investigate the translation of terms from English into German and vice versa in the isolation of an ontology vocabulary .", "For this study we built new domainspecific resources from the translation search engine Linguee and from the online encyclopedia Wikipedia ."]}
{"orig_sents": ["5", "3", "2", "1", "0", "6", "4"], "shuf_sents": ["Considering them as a single entity improves the translation of the verbal construct and thus the overall quality of the translation .", "Identification of verb parts in a sentence is essential for its understanding and they constitute as if they are a single entity .", "Mere statistical methods of machine translation are not sufficient enough to consider this aspect .", "In translating English to morphologically richer language like Hindi , the organization and the order of verbal constructs contributes to the fluency of the language .", "The steps taken towards reducing sparsity further helped in improving the translation results .", "Verb plays a crucial role of specifying the action or function performed in a sentence .", "The paper describes a strategy for pre-processing and for identification of verb parts in source and target language corpora ."]}
{"orig_sents": ["4", "1", "3", "0", "6", "2", "5"], "shuf_sents": ["We introduced ? antecedent F-measure ?", "Such Japanese sentences are often translated by Japanese-English statistical machine translation to the English sentence whose subjective , objective and possessive cases are omitted , and it causes to decrease the quality of translation .", "As a result , we found that it improves the scores of antecedent F-measure while the BLEU scores were almost unchanged .", "We performed experiments of J-E phrase based translation using Japanese sentence , whose omitted pronouns are complemented by human .", "In Japanese , particularly , spoken Japanese , subjective , objective and possessive cases are very often omitted .", "Every effectiveness of the zero pronoun resolution differs depending on the type and case of each zero pronoun .", "as a score for measuring quality of the translated English ."]}
{"orig_sents": ["0", "3", "2", "4", "1"], "shuf_sents": ["Comparisons play a critical role in scientific communication by allowing an author to situate their work in the context of earlier research problems , experimental approaches , and results .", "Experiments show an F1 score of 0.71 , 0.69 , and 0.74 on the development set and 0.76 , 0.65 , and 0.74 on a validation set for the NB , SVM and BN , respectively .", "In this paper , we introduce a set of semantic and syntactic features that characterize a sentence and then demonstrate how those features can be used in three different classifiers : Na ? ve Bayes ( NB ) , a Support Vector Machine ( SVM ) and a Bayesian network ( BN ) .", "Our goal is to identify comparison claims automatically from full-text scientific articles .", "Experiments were conducted on 122 full-text toxicology articles containing 14,157 sentences , of which 1,735 ( 12.25 % ) were comparisons ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Key knowledge components of biological research papers are conveyed by structurally and rhetorically salient sentences that summarize the main findings of a particular experiment .", "We assume that CKUs can be detected automatically with state-ofthe-art text analysis tools , and suggest some applications for presenting CKUs in knowledge bases and scientific browsing interfaces .", "We provide evidence that CKUs convey the most important new factual information , and thus demonstrate that rhetorical salience is a systematic discourse structure indicator in biology articles along with structural salience .", "In this article we define such sentences as Claimed Knowledge Updates ( CKUs ) , and propose using them in text mining tasks ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["Current research assumes that using just the citation sentence is enough for detecting sentiment .", "We explore methods to automatically identify these mentions and show that the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment .", "Sentiment analysis of citations in scientific papers is a new and interesting problem which can open up many exciting new applications in bibliometrics .", "We present a new corpus in which all mentions of a cited paper have been annotated .", "In this paper , we show that this approach misses much of the existing sentiment ."]}
{"orig_sents": ["3", "0", "4", "5", "6", "1", "2"], "shuf_sents": ["Although a number of resources and methods addressing aspects of the task have been introduced , there have so far been no annotated corpora for training and evaluating systems for broad-coverage , open-domain anatomical entity mention detection .", "The combined system demonstrates a promising level of performance , approaching 80 % F-score for mention detection for a relaxed matching criterion .", "The corpus and other introduced resources are available under open licences from http : // www.nactem.ac.uk/anatomy/ .", "Anatomical entities such as kidney , muscle and blood are central to much of biomedical scientific discourse , and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts .", "We introduce the AnEM corpus , a domain- and species-independent resource manually annotated for anatomical entity mentions using a fine-grained classification system .", "The corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature .", "We demonstrate the use of the corpus through an evaluation of the broad-coverage MetaMap tagger and a CRF-based system trained on the corpus data , considering also a combination of these two methods ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We present our analysis of the comparison , and a discussion of the contributions of each scheme .", "ay Perspective on Scientific Discourse Annotation for Knowledge Extraction Maria Liakata Aberystwyth University , UK / EMBL-EBI , UK liakata @ ebi.ac.uk Paul Thompson University of Manchester , UK paul.thompson @ manchester.ac.uk Anita de Waard Elsevier Labs , USA / UiL-OTS , Universiteit Utrecht , NL a.dewaard @ elsevier.com Raheel Nawaz University of Manchester , UK raheel.nawaz @ cs.man.ac.uk Henk Pander Maat UiL-OTS , Universiteit Utrecht , NL h.l.w.pandermaat @ uu.nl Sophia Ananiadou University of Manchester , UK sophia.ananiadou @ manchester.ac.uk Abstract This paper presents a three-way perspective on the annotation of discourse in scientific literature .", "One scheme seeks to identify the core components of scientific investigations at the sentence level , a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level .", "We use three different schemes , each of which focusses on different aspects of discourse in scientific articles , to annotate a corpus of three full-text papers , and compare the results ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["In particular , we find that matrix clauses with a reporting verb of the form ? These results suggest ? , are the predominant feature indicating knowledge attribution in scientific text .", "Based on a literature review , we investigate four linguistic features that mark different types epistemic evaluation ( modal auxiliary verbs , adverbs/adjectives , reporting verbs and references ) .", "Waard Henk Pander Maat Elsevier Labs Utrecht Institute of Linguistics Jericho , VT , USA Utrecht , The Netherlands a.dewaard @ elsevier.com h.l.w.pandermaat @ uu.nl Abstract We propose a model for knowledge attribution and epistemic evaluation in scientific discourse , consisting of three dimensions with different values : source ( author , other , unknown ) ; value ( unknown , possible , probable , presumed true ) and basis ( reasoning , data , other ) .", "A corpus study on two biology papers indicates the usefulness of this model , and suggest some typical trends ."]}
{"orig_sents": ["0", "4", "3", "6", "5", "2", "1"], "shuf_sents": ["This report documents the Machine Transliteration Shared Task conducted as a part of the Named Entities Workshop ( NEWS 2012 ) , an ACL 2012 workshop .", "We believe that the shared task has successfully achieved its objective by providing a common benchmarking platform for the research community to evaluate the state-of-the-art technologies that benefit the future research and development .", "We report the results with 4 performance metrics .", "In total , 14 tasks are provided .", "The shared task features machine transliteration of proper names from English to 11 languages and from 3 languages to English .", "Finally , 57 standard and 1 non-standard runs are submitted , where diverse transliteration methodologies are explored and reported on the evaluation data .", "7 teams participated in the evaluations ."]}
{"orig_sents": ["2", "4", "1", "0", "5", "3"], "shuf_sents": ["The results are strong , with F > 0.85 for purely unsupervised namedentity recognition across languages , compared to just F = 0.35 on the same data for supervised cross-domain named-entity recognition within a language .", "It is completely unsupervised , with no manually labeled items , no external resources , only using parallel text that does not need to be easily alignable .", "We present a new approach to named-entity recognition that jointly learns to identify named-entities in parallel text .", "We conclude that we have found a viable new strategy for unsupervised named-entity recognition across lowresource languages and for domain-adaptation within high-resource languages .", "The system generates seed candidates through local , cross-language edit likelihood and then bootstraps to make broad predictions across both languages , optimizing combined contextual , word-shape and alignment models .", "A combination of unsupervised and supervised methods increases the accuracy to F = 0.88 ."]}
{"orig_sents": ["0", "7", "3", "2", "8", "6", "1", "5", "4"], "shuf_sents": ["Transliteration has been usually recognized by spelling-based supervised models .", "However , this model , which can be formulated as unigram mixture , is prone to overfitting since it is based on maximum likelihood estimation .", "and ? target ? .", "in ? piaget ?", "We have shown that the proposed method considerably outperform the conventional transliteration models .", "We propose a novel latent semantic transliteration model based on Dirichlet mixture , where a Dirichlet mixture prior is introduced to mitigate the overfitting problem .", "In contrast to their model which requires an explicitly tagged training corpus with language origins , Hagiwara and Sekine ( 2011 ) have proposed the latent class transliteration model , which models language origins as latent classes and train the transliteration table via the EM algorithm .", "However , a single model can not deal with mixture of words with different origins , such as ? get ?", "Li et al ( 2007 ) propose a class transliteration method , which explicitly models the source language origins and switches them to address this issue ."]}
{"orig_sents": ["2", "4", "5", "1", "3", "0"], "shuf_sents": ["The corpora are freely available .", "In contrast to recent work , we apply a new method , which maps the DBpedia classes into CoNLL NE types .", "Supervised Named Entity Recognizers require large amounts of annotated text .", "Since our method is mainly languageindependent , we used it to generate corpora for English and Hungarian .", "Since manual annotation is a highly costly procedure , reducing the annotation cost is essential .", "We present a fully automatic method to build NE annotated corpora from Wikipedia ."]}
{"orig_sents": ["0", "3", "6", "1", "5", "4", "2"], "shuf_sents": ["This paper describes our syllable-based phrase transliteration system for the NEWS 2012 shared task on English-Chinese track and its back .", "In this paper we utilize Phrase-based model to solve machine transliteration with the mapping between Chinese characters and English syllables rather than English characters .", "The primary system achieved 0.330 on Chinese-English and 0.177 on English-Chinese in terms of top-1 accuracy .", "Grapheme-based Transliteration maps the character ( s ) in the source side to the target character ( s ) directly .", "This transliteration model also incorporates three phonetic features to enhance discriminative ability for phrase .", "Two heuristic rulebased syllable segmentation algorithms are applied .", "However , character-based segmentation on English side will cause ambiguity in alignment step ."]}
{"orig_sents": ["3", "4", "1", "5", "2", "0"], "shuf_sents": ["Our standard and non-standard runs achieves 0.398 and 0.458 in top-1 accuracy in the generation task .", "Then , we use two reranking methods to select the best transliteration among the prediction results from the different models .", "The other one is the JLIS-Reranking method which is based on the features from the alignment results .", "In this paper , we describe our approach to English-to-Korean transliteration task in NEWS 2012 .", "Our system mainly consists of two components : an letter-to-phoneme alignment with m2m-aligner , and transliteration training model DirecTL-p. We construct different parameter settings to train several transliteration models .", "One re-ranking method is based on the co-occurrence of the transliteration pair in the web corpora ."]}
{"orig_sents": ["2", "0", "3", "1", "5", "4"], "shuf_sents": ["Our results show that mpaligner is greatly better than m2m-aligner , and the Japanese-specific heuristics are effective for JnJk and EnJa tasks .", "In JnJk and EnJa tasks , it is crucial to handle long alignment .", "We developed a machine transliteration system combining mpaligner ( an improvement of m2m-aligner ) , DirecTL+ , and some Japanesespecific heuristics for the purpose of NEWS 2012 .", "While m2m-aligner is not good at long alignment , mpaligner performs well at longer alignment without any length limit .", "In EnJa task , it is shown that mora is the best alignment unit for Japanese language .", "An experimental result revealed that de-romanization , which is reverse operation of romanization , is crucial for JnJk task ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["We then present two innovations : a training objective that optimizes toward any of a set of possible correct labels ( since more than one transliteration is often possible for a particular input ) , and a k-best reranking stage to incorporate nonlocal features .", "We consider the task of generating transliterated word forms .", "To allow for a wide range of interacting features , we use a conditional random field ( CRF ) sequence labeling model .", "This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["In particular , we investigate a syllable-based Pinyin intermediate representation for Chinese , and a letter mapping for Arabic .", "We report the results of our transliteration experiments with language-specific adaptations in the context of two language pairs : English to Chinese , and Arabic to English ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Experiment results show that two-stage CRF method outperforms the one-stage opponent since the former costs less to encode more features and finer grained labels than the latter .", "This work presents an English-to-Chinese ( E2C ) machine transliteration system based on two-stage conditional random fields ( CRF ) models with accessor variety ( AV ) as an additional feature to approximate local context of the source language ."]}
{"orig_sents": ["7", "6", "5", "2", "8", "4", "1", "3", "0"], "shuf_sents": ["Having a standard test set and standard evaluation parameters , all based on a resource that provides multiple integrated annotation layers ( syntactic parses , semantic roles , word senses , named entities and coreference ) and in multiple languages could support joint modeling and help ground and energize ongoing research in the task of entity and event coreference .", "The task of coreference has had a complex evaluation history .", "OntoNotes provides a largescale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types , and covers multiple languages .", "Potentially many evaluation conditions , have , in the past , made it difficult to judge the improvement in new algorithms over previously reported results .", "This paper describes the OntoNotes annotation ( coreference and other layers ) and then describes the parameters of the shared task including the format , pre-processing information , evaluation criteria , and presents and discusses the results achieved by the participating systems .", "Until the creation of the OntoNotes corpus , resources in this sub-field of language processing were limited to noun phrase coreference , often on a restricted set of entities , such as the ACE entities .", "It was a follow-on to the English-only task organized in 2011 .", "The CoNLL-2012 shared task involved predicting coreference in English , Chinese , and Arabic , using the final version , v5.0 , of the OntoNotes corpus .", "OntoNotes also provides additional layers of integrated annotation , capturing additional shallow semantic structure ."]}
{"orig_sents": ["3", "0", "2", "4", "1", "5"], "shuf_sents": ["The proposed latent tree modeling turns the learning problem computationally feasible .", "We apply the same system to all languages , except for minor adaptations on some language dependent features , like static lists of pronouns .", "Additionally , using an automatic feature induction method , we are able to efficiently build nonlinear models and , hence , achieve high performances with a linear learning algorithm .", "We describe a machine learning system based on large margin structure perceptron for unrestricted coreference resolution that introduces two key modeling techniques : latent coreference trees and entropy guided feature induction .", "Our system is evaluated on the CoNLL2012 Shared Task closed track , which comprises three languages : Arabic , Chinese and English .", "Our system achieves an official score of 58.69 , the best one among all the competitors ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper describes our contribution to the CoNLL 2012 Shared Task.1 We present a novel decoding algorithm for coreference resolution which is combined with a standard pair-wise coreference resolver in a stacking approach .", "We obtain an official overall score of 58.25 which is the second highest in the Shared Task .", "The stacked decoders are evaluated on the three languages of the Shared Task ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["In particular , our score on the Chinese test set is the best among the participating teams .", "We adopt a hybrid approach to coreference resolution , which combines the strengths of rule-based methods and learningbased methods .", "We describe our system for the CoNLL-2012 shared task , which seeks to model coreference in OntoNotes for English , Chinese , and Arabic .", "Our official combined score over all three languages is 56.35 ."]}
{"orig_sents": ["3", "5", "7", "1", "0", "2", "6", "4"], "shuf_sents": ["We carefully redesigned the features so that they reflect more complex linguistic phenomena as well as discourse properties .", "We designed a new mention detection module that removes pleonastic pronouns , prunes constituents , and recovers mentions when they do not match exactly a noun phrase .", "Finally , we introduced a minimal cluster model grounded in the first mention of an entity .", "This paper describes the structure of the LTH coreference solver used in the closed track of the CoNLL 2012 shared task ( Pradhan et al , 2012 ) .", "We obtained the respective scores of 59.57 , 56.62 , and 48.25 on English , Chinese , and Arabic on the development set , 59.36 , 56.85 , and 49.43 on the test set , and the combined official score of 55.21 .", "The solver core is a mention classifier that uses Soon et al ( 2001 ) ? s algorithm and features extracted from the dependency graphs of the sentences .", "We optimized the feature sets for the three languages : We carried out an extensive evaluation of pairs of features and we complemented the single features with associations that improved the CoNLL score .", "This system builds on Bjo ? rkelund and Nugues ( 2011 ) ? s solver that we extended so that it can be applied to the three languages of the task : English , Chinese , and Arabic ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["We investigate a projection-based model in which we first translate Chinese and Arabic into English , run a publicly available coreference system , and then use a new projection algorithm to map the coreferring entities back from English into mention candidates detected in the Chinese and Arabic source .", "In this paper , we present our system description for the CoNLL-2012 coreference resolution task on English , Chinese and Arabic .", "We compare to a baseline that just runs the English coreference system on the supplied parses for Chinese and Arabic .", "Because our method does not beat the baseline system on the development set , we submit outputs generated by the baseline system as our final submission ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["We separate the two main stages of our model , mention detection and coreference resolution , into several sub-tasks which are solved by machine learning method and deterministic rules based on multi-filters , such as lexical , syntactic , semantic , gender and number information .", "This paper presents a mixed deterministic model for coreference resolution in the CoNLL-2012 shared task .", "Finally , we reach the average F1 scores 58.68 , 60.69 and 61.02 on the English closed task , Chinese closed and open tasks .", "We participate in the closed track for English and Chinese , and also submit an open result for Chinese using tools to generate the required features ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["Maximum entropy models are used for our system as classifiers to determine the coreference relationship between every two mentions ( usually noun phrases and pronouns ) in each document .", "We exploit rich lexical , syntactic and semantic features for the system , and the final features are selected using a greedy forward and backward strategy from an initial feature set .", "This paper describes our system participating in the CoNLL-2012 shared task : Modeling Multilingual Unrestricted Coreference in Ontonotes .", "Our system participated in the closed track for both English and Chinese languages ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Our system deals with all three languages : Arabic , Chinese and English .", "For Arabic and Chinese , the system produces high precision , while for English , precision and recall are balanced , which leads to the highest results across languages .", "The system results show that UBIU works reliably across all three languages , reaching an average score of 40.57 for Arabic , 46.12 for Chinese , and 48.70 for English .", "The current work presents the participation of UBIU ( Zhekova and Ku ? bler , 2010 ) in the CoNLL-2012 Shared Task : Modeling Multilingual Unrestricted Coreference in OntoNotes ( Pradhan et al , 2012 ) ."]}
{"orig_sents": ["0", "2", "1", "4", "3"], "shuf_sents": ["We in this paper present the model for our participation ( BCMI ) in the CoNLL-2012 Shared Task .", "Different filters handle different situations and the filtering strategies are designed manually .", "This paper describes a pure rule-based method , which assembles different filters in a proper order .", "We participated in the Chinese and English closed tracks , scored 51.83 and 59.24 respectively .", "These filters are assigned to different ordered tiers from general to special cases ."]}
{"orig_sents": ["5", "3", "2", "0", "4", "1"], "shuf_sents": ["Entities are obtained via greedy clustering .", "Our system ranked second in the English closed task .", "Our system employs a simple multigraph representation of the relation between mentions in a document , where the nodes correspond to mentions and the edges correspond to relations between the mentions .", "coreference resolution system that participated in the CoNLL2012 shared task on multilingual unrestricted coreference resolution .", "We participated in the closed tasks for English and Chinese .", "This paper presents HITS ?"]}
{"orig_sents": ["1", "0", "5", "8", "6", "10", "7", "4", "3", "2", "9"], "shuf_sents": ["The system performs coreference resolution through the mention pair classification and linking .", "This paper describes a coreference resolution system for CONLL 2012 shared task developed by HLT_HITSZ group , which incorporates rule-based and statistic-based techniques .", "As for the test dataset , the achieved F1 scores are 0.5749 and 0.6508 , respectively .", "It achieves 0.5861 and 0.6003 F1 score on the development data of English and Chinese , respectively .", "This system is evaluated on English and Chinese sides ( Closed Track ) , respectively .", "For each detected mention pairs in the text , a Decision Tree ( DT ) based binary classifier is applied to determine whether they form a coreference .", "Meanwhile , a rule-based classifier is applied to recognize some specific types of coreference , especially the ones with long distances .", "Next , the recognized coreferences are linked to generate the final coreference chain .", "This classifier incorporates 51 and 61 selected features for English and Chinese , respectively .", "This encouraging performance shows the effectiveness of our proposed coreference resolution system .", "The outputs of these two classifiers are merged ."]}
{"orig_sents": ["3", "4", "0", "1", "2"], "shuf_sents": ["In this paper , we present the improvements of Illinois-Coref system from last year .", "We focus on improving mention detection and pronoun coreference resolution , and present a new learning protocol .", "These new strategies boost the performance of the system by 5 % MUC F1 , 0.8 % BCUB F1 , and 1.7 % CEAF F1 on the OntoNotes-5.0 development set .", "The CoNLL-2012 shared task is an extension of the last year ? s coreference task .", "We participated in the closed track of the shared tasks in both years ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Our system is based on the Stanford ? s dcoref deterministic system which applies multiple sieves with the order from high precision to low precision to generate coreference chains .", "We evaluate the system using OntoNotes data set and report our results of average F-score 58.25 in the closed track .", "This paper describes our coreference resolution system for the CoNLL-2012 shared task .", "We introduce the newly added constraints and sieves and discuss the improvement on the original system ."]}
{"orig_sents": ["0", "1", "3", "2", "4"], "shuf_sents": ["This paper describes the UniTN/Essex submission to the CoNLL-2012 Shared Task on the Multilingual Coreference Resolution .", "We have extended our CoNLL-2011 submission , based on BART , to cover two additional languages , Arabic and Chinese .", "In particular , we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language .", "This paper focuses on adapting BART to new languages , discussing the problems we have encountered and the solutions adopted .", "We also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system ."]}
{"orig_sents": ["0", "3", "1", "2", "4"], "shuf_sents": ["Coreference resolution , which aims at correctly linking meaningful expressions in text , is a much challenging problem in Natural Language Processing community .", "The system takes a supervised learning strategy , and consists of two cascaded components : one for detecting mentions , and the other for clustering mentions .", "To make the system applicable for multiple languages , generic syntactic and semantic features are used to model coreference in text .", "This paper describes the multilingual coreference modeling system of Web Information Processing Group , Henan University of Technology , China , for the CoNLL-2012 shared task ( closed track ) .", "The system obtained combined official score 41.88 over three languages ( Arabic , Chinese , and English ) and ranked 7th among the 15 systems in the closed track ."]}
{"orig_sents": ["2", "5", "4", "6", "1", "0", "3"], "shuf_sents": ["Tests are run on real-world text acquired from freely available sources .", "Special reference is made to on-line three-way composition of the input , the error model and the language model .", "We inspect the viability of finite-state spellchecking and contextless correction of nonword errors in three languages with a large degree of morphological variety .", "We show that the finite-state approaches discussed are sufficiently fast for high-quality correction , even for Greenlandic which , due to its morphological complexity , is a difficult task for non-finite-state approaches .", "English , Finnish and Greenlandic ?", "Overviewing previous work , we conduct large-scale tests involving three languages ?", "and a variety of error models and algorithms , including proposed improvements of our own ."]}
{"orig_sents": ["4", "3", "1", "0", "5", "2"], "shuf_sents": ["In this paper we extend the matching approach to show how not only markedness constraints , but also faithfulness constraints and the interaction of the two types of constraints can be captured by the matching method .", "method , where constraint violations in alternative strings are matched through violation alignment in order to remove suboptimal candidates .", "We also provide a new proof of nonregularity of simple OT grammars .", "method where constraint violations are counted and filtered out to some set limit of approximability in a finite-state system , and the ? matching ?", "Previous work for encoding Optimality Theory grammars as finite-state transducers has included two prominent approaches : the socalled ? counting ?", "This often produces exact and small FST representations for OT grammars which we illustrate with two practical example grammars ."]}
{"orig_sents": ["0", "2", "3", "5", "1", "4"], "shuf_sents": ["aalan and Mohammed Attia Faculty of Engineering & IT , The British University in Dubai khaled.shaalan @ buid.ac.ae mohammed.attia @ buid.ac.ae Abstract A morphological analyser only recognizes words that it already knows in the lexical database .", "Our method is tested on a manually-annotated gold standard of 1,310 forms and yields good results despite the complexity of the task .", "It needs , however , a way of sensing significant changes in the language in the form of newly borrowed or coined words with high frequency .", "We develop a finite-state morphological guesser in a pipelined methodology for extracting unknown words , lemmatizing them , and giving them a priority weight for inclusion in a lexicon .", "Our work shows the usability of a highly non-deterministic finite state guesser in a practical and complex application .", "The processing is performed on a large contemporary corpus of 1,089,111,204 words and passed through a machine-learning-based annotation tool ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Roman transliterator based solely on a nonprobabilistic finite state transducer that solves the encountered scriptural issues via a particular architectural design in combination with a set of restrictions .", "This paper introduces a two-way Urdu ?", "In order to deal with the enormous amount of overgenerations caused by inherent properties of the Urdu script , the transliterator depends on a set of phonological and orthographic restrictions and a word list ; additionally , a default component is implemented to allow for unknown entities to be transliterated , thus ensuring a large degree of flexibility in addition to robustness ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["In this paper will be presented my approach to the integration of aspectually relevant properties of verbs into a morphological analyzer for English .", "The integration of semantic properties into morphological analyzers can significantly enhance the performance of any tool that uses their output as input , e.g. , for derivation or for syntactic parsing ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The work is based on a model introduced by ( Kamimura and Slutzki , 1981 ; Kamimura and Slutzki , 1982 ) , with a few changes to make the automata more applicable to natural language processing .", "Available algorithms include membership checking in bottom-up dag acceptors , transduction of dags to trees ( bottom-up dag-to-tree transducers ) , k-best generation and basic operations such as union and intersection .", "This paper presents DAGGER , a toolkit for finite-state automata that operate on directed acyclic graphs ( dags ) ."]}
{"orig_sents": ["0", "1", "2", "4", "3"], "shuf_sents": ["This paper introduces a new open source , WFST-based toolkit for Grapheme-toPhoneme conversion .", "The toolkit is efficient , accurate and currently supports a range of features including EM sequence alignment and several decoding techniques novel in the context of G2P .", "Experimental results show that a combination RNNLM system outperforms all previous reported results on several standard G2P test sets .", "The toolkit is implemented using OpenFst .", "Preliminary experiments applying Lattice Minimum Bayes-Risk decoding to G2P conversion are also provided ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["Kleene has been approved by SAP AG for release as free , open-source code under the Apache License , Version 2.0 , and will be available by August 2012 for downloading from http : // www.kleene-lang.org .", "The design , implementation , development status and future plans for the language are discussed .", "Users can program using regular expressions , alternation-rule syntax and right-linear phrase-structure grammars ; and Kleene provides variables , lists , functions and familiar program-control syntax .", "Kleene is a high-level programming language , based on the OpenFst library , for constructing and manipulating finite-state acceptors and transducers ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["In addition to describing the method , we present illustrative examples .", "operator and preference relations .", "bac , Miikka Silfverberg , and Anssi Yli-Jyr ?", "Our modular approach combines various preference constraints to form differ-ent replace rules .", "University of Helsinki Department of Modern Languages Unioninkatu 40 A FI-00014 Helsingin yliopisto , Finland { senka.drobac , miikka.silfverberg , anssi.yli-jyra } @ helsinki.fi Abstract We explain the implementation of replace rules with the .r-glc ."]}
{"orig_sents": ["5", "7", "2", "4", "6", "1", "0", "3"], "shuf_sents": ["That is , these transducers allowed not only exact matching but also approximate matching .", "In an attempt to increase the coverage of the data , our work centered on applying a set of finite-state transducers that helped the matching process between the positive samples and a set of new entries .", "As a starting point , a set of manually coded medical records were collected in order to code new medical records on the basis of this set of positive samples .", "While there are related works in languages such as English , this work presents the first results on automatic assignment of disease codes to medical records written in Spanish .", "Since the texts are written in natural language by doctors , the same Diagnostic Term might show alternative forms .", "This paper presents an application of finitestate transducers to the domain of medicine .", "Hence , trying to code a new medical record by exact matching the samples in the set is not always feasible due to sparsity of data .", "The objective is to assign disease codes to each Diagnostic Term in the medical records generated by the Basque Health Hospital System ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Due to the distance between Spanish and Basque , the verbal-chain transfer is a very complex module in the overall system .", "This paper presents the current status of development of a finite state transducer grammar for the verbal-chain transfer module in Matxin , a Rule Based Machine Translation system between Spanish and Basque .", "The grammar is compiled with foma , an open-source finitestate toolkit , and yields a translation execution time of 2000 verb chains/second ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["In the case of Arabic , this is illustrated through the addition of the ability to correctly parse partially vocalized forms without overgeneration , something not possible in the original analyzer , as well as to serve both as an analyzer and a generator .", "Apart from application speed , an FST representation immediately offers various possibilities to flexibly modify a grammar .", "In this paper we describe a conversion of the Buckwalter Morphological Analyzer for Arabic , originally written as a Perl-script , into a pure finite-state morphological analyzer .", "Representing a morphological analyzer as a finite-state transducer ( FST ) confers many advantages over running a procedural affix-matching algorithm ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["In this work , we describe a methodology based on the Stochastic Finite State Transducers paradigm for Spoken Language Understanding ( SLU ) for obtaining concept graphs from word graphs .", "In the edges of these concept graphs , both semantic and lexical information are represented .", "This makes these graphs a very useful representation of the information for SLU .", "The best path in these concept graphs provides the best sequence of concepts ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The approach is applied to an interval temporal logic linked to TimeML , a standard mark-up language for time and events , for which various finite-state mechanisms are proposed .", "A finite-state approach to temporal ontology for natural language text is described under which intervals ( of the real line ) paired with event descriptions are encoded as strings ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Moses , a well-known state-of-the-art system , is used as a machine translation reference in order to validate our results by comparison .", "Experiments on the TED corpus achieve a similar performance to that yielded by Moses .", "This paper presents a finite-state approach to phrase-based statistical machine translation where a log-linear modelling framework is implemented by means of an on-the-fly composition of weighted finite-state transducers ."]}
{"orig_sents": ["5", "0", "1", "2", "3", "8", "7", "6", "4"], "shuf_sents": ["The major drawback of this two-pass decoding approach lies in the fact that the translation system has to cope with the errors derived from the speech recognition system .", "There is hardly any cooperation between the acoustic and the translation knowledge sources .", "There is a line of research focusing on alternatives to implement speech translation efficiently : ranging from semi-decoupled to tightly integrated approaches .", "The goal of integration is to make acoustic and translation models cooperate in the underlying decision problem .", "Evidence of the performance is given through experimental results on a limited-domain task .", "Speech translation can be tackled by means of the so-called decoupled approach : a speech recognition system followed by a text translation system .", "The aim of this paper is to assess the quality of the hypotheses explored within different speech translation approaches .", "As a side-advantage of the integrated approaches , the translation is obtained in a single-pass decoding strategy .", "That is , the translation is built by virtue of the joint action of both models ."]}
{"orig_sents": ["5", "1", "4", "0", "3", "2"], "shuf_sents": ["In addition , the relative functional specificity of parse trees gives rise to a measure of parse quality .", "The proposed interface extends Gaifman ? s ( 1965 ) classical dependency rule formalism by separating lexical word forms and morphological categories from syntactic categories .", "Finally , we present a synthesis of strict grammar parsing and robust text parsing by connecting fragmental parses into trees with additional linear successor links .", "By filtering worse parses out from the parse forest using finite-state techniques , the best parses are saved .", "The separation lets the linguist take advantage of the morphological features in order to reduce the number of dependency rules and to make them lexically selective .", "This work complements a parallel paper of a new finite-state dependency parser architecture ( Yli-Jyra ? , 2012 ) by a proposal for a linguistically elaborated morphology-syntax interface and its finite-state implementation ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We demonstrate that the MERT line optimisation can be modelled as computing the shortest distance in a weighted finite-state transducer using a tropical polynomial semiring .", "Minimum Error Rate Training ( MERT ) is a method for training the parameters of a loglinear model .", "One advantage of this method of training is that it can use the large number of hypotheses encoded in a translation lattice as training data ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data , and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system .", "While automatic metrics of translation quality are invaluable for machine translation research , deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality .", "We show that Word Sense Disambiguation ( WSD ) can be used to evaluate the quality of machine translation lexical choice , by applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task ."]}
{"orig_sents": ["5", "0", "7", "2", "1", "4", "3", "6"], "shuf_sents": ["This paper points out some of the core issues on switching a language script and its repercussion in the phrase based statistical machine translation system development .", "Two independent views on Bengali script based SMT and transliterated Meitei Mayek based SMT systems of the training data and language models are presented and compared .", "a ) Manipuri using Bengali script , b ) Manipuri using transliterated Meetei Mayek script .", "The BLEU and NIST score shows that Bengali script based phrase based SMT ( PBSMT ) outperforms over the Meetei Mayek based English to Manipuri SMT system .", "The impact of various language models is commendable in such scenario .", "The statistical machine translation ( SMT ) system heavily depends on the sentence aligned parallel corpus and the target language model .", "However , subjective evaluation shows slight variation against the automatic scores .", "The present task reports on the outcome of EnglishManipuri language pair phrase based SMT task on two aspects ?"]}
{"orig_sents": ["1", "4", "3", "0", "2"], "shuf_sents": ["We identify five clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these five as nonterminal labels for the Hiero productions .", "Selecting a set of nonterminals for the synchronous CFGs underlying the hierarchical phrase-based models is usually done on the basis of a monolingual resource ( like a syntactic parser ) .", "Although this is our first non-optimized instantiation of the idea , our experiments show competitive performance with the Hiero baseline , exemplifying certain merits of this novel approach .", "In this paper we explore a first version of this idea based on a hierarchical decomposition of word alignments into recursive tree representations .", "However , a standard bilingual resource like word alignments is itself rich with reordering patterns that , if clustered somehow , might provide labels of different ( possibly complementary ) nature to monolingual labels ."]}
{"orig_sents": ["0", "2", "3", "4", "1"], "shuf_sents": ["In this paper , we empirically investigate the impact of critical configuration parameters in the popular cube pruning algorithm for decoding in hierarchical statistical machine translation .", "Besides standard hierarchical grammars , we also explore search with restricted recursion depth of hierarchical rules based on shallow-1 grammars .", "Specifically , we study how the choice of the k-best generation size affects translation quality and resource requirements in hierarchical search .", "We furthermore examine the influence of two different granularities of hypothesis recombination .", "Our experiments are conducted on the large-scale Chinese ? English and Arabic ? English NIST translation tasks ."]}
{"orig_sents": ["4", "1", "0", "5", "6", "3", "2"], "shuf_sents": ["The strengths however can be combined to achieve further improvements .", "Our results show that each of the presented reordering methods leads to improved translation quality on its own .", "A human analysis , comparing subjective translation quality as well as a detailed error analysis show the impact of our presented tree-based rules in terms of improved sentence quality and reduction of errors related to missing verbs and verb positions .", "Up to 1.1 BLEU points can be gained by POS and tree-based reordering over a baseline with lexicalized reordering .", "We describe a novel approach to combining lexicalized , POS-based and syntactic treebased word reordering in a phrase-based machine translation system .", "We present experiments on German-English and GermanFrench translation .", "We report improvements of 0.7 BLEU points by adding tree-based and lexicalized reordering ."]}
{"orig_sents": ["3", "0", "4", "2", "5", "1"], "shuf_sents": ["Previous approaches have relied on incrementally building larger rules by chunking smaller rules bottomup ; we introduce a complementary top-down model that incrementally builds shorter rules by segmenting larger rules .", "We show empirically that combining the more liberal rule chunking model with a more conservative rule segmentation model results in significantly better translations than either strategy in isolation .", "These integrate seamlessly because both stay strictly within a pure transduction grammar framework inducing under matching models during both training and testing ? instead of decoding under a completely different model architecture than what is assumed during the training phases , which violates an elementary principle of machine learning and statistics .", "We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone .", "Specifically , we combine iteratively chunked rules from Saers et al ( 2012 ) with our new iteratively segmented rules .", "To be able to drive induction top-down , we introduce a minimum description length objective that trades off maximum likelihood against model size ."]}
{"orig_sents": ["4", "1", "3", "2", "0", "5"], "shuf_sents": ["As a first sanity check , we report extensive coverage results for ITG on automatic and manual alignments .", "But what does it mean to parse a word alignment by a synchronous grammar ?", "This paper proposes an initial , formal characterization of alignment coverage as intersecting two partially ordered sets ( graphs ) of translation equivalence units , one derived by a grammar instance and another defined by the word alignment .", "This is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments .", "Deciding whether a synchronous grammar formalism generates a given word alignment ( the alignment coverage problem ) depends on finding an adequate instance grammar and then using it to parse the word alignment .", "Even for the ITG formalism , our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work ."]}
{"orig_sents": ["0", "2", "3", "1", "4"], "shuf_sents": ["We propose synchronous linear context-free rewriting systems as an extension to synchronous context-free grammars in which synchronized non-terminals span k ?", "As part of our investigations concerning the minimal k that is required for inducing manual alignments , we present a hierarchical aligner in form of a deduction system .", "1 continuous blocks on each side of the bitext .", "Such discontinuous constituents are required for inducing certain alignment configurations that occur relatively frequently in manually annotated parallel corpora and that can not be generated with less expressive grammar formalisms .", "We find that by restricting k to 2 on both sides , 100 % of the data can be covered ."]}
{"orig_sents": ["6", "0", "1", "4", "3", "2", "5"], "shuf_sents": ["The paper then compares the rate of agreement between the systems for each genre and sub-class .", "Each of the identification systems is based , explicitly or implicitly , on a theory of metaphor which hypothesizes that certain properties are essential to metaphor-inlanguage .", "At the same time , the different systems achieve similar success rates on each even though they show low agreement among themselves .", "The success of the identification systems varies significantly across genres and sub-classes of metaphor .", "The goal of this paper is to see what the success or failure of these systems can tell us about the essential properties of metaphorin-language .", "This is taken to be evidence that there are several sub-types of metaphor-in-language and that the ideal metaphor identification system will first define these sub-types and then model the linguistic properties which can distinguish these sub-types from one another and from nonmetaphors .", "This paper evaluates four metaphor identification systems on the 200,000 word VU Amsterdam Metaphor Corpus , comparing results by genre and by sub-class of metaphor ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["The quality of argumentation being the focus of the project , we developed a metaphor annotation protocol that targets metaphors that are relevant for the writer ? s arguments .", "We also describe encouraging findings regarding the potential of metaphor identification to contribute to automated scoring of essays .", "This article discusses metaphor annotation in a corpus of argumentative essays written by test-takers during a standardized examination for graduate school admission .", "We found a moderate-to-strong correlation ( r=0.51-0.57 ) between the percentage of metaphorically used words in an essay and the writing quality score .", "The reliability of the protocol is ? =0.58 , on a set of 116 essays ( the total of about 30K content-word tokens ) ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["We present a method to detect linguistic metaphors by inducing a domainaware semantic signature for a given text and compare this signature against a large index of known metaphors .", "Unfortunately , it is also a feature that serves to confound a computer ? s ability to comprehend natural human language .", "Metaphor is a pervasive feature of human language that enables us to conceptualize and communicate abstract concepts using more concrete terminology .", "By training a suite of binary classifiers using the results of several semantic signature-based rankings of the index , we are able to detect linguistic metaphors in unstructured text at a significantly higher precision as compared to several baseline approaches ."]}
{"orig_sents": ["0", "1", "2", "3", "5", "4"], "shuf_sents": ["We present the CSF - Common Semantic Features method for metaphor detection .", "This method has two distinguishing characteristics : it is cross-lingual and it does not rely on the availability of extensive manually-compiled lexical resources in target languages other than English .", "A metaphor detecting classifier is trained on English samples and then applied to the target language .", "The method includes procedures for obtaining semantic features from sentences in the target language .", "We obtain state-ofthe-art performance in both languages .", "Our experiments with Russian and English sentences show comparable results , supporting our hypothesis that a CSF-based classifier can be applied across languages ."]}
{"orig_sents": ["5", "2", "0", "1", "3", "4"], "shuf_sents": ["Identifying metaphors is thus an important step in language understanding .", "However , since almost any word can serve as a metaphor , they are impossible to list .", "Metaphors are ubiquitous and they present NLP with a range of challenges for WSD , IE , etc .", "To identify metaphorical use , we assume that it results in unusual semantic patterns between the metaphor and its dependencies .", "To identify these cases , we use SVMs with tree-kernels on a balanced corpus of 3872 instances , created by bootstrapping from available metaphor lists.1 We outperform two baselines , a sequential and a vectorbased approach , and achieve an F1-score of 0.75 .", "A metaphor is a figure of speech that refers to one concept in terms of another , as in ? He is such a sweet person ? ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["We aim to investigate cross-cultural patterns of thought through cross-linguistic investigation of the use of metaphor .", "In contrast to previous work which relies on resources like syntactic parsing and WordNet , our system is based on LDA topic modeling , enabling its application even to low-resource languages , and requires no labeled data .", "As a first step , we produce a system for locating instances of metaphor in English and Spanish text .", "We achieve an F-score of 59 % for English ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification .", "We use Topical Structure and Tracking , an Imageability score , and innova-tive methods to build an effective metaphor identification system that is fully automated and performs well over baseline .", "traction of Metaphors from Novel Data Tomek Strzalkowski1 , George Aaron Broadwell1 , Sarah Taylor2 , Laurie Feldman1 , Boris Yamrom1 , Samira Shaikh1 , Ting Liu1 , Kit Cho1 , Umit Boz1 , Ignacio Cases1 and Kyle El-liott3 1State University of New York 2Sarah M. Taylor Consulting LLC 3Plessas Experts University at Albany 121 South Oak St. Network Inc. Albany NY USA 12222 Falls Church VA USA 22046 Herndon VA 20171 tomek @ albany.edu talymail59 @ gmail.com kelliot @ plessas.net Abstract This article describes our novel approach to the automated detection and analysis of meta-phors in text .", "We employ robust , quantitative language processing to implement a system prototype combined with sound social science methods for validation ."]}
{"orig_sents": ["6", "1", "5", "2", "0", "3", "4"], "shuf_sents": ["The rest of the first part describes the classes of metaphorrelated words and the rules of their annotation with BRAT .", "The first part of the article is devoted to the procedure of `` shallow '' annotation in which metaphor-related words are identified according to a slightly modified version of the MIPVU procedure .", "Further on , the article gives a brief account of the linguistic problems that were encountered in adapting MIPVU to Russian .", "The examples of annotation show how the visualization functionalities of BRAT allow the researcher to describe the multifaceted nature of metaphor related words and the complexity of their relations .", "The second part of the paper speaks about the annotation of conceptual metaphors ( the `` deep '' annotation ) , where formulations of conceptual metaphors are inferred from the basic and contextual meanings of metaphor-related words from the `` shallow '' annotation , which is expected to make the metaphor formulation process more controllable .", "The paper presents the results of two reliability tests and the measures of inter-annotator agreement obtained in them .", "This work presents the tentative version of the protocol designed for annotation of a Russian metaphor corpus using the rapid annotation tool BRAT ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["The first delivery , PersPred 11 , contains 700 CPs , for which 22 fields of lexical , syntactic and semantic information are encoded .", "This paper introduces PersPred , the first manually elaborated syntactic and semantic database for Persian Complex Predicates ( CPs ) .", "Beside their theoretical interest , Persian CPs constitute an important challenge in Persian lexicography and for NLP .", "The semantic classification PersPred provides allows to account for the productivity of these combinations in a way which does justice to their compositionality without overlooking their idiomaticity ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["While a dictionarybased approach suffers from limited recall , precision is high ; hence it is best employed alongside an approach with complementing properties , such as an n-gram language model .", "We evaluate the method on data from the English-German translation part of the crosslingual word sense disambiguation task in the 2010 semantic evaluation exercise ( SemEval ) .", "The paper describes a method for identifying and translating multiword expressions using a bi-directional dictionary .", "The output of a baseline disambiguation system based on n-grams was substantially improved by matching the target words and their immediate contexts against compound and collocational words in a dictionary ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["We find ( i ) that our ratings are highly robust against aggressive filtering ; ( ii ) Z-score filtering fails to detect unreliable item ratings ; and ( iii ) Minimum Subject Agreement is highly effective at detecting unreliable subjects .", "This paper assesses two standard cleansing approaches on two sets of compositionality ratings for German noun-noun compounds , in their ability to produce compositionality ratings of higher consistency , while reducing data quantity .", "However , despite their importance , to our knowledge there has been no extensive look at the effects of cleansing methods on human rating data .", "Human ratings are an important source for evaluating computational models that predict compositionality , but like many data sets of human semantic judgements , are often fraught with uncertainty and noise ."]}
{"orig_sents": ["3", "4", "2", "0", "1"], "shuf_sents": ["The vectors were obtained by Latent Semantic Analysis ( LSA ) applied to the ukWaC corpus .", "Our results outperform those of all the participants in the Distributional Semantics and Compositionality ( DISCO ) 2011 shared task .", "We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components .", "This research focuses on determining semantic compositionality of word expressions using word space models ( WSMs ) .", "We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs , corpora , preprocessing techniques , methods for determining compositionality , and evaluation testbeds ."]}
{"orig_sents": ["0", "12", "4", "1", "2", "14", "10", "6", "8", "9", "11", "5", "13", "7", "3"], "shuf_sents": ["While working on valency lexicons for Czech and English , it was necessary to define treatment of multiword entities ( MWEs ) with the verb as the central lexical unit .", "We present a corpus-based study , concentrating on multilayer specification of verbal MWEs , their properties in Czech and English , and a comparison between the two languages using the parallel Czech-English Dependency Treebank ( PCEDT ) .", "This comparison revealed interesting differences in the use of verbal MWEs in translation ( discovering that such MWEs are actually rarely translated as MWEs , at least between Czech and English ) as well as some inconsistencies in their annotation .", "25 , 11800 Prague 1 , Czech Republic", "Such a formal specification has also been used for automated quality control of the annotation vs. the lexicon entries .", "?", "?", "full address : Institute of Formal and Applied Linguistics , Charles University in Prague , Faculty of Mathematics and Physics , Malostranske nam .", "This work has been supported by the Grant No .", "GPP406/13/03351P of the Grant Agency of the Czech Republic .", "Since Czech and English are typologically different languages , we believe that our findings will also contribute to a better understanding of verbal MWEs and possibly their more unified treatment across languages .", "The data used have been provided by the LINDAT/Clarin infrastructural project LM2010013 supported by the MSMT CR ( http : //lindat.cz ) .", "Morphological , syntactic and semantic properties of such MWEs had to be formally specified in order to create lexicon entries and use them in treebank annotation .", "Authors ?", "Adding MWE-based checks should thus result in better quality control of future treebank/lexicon annotation ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This paper presents a supervised machine learning approach that uses a machine learning algorithm called Random Forest for recognition of Bengali noun-noun compounds as multiword expression ( MWE ) from Bengali corpus .", "Our proposed approach to MWE recognition has two steps : ( 1 ) extraction of candidate multi-word expressions using Chunk information and various heuristic rules and ( 2 ) training the machine learning algorithm to recognize a candidate multi-word expression as Multi-word expression or not .", "The proposed system is tested on a Bengali corpus for identifying noun-noun compound MWEs from the corpus .", "A variety of association measures , syntactic and linguistic clues are used as features for identifying MWEs ."]}
{"orig_sents": ["2", "6", "5", "4", "1", "0", "3"], "shuf_sents": ["The particular values of the categories are sorted according to a frequency ratio .", "Categories with the largest divergence are considered to be the most significant .", "This paper presents an algorithm that allows the user to issue a query pattern , collects multi-word expressions ( MWEs ) that match the pattern , and then ranks them in a uniform fashion .", "As a result , we obtain morphosyntactic profiles of a given pattern , which includes the most stable category of the pattern , and their values .", "For every part of speech , and for all of its categories , we calculate a normalized Kullback-Leibler divergence between the category ? s distribution in the pattern and its distribution in the corpus overall .", "The algorithm collects the frequency of morphological categories of the given pattern on a unified scale in order to choose the stable categories and their values .", "This is achieved by quantifying the strength of all possible relations between the tokens and their features in the MWEs ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["High frequency can convert a word sequence into a multiword expression ( MWE ) , i.e. , a collocation .", "In this paper , we use collocations as well as syntactically-flexible , lexicalized phrases to analyze ? job specification documents ?", "Such patterns and its automated processing are the basis for identifying organizational domain knowledge and business information which is used later for the first instances of requirement elicitation processes in software engineering .", "( a kind of corporate technical document ) for subsequent acquisition of automated knowledge elicitation .", "We propose the definition of structural and functional patterns of specific corporate documents by analyzing the contexts and sections in which the expression occurs ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Our objective is to create a valuable resource , which will allow for the automatic identification MWE in running text and for a deeper understanding of these expressions in their context .", "Based on a lexicon of Portuguese MWE , this presentation focuses on an ongoing work that aims at the creation of a typology that describes these expressions taking into account their semantic , syntactic and pragmatic properties .", "We also plan to annotate each MWEentry in the mentioned lexicon according to the information obtained from that typology ."]}
{"orig_sents": ["2", "3", "5", "4", "0", "1"], "shuf_sents": ["The availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources , such as the Portuguese versions of Wordnet , Propbank and VerbNet .", "Moreover , it will allow the revision of parsers and dictionaries already in use .", "A challenging topic in Portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se , which impacts NLP tasks such as syntactic parsing , semantic role labeling and machine translation .", "Aiming to give a step forward towards the automatic disambiguation of se , our study focuses on the identification of pronominal verbs , which correspond to one of the six uses of se as a clitic pronoun , when se is considered a CONSTITUTIVE PARTICLE of the verb lemma to which it is bound , as a multiword unit .", "This process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task .", "Our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se ."]}
{"orig_sents": ["0", "1", "2", "4", "3"], "shuf_sents": ["We deal with syntactic identification of occurrences of multiword expression ( MWE ) from an existing dictionary in a text corpus .", "The MWEs we identify can be of arbitrary length and can be interrupted in the surface sentence .", "We analyse and compare three approaches based on linguistic analysis at a varying level , ranging from surface word order to deep syntax .", "We use the dictionary of multiword expressions SemLex , that was compiled by annotating the Prague Dependency Treebank and includes deep syntactic dependency trees of all MWEs .", "The evaluation is conducted using two corpora : the Prague Dependency Treebank and Czech National Corpus ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["The results show the major role of distributional similarity , which measures compositionality , in the extraction and classification of MWEs , especially , as expected , in the case of idioms .", "Even though cooccurrence and some aspects of morphosyntactic flexibility contribute to this task in a more limited measure , ML experiments make benefit of these sources of knowledge , allowing to improve the results obtained using exclusively distributional similarity features .", "We present an experimental study of how different features help measuring the idiomaticity of noun+verb ( NV ) expressions in Basque .", "After testing several techniques for quantifying the four basic properties of multiword expressions or MWEs ( institutionalization , semantic non-compositionality , morphosyntactic fixedness and lexical fixedness ) , we test different combinations of them for classification into idioms and collocations , using Machine Learning ( ML ) and feature selection ."]}
{"orig_sents": ["5", "2", "4", "0", "3", "1"], "shuf_sents": ["The creation of frame files is usually done manually , but we propose an automatic method to expedite this process .", "Our method perfectly predicts 65 % of the roles in 3015 unique noun-verb combinations , with an additional 22 % partial predictions , giving us 87 % useful predictions to build our annotation resource .", "For semantic role labelling , each argument of the noun-verb complex predicate must be given a role label .", "We use two resources for this method : Hindi PropBank frame files for simple verbs and the annotated Hindi Treebank .", "For complex predicates , frame files need to be created specifying the role labels for each noun-verb complex predicate .", "The linguistic annotation of noun-verb complex predicates ( also termed as light verb constructions ) is challenging as these predicates are highly productive in Hindi ."]}
{"orig_sents": ["5", "3", "4", "1", "2", "0"], "shuf_sents": ["A comparative quantitative corpus analysis demonstrates that this strategy , though paradigmatically limited , is nonetheless widely exploited : From a distributional point of view , some of these CIAs only combine with one or a few adjectives and form MWEs that appear to be completely lexicalized , while some others modify wider classes of adjectives thus displaying a certain degree of productivity .", "This contribution deals with a particular analytic structure of superlative in Italian that is still neglected in the literature .", "This is what we will call Constructional Intensifying Adjectives ( CIAs ) , adjectives which modify the intensity of other adjectives on the basis of regular semantic patterns , thus giving rise to multiword superlative constructions of the type : ADJX+ADJINTENS .", "Information on degree is grammatically relevant and constitutes what Lazard ( 2006 ) calls a primary domain of grammaticalization : According to typological studies ( Cuzzolin & Lehmann , 2004 ) , many languages of the world have in fact at their disposal multiple grammatical devices to express gradation .", "In Italian , the class of superlativizing structures alternative to the morphological superlative is very rich and consists , among others , of adverbs of degree , focalizing adverbs and prototypical comparisons .", "Grading is a primary cognitive operation that has an important expressive function ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper reports our ongoing project for constructing an English multiword expression ( MWE ) dictionary and NLP tools based on the developed dictionary .", "We extracted functional MWEs from the English part of Wiktionary , annotated the Penn Treebank ( PTB ) with MWE information , and conducted POS tagging experiments .", "We report how the MWE annotation is done on PTB and the results of POS and MWE tagging experiments ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["We explore improving parsing social media and other web data by altering the input data , namely by normalizing web text , and by revising output parses .", "We find that text normalization improves performance , though spell checking has more of a mixed impact .", "The results also demonstrate that , more than the size of the training data , the goodness of fit of the data has a great impact on the parser .", "We also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model ."]}
{"orig_sents": ["4", "3", "1", "0", "2"], "shuf_sents": ["This suggests that when social media writing transcribes phonological properties of speech , it is not merely a case of inventing orthographic transcriptions .", "Not only does this variable appear frequently , but it displays the same sensitivity to linguistic context as in spoken language .", "Rather , social media displays influence from structural properties of the phonological system .", "This paper investigates examples of the phonological variable of consonant cluster reduction in Twitter .", "Does phonological variation get transcribed into social media text ?"]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["Our preliminary results show that summaries created by our method are more concise and news-worthy than SumBasic according to human judges .", "Although the ideal length of summaries differs greatly from topic to topic on Twitter , previous work has only generated summaries of a pre-fixed length .", "In particular , we extend the Pageranklike ranking algorithm from previous work to partition event graphs and thereby detect finegrained aspects of the event to be summarized .", "We also provide a brief survey of datasets and evaluation design used in previous work to highlight the need of developing a standard evaluation for automatic tweet summarization task .", "In this paper , we propose an event-graph based method using information extraction techniques that is able to create summaries of variable length for different topics ."]}
{"orig_sents": ["3", "4", "5", "7", "6", "1", "2", "0"], "shuf_sents": ["Our first phase , using crowdsourced nasty indicators , achieves 58 % precision and 49 % recall , which increases to 75 % precision and 62 % recall when we bootstrap over the first level with generalized syntactic patterns .", "We then use general syntactic patterns from previous work to create more general sarcasm indicators , improving precision to 62 % and recall to 52 % .", "To further test the generality of the method , we then apply it to bootstrapping a classifier for nastiness dialogic acts .", "More and more of the information on the web is dialogic , from Facebook newsfeeds , to forum conversations , to comment threads on news articles .", "In contrast to traditional , monologic Natural Language Processing resources such as news , highly social dialogue is frequent in social media , making it a challenging context for NLP .", "This paper tests a bootstrapping method , originally proposed in a monologic domain , to train classifiers to identify two different types of subjective language in dialogue : sarcasm and nastiness .", "The best performing classifier for the first phase achieves 54 % precision and 38 % recall for sarcastic utterances .", "We explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall ."]}
{"orig_sents": ["5", "0", "3", "4", "1", "2"], "shuf_sents": ["We characterize each participant ? s attitude to-wards topics as Topical Positioning , employ Topical Positioning Map to represent the posi-tions of participants with respect to each other and track attitude shifts over time .", "Our approach can work across different types of social media , such as Twitter discussion and online chat room .", "In this article , we show results on Twitter data .", "We also discuss how we used participants ?", "attitudes towards system-detected meso-topics to re-flect their attitudes towards the overall topic of conversation .", "ositioning : A New Method for Predicting Opinion Changes in Conversation Ching-Sheng Lin1 , Samira Shaikh1 , Jennifer Stromer-Galley1,2 , Jennifer Crowley1 , Tomek Strzalkowski1,3 , Veena Ravishankar1 1State University of New York - University at Albany , NY 12222 USA 2Syracuse University 3Polish Academy of Sciences clin3 @ albany.edu , sshaikh @ albany.edu , tomek @ albany.edu Abstract In this paper , we describe a novel approach to automatically detecting and tracking discus-sion dynamics in Internet social media by fo-cusing on attitude modeling of topics ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Even though tweets that have been labelled as sarcastic have been omitted from this set , it still represents a difficult test set and the highest accuracy we achieve is 61.6 % using supervised learning and a feature set consisting of subjectivity-lexicon-based scores , Twitterspecific features and the top 1,000 most discriminative words .", "We perform a series of 3-class sentiment classification experiments on a set of 2,624 tweets produced during the run-up to the Irish General Elections in February 2011 .", "This is superior to various naive unsupervised approaches which use subjectivity lexicons to compute an overall sentiment score for a < tweet , political party > pair ."]}
{"orig_sents": ["3", "4", "2", "1", "0"], "shuf_sents": ["We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases .", "To overcome the limitations of the short lengths of these comments , we use an voting scheme to combine predictions made on individual user entries .", "Our dataset is composed of the comments made by the editors on the talk pages .", "This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia .", "Sockpuppets are fake accounts created by malicious users to bypass Wikipedia ? s regulations ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["These features are incorporated in a supervised classifier and compared against standard features that are widely used for various tasks in natural language processing , such as bag of words , part-of speech and syntactic parse information .", "For that purpose , we created a specially annotated web corpus from forum entries discussing the healthiness of certain food items .", "We examine a set of task-specific features ( mostly ) based on linguistic insights that are instrumental in finding utterances that are commonly perceived as reliable .", "We investigate the task of detecting reliable statements about food-health relationships from natural language texts ."]}
{"orig_sents": ["4", "1", "2", "0", "3"], "shuf_sents": ["For a Twitter feed that would have been otherwise difficult to translate , we report significant gains in translation quality using this strategy .", "In this study , we report the experimental results we obtained when translating 12 Twitter feeds published by agencies and organizations of the government of Canada , using a state-ofthe art Statistical Machine Translation ( SMT ) engine as a black box translation device .", "We mine parallel web pages linked from the URLs contained in English-French pairs of tweets in order to create tuning and training material .", "Furthermore , we give a detailed account of the problems we still face , such as hashtag translation as well as the generation of tweets of legal length .", "While the automatic translation of tweets has already been investigated in different scenarios , we are not aware of any attempt to translate tweets created by government agencies ."]}
{"orig_sents": ["1", "4", "3", "2", "5", "0"], "shuf_sents": ["Through a use case on earthquakes in Southeast Asia , we demonstrate GAF flexibility and ability to reason over events with the aid of extra-linguistic resources .", "This paper introduces GAF , a grounded annotation framework to represent events in a formal context that can represent information from both textual and extra-textual sources .", "This allows us to complete textual information with external sources and facilitates reasoning .", "Instances are represented by RDF compliant URIs that are shared across different research disciplines .", "GAF makes a clear distinction between mentions of events in text and their formal representation as instances in a semantic layer .", "The semantic layer can integrate any linguistic information and is compatible with previous event representations in NLP ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The results obtained can be used for the improvement of Information Extraction .", "We collect and analyze statistics about subject-verb-object triplets and their content , which helps us compare corpora belonging to the same domain but to different genre/text type .", "We argue that event structure is strongly related to the genre of the corpus , and propose statistical properties that are able to capture these genre differences .", "This paper describes an approach for investigating the representation of events and their distribution in a corpus ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["I conclude that a tool providing these functions would need events to be defined and represented as features of discourses about the world rather than objectively existing things in the world .", "The functions include individuation , selection , and contextualization of events .", "I present a set of functional requirements for a speculative tool informing users about events in historical discourse , in order to demonstrate what these requirements imply about how we should define and represent historical events ."]}
{"orig_sents": ["5", "3", "1", "2", "4", "0"], "shuf_sents": ["We propose an annotation schema to capture this information as a tuple of < location , attribute , value , change-of-state , time-reference > .", "In this paper , we consider the importance of identifying the change of state for events , in particular , events that measure and compare multiple states across time .", "Change of state is important to the clinical diagnosis of pneumonia ; in the example ? there are bibasilar opacities that are unchanged ? , the presence of bibasilar opacities alone may suggest pneumonia , but not when they are unchanged , which suggests the need to modify events with change of state information .", "Our task is the identification of phenotypes , specifically , pneumonia , from clinical narratives .", "Our corpus is comprised of chest Xray reports , where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas .", "Understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text ."]}
{"orig_sents": ["2", "0", "5", "3", "1", "4"], "shuf_sents": ["An image and its caption convey different information , but are generated by the same underlying concepts .", "We capture visual properties such as color , texture , shape , and orientation by computing low-level image features , and measure the contribution of each type of visual feature towards the accuracy of the model .", "We are interested in the task of image annotation using noisy natural text as training data .", "We use the trained model to annotate test images without corresponding text .", "Our model significantly outperforms both a competitive baseline and a previous topic model-based system .", "In this paper , we learn latent mixtures of topics that generate image and product descriptions on shopping websites by adapting a topic model for multilingual data ( Mimno et al , 2009 ) ."]}
{"orig_sents": ["0", "2", "4", "1", "5", "3"], "shuf_sents": ["We present a holistic data-driven technique that generates natural-language descriptions for videos .", "We show that this knowledge , automatically mined from web-scale text corpora , enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification .", "We combine the output of state-ofthe-art object and activity detectors with ? realworld ?", "We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61 % of the time .", "knowledge to select the most probable subject-verb-object triplet for describing a video .", "Unlike previous methods , our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus ."]}
{"orig_sents": ["6", "0", "5", "4", "2", "3", "1"], "shuf_sents": ["Classic exploratory data analysis methods , such as agglomerative hierarchical clustering , only provide a means of obtaining a tree-structured partitioning of the data .", "We also propose appropriate performance measures , and demonstrate superior performance compared to other hierarchical clustering algorithms .", "Our approach is based on a generative model , which relates the attribute descriptions associated with each node , and the node assignments of the data instances , in a probabilistic fashion .", "We furthermore use a nonparametric Bayesian prior , known as the tree-structured stick breaking process , which allows for the structure of the tree to be learned in an unsupervised fashion .", "On the other hand , in this work we propose to learn a hierarchy of linguistic descriptions , referred to as attributes , which allows for a textual description of the semantic content that is captured by the hierarchy .", "This requires the user to go through the images first , in order to reveal the semantic relationship between the different nodes .", "We propose a method to learn succinct hierarchical linguistic descriptions of visual datasets , which allow for improved navigation efficiency in image collections ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["justifies the inherent difficulties .", "There are cultural barriers to collaborative effort between literary scholars and computational linguists .", "In this work , we discuss some of these problems in the context of our ongoing research project , an exploration of free indirect discourse in Virginia Woolf ? s To The Lighthouse , ultimately arguing that the advantages of taking each field out of its ? comfort zone ?"]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The method was also used to build a prototype search engine for classical Arabic poems .", "The method utilizes the basic classical Arabic poem features such as structure , rhyme , writing style , and word usage .", "The proposed method achieves a precision of 96.94 % while keeping a high recall value at 92.24 % .", "This work presents a novel method for recognizing and extracting classical Arabic poems found in textual sources ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["In this corpus study , we develop and automatically extract three features to show that the classical character of Chinese poetry decreased across the century .", "Scholars of Chinese literature note that China ? s tumultuous literary history in the 20th century centered around the uncomfortable tensions between tradition and modernity .", "We also find that Taiwan poets constitute a surprising exception to the trend , demonstrating an unusually strong connection to classical diction in their work as late as the ? 50s and ? 60s ."]}
{"orig_sents": ["5", "9", "3", "0", "7", "2", "6", "8", "1", "4"], "shuf_sents": ["are important for understanding how readers understand literature and how literary works become popular .", "LSA-based similarity between book fragments and response may be able to reveal the book fragments that most affected readers .", "This corpus should include context information about the responses and should remain open to additional material .", "sites , etc . )", "The paper argues that a corpus of book responses can be an important instrument for research into reading behavior , reader response , book reviewing and literary appreciation .", "This position paper argues the need for a comprehensive corpus of online book responses .", "Based on a pilot study for the creation of a corpus of Dutch online book response , the paper shows how linguistic tools can find differences in word usage between responses from various sites .", "A sufficiently large , varied and representative corpus of online responses to books will facilitate research into these processes .", "They can also reveal response type by clustering responses based on usage of either words or their POS-tags , and can show the sentiments expressed in the responses .", "Responses to books ( in traditional reviews , book blogs , on booksellers ?"]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["Eliot ? s modernist poem The Waste Land is often interpreted as collection of voices which appear multiple times throughout the text .", "T.S .", "Here , we investigate whether we can automatically cluster existing segmentations of the text into coherent , expert-identified characters .", "We show that clustering The Waste Land is a fairly difficult task , though we can do much better than random baselines , particularly if we begin with a good initial segmentation ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Nine segmentations of the poem titled Kubla Khan ( Coleridge , 1816 , pp .", "This work performs some basic research upon topical poetry segmentation in a pilot study designed to test some initial assumptions and methodologies .", "Analyses and discussions of these codings focus upon how to improve agreement and outline some initial results on the nature of topics in this poem .", "55-58 ) are collected and analysed , producing low but comparable inter-coder agreement ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["The tool displays words , their significant co-occurrences , and contains a new visualization for significant concordances .", "Contexts of words and co-occurrences can be displayed .", "We present CoocViewer , a graphical analysis tool for the purpose of quantitative literary analysis , and demonstrate its use on a corpus of crime novels .", "After reviewing previous research and current challenges in the newly emerging field of quantitative literary research , we demonstrate how CoocViewer allows comparative research on literary corpora in a project-specific study , and how we can confirm or enhance our hypotheses through quantitative literary analysis ."]}
{"orig_sents": ["6", "5", "0", "3", "8", "1", "7", "4", "2"], "shuf_sents": ["In this paper , we characterize two prose genres syntactically : chick lit ( humorous novels on the challenges of being a modern-day urban female ) and high literature .", "The results show that literature contains more complex ( subordinating ) sentences than chick lit .", "Our results indicate that detailed insight into stylistic differences can be obtained by combining computational linguistic analysis with literary theory .", "First , we develop a top-down computational method based on existing literary-linguistic theory .", "This shows that the two genres can be distinguished along certain features .", "Since the models used are typically black boxes , they give little insight into the stylistic differences they detect .", "Stylometric analysis of prose is typically limited to classification tasks such as authorship attribution .", "Secondly , a bottom-up analysis is made of specific morphological and syntactic features in both genres , based on the parser ? s output .", "Using an off-the-shelf parser we obtain syntactic structures for a Dutch corpus of novels and measure the distribution of sentence types in chick-lit and literary novels ."]}
{"orig_sents": ["0", "4", "2", "1", "3"], "shuf_sents": ["This paper discusses user study outcomes with teachers who used Language MuseSM a webbased teacher professional development ( TPD ) application designed to enhance teachers ?", "Measurement outcomes of user piloting with teachers in a TPD setting indicated that application use increased teachers ' linguistic knowledge and awareness , and their ability to develop appropriate language-based instruction for ELLs .", "System development was grounded in literature that supports the notion that instruction incorporating language support for ELLs can improve their accessibility to content-area classroom texts ? in terms of access to content , and improvement of language skills .", "Instruction developed during the pilot was informed by the application ? s linguistic analysis feedback , provided by natural language processing capabilities in Language Muse .", "linguistic awareness , and support teachers in the development of language-based instructional scaffolding ( support ) for their English language learners ( ELL ) ."]}
{"orig_sents": ["4", "1", "3", "0", "2", "5"], "shuf_sents": ["The tool applies a series of automatic transformations to user documents to identify and remove the reading obstacles to comprehension .", "A significant percentile of them have inadequate reading comprehension skills .", "We focus on three semantic components : an Image component that retrieves images for the concepts in the text , an idiom detection component and a topic model component .", "In the ongoing FIRST project we build a multilingual tool called Open Book that helps the ASD people to better understand the texts .", "Persons affected by Autism Spectrum Disorders ( ASD ) present impairments in social interaction .", "Moreover , we present the personalization component that adapts the system output to user preferences ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["Seen from the perspective of the non-native as an ever-learning reader , we show how translation may be of more harm than help in understanding and retaining the meaning of a word while simplification holds promise .", "One of the populations that often needs some form of help to read everyday documents is non-native speakers .", "This paper discusses aid at the word and word string levels and focuses on the possibility of using translation and simplification .", "We conclude that if reading everyday documents can be considered as a learning activity as well as a practical necessity , then our study reinforces the arguments that defend the use of simplification to make documents that non-natives need to read more accessible ."]}
{"orig_sents": ["1", "3", "4", "2", "0", "5"], "shuf_sents": ["Lexical tightness captures aspects of prose complexity that are not covered by classic readability indexes , especially for literary texts .", "We present a computational notion of Lexical Tightness that measures global cohesion of content words in a text .", "Lexical tightness strongly correlates with grade level in a collection of expertly rated reading materials .", "Lexical tightness represents the degree to which a text tends to use words that are highly inter-associated in the language .", "We demonstrate the utility of this measure for estimating text complexity as measured by US school grade level designations of texts .", "We also present initial findings on the utility of this measure for automated estimation of complexity for poetry ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The results are discussed in comparison to conclusions obtained from a prior empirical survey .", "We have worked with a collection of newspaper articles with a significant number of numerical expressions .", "The purpose of this paper is to motivate and describe a system that simplifies numerical expression in texts , along with an evaluation study in which experts in numeracy and literacy assessed the outputs of this system ."]}
{"orig_sents": ["1", "4", "3", "0", "2", "5"], "shuf_sents": ["Next , at Stage 2 , a complexity score is generated for each text by applying one or another of three possible prediction models : one optimized for application to informational texts , one optimized for application to literary texts , and one optimized for application to mixed texts .", "Many existing approaches for measuring text complexity tend to overestimate the complexity levels of informational texts while simultaneously underestimating the complexity levels of literary texts .", "Each model combines lexical , syntactic and discourse features , as appropriate , to best replicate human complexity judgments .", "At Stage 1 , each text is classified into one or another of three possible genres : informational , literary or mixed .", "We present a two-stage estimation technique that successfully addresses this problem .", "We demonstrate that resulting text complexity predictions are both unbiased , and highly correlated with classifications provided by experienced educators ."]}
{"orig_sents": ["5", "4", "0", "3", "1", "2"], "shuf_sents": ["Our approach starts with a small number of seed hashtags for each emotion , which we use to automatically label tweets as initial training data .", "We select the hashtags with the highest scores , use them to automatically harvest new tweets from Twitter , and repeat the bootstrapping process .", "We show that the learned hashtag lists help to improve emotion classification performance compared to an N-gram classifier , obtaining 8 % microaverage and 9 % macro-average improvements in F-measure .", "We then train emotion classifiers and use them to identify and score candidate emotion hashtags .", "Using the bootstrapping framework , we learn lists of emotion hashtags from unlabeled tweets .", "We present a bootstrapping algorithm to automatically learn hashtags that convey emotion ."]}
{"orig_sents": ["4", "1", "3", "2", "0", "7", "6", "5"], "shuf_sents": ["Our emotion annotation method also produced a corpus of emotion labeled sports tweets .", "We create an Amazon Mechanical Turk1 task that elicits emotion labels and phrase-emotion associations from the participants .", "GEW is the first computational resource that can be used to assign emotion labels with such a high level of granularity .", "Using the proposed method , we create an emotion lexicon , compatible with the 20 emotion categories of the Geneva Emotion Wheel .", "In this paper , we detail a method for domain specific , multi-category emotion recognition , based on human computation .", "The performance gains are most pronounced for the fine-grained emotion classification , where we achieve an accuracy twice higher than the benchmark.2", "We show that the presented domain-targeted lexicon outperforms the existing general purpose ones in both settings .", "We compared the crossvalidated version of the lexicon with existing resources for both the positive/negative and multi-emotion classification problems ."]}
{"orig_sents": ["4", "1", "2", "0", "3"], "shuf_sents": ["This paper describes the Spanish DAL , a knowledge base formed by more than 2500 words manually rated by humans along the same three dimensions .", "An early , influential work by Cynthia Whissell , the Dictionary of Affect in Language ( DAL ) , allows rating words along three dimensions : pleasantness , activation and imagery .", "Given the lack of such tools in Spanish , we decided to replicate Whissell ? s work in that language .", "We evaluated its usefulness on two sentiment analysis tasks , which showed that the knowledge base managed to capture relevant information regarding the three affective dimensions .", "The topic of sentiment analysis in text has been extensively studied in English for the past 30 years ."]}
{"orig_sents": ["2", "6", "0", "4", "3", "5", "7", "1"], "shuf_sents": ["Assuming that the human labeling is correct ( annotation of a sample indicates that about 85 % of these tweets are indeed sarcastic ) , we train a machine learning classifier on the harvested examples , and apply it to a test set of a day ? s stream of 3.3 million Dutch tweets .", "We hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of nonverbal expressions that people employ in live interaction when conveying sarcasm .", "To avoid a sarcastic message being understood in its unintended literal meaning , in microtexts such as messages on Twitter.com sarcasm is often explicitly marked with the hashtag ? # sarcasm ? .", "We annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag .", "Of the 135 explicitly marked tweets on this day , we detect 101 ( 75 % ) when we remove the hashtag .", "30 % of the top-250 ranked tweets are indeed sarcastic .", "We collected a training corpus of about 78 thousand Dutch tweets with this hashtag .", "Analysis shows that sarcasm is often signalled by hyperbole , using intensifiers and exclamations ; in contrast , non-hyperbolic sarcastic messages often receive an explicit marker ."]}
{"orig_sents": ["4", "0", "6", "7", "5", "1", "3", "2"], "shuf_sents": ["Such reviews are a very important source of information for customers and companies .", "In this paper we focus on the detection of deceptive opinion spam , which consists of fictitious opinions that have been deliberately written to sound authentic , in order to deceive the consumers .", "Evaluation results in a corpus of hotel reviews demonstrate the appropriateness of the proposed method for real applications since it reached a f-measure of 0.84 in the detection of deceptive opinions using only 100 positive examples for training .", "In particular we propose a method based on the PU-learning approach which learns only from a few positive examples and a set of unlabeled data .", "Nowadays a large number of opinion reviews are posted on the Web .", "Due to the economic importance of these reviews there is a growing trend to incorporate spam on such sites , and , as a consequence , to develop methods for opinion spam detection .", "The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients ?", "expectations ."]}
{"orig_sents": ["1", "5", "3", "0", "2", "4"], "shuf_sents": ["Additionally , we propose a ring-based strategy , in which the chaining process is iterated several times , with the goal of further improving the performance of our method .", "This paper describes a novel approach for sexual predator detection in chat conversations based on sequences of classifiers .", "We report experimental results on the corpus used in the first international competition on sexual predator identification ( PAN ? 12 ) .", "Local classifiers are trained for each part of the documents and their outputs are combined by a chain strategy : predictions of a local classifier are used as extra inputs for the next local classifier .", "Experimental results show that the proposed method outperforms a standard ( global ) classification technique for the different settings we consider ; besides the proposed method compares favorably with most methods evaluated in the PAN ? 12 competition .", "The proposed approach divides documents into three parts , which , we hypothesize , correspond to the different stages that a predator employs when approaching a child ."]}
{"orig_sents": ["0", "4", "5", "1", "3", "2"], "shuf_sents": ["Though much research has been conducted on Subjectivity and Sentiment Analysis ( SSA ) during the last decade , little work has focused on Arabic .", "We adopted a random graph walk approach to extend the Arabic SSA lexicon using ArabicEnglish phrase tables , leading to improvements for SSA on Arabic microblogs .", "Our classification features yield results that surpass Arabic SSA results in the literature .", "We used different features for both subjectivity and sentiment classification including stemming , part-of-speech tagging , as well as tweet specific features .", "In this work , we focus on SSA for both Modern Standard Arabic ( MSA ) news articles and dialectal Arabic microblogs from Twitter .", "We showcase some of the challenges associated with SSA on microblogs ."]}
{"orig_sents": ["5", "3", "4", "6", "1", "0", "2"], "shuf_sents": ["Moreover , in addition to our newly created social media dataset , we also report results on other widely popular domains , such as movie and product reviews .", "We explore different pre-processing techniques and employ various features and classifiers .", "We believe that this article will not only extend the current sentiment analysis research to another family of languages , but will also encourage competition which potentially leads to the production of high-end commercial solutions .", "Whereas in English , Chinese , or Spanish this field has a long history and evaluation datasets for various domains are widely available , in case of Czech language there has not yet been any systematical research conducted .", "We tackle this issue and establish a common ground for further research by providing a large humanannotated Czech social media corpus .", "This article provides an in-depth research of machine learning methods for sentiment analysis of Czech social media .", "Furthermore , we evaluate state-of-the-art supervised machine learning methods for sentiment analysis ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We discuss a tagging scheme to tag data for training information extraction models which can extract the features of a product/service and opinions about them from textual reviews , and which can be used across different domains with minimal adaptation .", "We show that by using minor modifications to this simple tagging scheme the number of domain dependent opinion phrases are reduced from 36 % to 17 % , which leads to models more useful across domains .", "A simple tagging scheme results in a large number of domain dependent opinion phrases and impedes the usefulness of the trained models across domains ."]}
{"orig_sents": ["1", "0", "5", "4", "3", "2"], "shuf_sents": ["SentiStrength ( Thelwall et al , 2012 ) and SO-CAL ( Taboada et al , 2011 ) ?", "We compare the performance of two lexiconbased sentiment systems ?", "A qualitative error analysis then identifies classes of typical problems the two systems have with tweets .", "After the initial comparison , we successively enrich the SO-CAL-based analysis with tweet-specific mechanisms and observe that in some cases , this improves the performance .", "While SentiStrength has been geared specifically toward short social-media text , SO-CAL was built for general , longer text .", "on the two genres of newspaper text and tweets ."]}
{"orig_sents": ["1", "3", "0", "2", "4"], "shuf_sents": ["This paper presents a set of experiments on English and Spanish product reviews .", "Up until now most of the methods published for polarity classification are applied to English texts .", "Using a comparable corpus , a supervised method and two unsupervised methods have been assessed .", "However , other languages on the Internet are becoming increasingly important .", "Furthermore , a list of Spanish opinion words is presented as a valuable resource ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["In this paper we propose a method that uses corpora where phrases are annotated as Positive , Negative , Objective and Neutral , to achieve new sentiment resources involving words dictionaries with their associated polarity .", "Through this process a graph-based algorithm is used to obtain auto-balanced values that characterize sentiment polarities well used on Sentiment Analysis tasks .", "To assessment effectiveness of the obtained resource , sentiment classification was made , achieving objective instances over 80 % .", "Our method was created to build sentiment words inventories based on sentisemantic evidences obtained after exploring text with annotated sentiment polarity information ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["We experiment with sentiment analysis on two datasets from TWITA : a generic collection and a topic-specific collection .", "We observe that albeit shallow , our simple system captures polarity distinctions matching reasonably well the classification done by human judges , with differences in performance across polarity values and on the two sets .", "We describe TWITA , the first corpus of Italian tweets , which is created via a completely automatic procedure , portable to any other language .", "The only resource we use is a polarity lexicon , which we obtain by automatically matching three existing resources thereby creating the first polarity database for Italian ."]}
{"orig_sents": ["2", "3", "1", "5", "4", "0"], "shuf_sents": ["For this reason , these machine learning models can be applied to any language ; i.e. , there is no lexical , grammatical , syntactical analysis used in the classification process .", "We present a novel ? Pruned ICF Weighting Coefficient , ?", "In this work , we attempt to detect sentencelevel subjectivity by means of two supervised machine learning approaches : a Fuzzy Control System and Adaptive Neuro-Fuzzy Inference System .", "Even though these methods are popular in pattern recognition , they have not been thoroughly investigated for subjectivity analysis .", "Our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any lexical knowledge .", "which improves the accuracy for subjectivity detection ."]}
{"orig_sents": ["3", "4", "2", "7", "1", "0", "8", "5", "6"], "shuf_sents": ["A Hybrid feature selection method based on RST and Information Gain ( IG ) is proposed for sentiment classification .", "In this paper , Rough Set Theory ( RST ) based feature selection method is applied for sentiment classification .", "Therefore , a feature selection method is required to eliminate the irrelevant and noisy features from the feature vector for efficient working of ML algorithms .", "Sentiment analysis means to extract opinion of users from review documents .", "Sentiment classification using Machine Learning ( ML ) methods faces the problem of high dimensionality of feature vector .", "Movie review , product ( book , DVD and electronics ) review dataset .", "Experimental results show that Hybrid feature selection method outperforms than other feature selection methods for sentiment classification .", "Rough Set Theory based feature selection method finds the optimal feature subset by eliminating the redundant features .", "Proposed methods are evaluated on four standard datasets viz ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper presents a method for sentiment analysis specifically designed to work with Twitter data ( tweets ) , taking into account their structure , length and specific language .", "The approach employed makes it easily extendible to other languages and makes it able to process tweets in near real time .", "The main contributions of this work are : a ) the pre-processing of tweets to normalize the language and generalize the vocabulary employed to express sentiment ; b ) the use minimal linguistic processing , which makes the approach easily portable to other languages ; c ) the inclusion of higher order n-grams to spot modifications in the polarity of the sentiment expressed ; d ) the use of simple heuristics to select features to be employed ; e ) the application of supervised learning using a simple Support Vector Machines linear classifier on a set of realistic data .", "We show that using the training models generated with the method described we can improve the sentiment classification performance , irrespective of the domain and distribution of the test sets ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We investigate the relationship between coding detail and diagnostic classification performance , and find that a simple coding scheme is of high diagnostic utility .", "We propose a simple method to automate the pared down coding scheme , and find that these automatic codes are of diagnostic utility .", "We find that certain manual codes for linguistic errors are useful for distinguishing between diagnostic groups .", "We investigate the utility of linguistic features for automatically differentiating between children with varying combinations of two potentially comorbid neurodevelopmental disorders : autism spectrum disorder and specific language impairment ."]}
{"orig_sents": ["1", "0", "5", "3", "6", "4", "2"], "shuf_sents": ["The goal of the corpus is to provide a large data resource for the development and evaluation of grammatical error correction systems .", "We describe the NUS Corpus of Learner English ( NUCLE ) , a large , fully annotated corpus of learner English that is freely available for research purposes .", "Finally , we present statistics on the distribution of grammatical errors in the NUCLE corpus .", "In this paper , we address this need .", "Most importantly , we report on an unpublished study of annotator agreement for grammatical error correction .", "Although NUCLE has been available for almost two years , there has been no reference paper that describes the corpus in detail .", "We describe the annotation schema and the data collection and annotation process of NUCLE ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["The system has been tested by learners in a range of educational institutions , and their feedback has guided its development .", "This paper presents a self-assessment and tutoring system which combines an holistic score with detection and correction of frequent errors and furthermore provides a qualitative assessment of each individual sentence , thus making the language learner aware of potentially problematic areas rather than providing a panacea .", "Automated feedback on writing may be a useful complement to teacher comments in the process of learning a foreign language ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["The system is evaluated on a corpus of 1 702 essays , each graded independently by the student ? s own teacher and also in a blind re-grading process by another teacher .", "We present the first system developed for automated grading of high school essays written in Swedish .", "We show that our system ? s performance is fair , given the low agreement between the two human graders , and furthermore show how it could improve efficiency in a practical setting where one seeks to identify incorrectly graded essays .", "The system uses standard text quality indicators and is able to compare vocabulary and grammar to large reference corpora of blog posts and newspaper articles ."]}
{"orig_sents": ["4", "2", "0", "3", "5", "1"], "shuf_sents": ["While there has been a growing body of work in NLI , it has been difficult to compare methodologies because of the different approaches to pre-processing the data , different sets of languages identified , and different splits of the data used .", "A total of 29 teams from around the world competed across three different sub-tasks .", "This problem area has seen a spike in interest in recent years as it can have an impact on educational applications tailored towards non-native speakers of a language , as well as authorship profiling .", "In this shared task , the first ever for Native Language Identification , we sought to address the above issues by providing a large corpus designed specifically for NLI , in addition to providing an environment for systems to be directly compared .", "Native Language Identification , or NLI , is the task of automatically classifying the L1 of a writer based solely on his or her essay written in another language .", "In this paper , we report the results of the shared task ."]}
{"orig_sents": ["0", "1", "5", "4", "2", "6", "8", "7", "3"], "shuf_sents": ["Vector Space Models ( VSM ) have been widely used in the language assessment field to provide measurements of students ?", "vocabulary choices and content relevancy .", "Our experiments conducted on data from a non-native English speaking test suggest that the unsupervised topic clustering is better at selecting responses to train RVs than random selection .", "Index Terms : Vector Space Model ( VSM ) , speech assessment , unsupervised learning , document clustering", "To address this limitation , we applied unsupervised learning methods to reduce or even eliminate the human scoring step required for training RVs .", "However , training reference vectors ( RV ) in a VSM requires a time-consuming and costly human scoring process .", "In addition , we conducted an experiment to totally eliminate the need of human scoring .", "We obtained VSM-derived features that show promisingly high correlations to human-holistic scores , indicating that the costly human scoring process can be eliminated .", "Instead of using human rated scores to train RVs , we used used the machine-predicted scores from an automated speaking assessment system for training RVs ."]}
{"orig_sents": ["1", "2", "3", "0", "4"], "shuf_sents": ["We also show that re-formulating the classification problem as a multi-stage cascaded classification improves the classification accuracy .", "We developed an approach to predict the proficiency level of Estonian language learners based on the CEFR guidelines .", "We performed learner classification by studying morphosyntactic variation and lexical richness in texts produced by learners of Estonian as a second language .", "We show that our features which exploit the rich morphology of Estonian by focusing on the nominal case and verbal mood are useful predictors for this task .", "Finally , we also studied the effect of training data size on classification accuracy and found that more training data is beneficial in only some of the cases ."]}
{"orig_sents": ["3", "4", "2", "0", "1"], "shuf_sents": ["In order to automatically score the content accuracy of these spoken responses , we propose three categories of robust features , inspired from flexible text matching , n-grams , as well as string edit distance metrics .", "The experimental results indicate that even based on speech recognizer output , most of the feature correlations with human expert rater scores are in the range of r = 0.4 to r = 0.5 , and further , that a scoring model for predicting human rater proficiency scores that includes our content features can significantly outperform a baseline without these features ( r = 0.56 vs. r = 0.33 ) .", "However , the assessment discussed in this paper focuses on essential speaking skills that English teachers need in order to be effective communicators in their classrooms and elicits mostly responses that fall in between these extremes and are moderately predictable .", "This paper presents and evaluates approaches to automatically score the content correctness of spoken responses in a new language test for teachers of English as a foreign language who are non-native speakers of English .", "Most existing tests of English spoken proficiency elicit responses that are either very constrained ( e.g. , reading a passage aloud ) or are of a predominantly spontaneous nature ( e.g. , stating an opinion on an issue ) ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["our system achieved an accuracy of 43 % on the test data , improved to 63 % with improved feature normalization .", "We present a system for automatically identifying the native language of a writer .", "We experiment with a large set of features and train them on a corpus of 9,900 essays written in English by speakers of 11 different languages .", "In this paper , we present the features used in our system , describe our experiments and provide an analysis of our results ."]}
{"orig_sents": ["4", "0", "1", "3", "2"], "shuf_sents": ["I explore the given manually annotated data using word features such as the length , endings and character trigrams .", "Furthermore , I employ k-NN classification .", "The distance between two documents is calculated combining n-grams of word lengths and endings , and character trigrams .", "Modified TFIDF is used to generate a stop-word list automatically .", "This paper describes the system developed for the NLI 2013 Shared Task , requiring to identify a writer ? s native language by some text written in English ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["We decribe the submissions made by the National Research Council Canada to the Native Language Identification ( NLI ) shared task .", "Somewhat surprisingly , a classifier relying on purely lexical features performed very well and proved difficult to outperform significantly using various combinations of feature spaces .", "Our submissions rely on a Support Vector Machine classifier , various feature spaces using a variety of lexical , spelling , and syntactic features , and on a simple model combination strategy relying on a majority vote between classifiers .", "However , the combination of multiple predictors allowed to exploit their different strengths and provided a significant boost in performance ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["We describe the variety of machine learning approaches that we explored , including Winnow , language modeling , logistic regression and maximum-entropy models .", "This paper describes MITRE ? s participation in the native language identification ( NLI ) task at BEA-8 .", "We also describe several ensemble methods that we employed for combining these base systems .", "Our primary features were word and character n-grams .", "Our best effort performed at an accuracy of 82.6 % in the eleven-way NLI task , placing it in a statistical tie with the best performing systems ."]}
{"orig_sents": ["2", "0", "1", "6", "4", "3", "5"], "shuf_sents": ["catholique de Louvain Department of Linguistic Department of Linguistics Centre for English Corpus Linguistics Aesthetic and Literary Studies Athens , OH , USA Louvain-la-Neuve , Belgium University of Bergen , Norway jarvis @ ohio.edu yves.bestgen @ uclouvain.be pepper.steve @ gmail.com Abstract This paper reports our contribution to the 2013 NLI Shared Task .", "The purpose of the task was to train a machine-learning system to identify the native-language affiliations of 1,100 texts written in English by nonnative speakers as part of a high-stakes test of gen-eral academic English proficiency .", "g Classification Accuracy in Native Language Identification Scott Jarvis Yves Bestgen Steve Pepper Ohio University Universit ?", "Our system identified the correct native-language affiliations of 83.6 % of the texts in the test set .", "Our final system used an SVM classifier with over 400,000 unique features consisting of lexical and POS n-grams occur-ring in at least two texts in the training set .", "This was the highest classification accuracy achieved in the 2013 NLI Shared Task .", "We trained our system on the new TOEFL11 corpus , which includes 11,000 essays written by nonnative speakers from 11 native-language backgrounds ."]}
{"orig_sents": ["4", "1", "2", "3", "0"], "shuf_sents": ["Our official system achieves accuracy of 0.773 , which ranks it 18th among the 29 teams in the closed track .", "With the availability of relevant data resources , much work has been done to explore the native language of a foreign language learner .", "In this report , we present our system for the first shared task in Native Language Identification ( NLI ) .", "We use a linear SVM classifier and explore features of words , word and character n-grams , style , and metadata .", "Native Language Identification ( NLI ) , which tries to identify the native language ( L1 ) of a second language learner based on their writings , is helpful for advancing second language learning and authorship profiling in forensic linguistics ."]}
{"orig_sents": ["1", "4", "0", "2", "3"], "shuf_sents": ["We found that the choice of ensemble combination method did not lead to much difference in results , although exploiting the varying behaviours of linear versus logistic regression SVM classifiers could be promising in future work ; but part-of-speech tagsets showed noticeable differences .", "Our submission for this NLI shared task used for the most part standard features found in recent work .", "We also note that the overall architecture , with its feature set and ensemble approach , had an accuracy of 83.1 % on the test set when trained on both the training data and development data supplied , close to the best result of the task .", "This suggests that basically throwing together all the features of previous work will achieve roughly the state of the art .", "Our focus was instead on two other aspects of our system : at a high level , on possible ways of constructing ensembles of multiple classifiers ; and at a low level , on the granularity of part-of-speech tags used as features ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper describes the Nara Institute of Science and Technology ( NAIST ) native language identification ( NLI ) system in the NLI 2013 Shared Task .", "Our system ranked ninth in the closed track , third in open track 1 and fourth in open track 2 .", "We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We expand a set of common language identification features to include cognate interference and spelling mistakes .", "We apply Support Vector Machines to differentiate between 11 native languages in the 2013 Native Language Identification Shared Task .", "Our best results are obtained with a classifier which includes both the cognate and the misspelling features , as well as word unigrams , word bigrams , character bigrams , and syntax production rules ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["The use of different formalisms captures complementary information from second language data , and can be used in combination to yield classification performance superior to any formalism taken on its own .", "However , this class of features can be applied to any general constituent based model of grammar and previous work has done little to explore these options , relying primarily on the common Penn Treebank annotation standard .", "Tree Substitution Grammar rules form a large and expressive class of features capable of representing syntactic and lexical patterns that provide evidence of an author ? s native language .", "In this work we contrast the performance of syntactic features for Native Language Indentification using five different formalisms ."]}
{"orig_sents": ["1", "5", "3", "0", "2", "4"], "shuf_sents": ["We demonstrate that even using word level n-grams as features , and support vector machine ( SVM ) as a classifier can yield nearly 80 % accuracy .", "Native language identification ( NLI ) is the task to determine the native language of the author based on an essay written in a second language .", "We observe that the accuracy of a binary-based word level ngram representation ( ~80 % ) is much better than the performance of a frequency-based word level n-gram representation ( ~20 % ) .", "In this paper , we use the TOEFL11 data set which consists of more data , in terms of the amount of essays and languages , and less biased across prompts , i.e. , topics , of essays .", "Notably , comparable results can be achieved without removing punctuation marks , suggesting a very simple baseline system for NLI .", "NLI is often treated as a classification problem ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The results show that single highest performing promptbased content feature measures the number of unique lexical types that overlap with the listening materials and are not contained in either the reading materials or a sample response , with a correlation of r = 0.450 with holistic proficiency scores provided by humans .", "This paper investigates the use of promptbased content features for the automated assessment of spontaneous speech in a spoken language proficiency assessment .", "Furthermore , linear regression scoring models that combine the proposed promptbased content features with additional spoken language proficiency features are shown to achieve competitive performance with scoring models using content features based on prescored responses ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We derive NLP features from the holistic rubric used to score the summaries written by students for such tasks and use them to design a preliminary , automated scoring system .", "We introduce a cognitive framework for measuring reading comprehension that includes the use of novel summary writing tasks .", "Our results show that the automated approach performs well on summaries written by students for two different passages ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["In the process , we also develop ways to calculate agreements for sets of dependencies .", "Reliably-annotated learner corpora are a necessary step for the development of POS tagging and parsing of learner language .", "In our study , three annotators marked several layers of annotation over different levels of learner texts , and they were able to obtain generally high agreement , especially after discussing the disagreements among themselves , without researcher intervention , illustrating the feasibility of the scheme .", "This paper reports on a study of interannotator agreement ( IAA ) for a dependency annotation scheme designed for learner English .", "We pinpoint some of the problems in obtaining full agreement , including annotation scheme vagueness for certain learner innovations , interface design issues , and difficult syntactic constructions ."]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["The task is to automatically detect the native language of the TOEFL essays authors in a set of given test documents in English .", "The task was solved by a system that used the PPM compression algorithm based on an n-gram statistical model .", "We submitted four runs ; word-based PPMC algorithm with normalization and without , character-based PPMC algorithm with normalization and without .", "This paper reports on our work in the NLI shared task 2013 on Native Language Identification .", "The worst result was obtained on training and testing data during the evaluation procedure using the character-based PPM method and normalization : accuracy = 31.9 % ; the best one was macroaverage F-measure = 0.708 with the word-based PPMC algorithm without normalization ."]}
{"orig_sents": ["4", "3", "2", "1", "0", "5"], "shuf_sents": ["open-training task 2 ) .", "This method can also be used to boost performance even when training data from the same corpus is available ( i.e .", "open-training task 1 ) , particularly when some form of domain adaptation is also applied .", "We show that including training data from multiple corpora is highly effective at robust , cross-corpus NLI ( i.e .", "Our efforts in the 2013 NLI shared task focused on the potential benefits of external corpora .", "However , in the closed-training task , despite testing a number of new features , we did not see much improvement on a simple model based on earlier work ."]}
{"orig_sents": ["5", "2", "3", "4", "1", "0"], "shuf_sents": ["Overall , across all three tasks , our best accuracy of 83.5 % for the standard TOEFL11 test set came in second place .", "In the open-1 task , the word-based recurring ngrams outperformed the ensemble , yielding 38.5 % ( rank 2 ) .", "Starting with recurring word-based ngrams ( Bykh and Meurers , 2012 ) , we tested different linguistic abstractions such as partof-speech , dependencies , and syntactic trees as features for NLI .", "We also experimented with features encoding morphological properties , the nature of the realizations of particular lemmas , and several measures of complexity developed for proficiency and readability classification ( Vajjala and Meurers , 2012 ) .", "Employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2 % ( rank 5 ) in the closed task and 83.5 % ( rank 1 ) in the open-2 task .", "We explore a range of features and ensembles for the task of Native Language Identification as part of the NLI Shared Task ( Tetreault et al , 2013 ) ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In this paper , we describe our approach to native language identification and discuss the results we submitted as participants to the First NLI Shared Task .", "By resorting to a wide set of general ? purpose features qualifying the lexical and grammatical structure of a text , rather than to ad hoc features specifically selected for the NLI task , we achieved encouraging results , which show that the proposed approach is general ? purpose and portable across different tasks , domains and languages ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["This paper presents a Native Language Identification ( NLI ) system based on TF-IDF weighting schemes and using linear classifiers - support vector machines , logistic regressions and perceptrons .", "Furthermore , with subsequent evaluations using 10-fold cross-validation ( as given by the organizers ) on the combined training and development data , the best average accuracy obtained is 0.8455 and the features that contributed to this accuracy are the TF-IDF of the combined unigrams and bigrams of words .", "The system was one of the participants of the 2013 NLI Shared Task in the closed-training track , achieving 0.814 overall accuracy for a set of 11 native languages .", "This accuracy was only 2.2 percentage points lower than the winner ? s performance ."]}
{"orig_sents": ["3", "1", "4", "5", "2", "6", "0"], "shuf_sents": ["We evaluated our approach over a set of 11 native languages reaching 75 % accuracy .", "NLI as a sub area of author profiling focuses on identifying the first language of an author given a text in his second language .", "In our approaches , we selected lexical and syntactic features based on n-grams of characters , words , Penn TreeBank ( PTB ) and Universal Parts Of Speech ( POS ) tagsets , and perplexity values of character of n-grams to build four different models .", "This paper describes our approaches to Native Language Identification ( NLI ) for the NLI shared task 2013 .", "Researchers have reported several sets of features that have achieved relatively good performance in this task .", "The type of features used in such works are : lexical , syntactic and stylistic features , dependency parsers , psycholinguistic features and grammatical errors .", "We also combine all the four models using an ensemble based approach to get the final result ."]}
{"orig_sents": ["2", "3", "0", "5", "4", "1"], "shuf_sents": ["Out of key concepts of machine learning , we focus on feature engineering .", "We trained four different SVM models and combined them through majority voting achieving accuracy 72.5 % .", "Our goal is to predict the first language ( L1 ) of English essays ? s authors with the help of the TOEFL11 corpus where L1 , prompts ( topics ) and proficiency levels are provided .", "Thus we approach this task as a classification task employing machine learning methods .", "During system development , we experimented with various techniques for feature filtering and combination optimized with respect to the notion of mutual information and information gain .", "We design features across all the L1 languages not making use of knowledge of prompt and proficiency level ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["We also present a discussion of feature analysis based on information gain , and an overview on the performance of different word network features in the Native Language Identification task .", "Our feature sets were inspired by existing literature on native language identification and word networks .", "Experiments show that word networks have competitive performance against the baseline feature set , which is a promising result .", "We report on the performance of two different feature sets in the Native Language Identification Shared Task ( Tetreault et al , 2013 ) ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages .", "Our submission uses a Maximum Entropy classifier , using as features character and chunk n-grams , spelling and grammatical mistakes , and lexical preferences .", "This paper describes LIMSI ? s participation to the first shared task on Native Language Identification ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The features were collected from sequences and collocations of bare word forms , suffixes and character n-grams amounting to a feature set of several hundred thousand features .", "This paper describes an effort to perform Native Language Identification ( NLI ) using machine learning on a large amount of lexical features .", "These features were used to train a linear Support Vector Machine ( SVM ) classifier for predicting the native language category ."]}
{"orig_sents": ["4", "1", "5", "0", "2", "3", "6"], "shuf_sents": ["While string kernels have been used before in text analysis tasks , LRD is a distance measure designed to work on DNA sequences .", "More precisely , we used several string kernels and a kernel based on Local Rank Distance ( LRD ) .", "In this work , LRD is applied with success in native language identification .", "Finally , the Unibuc team ranked third in the closed NLI Shared Task .", "This paper presents our approach to the 2013 Native Language Identification shared task , which is based on machine learning methods that work at the character level .", "Actually , our best system was a kernel combination of string kernel and LRD .", "This result is more impressive if we consider that our approach is language independent and linguistic theory neutral ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We describe the various features used for classification , as well as the settings of the classifier that yielded the highest accuracy .", "We show that it is possible to learn to identify , with high accuracy , the native language of English test takers from the content of the essays they write .", "Our method uses standard text classification techniques based on multiclass logistic regression , combining individually weak indicators to predict the most probable native language from a set of 11 possibilities ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["In this paper , several LM adaptation methods that require either no manual transcription process or just a small amount of transcriptions have been evaluated .", "Index Terms : language model adaptation , unsupervised training , Web as a corpus", "In automated speech assessment , adaptation of language models ( LMs ) to test questions is important to achieve high recognition accuracy However , for large-scale language tests , the ordinary supervised training , which uses an expensive and time-consuming manual transcription process , is hard to utilize for LM adaptation .", "Our experiments suggest that these LM adaptation methods can allow us to obtain considerable recognition accuracy gain with no or low human transcription cost ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["This improves on a previous result which used a similar approach but trained the classifier on a substantially larger data set containing all student utterances .", "We present an experiment aimed at improving interpretation robustness of a tutorial dialogue system that relies on detailed semantic interpretation and dynamic natural language feedback generation .", "We show that we can improve overall interpretation quality by combining the output of a semantic interpreter with that of a statistical classifier trained on the subset of student utterances where semantic interpretation fails .", "Finally , we discuss how the labels from the statistical classifier can be integrated effectively with the dialogue system ? s existing error recovery policies ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Our method goes beyond a purely dictionary-based approach and also takes context into account .", "We evaluate our model on artificially generated data as well as naturally occurring learner text .", "Our best-performing model achieves high precision and reasonable recall , making it suitable for inclusion in a system that gives feedback to language learners .", "We present a method for automatically detecting missing hyphens in English text ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We describe a study in which we apply the metric TERp ( Translation Edit Rate Plus ) to a corpus of student-written translations from Spanish to English and compare the judgments of TERp against assessments provided by a translation instructor .", "This paper discusses preliminary work investigating the application of Machine Translation ( MT ) metrics toward the evaluation of translations written by human novice ( student ) translators ."]}
{"orig_sents": ["0", "4", "1", "2", "3"], "shuf_sents": ["This research analyzed the clinical notes of epilepsy patients using techniques from corpus linguistics and machine learning and predicted which patients are candidates for neurosurgery , i.e .", "Information-theoretic and machine learning techniques are used to determine whether and how sets of clinic notes from patients with intractable and nonintractable epilepsy are different .", "The results show that it is possible to predict from an early stage of treatment which patients will fall into one of these two categories based only on text data .", "These results have broad implications for developing clinical decision support systems .", "have intractable epilepsy , and which are not ."]}
{"orig_sents": ["0", "2", "1", "3", "4"], "shuf_sents": ["Identification of complex clinical phenotypes among critically ill patients is a major challenge in clinical research .", "In this paper , we describe a text processing method that uses Natural Language Processing ( NLP ) and supervised text classification methods to identify patients who are positive for Acute Lung Injury ( ALI ) based on the information available in free-text chest x-ray reports .", "The overall research goal of our work is to develop automated approaches that accurately identify critical illness phenotypes to prevent the resource intensive manual abstraction approach .", "To increase the classification performance we enhanced the baseline unigram representation with bigram and trigram features , enriched the n-gram features with assertion analysis , and applied statistical feature selection .", "We used 10-fold cross validation for evaluation and our best performing classifier achieved 81.70 % precision ( positive predictive value ) , 75.59 % recall ( sensitivity ) , 78.53 % f-score , 74.61 % negative predictive value , 76.80 % specificity in identifying patients with ALI ."]}
{"orig_sents": ["3", "1", "2", "0", "4"], "shuf_sents": ["We use support vector machines with composite kernels , which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees .", "Events , time expressions , and temporal relations convey information about the time course of a patient ? s clinical record that must be understood for many applications of interest .", "In this paper , we focus on extracting information about how time expressions and events are related by narrative containers .", "The clinical narrative contains a great deal of valuable information that is only understandable in a temporal context .", "Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classification for this task ."]}
{"orig_sents": ["4", "1", "3", "2", "0"], "shuf_sents": ["Although we operate with an incomplete , RadLex-based linguistic resource , the obtained results show the effectiveness of our approach by identifying a recall value of 74.3 % for the classification task .", "Our discussions with radiologists revealed that anatomical entities with pathological findings are of particular interest when linking radiology text and images .", "In this paper , we introduce our syntacto-semantic parsing approach to classify sentences in radiology reports as either pathological or non-pathological based on the findings they describe .", "Previous research to identify pathological findings focused on simplistic approaches that recognize diseases or negated findings , but failed to establish a holistic approach .", "In order to integrate heterogeneous clinical information sources , semantically correlating information entities have to be linked ."]}
{"orig_sents": ["2", "1", "6", "3", "0", "5", "4"], "shuf_sents": ["A medical expert inspects two thousand term pairs generated by two semantic spaces ?", "Developing terminological resources manually is , however , prohibitively expensive and likely to result in low coverage , especially given the high variability of language use in clinical text .", "The various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as SNOMED CT .", "In this paper , we exemplify the potential of our proposed method using the Swedish version of SNOMED CT , which currently lacks synonyms .", "for one hundred preferred terms of the semantic types disorder and finding .", "one of which models multiword terms in addition to single words ?", "To support this process , distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["specificity of linguistic constructions .", "In this paper , a new self ? training method for domain adaptation is illustrated , where the selection of reliable parses is carried out by an unsupervised linguistically ?", "The method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines , which demonstrates its ability to capture both reliability of parses and domain ?", "driven algorithm , ULISSE ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We present a rule-based methodology that relies on lexical and syntactic information as well as anaphora/ellipsis resolution to construct structured representations of questions ( frames ) .", "Our results indicate the viability of our approach and demonstrate the important role played by anaphora and ellipsis in interpreting consumer health questions .", "In this paper , we focus on the task of consumer health question understanding .", "While interest in biomedical question answering has been growing , research in consumer health question answering remains relatively sparse ."]}
{"orig_sents": ["2", "0", "3", "4", "1"], "shuf_sents": ["Community-wide challenges in the BioNLP research field provide goldstandard datasets and rigorous evaluation criteria , allowing for a meaningful comparison between techniques as well as measuring progress within the field .", "In this work , we perform a critical assessment of a large-scale text mining resource , identifying systematic errors and determining their underlying causes through semi-automated analyses and manual evaluations1 .", "Text mining methods for the biomedical domain have matured substantially and are currently being applied on a large scale to support a variety of applications in systems biology , pathway curation , data integration and gene summarization .", "However , such evaluations are typically conducted on relatively small training and test datasets .", "On a larger scale , systematic erratic behaviour may occur that severely influences hundreds of thousands of predictions ."]}
{"orig_sents": ["5", "1", "0", "2", "4", "3", "6"], "shuf_sents": ["This paper describes a publicly available set of tools for sublanguage recognition .", "However , practical methods for recognizing or characterizing them have been lacking .", "Closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model .", "A number of examples of implications of the sublanguage characteristics for natural language processing are pointed out .", "Scientific journal articles are compared to general English text , and it is shown that the journal articles fit the sublanguage model , while the general English text does not .", "It has long been realized that sublanguages are relevant to natural language processing and text mining .", "The software is made publicly available at [ edited for anonymization ] ."]}
{"orig_sents": ["16", "5", "1", "14", "4", "2", "11", "10", "9", "13", "17", "12", "15", "6", "7", "3", "0", "8"], "shuf_sents": ["In this paper , we briefly introduce BEL and investigate how far BioNLP-shared task an-notations could be converted to BEL state-ments and in such a way directly support BEL statement generation .", "? Peitsch2 ? 1Fraunhofer ? Institute ? for ? Algorithms ? and ? Scientific ? Computing , ? Schloss ? Birlinghoven , ? Sankt ? Augustin , ? Germany .", "? Aachen ? International ? Centre ? for ? Information ? Technology , ? Dahlmannstr .", "On the other hand , BEL statements and the corresponding evidence sentences might be a valuable resource for fu-ture BioNLP shared task training data genera-tion .", "? 3Bonn- ?", "? Apitius1,3 , ? Manuel ? C .", "With the rapid growth of biomedical litera-ture , automated methods are a crucial prereq-uisite for handling and encoding the available knowledge .", "The BioNLP shared tasks support the development of such tools and provide a linguistically motivated format for the anno-tation of relations .", "We present the first results of the automatic BEL statement generation and emphasize the need for more training data that captures the underlying bio-logical meaning .", "?", "? Germany ?", "? 2 , ? Bonn , ?", "The Biological expres-sion language ( BEL ) encodes the data in form of causal relationships , which describe the as-sociation between biological events .", "?", "? 2Philip ? Morris ? International ? R & D , ? Philip ? Morris ? Products ? S.A. , ? Quai ? Jeanrenaud ? 5 , ? 2000 ? Neuch ? tel , ? Switzerland .", "BEL can successfully be applied to large data and sup-port causal reasoning and hypothesis genera-tion .", "rks derived from qualitative translations of BioNLP Shared Task annotations ? Juliane ? Fluck1* , ? Alexander ? Klenner1 , ? Sumit ? Madan1 , ? Sam ? Ansari2 , ? Tamara ? Bobic1,3 , ? Julia ? Hoeng2 , ? Martin ? Hofmann- ?", "{ jfluck , smadan , aklenner , tbobic , mhofmann-apitius } @ scai.fraunhofer.de , { sam.ansari , julia.hoeng , manuel.peitsch } @ pmi.com Abstract Interpreting the rapidly increasing amount of experimental data requires the availability and representation of biological knowledge in a computable form ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["We observed significant differences in the z-scores of both populations for most of the metrics .", "We used these metrics to analyze linguistic patterns in spontaneous narratives from children developing typically and children identified as having a language impairment .", "These findings suggest we can use these metrics to aid in the task of language assessment in children .", "We present a set of new measures designed to reveal latent information of language use in children at the lexico-syntactic level ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["For all but one of the ten sentence types affected by these two rules , the parsing was improved by pre-processing .", "Parsings by a syntactic dependency parser , trained on standard Swedish , were manually analysed for the 33 sentence types most typical to clinical text .", "This analysis resulted in the identification of eight error types , and for two of these error types , preprocessing rules were constructed to improve the performance of the parser .", "Sentence types typical to Swedish clinical text were extracted by comparing sentence part-of-speech tag sequences in clinical and in standard Swedish text ."]}
{"orig_sents": ["2", "7", "4", "5", "0", "8", "3", "6", "1"], "shuf_sents": ["We have performed an intrinsic evaluation on predicting argumentative labels for non-structured abstracts and an extrinsic evaluation to predict argumentative labels on abstracts relevant to Gene Reference Into Function ( GeneRIF ) indexing .", "On the other hand , the algorithms that model the argumentative structure of the abstracts obtain lower performance in the extrinsic evaluation .", "MEDLINE/PubMed contains structured abstracts that can provide argumentative labels .", "Algorithms that model the argumentative structure seem to perform better than other algorithms .", "These abstracts make up less than one quarter of all the abstracts in MEDLINE/PubMed , so it is worthwhile to learn how to automatically label the non-structured ones .", "We have compared several machine learning algorithms trained on structured abstracts to identify argumentative labels .", "Extrinsic results show that assigning argumentative labels to non-structured abstracts improves the performance on GeneRIF indexing .", "Selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks .", "Intrinsic evaluation shows that argumentative labels can be assigned effectively to structured abstracts ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["We also observed improved performance for the automatic prediction of coherence and language impairment when we use features derived from the topic words provided by LDA .", "Our experiments show LDA is useful for detecting the topics that correspond to the narrative structure .", "In this paper , we explore the use of Latent Dirichlet Allocation ( LDA ) for detecting topics from narratives , and use the topics derived from LDA in two classification tasks : automatic prediction of coherence and language impairment .", "Child language narratives are used for language analysis , measurement of language development , and the detection of language impairment ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues .", "In this paper we take a fresh look at parallels between linguistics and biology ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["Toward knowledge based construction , the task is modified in a number of points .", "As the final results , it received 12 submissions , among which 2 were withdrawn from the final report .", "The Genia Event Extraction task is organized for the third time , in BioNLP Shared Task 2013 .", "This paper presents the task setting , data sets , and the final results with discussion for possible future directions ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["In version 2.1 we introduce an automated annotation scheme learning system , which derives task-specific event rules and constraints from the training data , and uses these to automatically adapt the system for new corpora with no additional programming required .", "TEES is a support vector machine ( SVM ) based text mining system for the extraction of events and relations from natural language texts .", "TEES 2.1 is shown to have good generalizability and good performance across the BioNLP 2013 task corpora , achieving first place in four out of eight tasks .", "We participate in the BioNLP 2013 Shared Task with Turku Event Extraction System ( TEES ) version 2.1 ."]}
{"orig_sents": ["2", "4", "0", "5", "3", "1"], "shuf_sents": ["However , when humans interpret biomolecular research articles , they usually build upon extensive background knowledge of their favorite genes and pathways .", "We participated in the Genia Event Extraction ( GE ) and Gene Regulation Network ( GRN ) tasks , ranking first in the former and fifth in the latter .", "During the past few years , several novel text mining algorithms have been developed in the context of the BioNLP Shared Tasks on Event Extraction .", "In this paper , we introduce our participation in the latest Shared Task using the largescale text mining resource EVEX which we previously implemented using state-ofthe-art algorithms , and which was applied to the whole of PubMed and PubMed Central .", "These algorithms typically aim at extracting biomolecular interactions from text by inspecting only the context of one sentence .", "To make such world knowledge available to a text mining algorithm , it could first be applied to all available literature to subsequently make a more informed decision on which predictions are consistent with the current known data ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["The Genia Event ( GE ) extraction task of the BioNLP Shared Task addresses the extraction of biomedical events from the natural language text of the published literature .", "We explore the impact of these two aspects of the system and conclude that the change in parser limits recall to an extent that can not be offset by the large quantities of training data .", "In our submission , we modified an existing system for learning of event patterns via dependency parse subgraphs to utilise a more accurate parser and significantly more , but noisier , training data .", "However , our extensions of the system to extract modification events shows promise ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["This model facilitates inference compared to global models while relying on richer information compared to usual pipeline approaches .", "This system is based on a pairwise model that transforms trigger classification in a simple multi-class problem in place of the usual multi-label problem .", "The HDS4NLP system ranked 6th on the Genia task ( 43.03 % f-score ) , and after fixing a bug discovered after the final submission , it outperforms the winner of this task ( with a f-score of 51.15 % ) .", "This paper describes the HDS4NLP entry to the BioNLP 2013 shared task on biomedical event extraction ."]}
{"orig_sents": ["4", "0", "2", "3", "7", "1", "6", "5"], "shuf_sents": ["The CG task is an information extraction task targeting the recognition of events in text , represented as structured n-ary associations of given physical entities .", "This level of performance is broadly comparable with the state of the art for established molecular-level extraction tasks , demonstrating that event extraction resources and methods generalize well to higher levels of biological organization and are applicable to the analysis of scientific texts on cancer .", "In addition to addressing the cancer domain , the CG task is differentiated from previous event extraction tasks in the BioNLP ST series in addressing a wide range of pathological processes and multiple levels of biological organization , ranging from the molecular through the cellular and organ levels up to whole organisms .", "Final test set submissions were accepted from six teams .", "We present the design , preparation , results and analysis of the Cancer Genetics ( CG ) event extraction task , a main task of the BioNLP Shared Task ( ST ) 2013 .", "bionlp-st.org/ .", "The CG task continues as an open challenge to all interested parties , with tools and resources available from http : //2013 .", "The highest-performing system achieved an Fscore of 55.4 % ."]}
{"orig_sents": ["1", "4", "0", "5", "2", "3"], "shuf_sents": ["The task setting , representation and semantics are defined with respect to pathway model standards and ontologies ( SBML , BioPAX , SBO ) and documents selected by relevance to specific model reactions .", "We present the Pathway Curation ( PC ) task , a main event extraction task of the BioNLP shared task ( ST ) 2013 .", "The highest achieved Fscore , 52.8 % , indicates that event extraction is a promising approach to supporting pathway curation efforts .", "The PC task continues as an open challenge with data , resources and tools available from http : //2013.bionlp-st.org/", "The PC task concerns the automatic extraction of biomolecular reactions from text .", "Two BioNLP ST 2013 participants successfully completed the PC task ."]}
{"orig_sents": ["1", "0", "2", "5", "4", "6", "3"], "shuf_sents": ["Our event extraction is based on the system we recently proposed for mining relations and events involving genes or proteins in the biomedical literature using a novel , approximate subgraph matching-based approach .", "We participated in the BioNLP 2013 shared tasks , addressing the GENIA ( GE ) and the Cancer Genetics ( CG ) event extraction tasks .", "In addition to handling the GE task involving 13 event types uniformly related to molecular biology , we generalized our system to address the CG task targeting a challenging set of 40 event types related to cancer biology with various arguments involving 18 kinds of biological entities .", "The consistent performance confirms that our system generalizes well to various event extraction tasks and scales to handle a large number of event and entity types .", "In addition , we evaluated the impact of using paths of all possible lengths among event participants as key contextual dependencies to extract potential events as compared to using only the shortest paths within the framework of our system .", "Moreover , we attempted to integrate a distributional similarity model into our system to extend the graph matching scheme for more events .", "We achieved a 46.38 % F-score in the CG task and a 48.93 % F-score in the GE task , ranking 3rd and 4th respectively ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["We tested a linguistically motivated rulebased system in the Cancer Genetics task of the BioNLP13 shared task challenge .", "The performance of the system was very moderate , ranging from 52 % against the development set to 45 % against the test set .", "Interestingly , the performance of the system did not change appreciably when using only entities tagged by the inbuilt tagger as compared to performance using the gold-tagged entities .", "The lack of an event anaphoric module , as well as problems in reducing events generated by a large trigger class to the task-specific event subset , were likely major contributory factors to the rather moderate performance ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes NaCTeM entries for the Cancer Genetics ( CG ) and Pathway Curation ( PC ) tasks in the BioNLP Shared Task 2013 .", "We have applied a state-ofthe-art event extraction system EventMine to the tasks in two different settings : a single-corpus setting for the CG task and a stacking setting for the PC task .", "EventMine was applicable to the two tasks with simple task specific configuration , and it produced a reasonably high performance , positioning second in the CG task and first in the PC task ."]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["The supporting resources described in this paper will continue to be publicly available from the shared task homepage http : //2013.bionlp-st.org/", "Following the tradition of the previous two BioNLP Shared Task events , the task organisers and several external groups sought to make system development easier for the task participants by providing automatically generated analyses using a variety of automated tools .", "Such evaluation can improve understanding of the applicability and benefits of specific tools and representations .", "This paper describes the technical contribution of the supporting resources provided for the BioNLP Shared Task 2013 .", "Providing analyses created by different tools that address the same task also enables extrinsic evaluation of the tools through the evaluation of their contributions to the event extraction task ."]}
{"orig_sents": ["3", "0", "5", "4", "1", "2"], "shuf_sents": ["Our system consists of two phases .", "When evaluated on the GENIA event extraction task of the BioNLP 2013 shared task , the system obtained the best results on strict matching and the third best on approximate span and recursive matching , with F-scores of 48.92 and 50.68 , respectively .", "Moreover , it has excellent performance in terms of speed .", "In this paper we present a biomedical event extraction system for the BioNLP 2013 event extraction task .", "In the extraction phase , the dictionary and obtained patterns are applied to extract events from input text .", "In the learning phase , a dictionary and patterns are generated automatically from annotated events ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["On the test set of BioNLP ? 13 , it achieves an F-score of 47.56 % on the primary task obtaining the 5th place in task 1 , which is 1.78 percentage points higher than the baseline ( following the Uturku system ) , demonstrating that the proposed method is efficient .", "The proposed system is characterized by a wide array of features based on dependency parse graphs and additional argument information in the second trigger detection .", "Based on the Uturku system which is the best one in the BioNLP ? 09 Shared Task , we improve the performance of biomedical event extraction by reducing illegal events and false positives in the second trigger detection and the second argument detection .", "We describe a system for extracting biomedical events among genes and proteins from biomedical literature , using the corpus from the BioNLP ? 13 Shared Task on Event Extraction .", "On the development set of BioNLP ? 13 , the system achieves an F-score of 50.96 % on the primary task ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We describe a biological event detection method implemented for the Genia Event Extraction task of BioNLP 2013 .", "The method relies on syntactic dependency relations provided by a general NLP pipeline , supported by statistics derived from Maximum Entropy models for candidate trigger words , for potential arguments , and for argument frames ."]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["We use dictionary and support vector machine classifier to detect event triggers .", "In this paper we propose a system which uses hybrid methods that combine both rule-based and machine learning ( ML ) -based approaches to solve GENIA Event Extraction of BioNLP Shared Task 2013 .", "There are three main stages in model : Pre-processing , trigger detection and biomedical event detection .", "Event detection is applied on syntactic patterns which are combined with features extracted for classification .", "We apply UIMA1 Framework to support coding ."]}
{"orig_sents": ["0", "1", "4", "2", "3"], "shuf_sents": ["We describe our system to extract genia events that was developed for the BioNLP 2013 Shared Task .", "Our system uses a supervised information extraction platform based on Support Vector Machines ( SVM ) and separates the process of event classification into multiple stages .", "We find that this optimisation improves the performance of our approach .", "Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure ( strict matching ) .", "For each event type the SVM parameters are adjusted and feature selection carried out ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["We describe a high precision system for extracting events of biomedical significance that was developed during the BioNLP shared task 2013 and tested on the Cancer Genetics data set .", "However , precision was the second highest ranked on the task at 62.73 .", "The system achieved an F-score on the development data of 73.67 but was ranked 5th out of six with an F-score of 29.94 on the test data .", "Analysis suggests the need to continue to improve our system for complex events particularly taking into account cross-domain differences in argument distributions ."]}
{"orig_sents": ["4", "0", "1", "2", "3"], "shuf_sents": ["This paper describes our entry in the Gene Regulation Network in Bacteria ( GRN ) part , for which our system finished in second place ( out of five ) .", "To tackle this relation extraction task , we employ a basic Support Vector Machine framework .", "We discuss our findings in constructing local and contextual features , that augment our precision with as much as 7.5 % .", "We touch upon the interaction type hierarchy inherent in the problem , and the importance of the evaluation procedure to encourage exploration of that structure .", "The BioNLP Shared Task 2013 is organised to further advance the field of information extraction in biomedical texts ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The combination of these three sources of rules leads to good results with a SER measure close to the winner and a best F-measure .", "In the perspective of annotating a text with respect to an ontology , we have participated in the subtask 1 of the BB BioNLPST whose aim is to detect , in the text , Bacteria Habitats and associate to them one or several categories from the OntoBiotope ontology provided for the task .", "We have used a rule-based machine learning algorithm ( WHISK ) combined with a rule-based automatic ontology projection method and a rote learning technique ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["In this paper , we present the methods we used to extract bacteria and biotopes names and then to identify the relation between those entities while participating to the BioNLP ? 13 Bacteria and Biotopes Shared Task .", "We achieved poor results : an SER of 0.66 in sub-task 1 , and a 0.06 F-measure in both sub-tasks 2 and 3 .", "We used machine-learning based approaches for this task , namely a CRF to extract bacteria and biotopes names and a simple matching algorithm to predict the relations ."]}
{"orig_sents": ["3", "1", "2", "4", "0"], "shuf_sents": ["The paper details the corpus specifications , the evaluation metrics , and it summarizes and discusses the participant results .", "Math ? matique , Informatique et G ? nome Institut National de la Recherche Agronomique UR1077 , F78352 Jouy-en-Josas , France forename.name @ jouy.inra.fr Abstract The goal of the Genic Regulation Network task ( GRN ) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium Bacillus subtilis .", "It is an extension of the BI task of BioNLP-ST ? 11 .", "ssy , Philippe Bessi ? res , Claire N ? dellec Unit ?", "The corpus is composed of sentences selected from publicly available PubMed scientific abstracts ."]}
{"orig_sents": ["1", "4", "7", "6", "5", "0", "2", "3"], "shuf_sents": ["The Bacteria Biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the OntoBiotope ontology .", "ssy1 , Wiktoria Golik1 , Zorana Ratkovic1,2 , Philippe Bessi ? res1 , Claire N ? dellec1 1Unit ?", "Bacteria locations are crucial knowledge in biology for phenotype studies .", "The paper details the corpus specifications , the evaluation metrics , and it summarizes and discusses the participant results .", "Math ? matique , Informatique et G ? nome MIG INRA UR1077 ?", "France forename.name @ jouy.inra.fr Abstract This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013 , which follows BioNLP-ST-11 .", "France 2LaTTiCe UMR 8094 CNRS , 1 rue Maurice Arnoux , F-92120 Montrouge ?", "F-78352 Jouy-en-Josas ?"]}
{"orig_sents": ["5", "2", "0", "1", "3", "4"], "shuf_sents": ["Two systems are explained : Sub-task 1 system for identifying habitat mentions in unstructured biomedical text and normalizing them through the OntoBiotope ontology and Sub-task 2 system for extracting localization and partof relations between bacteria and habitats .", "Both approaches rely on syntactic rules designed by considering the shallow linguistic analysis of the text .", "This paper reports the results to a challenge , set forth by the Bacteria Biotopes Task of the BioNLP Shared Task 2013 .", "Sub-task 2 system also makes use of discourse-based rules .", "The two systems achieve promising results on the shared task test data set .", "The absence of a comprehensive database of locations where bacteria live is an important obstacle for biologists to understand and study the interactions between bacteria and their habitats ."]}
{"orig_sents": ["2", "0", "5", "1", "4", "6", "3"], "shuf_sents": ["Dedicated computational approaches are required to sip through large volumes of text and infer gene interactions .", "Also , we introduce a new skip-mention data representation to enable distant relation extraction using first-order models .", "Published literature in molecular genetics may collectively provide much information on gene regulation networks .", "Our approach was ranked first of five , with a slot error rate of 0.73 .", "To account for a variety of relation types , multiple models are inferred .", "We propose a novel sieve-based relation extraction system that uses linear-chain conditional random fields and rules .", "The system was applied to the BioNLP 2013 Gene Regulation Network Shared Task ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["This paper describes the information extraction techniques developed in the framework of the participation of IRISATexMex to the following BioNLP-ST13 tasks : Bacterial Biotope subtasks 1 and 2 , and Graph Regulation Network .", "They are classically based on machine learning techniques , but we put the emphasis on the use of similarity measures inherited from the information retrieval domain ( Okapi-BM25 ( Robertson et al , 1998 ) , language modeling ( Hiemstra , 1998 ) ) .", "Through the good results obtained for these tasks , we show that these simple settings are competitive provided that the representation and similarity chosen are well suited for the task .", "The approaches developed are general-purpose ones and do not rely on specialized preprocessing , nor specialized external data , and they are expected to work independently of the domain of the texts processed ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["Given a precise alignment at the word level , the complete surface form of a meaning representations can be deduced using a simple declarative rule .", "By reformatting semantic representations as graphs , fine-grained alignment can be obtained .", "Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of text ? meaning pairs .", "Even though such corpora exist nowadays , or could be constructed using robust semantic parsing , the simple alignment between text and meaning representation is too coarse for developing robust ( statistical ) NLG systems ."]}
{"orig_sents": ["3", "0", "2", "1", "4"], "shuf_sents": ["In this paper we develop and evaluate a Natural Language Generation ( NLG ) system that converts RDF data into natural language text based on an ontology and an associated ontology lexicon .", "We apply the developed approach to the cooking domain , providing both an ontology and an ontology lexicon in lemon format .", "While it follows a classical NLG pipeline , it diverges from most current NLG systems in that it exploits an ontology lexicon in order to capture context-specific lexicalisations of ontology concepts , and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-specific corpus .", "The increasing amount of machinereadable data available in the context of the Semantic Web creates a need for methods that transform such data into human-comprehensible text .", "Finally , we evaluate fluency and adequacy of the generated recipes with respect to two target audiences : cooking novices and advanced cooks ."]}
{"orig_sents": ["0", "3", "1", "5", "4", "2"], "shuf_sents": ["In this paper we describe a natural language generation system which produces complex sentences from a biology knowledge base .", "questions in an e-textbook application .", "Our referring expression module is available for download as the open source Antfarm tool1 .", "The NLG system allows domain experts to discover errors in the knowledge base and generates certain parts of answers in response to users ?", "The system is capable of dealing with certain types of incomplete inputs arising from a knowledge base which is constantly edited and includes a referring expression generation module which keeps track of discourse history .", "The system allows domain experts to customise its lexical resources and to set parameters which influence syntactic constructions in generated sentences ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["After first presenting a validation experiment for naturalness ratings of SRC texts gathered using Amazon ? s Mechanical Turk , we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them .", "We conclude with a discussion of possible ways of improving the ranker in future work .", "We show that Nakatsu & White ? s ( 2010 ) proposed enhancements to the SPaRKy Restaurant Corpus ( SRC ; Walker et al , 2007 ) for better expressing contrast do indeed make it possible to generate better texts , including ones that make effective and varied use of contrastive connectives and discourse adverbials ."]}
{"orig_sents": ["4", "5", "2", "3", "0", "1"], "shuf_sents": ["Finally , we evaluate an existing surface realiser on the resulting dataset .", "We show that , after rewriting , the generator achieves a coverage of 76 % and a BLEU score of 0.74 on the elliptical data .", "We show that 9 % of the data contains an ellipsis and that both coverage and BLEU score markedly decrease for elliptic input ( from 82.3 % coverage for non-elliptic sentences to 65.3 % for elliptic sentences and from 0.60 BLEU score to 0.47 ) .", "We argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the SR data .", "In this paper , we focus on the task of generating elliptic sentences .", "We extract from the data provided by the Surface Realisation ( SR ) Task ( Belz et al , 2011 ) 2398 input whose corresponding output sentence contain an ellipsis ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["Experiments with two ontologies confirm that it leads to more compact texts , compared to a pipeline with the same components , with no deterioration in the perceived quality of the generated texts .", "We present an Integer Linear Programming model of content selection , lexicalization , and aggregation that we developed for a system that generates texts from OWL ontologies .", "Unlike pipeline architectures , our model jointly considers the available choices in these three text generation stages , to avoid greedy decisions and produce more compact texts .", "We also present an approximation of our model , which allows longer texts to be generated efficiently ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["Planning-based approaches to reference provide a uniform treatment of linguistic decisions , from content selection to lexical choice .", "Because the number of distinct denotations it searches grows doublyexponentially with the size of the referential domain , we present representational and search strategies that make generation and interpretation tractable .", "In this paper , we show how the issues of lexical ambiguity , vagueness , unspecific descriptions , ellipsis , and the interaction of subsective modifiers can be expressed using a belief-state planner modified to support context-dependent actions ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["Further , it can be extended to capture speaker variation , reaching an 82.83 % Dice overlap with human-produced expressions .", "The NetherlandsAbstract When they introduced the Graph-Based Algorithm ( GBA ) for referring expression generation , Krahmer et al ( 2003 ) flaunted the natural way in which it deals with relations between objects ; but this feature has never been tested empirically .", "We compare the original GBA against a variant that we introduce to better reflect human reference , and find that although the original GBA performs reasonably well , our new algorithm offers an even better match to human data ( 77.91 % Dice ) .", "We fill this gap in this paper , exploring referring expression generation from the perspective of the GBA and focusing in particular on generating human-like expressions in visual scenes with spatial relations ."]}
{"orig_sents": ["2", "6", "8", "4", "0", "5", "3", "7", "1"], "shuf_sents": ["by letting the pointing gesture indicate locative information , with other , nonlocative properties of a referent included in the description .", "In particular , pointing gestures are strongly associated with the use of locative features in referring expressions .", "Pointing gestures are pervasive in human referring actions , and are often combined with spoken descriptions .", "This paper investigates this question empirically , using machine-learning techniques on a new corpus of dialogues involving multimodal references to objects .", "In particular , it is not clear whether , in planning a pointing gesture in conjunction with a description , an NLG system should seek to minimise the redundancy between them , e.g .", "This question has a bearing on whether the gestural and spoken parts of referring acts are planned separately or arise from a common underlying computational mechanism .", "Combining gesture and speech naturally to refer to objects is an essential task in multimodal NLG systems .", "Our results indicate that human pointing strategies interact with descriptive strategies .", "However , the way gesture and speech should be combined in a referring act remains an open question ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["The task to perform was described in the challenge ? s call as follows : given a set of RDF triples containing facts about a celebrity , select those triples that are reflected in the target text ( i.e. , a short biography about that celebrity ) .", "From the initial nine expressions of interest , finally two participants submitted their systems for evaluation .", "In this overview paper we present the outcome of the first content selection challenge from open semantic web data , focusing mainly on the preparatory stages for defining the task and annotating the data ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Natural language processing technology can be used to automatically generate such questions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs .", "While we have not yet incorporated the full learning context into our approach , our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning .", "When instructors prepare learning materials for students , they frequently develop accompanying questions to guide learning .", "We introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning ."]}
{"orig_sents": ["0", "1", "3", "5", "4", "6", "7", "2", "8"], "shuf_sents": ["We describe a statistical Natural Language Generation ( NLG ) method for summarisation of time-series data in the context of feedback generation for students .", "In this paper , we initially present a method for collecting time-series data from students ( e.g .", "Our findings suggest that the learning agent needs to take into account both the student and lecturers ?", "marks , lectures attended ) and use example feedback from lecturers in a datadriven approach to content selection .", "method of providing feedback .", "We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturers ?", "We evaluate our system with undergraduate students by comparing it to three baseline systems : a rule-based system , lecturerconstructed summaries and a Brute Force system .", "Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers .", "preferences ."]}
{"orig_sents": ["5", "1", "0", "4", "2", "3"], "shuf_sents": ["These findings were incorporated into a framework for literature reviews , focusing on their macro-level document structure and the sentence-level templates , as well as the information summarization strategies .", "The first part of the study presents the results of a multi-level discourse analysis to investigate their discourse and content characteristics .", "Summaries generated from a partial implementation are evaluated against human written summaries and assessors ?", "comments are discussed to formulate recommendations for future work .", "The second part of this study discusses insights from this analysis , and how the framework can be adapted to automatic summaries resembling human written literature reviews .", "This study is conducted in the area of multidocument summarization , and develops a literature review framework based on a deconstruction of human-written literature review sections in information science research papers ."]}
{"orig_sents": ["2", "3", "4", "1", "5", "0"], "shuf_sents": ["Moreover , the longer sentences generated by our method are competitive with shorter sentences generated by the previous word graph model in terms of grammaticality .", "We exploit a ranking strategy to select the best path in the word graph as an abstract sentence .", "We propose a novel end-to-end framework for abstractive meeting summarization .", "We cluster sentences in the input into communities and build an entailment graph over the sentence communities to identify and select the most relevant sentences .", "We then aggregate those selected sentences by means of a word graph model .", "Despite not relying on the syntactic structure , our approach significantly outperforms previous models for meeting summarization in terms of informativeness ."]}
{"orig_sents": ["4", "3", "1", "2", "0"], "shuf_sents": ["Our empirical evaluation using a manually annotated corpus in Japanese demonstrates that the proposed model achieved 0.758 in F-score , outperforming the two baseline models .", "However , to the best of our knowledge , the NLG community has been less concerned with explicit voice selection .", "In this paper , we propose an automatic voice selection model based on various linguistic information , ranging from lexical to discourse information .", "Automatic voice selection is essential for realising more sophisticated MT and summarisation systems , because it impacts the readability of generated texts .", "This paper focuses on a subtask of natural language generation ( NLG ) , voice selection , which decides whether a clause is realised in the active or passive voice according to its contextual information ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["The cross-disciplinary MIME project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the first person on scene and ambulance clinicians .", "NLG is used to produce a textual handover report at any time , summarising data from novel medical sensors , as well as observations and actions recorded by the carer .", "We describe the MIME project with a focus on the NLG algorithm and an initial evaluation of the generated reports ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["These results suggest that some subtle contextual factors govern the choice between different types of QREs , and that numerals are highly preferred for subitizable quantities despite the availability of coarser-grained expressions .", "We present the results from an elicitation experiment in which human speakers were asked to produced quantified referring expressions ( QREs ) , as in ? The crate with 10 apples ? , ? The crate with many apples ? , etc ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["In this paper we present the preliminary work of a Basque poetry generation system .", "Basically , we have extracted the POS-tag sequences from some verse corpora and calculated the probability of each sequence .", "Finally we evaluate those strategies using a Turing Test-like evaluation .", "For the generation process we have defined 3 different experiments : Based on a strophe from the corpora , we ( a ) replace each word with other according to its POS-tag and suffixes , ( b ) replace each noun and adjective with another equally inflected word and ( c ) replace only nouns with semantically related ones ( inflected ) ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["We present first results of our project on the generation of contextually adequate greeting exchanges in video role playing games .", "An evaluation , comparing dialog from the video role playing game Skyrim to dialog determined by our algorithm , shows that our algorithm is able to generate greeting exchanges that are contextually more adequate than those featured by Skyrim .", "To make greeting exchanges computable , an analysis of the factors influencing greeting behavior as well as the factors influencing greeting exchanges is given .", "Based on the politeness model proposed by Brown & Levinson ( 1987 ) we develop a simple algorithm for the generation of greeting exchanges ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper introduces the problem of generating descriptions of n-dimensional spatial data by decomposing it via modelbased clustering .", "I apply the approach to the error function of supervised classification algorithms , a practical problem that uses Natural Language Generation for understanding the behaviour of a trained classifier .", "I demonstrate my system on a dataset taken from CoNLL shared tasks ."]}
{"orig_sents": ["2", "1", "0", "3", "4"], "shuf_sents": ["Based on various features collected from the training corpus , the system statistically learns template representations and document structure and produces well ? formed texts ( as evaluated by crowdsourced and expert evaluations ) .", "Given a domain corpus of historical texts , GenNext allows the user to generate a template bank organized by semantic concept via derived discourse representation structures in conjunction with general and domain-specific entity tags .", "We introduce GenNext , an NLG system designed specifically to adapt quickly and easily to different domains .", "In addition to domain adaptation , GenNext ? s hybrid approach significantly reduces complexity as compared to traditional NLG systems by relying on templates ( consolidating micro-planning and surface realization ) and minimizing the need for domain experts .", "In this description , we provide details of GenNext ? s theoretical perspective , architecture and evaluations of output ."]}
{"orig_sents": ["2", "0", "1", "4", "5", "3"], "shuf_sents": ["Universit ?", "de Montr ? al C.P .", "c Vaudry and Guy Lapalme RALI-DIRO ?", "Grammatical similarities between English and French that could be exploited and specifics of French that needed adaptation are discussed .", "6128 , Succ .", "Centre-Ville Montr ? al , Qu ? bec , Canada , H3C 3J8 { vaudrypl , lapalme } @ iro.umontreal.ca Abstract This paper describes SimpleNLG-EnFr , an adaption of the English realisation engine SimpleNLG ( Gatt and Reiter , 2009 ) for bilin-gual English-French realisation ."]}
{"orig_sents": ["3", "2", "0", "1", "4"], "shuf_sents": ["This paper presents the first study towards Turkish paraphrase alignment .", "We perform an analysis of different types of paraphrases on a modest Turkish paraphrase corpus and present preliminary results on that analysis from different standpoints .", "Although previous work has addressed linguistic variations at different levels of language , paraphrasing in Turkish has not been yet thoroughly studied .", "Paraphrasing is expressing the same semantic content using different linguistic means .", "We also explore the impact of human interpretation of paraphrasing on the alignment of paraphrase sentence pairs ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["This paper describes the first steps towards the implementation of a system which collects information from heart rate and respiration rate using a wearable sensor .", "The paper further outlines the direction for future work and in particular the challenges for NLG in this application domain .", "This position paper presents an on-going work on a natural language generation framework that is particularly tailored for summary text generation from body area networks .", "We present an overview of the main challenges when considering this type of sensor devices used for at home monitoring of health parameters ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["We present the first prototype of a handover report generator developed for the MIME ( Managing Information in Medical Emergencies ) project .", "NLG applications in the medical domain have been varied but most are deployed in clinical situations .", "We develop a mobile device for prehospital care which receives streamed sensor data and user input , and converts these into a handover report for paramedics ."]}
{"orig_sents": ["0"], "shuf_sents": ["This demo showcases Thoughtland , an end-to-end system that takes training data and a selected machine learning model , produces a cloud of points via crossvalidation to approximate its error function , then uses model-based clustering to identify interesting components of the error function and natural language generation to produce an English text summarizing the error function ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Our model consists of heuristic rules based on co-occurrences of predicates in the training data .", "This paper described UIC-CSC , the entry we submitted for the Content Selection Challenge 2013 ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["This year , 143 machine translation systems were submitted to the ten translation tasks from 23 institutions .", "We present the results of the WMT13 shared tasks , which included a translation task , a task for run-time estimation of machine translation quality , and an unofficial metrics task .", "An additional 6 anonymized systems were included , and were then evaluated both automatically and manually , in our largest manual evaluation to date .", "The quality estimation task had four subtasks , with a total of 14 teams , submitting 55 entries ."]}
{"orig_sents": ["3", "4", "1", "2", "0"], "shuf_sents": ["Collected scores were evaluated in terms of system level correlation ( how well each metric ? s scores correlate with WMT13 official human scores ) and in terms of segment level correlation ( how often a metric agrees with humans in comparing two translations of a particular sentence ) .", "We collected scores of 16 metrics from 8 research groups .", "In addition to that we computed scores of 5 standard metrics such as BLEU , WER , PER as baselines .", "This paper presents the results of the WMT13 Metrics Shared Task .", "We asked participants of this task to score the outputs of the MT systems involved in WMT13 Shared Translation Task ."]}
{"orig_sents": ["5", "6", "1", "7", "3", "2", "0", "4"], "shuf_sents": ["We conclude that HMEANT is a step in the right direction , but has some serious flaws .", "The human variant , HMEANT , has largely been evaluated using correlation with human contrastive evaluations , the standard human evaluation metric for the WMT shared tasks .", "Most importantly , however , a human metric must be discerning .", "It needs to be reliable ; it needs to work across different language pairs ; and it needs to be lightweight .", "The reliance on verbs as heads of frames , and the assumption that annotators need minimal guidelines are particularly problematic .", "There has been a recent surge of interest in semantic machine translation , which standard automatic metrics struggle to evaluate .", "A family of measures called MEANT has been proposed which uses semantic role labels ( SRL ) to overcome this problem .", "In this paper we claim that for a human metric to be useful , it needs to be evaluated on intrinsic properties ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We report results for French-English , German-English and Spanish-English in both directions .", "The main novelties of this year ? s participation are the following : our first participation to the Spanish-English task ; experiments with source pre-ordering ; a tighter integration of continuous space language models using artificial text generation ( for German ) ; and the use of different tuning sets according to the original language of the text to be translated .", "Our submissions use n-code , an open source system based on bilingual n-grams , and continuous space models in a post-processing step .", "This paper describes LIMSI ? s submissions to the shared WMT ? 13 translation task ."]}
{"orig_sents": ["1", "2", "0", "3", "4"], "shuf_sents": ["English , and English ? Russian .", "We describe the CMU systems submitted to the 2013 WMT shared task in machine translation .", "We participated in three language pairs , French ? English , Russian ?", "Our particular innovations include : a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create ? synthetic translation options ?", "that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context ."]}
{"orig_sents": ["3", "0", "2", "1", "4", "5", "6"], "shuf_sents": ["We develop parallel FDA for solving computational scalability problems caused by the abundance of training data for SMT models and LM models and still achieve SMT performance that is on par with using all of the training data or better .", "Parallel FDA can also be used for selecting the LM corpus based on the training set selected by parallel FDA .", "Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later .", "We use feature decay algorithms ( FDA ) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction .", "The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems .", "The relevancy of the selected LM corpus can reach up to 86 % reduction in the number of OOV tokens and up to 74 % reduction in the perplexity .", "We perform SMT experiments in all language pairs in the WMT13 translation task and obtain SMT performance close to the top systems using significantly less resources for training and development ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We describe our experiments with phrase-based machine translation for the WMT 2013 Shared Task .", "For the pairs containing Russian , we describe a set of independent experiments with slightly different translation models .", "We describe a set of results with different training data sizes and subsets .", "We trained one system for 18 translation directions between English or Czech on one side and English , Czech , German , Spanish , French or Russian on the other side ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["because it combines on three diverse approaches : TectoMT , a system with transfer at the deep syntactic level of representation , factored phrase-based translation using Moses , and finally automatic rule-based correction of frequent grammatical and meaning errors .", "This paper describes our WMT submissions CU-BOJAR and CU-DEPFIX , the latter dubbed ? CHIMERA ?", "We do not use any off-the-shelf systemcombination method ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We adopted phrase-based SMT approach and evaluated a number of different techniques , including data filtering , spelling correction , alignment of lemmatized word forms and transliteration .", "We also report on the experiments that did not have any positive effect and provide an analysis of the problems we encountered during the development of our systems .", "Altogether they yielded +2.0 and +1.5 BLEU improvement for ru-en and enru language pairs .", "This paper describes the English-Russian and Russian-English statistical machine translation ( SMT ) systems developed at Yandex School of Data Analysis for the shared translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task .", "Translations for English ? German and English ? French were generated using a phrase-based translation system which is extended by additional models such as bilingual , fine-grained part-ofspeech ( POS ) and automatic cluster language models and discriminative word lexica ( DWL ) .", "In addition , we combined reordering models on different sentence abstraction levels ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We present the results of using a big tuning data and the effect of averaging tuning weights of different seeds .", "Additionally , we performed a linguistically motivated compound splitting in the Germanto-English SMT system .", "We implement phrase-based SMT systems with standard parameters .", "This paper describes T ? UB ? ITAK-B ? ILGEM statistical machine translation ( SMT ) systems submitted to the Eighth Workshop on Statistical Machine Translation ( WMT ) shared translation task for German-English language pair in both directions ."]}
{"orig_sents": ["3", "5", "4", "6", "1", "0", "2"], "shuf_sents": ["For Russian-English we transliterated the unknown words .", "For German-English we used constituent parsing for reordering and compound splitting as preprocessing steps .", "The transliteration system is learned with the help of an unsupervised transliteration mining algorithm .", "This paper describes Munich-EdinburghStuttgart ? s submissions to the Eighth Workshop on Statistical Machine Translation .", "The systems described in this paper use OSM ( Operation Sequence Model ) .", "We report results of the translation tasks from German , Spanish , Czech and Russian into English and from English to German , Spanish , Czech , French and Russian .", "We explain different pre-/post-processing steps that we carried out for different language pairs ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["We describe how we integrate with MapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set .", "We present the system we developed to provide efficient large-scale feature-rich discriminative training for machine translation .", "We report our findings on German-English and RussianEnglish translation , and discuss benefits , as well as obstacles , to tuning on larger development sets drawn from the parallel training data ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["This paper describes the TALP participation in the WMT13 evaluation campaign .", "Our participation is based on the combination of several statistical machine translation systems : based on standard phrasebased Moses systems .", "Variations include techniques such as morphology generation , training sentence filtering , and domain adaptation through unit derivation .", "The results show a coherent improvement on TER , METEOR , NIST , and BLEU scores when compared to our baseline system ."]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["We confirm that PHRASEFIX ( SPE ) improves the output of TECTOMT , and we use this to analyze errors in TECTOMT .", "We present two English-to-Czech systems that took part in the WMT 2013 shared task : TECTOMT and PHRASEFIX .", "In a brief survey , we put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique : using synthetic parallel data from TECTOMT to train a statistical MT system ( SMT ) .", "However , we also show that extending data for SMT is more effective .", "The former is a deep-syntactic transfer-based system , the latter is a more-or-less standard statistical post-editing ( SPE ) applied on top of TECTOMT ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We demonstrate the effectiveness of a new adaptive , online tuning algorithm that scales to large feature and tuning sets .", "For both English-French and English-German , the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods .", "We describe the Stanford University NLP Group submission to the 2013 Workshop on Statistical Machine Translation Shared Task ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We describe the LIA machine translation systems for the Russian-English and English-Russian translation tasks .", "Various factored translation systems were built using MOSES to take into account the morphological complexity of Russian and we experimented with the romanization of untranslated Russian words ."]}
{"orig_sents": ["2", "4", "1", "3", "0"], "shuf_sents": ["The latter improved the baseline Russian-to-English BLEU score from 30.1 to 31.3 % on a heldout test set .", "The system participated in the English-to-French and Russian-to-English WMT evaluation tasks with competitive results .", "This paper describes OmnifluentTM Translate ?", "The features which contributed the most to high translation quality were training data sub-sampling methods , document-specific models , as well as rule-based morphological normalization for Russian .", "a state-of-the-art hybrid MT system capable of high-quality , high-speed translations of text and speech ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order .", "Our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features .", "This is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order , driven by a classifier trained on a parallel corpus ."]}
{"orig_sents": ["4", "5", "1", "2", "0", "3"], "shuf_sents": ["Our results show that the improvement is 0.8 points absolute ( BLEU ) for EN-ES and 0.7 points for ES-EN compared to the standard PBSMT system ( single best system ) .", "Therefore the paper does not present a multi-engine system combination .", "We investigate three types of shallow semantics : ( i ) Quality Estimation ( QE ) score , ( ii ) genre ID , and ( iii ) context ID derived from context-dependent language models .", "It is important to note that we developed this method when the standard ( confusion network-based ) system combination is ineffective such as in the case when the input is only two .", "This paper describes shallow semantically-informed Hierarchical Phrase-based SMT ( HPBSMT ) and Phrase-Based SMT ( PBSMT ) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at the Workshop on Statistical Machine Translation ( WMT 13 ) .", "The system uses PBSMT and HPBSMT decoders with multiple LMs , but will run only one decoding path decided before starting translation ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University , Karlsruhe Institute of Technology ( KIT ) , LIMSI-CNRS and SYSTRAN Software , Inc .", "This paper describes the joint submission of the QUAERO project for the German ? English translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation ( WMT 2013 ) .", "The translations were joined using the RWTH ? s system combination approach .", "Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation ."]}
{"orig_sents": ["4", "1", "3", "0", "2"], "shuf_sents": ["A number of different techniques are evaluated , including hierarchical phrase reordering , translation model interpolation , domain adaptation techniques , weighted phrase extraction , word class language model , continuous space language model and system combination .", "We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions .", "By application of these methods we achieve considerable improvements over the respective baseline systems .", "Both hierarchical and phrase-based SMT systems are applied .", "This paper describes the statistical machine translation ( SMT ) systems developed at RWTH Aachen University for the translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation ( WMT 2013 ) ."]}
{"orig_sents": ["0", "1", "4", "5", "3", "2"], "shuf_sents": ["This paper describes the University of Cambridge submission to the Eighth Workshop on Statistical Machine Translation .", "We report results for the RussianEnglish translation task .", "Lattices are rescored with a higher order language model and minimum Bayes-risk objective .", "The decoder is HiFST , a hierarchical phrase-based decoder implemented using weighted finitestate transducers .", "We use multiple segmentations for the Russian input language .", "We employ the Hadoop framework to extract rules ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The main contributions this past year are significant improvements in both speed and usability of the grammar extraction and decoding steps .", "We describe improvements made over the past year to Joshua , an open-source translation system for parsing-based machine translation .", "We have also rewritten the decoder to use a sparse feature representation , enabling training of large numbers of features with discriminative training methods ."]}
{"orig_sents": ["1", "4", "0", "3", "2"], "shuf_sents": ["For the Spanish-English pair , the use of linguistic information to select parallel data is investigated .", "This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task .", "Finally , for the German-English system , we describe our work in addressing the long distance reordering problem and a system combination strategy .", "For the FrenchEnglish pair , the usefulness of the small indomain parallel corpus is evaluated , compared to an out-of-domain parallel data sub-sampling method .", "Three language pairs are considered : SpanishEnglish and French-English in both directions and German-English in that direction ."]}
{"orig_sents": ["2", "4", "3", "0", "1"], "shuf_sents": ["We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system .", "For the Russian to English translation direction , we apply linguistically motivated pre-processing on the Russian side of the data .", "This paper describes QCRI-MES ? s submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation .", "For tuning , we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus-level .", "We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA++ and build a phrase-based machine translation system ."]}
{"orig_sents": ["4", "3", "5", "0", "1", "2"], "shuf_sents": ["We also investigate cleaning of the noisy Common Crawl corpus .", "We show that we can use alignment-based filtering for cleaning with good results .", "Finally we investigate effects of corpus selection for recasing .", "We use the Docent decoder , a local search decoder that translates at the document level .", "We describe the Uppsala University system for WMT13 , for English-to-German translation .", "We add tunable distortion limits , that is , soft constraints on the maximum distortion allowed , to Docent ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The first three systems employ inflectional generalization , while the latter two employ parser-based reordering , and DE-EN performs compound splitting .", "We present 5 systems of the MunichEdinburgh-Stuttgart1 joint submissions to the 2013 SMT Shared Task : FR-EN , ENFR , RU-EN , DE-EN and EN-DE .", "For our experiments , we use standard phrase-based Moses systems and operation sequence models ( OSM ) ."]}
{"orig_sents": ["3", "4", "2", "1", "0", "5"], "shuf_sents": ["To this aim , we compare binary classifiers trained on different data : the human-annotated dataset from the 7th Workshop on Statistical Machine Translation ( WMT-12 ) , and an automatically labelled version of the same corpus .", "Focusing on this binary task , we show that subjective human judgements can be effectively replaced with an automatic annotation procedure .", "In Machine Translation ( MT ) Quality Estimation ( QE ) , for instance , using humanannotated data to train a binary classifier that discriminates between good ( useful for a post-editor ) and bad translations is not trivial .", "Supervised approaches to NLP tasks rely on high-quality data annotations , which typically result from expensive manual labelling procedures .", "For some tasks , however , the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications .", "Our results show that human labels are less suitable for the task ."]}
{"orig_sents": ["5", "4", "1", "3", "0", "2"], "shuf_sents": ["Empirical evaluations show that these methods are as accurate as ? and significantly faster than ?", "This paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other .", "Gibbs sampling and brute-force all-pairs search .", "We present ( 1 ) efficient , online inference for representing documents in several languages in a common topic space and ( 2 ) fast approximations for finding near neighbors in the probability simplex .", "Beyond their common application to exploratory data analysis , latent variable topic models have been used to represent text in a low-dimensional space , independent of vocabulary , where documents may be compared .", "Many tasks in NLP and IR require efficient document similarity computations ."]}
{"orig_sents": ["2", "1", "3", "5", "0", "4"], "shuf_sents": ["Then , we supplement the model ? s feature space with translation scores estimated over comparable corpora in order to improve accuracy .", "The learned models typically have both low accuracy ( incorrect translations and feature scores ) and low coverage ( high out-of-vocabulary rates ) .", "Statistical machine translation ( SMT ) performance suffers when models are trained on only small amounts of parallel data .", "In this work , we use an additional data resource , comparable corpora , to improve both .", "We observe improvements between 0.5 and 1.7 BLEU translating Tamil , Telugu , Bengali , Malayalam , Hindi , and Urdu into English .", "Beginning with a small bitext and corresponding phrase-based SMT model , we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Doing so , we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier .", "Our approach augments the English side of the phrase table using a classifier to predict where English articles might plausibly be added or removed , and then we decode as usual .", "We apply our technique to the problem of producing English determiners when translating from Russian and Czech , languages that lack definiteness morphemes .", "We propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation options ? phrasal translations that are generated by auxiliary translation and postediting processes ? to augment the default phrase inventory learned from parallel data ."]}
{"orig_sents": ["4", "5", "8", "0", "1", "7", "6", "3", "2"], "shuf_sents": ["Some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets ( Denkowski et al , 2012a ; Rarrick et al , 2011 ) , ? domain adaptation ?", "to arrive at data more suited to the task at hand ( Moore and Lewis , 2010 ; Axelrod et al , 2011 ) , or by specifically focusing on data reduction by keeping only as much data as is needed for building models e.g. , ( Eck et al , 2005 ) .", "Further , unlike other methods created specifically for data reduction that have similar effects on the data , our method scales to very large data , up to tens to hundreds of millions of parallel sentences .", "At the same time it reduces model sizes , improves training times , and , because it attempts to preserve contexts for all n-grams in a corpus , the cost in quality is minimal ( as measured by BLEU ) .", "Our field has seen significant improvements in the quality of machine translation systems over the past several years .", "The single biggest factor in this improvement has been the accumulation of ever larger stores of data .", "We have developed a very simple n-gram counting method that reduces the size of data sets dramatically , as much as 90 % , and is applicable independent of specific dev and test data .", "This paper focuses on techniques related to the latter efforts .", "However , we now find ourselves the victims of our own success , in that it has become increasingly difficult to train on such large sets of data , due to limitations in memory , processing power , and ultimately , speed ( i.e. , data to models takes an inordinate amount of time ) ."]}
{"orig_sents": ["5", "6", "4", "0", "2", "1", "3"], "shuf_sents": ["To investigate this question , we compare ? natural ?", "tasks defined as random shards in the context of patent SMT .", "tasks defined as sections of the International Patent Classification versus ? random ?", "We find that both versions of multi-task learning improve equally well over independent and pooled baselines , and gain nearly 2 BLEU points over standard MERT tuning .", "division of data into tasks that balance shared and individual knowledge , or whether its inherent regularization makes multi-task learning a broadly applicable remedy against overfitting .", "Multi-task learning has been shown to be effective in various applications , including discriminative SMT .", "We present an experimental evaluation of the question whether multi-task learning depends on a ? natural ?"]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["We present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario .", "Our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 BLEU points absolute , and a standard incremental adaptation approach by 2 BLEU points absolute .", "With the introduction of a simple online feature , we are able to adapt the translation model on the fly to the corrections made by the translators .", "Additionally , we do online adaption of the feature weights with a large margin algorithm ."]}
{"orig_sents": ["2", "1", "5", "4", "3", "0"], "shuf_sents": ["By interpolating the heuristic and the trained phrase table , we can improve over the baseline by 0.5 % BLEU and 0.5 % TER .", "Different from previous work , we completely avoid the use of a word alignment or phrase extraction heuristics , moving towards a more principled phrase generation and probability estimation .", "We present an iterative technique to generate phrase tables for SMT , which is based on force-aligning the training data with a modified translation decoder .", "The resulting phrase table shows only a small overlap with the heuristically extracted one , which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics .", "Experiments are carried out on the IWSLT 2011 Arabic-English task , where we are able to reach moderate improvements on a state-of-the-art baseline with our training method .", "During training , we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration ."]}
{"orig_sents": ["3", "2", "4", "1", "5", "0"], "shuf_sents": ["When these individual systems are used together for system combination , our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems .", "The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines .", "System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other .", "We present Positive Diversity Tuning , a newmethod for tuningmachine translation models specifically for improved performance during system combination .", "We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations , while also punishing them for translations that are too similar to those produced by other systems .", "We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets , while still obtaining good BLEU scores ."]}
{"orig_sents": ["1", "4", "0", "2", "3"], "shuf_sents": ["Post-editing time prediction uses regression models , additionally fed with new elaborate features from the Statistical MT decoding process .", "This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation ( MT ) output .", "These seem to be better indicators of post-editing time than blackbox features .", "Prior to training the models , feature scoring with ReliefF and Information Gain is used to choose feature sets of decent size and avoid computational complexity .", "Sentence-level ranking of alternative MT outputs is done with pairwise classifiers using Logistic Regression with blackbox features originating from PCFG Parsing , language models and various counts ."]}
{"orig_sents": ["3", "2", "4", "0", "1"], "shuf_sents": ["Using only 25 ( out of 160 ) features , our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask , while the active learning model reached 2nd place in the scoring variant using only ? 25 % of the available instances for training .", "These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation , and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive .", "Our submissions use the framework of Gaussian Processes to investigate lightweight approaches for this problem .", "We describe the results of our submissions to the WMT13 Shared Task on Quality Estimation ( subtasks 1.1 and 1.3 ) .", "We focus on two approaches , one based on feature selection and another based on active learning ."]}
{"orig_sents": ["0", "3", "4", "5", "1", "2"], "shuf_sents": ["We introduce referential translation machines ( RTM ) for quality estimation of translation outputs .", "We develop novel techniques for solving all subtasks in the WMT13 quality estimation ( QE ) task ( QET 2013 ) based on individual RTM models .", "Our results achieve improvements over last year ? s QE task results ( QET 2012 ) , as well as our previous results , provide new features and techniques for QE , and rank 1st or 2nd in all of the subtasks .", "RTMs are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain , which can be used for estimating the quality of translation outputs , judging the semantic similarity between text , and evaluating the quality of student answers .", "RTMs achieve top performance in automatic , accurate , and language independent prediction of sentence-level and word-level statistical machine translation ( SMT ) quality .", "RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["Our submissions were focused on tasks whose aim was predicting sentence-level Human-mediated Translation Edit Rate and sentence-level post-editing time ( Task 1.1 and 1.3 , respectively ) .", "Our models consistently overcome the baselines for both tasks and performed particularly well for Task 1.3 , ranking first among seven participants .", "We designed features that are built on resources such as automatic word alignment , n-best candidate translation lists , back-translations and word posterior probabilities .", "In this paper we present the approach and system setup of the joint participation of Fondazione Bruno Kessler and University of Edinburgh in the WMT 2013 Quality Estimation shared-task ."]}
{"orig_sents": ["3", "5", "7", "6", "4", "2", "0", "1"], "shuf_sents": ["values around 0.30 .", "The results on the test set dropped significantly , raising different discussions to be taken into account .", "Evaluation at development time showed considerably good results in a cross-validation experiment , with Kendall ? s ?", "This paper describes the TALP-UPC participation in the WMT ? 13 Shared Task on Quality Estimation ( QE ) .", "For that , we learned Random Forest classifiers especially tailored for the problem .", "Our participation is reduced to task 1.2 on System Selection .", "We approached system selection by means of pairwise ranking decisions .", "We used a broad set of features ( 86 for German-to-English and 97 for English-to-Spanish ) ranging from standard QE features to features based on pseudo-references and semantic similarity ."]}
{"orig_sents": ["5", "4", "3", "1", "0", "2", "6"], "shuf_sents": ["In Task 2 , to take the contextual information into account , we employed a discriminative undirected probabilistic graphical model Conditional random field ( CRF ) , in addition to the NB algorithm .", "In Task 1.2 , we utilized a probability model Na ? ve Bayes ( NB ) as a classification algorithm with the features borrowed from the traditional evaluation metrics .", "The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB .", "In Task 1.1 , we used an enhanced version of BLEU metric without using reference translations to evaluate the translation quality .", "We submitted the results for Task 1.1 ( sentence-level quality estimation ) , Task 1.2 ( system selection ) and Task 2 ( word-level quality estimation ) .", "This paper is to introduce our participation in the WMT13 shared tasks on Quality Estimation for machine translation without using reference translations .", "The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2 ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We participated in the 1.1 , 1.2 and 1.3 sub-tasks with our QE system trained on features from diverse information sources like MT decoder features , n-best lists , mono- and bi-lingual corpora and giza training models .", "Our system shows competitive results in the workshop shared task .", "In this paper we present our entry to the WMT ? 13 shared task : Quality Estimation ( QE ) for machine translation ( MT ) ."]}
{"orig_sents": ["7", "9", "6", "10", "2", "0", "8", "3", "1", "4", "5"], "shuf_sents": ["To this end , we use a Support Vector Machine with 66 features .", "We assign a score to each sentence of these corpora .", "We perform a linear regression of the feature space against scores in the range .", "For that , we use the post-edited and reference corpora during the training step .", "Then , we tune these scores on a development corpus .", "This leads to an improvement of 10.5 % on the development corpus , in terms of Mean Average Error , but achieves only a slight improvement on the test corpus .", "Each translated sentence is given a score between 0 and 1 .", "In this paper we present the system we submitted to the WMT13 shared task on Quality Estimation .", "In this paper , we propose to increase the size of the training corpus .", "We participated in the Task 1.1 .", "The score is obtained by using several numerical or boolean features calculated according to the source and target sentences ."]}
{"orig_sents": ["1", "8", "4", "7", "0", "5", "2", "3", "6"], "shuf_sents": ["strategy to keep only the best performing ones .", "This paper presents the LIG ? s systems submitted for Task 2 of WMT13 Quality Estimation campaign .", "classifiers to build a strong ? composite ?", "classifier by taking advantage of their complementarity is presented and experimented .", "We integrate a number of features of various types ( system-based , lexical , syntactic and semantic ) into the conventional feature set , for our baseline classifier training .", "Then , a method that combines multiple ? weak ?", "We then select the best systems for submission and present the official results obtained .", "After the experiments with all features , we deploy a ? Feature Selection ?", "This is a word confidence estimation ( WCE ) task where each participant was asked to label each word in a translated text as a binary ( Keep/Change ) or multi-class ( Keep/Substitute/Delete ) category ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["The two systems use a wide variety of features , of which the most effective are the word-alignment , n-gram frequency , language model , POS-tag-based and pseudoreferences ones .", "Both systems perform at a similarly high level in the two tasks of scoring and ranking translations , although there is some evidence that the systems are over-fitting to the training data .", "Task 1.1 involve estimating postediting effort for English-Spanish translation pairs in the news domain .", "We describe the two systems submitted by the DCU-Symantec team to Task 1.1. of the WMT 2013 Shared Task on Quality Estimation for Machine Translation ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output .", "This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task ."]}
{"orig_sents": ["2", "1", "0", "3", "4"], "shuf_sents": ["Input features are generated by applying automatic translation error analysis to the translation hypotheses and calculating the error category frequency differences .", "It is a machine learning-based metric that is trained on manual ranking data from WMT shared tasks 2008 ? 2012 .", "We describe TerrorCat , a submission to this year ? s metrics shared task .", "We additionally experiment with adding quality estimation features in addition to the error analysis-based ones .", "When evaluated against WMT ? 2012 rankings , the systemlevel agreement is rather high for several language pairs ."]}
{"orig_sents": ["5", "0", "4", "3", "1", "2", "6"], "shuf_sents": ["ACT relies on automatic word-level alignment ( using GIZA++ ) between a source sentence and respectively the reference and candidate translations , along with other heuristics for comparing translations of discourse connectives .", "The actual version of ACT is available only for a limited language pairs .", "Consequently , we are participating only for the English/French and English/German language pairs .", "The accuracy of the ACT metric was assessed by human judges on sample data for English/French , English/Arabic , English/Italian and English/German translations ; the ACT scores are within 2-5 % of human scores .", "Using a dictionary of equivalents , the translations are scored automatically or , for more accuracy , semi-automatically .", "This paper gives a detailed description of the ACT ( Accuracy of Connective Translation ) metric , a reference-based metric that assesses only connective translations .", "Our hypothesis is that ACT metric scores increase with better translation quality in terms of human evaluation ."]}
{"orig_sents": ["5", "4", "3", "0", "2", "1"], "shuf_sents": ["nLEPOR_baseline measures the similarity of the system output translations and the reference translations only on word sequences .", "The evaluation results of WMT13 show LEPOR_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using Pearson correlation criterion on English-to-other ( FR , DE , ES , CS , RU ) language pairs .", "LEPOR_v3.1 is a new version of LEPOR metric using the mathematical harmonic mean to group the factors and employing some linguistic features , such as the part-of-speech information .", "nLEPOR_baseline is an n-gram based language independent MT evaluation metric employing the factors of modified sentence length penalty , position difference penalty , n-gram precision and n-gram recall .", "In the Metrics task , we submitted two automatic MT evaluation systems nLEPOR_baseline and LEPOR_v3.1 .", "This paper is to describe our machine translation evaluation systems used for participation in the WMT13 shared Metrics Task ."]}
{"orig_sents": ["5", "1", "0", "3", "4", "2"], "shuf_sents": ["MEANT is optimized by tuning a small number of weights ? one for each semantic role label ? so as to maximize correlation with human adequacy judgment on a development set .", "In this paper , we describe HKUST ? s submission to the WMT 2013 metrics evaluation task , MEANT and UMEANT .", "Evaluated on test sets from the WMT 2012/2011 metrics evaluation , bothMEANT and UMEANT achieve competitive correlations with human judgments using nothing more than a monolingual corpus and an automatic shallow semantic parser .", "UMEANT is an unsupervised version where weights for each semantic role label are estimated via an inexpensive unsupervised approach , as opposed to MEANT ? s supervised method relying on more expensive grid search .", "In this paper , we present a battery of experiments for optimizing MEANT on different development sets to determine the set of weights that maximize MEANT ? s accuracy and stability .", "The linguistically transparentMEANT and UMEANT metrics are tunable , simple yet highly effective , fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarity between MT output and reference translations ."]}
{"orig_sents": ["0", "1", "4", "5", "3", "2"], "shuf_sents": ["In this paper we describe our participation to the WMT13 Shared Task on Quality Estimation .", "The main originality of our approach is to include features originally designed to classify text according to some author ? s style .", "In the remaining of this paper we focus on subtask 1.3 , but there is very little difference in the application of the approach to task 1.1 .", "This approach was also used by the first author in his submissions to subtask 1.1 , identified as TCD-CNGL OPEN and TCD-CNGL RESTRICTED1 .", "This implies the use of reference categories , which are meant to represent the quality of the MT output .", "Preamble This paper describes the approach followed in the two systems that we submitted to subtask 1.3 of the WMT13 Shared Task on Quality Estimation , identified as TCD-DCU-CNGL 1-3 SVM1 and TCD-DCU-CNGL 1-3 SVM2 ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In this paper , we propose a novel syntactic based MT evaluation metric which only employs the dependency information in the source side .", "Experimental results show that our method achieves higher correlation with human judgments than BLEU , TER , HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010 ."]}
{"orig_sents": ["1", "4", "0", "2", "3"], "shuf_sents": ["In this work , we review several solutions to improve the accuracy of German-English word reordering while preserving the efficiency of phrase-based decoding .", "Despite being closely related languages , German and English are characterized by important word order differences .", "Among these , we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena .", "Through an extensive evaluation including diverse translation quality metrics , we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT .", "Longrange reordering of verbs , in particular , represents a real challenge for state-of-theart SMT systems and is one of the main reasons why translation quality is often so poor in this language pair ."]}
{"orig_sents": ["1", "6", "0", "2", "3", "5", "4"], "shuf_sents": ["While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems ( Koehn et al , 2007 ) , it is however commonly not employed in hierarchical decoders .", "We introduce a lexicalized reordering model for hierarchical phrase-based machine translation .", "We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories , and show how orientations can be scored in hierarchical decoding .", "The model is empirically evaluated on the NIST Chinese ? English translation task .", "On a French ? German translation task , we obtain a gain of up to +0.4 % BLEU .", "We achieve a significant improvement of +1.2 % BLEU over a typical hierarchical baseline setup and an improvement of +0.7 % BLEU over a syntax-augmented hierarchical setup .", "The model scores monotone , swap , and discontinuous phrase orientations in the manner of the one presented by Tillmann ( 2004 ) ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper presents a dependencyconstrained hierarchical machine translation model that uses Moses open-source toolkit for rule extraction and decoding .", "Experiments are carried out for the German-English language pair in both directions for projective and non-projective dependencies .", "We examine effects on SCFG size and automatic evaluation results when constraints are applied with respect to projective or non-projective dependency structures and on the source or target language side ."]}
{"orig_sents": ["4", "5", "3", "2", "1", "0"], "shuf_sents": ["While the current implementation is limited in the size of inputs it can handle in reasonable time , our experiments provide insights towards obtaining future speedups , while staying in the same general framework .", "Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection , without recourse to pruning techniques that only provide approximate solutions .", "We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain .", "In hierarchical translation , inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model .", "We present a method for inference in hierarchical phrase-based translation , where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling .", "We also present a first implementation of that method along with experimental results shedding light on some fundamental issues ."]}
{"orig_sents": ["3", "1", "0", "2", "6", "4", "5", "7"], "shuf_sents": ["Evaluating aligners under noisy conditions would seem to require creating an evaluation dataset by manually annotating a noisy document for gold-standard alignments .", "Most aligners do not perform very well when the input is a noisy , rather than a highlyparallel , document pair .", "Such a costly process hinders our ability to evaluate an aligner under various types and levels of noise .", "Sentence alignment is an important step in the preparation of parallel data .", "Our approach is unique as it requires no manual labeling , instead relying on small parallel datasets ( already at the disposal of MT researchers ) to generate many evaluation datasets that mimic a variety of noisy conditions .", "We use our framework to perform a comprehensive comparison of three aligners under noisy conditions .", "In this paper , we propose a new evaluation framework for sentence aligners , which is particularly suitable for noisy-data evaluation .", "Furthermore , our framework facilitates the fine-tuning of a state-of-the-art sentence aligner , allowing us to substantially increase its recall rates by anywhere from 5 % to 14 % ( absolute ) across several language pairs ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["This approach leads to better machine translation results than a morphemeaware model that does not explicitly model morpheme reordering .", "We apply multi-rate HMMs , a tree structured HMM model , to the word-alignment problem .", "Multi-rate HMMs allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["Our model assumes that the alignment variables have a tree structure which is isomorphic to the target dependency tree and models the distortion probability based on the source dependency tree , thereby incorporating the syntactic structure from both sides of the parallel sentences .", "While our model was sensitive to posterior thresholds , it also showed a performance comparable to that of HMM alignment models .", "We propose a novel unsupervised word alignment model based on the Hidden Markov Tree ( HMT ) model .", "In English-Japanese word alignment experiments , our model outperformed an IBM Model 4 baseline by over 3 points alignment error rate ."]}
{"orig_sents": ["5", "2", "4", "0", "3", "1"], "shuf_sents": ["We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words .", "By using these methods we are able to improve the translation performance by up to 0.8 BLEU points compared to a system that uses a standard DWL .", "We present two ways to extend a DWL to improve its ability to model the word translation probability in a phrase-based machine translation ( PBMT ) system .", "Furthermore , as the standard DWL does not get any feedback from the MT system , we change the DWL training process to explicitly focus on addressing MT errors .", "While DWLs are able to model the global source information , they ignore the structure of the source and target sentence .", "The Discriminative Word Lexicon ( DWL ) is a maximum-entropy model that predicts the target word probability given the source sentence words ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We present two methods for automatic correction and extension of morphological annotations , and demonstrate their success on three divergent Egyptian Arabic corpora .", "For languages with complex morphologies , limited resources and tools , and/or lack of standard grammars , developing annotated resources can be a challenging task .", "Annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper presents a method for part-ofspeech tagging of historical data and evaluates it on texts from different corpora of historical German ( 15th ? 18th century ) .", "Spelling normalization is used to preprocess the texts before applying a POS tagger trained on modern German corpora .", "Using only 250 manually normalized tokens as training data , the tagging accuracy of a manuscript from the 15th century can be raised from 28.65 % to 74.89 % ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["However , constructing highquality annotated treebanks is a challenging task .", "Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank influenced the parsing results and real difficulties of Vietnamese parsing that required further improvements to existing parsing technologies .", "We utilized two publicly available parsers , Berkeley and MST parsers , for feedback on improving the quality of part-of-speech tagging for the Vietnamese Treebank .", "The recent success of statistical parsing methods has made treebanks become important resources for building good parsers ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated.1", "When creating a new resource , preprocessing the source texts before annotation is both ubiquitous and obvious .", "In this paper , we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline .", "How the preprocessing affects the annotation effort for various tasks is for the most part an open question , however ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We explore the use of continuous rating scales for human evaluation in the context of machine translation evaluation , comparing two assessor-intrinsic qualitycontrol techniques that do not rely on agreement with expert judgments .", "Experiments employing Amazon ? s Mechanical Turk service show that quality-control techniques made possible by the use of the continuous scale show dramatic improvements to intra-annotator agreement of up to +0.101 in the kappa coefficient , with inter-annotator agreement increasing by up to +0.144 when additional standardization of scores is applied ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We use crowdsourcing to obtain query and sentence chunking and show that entailment can not only be used as an effective evaluation metric to assess the quality of annotations , but it can also be employed to filter out noisy annotations .", "Hierarchical or nested annotation of linguistic data often co-exists with simpler non-hierarchical or flat counterparts , a classic example being that of annotations used for parsing and chunking .", "In this work , we propose a general strategy for comparing across these two schemes of annotation using the concept of entailment that formalizes a correspondence between them ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["We introduce a framework for lightweight dependency syntax annotation .", "We demonstrate the efficacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations .", "Our formalism builds upon the typical representation for unlabeled dependencies , permitting a simple notation and annotation workflow .", "Moreover , the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["The paper addresses the challenge of converting MIDT , an existing dependency ?", "based Italian treebank resulting from the harmonization and merging of smaller resources , into the Stanford Dependencies annotation formalism , with the final aim of constructing a standard ? compliant resource for the Italian language .", "Achieved results include a methodology for converting treebank annotations belonging to the same dependency ? based family , the Italian Stanford Dependency Treebank ( ISDT ) , and an Italian localization of the Stanford Dependency scheme ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["Finally , we highlight some language phenomena and give some remarks .", "Based on the annota-tion , we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects .", "In this work , we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09 .", "Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University , Taipei , Taiwan { hhhuang , jsyu , twchang , cklin } @ nlg.csie.ntu.edu.tw ; hhchen @ ntu.edu.tw Abstract Discourse relation may entail sentiment in-formation ."]}
{"orig_sents": ["5", "1", "8", "9", "6", "2", "4", "0", "3", "7"], "shuf_sents": ["We demonstrate this new functionality by using it to compare annotations belonging to two different approaches to discourse analysis , namely discourse relations and functional discourse annotations .", "Comparison and integration of multiple schemes have the potential to provide enhanced information .", "The extension works by allowing the construction of parallel subworkflows for each scheme within a single U-Compare workflow .", "Integrating these different annotation types within an interoperable environment allows us to study the correlations between different types of discourse and report on the new insights that this allows us to discover .", "The different types of discourse annotations produced by each sub-workflow can be either merged or visualised side-by-side for comparison .", "There exist various different discourse annotation schemes that vary both in the perspectives of discourse structure considered and the granularity of textual units that are annotated .", "In this paper , we present an extension of U-Compare that allows the easy comparison , integration and visualisation of resources that contain or output annotations based on multiple discourse annotation schemes .", "? The authors have contributed equally to the development of this work and production of the manuscript .", "However , the differing formats of corpora and tools that contain or produce such schemes can be a barrier to their integration .", "U-Compare is a graphical , UIMA-based workflow construction platform for combining interoperable natural language processing ( NLP ) resources , without the need for programming skills ."]}
{"orig_sents": ["6", "3", "1", "0", "5", "4", "7", "2"], "shuf_sents": ["This flexibility in extending type systems has resulted in the development of repositories of components that share one or several type systems ; however , components coming from different repositories , and thus not sharing type systems , remain incompatible .", "The components exchange data by sharing common type systems ? schemata of data type structures ? which extend a generic , top-level type system built into UIMA .", "The proposed solution encourages ad hoc conversions , enables the usage of heterogeneous components , and facilitates highly customised UIMA applications .", "The architecture defines common data structures and interfaces to support interoperability of individual processing components working together in a UIMA application .", "We alleviate this problem by introducing a conversion mechanism based on SPARQL , a query language for the data retrieval and manipulation of RDF graphs .", "Commonly , this problem has been solved programmatically by implementing UIMA components that perform the alignment of two type systems , an arduous task that is impractical with a growing number of type systems .", "Unstructured Information Management Architecture ( UIMA ) has been gaining popularity in annotating text corpora .", "We provide a UIMA component that serialises data coming from a source component into RDF , executes a user-defined , typeconversion query , and deserialises the updated graph into a target component ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["This access provides information about inter-layer relations and dependencies that have been previously difficult to explore , and which are highly valuable for continued development of language processing applications .", "We outline the process of mapping MASC ? s GrAF representation to ANNIS ? s internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in the corpus .", "This paper describes the importation of Manually Annotated Sub-Corpus ( MASC ) data and annotations into the linguistic database ANNIS , which allows users to visualize and query linguistically-annotated corpora ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper discusses the problem of annotating coreference relations with generic expressions in a large scale corpus .", "We present and analyze some existing theories of genericity , compare them to the approaches to generics that are used in the state-of-the-art coreference annotation guidelines and discuss how coreference of generic expressions is processed in the manual annotation of the Prague Dependency Treebank .", "After analyzing some typical problematic issues we propose some partial solutions that can be used to enhance the quality and consistency of the annotation ."]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["The results suggest that most of the crowd annotations were good enough to use as training data for resolving such anaphoric nouns .", "We examine the feasibility of annotating such anaphoric nouns using crowdsourcing .", "Anaphoric shell nouns such as this issue and this fact conceptually encapsulate complex pieces of information ( Schmid , 2000 ) .", "We also evaluated the quality of crowd annotation using experts .", "In particular , we present our methodology for reliably annotating antecedents of such anaphoric nouns and the challenges we faced in doing so ."]}
{"orig_sents": ["4", "2", "3", "1", "0"], "shuf_sents": ["The results indicate that our current approach to local discourse structure needs to accommodate properly contained arguments and relations , and partially overlapping as well as shared arguments ; deviating further from simple trees , but not as drastically as a chain graph structure would imply , since no genuine cases of structural crossing dependencies are attested in TDB .", "The effects of information structure in the surface form , which result in seemingly complex configurations with underlying simple dependencies , are introduced ; and the structural implications are discussed .", "This paper investigates the structure represented by the explicit connectives annotated in the multiplegenre Turkish Discourse Bank ( TDB ) .", "The dependencies that violate tree-constraints are analyzed .", "Various discourse theories have argued for data structures ranging from the simplest trees to the most complex chain graphs ."]}
{"orig_sents": ["0", "2", "4", "1", "3"], "shuf_sents": ["In this paper , we present an annotation tool developed specifically for manual sentiment analysis of social media posts .", "It is also designed as a SaaS ( Software as a Service ) .", "The tool provides facilities for general and target based opinion marking on different type of posts ( i.e .", "The tool ? s outstanding features are easy and fast annotation interface , detailed sentiment levels , multi-client support , easy to manage administrative modules and linguistic annotation capabilities .", "comparative , ironic , conditional ) with a web based UI which supports synchronous annotation ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We describe a new annotation scheme for formalizing relation structures in research papers .", "We report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the IPSJ Journal .", "Using the scheme , we are building a Japanese corpus to help develop information extraction systems for digital libraries .", "The scheme has been developed through the investigation of computer science papers ."]}
{"orig_sents": ["0", "4", "2", "1", "3"], "shuf_sents": ["Semantically annotated corpora play an important role in natural language processing .", "Upon the completion of this project , all annotated corpora will be made freely available .", "Each subcorpus is first sensetagged using a wordnet and then these synsets are linked .", "The multilingual corpora are designed to not only provide data for NLP tasks like machine translation , but also to contribute to the study of translation shift and bilingual lexicography as well as the improvement of monolingual wordnets .", "This paper presents the results of a pilot study on building a sense-tagged parallel corpus , part of ongoing construction of aligned corpora for four languages ( English , Chinese , Japanese , and Indonesian ) in four domains ( story , essay , news , and tourism ) from the NTU-Multilingual Corpus ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["In this paper , we discuss our efforts to annotate nominals in the Hindi Treebank with the semantic property of animacy .", "The suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution , syntactic parsing , verb classification and argument differentiation .", "Although the treebank already encodes lexical information at a number of levels such as morph and part of speech , the addition of animacy information seems promising given its relevance to varied linguistic phenomena ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["Automatic pre-annotation is often used to improve human annotation speed and accuracy .", "Our study design includes two different corpora , three pre-annotation schemes linked to two annotation levels , both expert and novice annotators , a questionnaire-based subjective assessment and a corpus-based quantitative assessment .", "We address here out-of-domain named entity annotation , and examine whether automatic pre-annotation is still beneficial in this setting .", "We observe that preannotation helps in all cases , both for speed and for accuracy , and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We hope that a sembank of simple , whole-sentence semantic structures will spur new work in statistical natural language understanding and generation , like the Penn Treebank encouraged work on statistical parsing .", "This paper gives an overview of AMR and tools associated with it .", "We describe Abstract Meaning Representation ( AMR ) , a semantic representation language in which we are writing down the meanings of thousands of English sentences ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper presents a case study of a difficult and important categorical annotation task ( word sense ) to demonstrate a probabilistic annotation model applied to crowdsourced data .", "It is argued that standard ( chance-adjusted ) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels .", "Compared to conventional agreement measures , application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["We investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation .", "Our task is to annotate the argumentative structure of short texts .", "We show that it is possible to isolate a reliable subgroup of annotators , so that aspects of the difficulty of the underlying task can be studied ."]}
{"orig_sents": ["2", "0", "1", "3", "4"], "shuf_sents": ["We review how paraphrase recognition has benefited from crowdsourcing in the past and identify two problems in paraphrase acquisition and semantic similarity evaluation that can be solved by employing a smart crowdsourcing strategy .", "First , we employ the CrowdFlower platform to conduct an experiment on sub-sentential paraphrase acquisition with early exclusion of lowaccuracy crowdworkers .", "Crowdsourcing , while ideally reducing both costs and the need for domain experts , is no all-purpose tool .", "Second , we compare two human intelligence task designs for evaluating phrase pairs on a semantic similarity scale .", "While the first experiment confirms our strategy successful at tackling the problem of missing gold in paraphrase generation , the results of the second experiment suggest that , for both semantic similarity evaluation on a continuous and a binary scale , querying crowdworkers for a semantic similarity value on a multi-grade scale yields better results than directly asking for a binary classification ."]}
{"orig_sents": ["1", "3", "2", "4", "0"], "shuf_sents": ["Our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches .", "This paper presents an analysis of an annotator ? s behaviour during her/his annotation process for eliciting useful information for natural language processing ( NLP ) tasks .", "Since an annotator ? s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation , analysing the process of text annotation has potential to reveal useful information for NLP tasks , in particular semantic and discourse processing that require deeper language understanding .", "Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems .", "We conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in Japanese texts ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We describe how we have improved our annotation guideline by using the evaluation ( in terms of precision , recall and F-Measure ) of a first round of annotation produced by two expert annotators and by our automatic annotation system .", "The annotation scheme we propose is based on the detection of predicative cues referring to an enunciative and/or modal variation - and their scope at a sentence level .", "In this paper we present the development of a corpus of French newswire texts annotated with enunciative and modal commitment information ."]}
{"orig_sents": ["6", "7", "8", "3", "4", "2", "1", "0", "5"], "shuf_sents": ["The best accuracy 0.679 was achieved with Na ?", "? ve Bayes Multinomial ) significantly outperform proposed knowledge-based method all obtained results are above baseline .", "Despite that supervised machine learning methods ( Support Vector Machine and Na ?", "We explore an influence of sentiment word dictionaries based on the different parts-of-speech ( adjectives , adverbs , nouns , and verbs ) for knowledge-based method ; different feature types ( bag-ofwords , lemmas , word n-grams , character n-grams ) for machine learning methods ; and pre-processing techniques ( emoticons replacement with sentiment words , diacritics replacement , etc . )", "for both approaches .", "? ve Bayes Multinomial and token unigrams plus bigrams , when pre-processing involved diacritics replacement .", "Despite many methods that effectively solve sentiment classification task for such widely used languages as English , there is no clear answer which methods are the most suitable for the languages that are substantially different .", "In this paper we attempt to solve Internet comments sentiment classification task for Lithuanian , using two classification approaches ?", "knowledge-based and supervised machine learning ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["The paper describes the state of the art in sentiment analysis in Russian , collection characteristics , track tasks and evaluation metrics .", "These initiatives took part within Russian Information Retrieval Seminar ( ROMIP ) , which is an annual TREC-like competition in Russian .", "In this paper we describe our experience in conducting the first open sentiment analysis evaluations in Russian in 2011-2012 .", "Several test and train collections were created for such tasks as sentiment classification in blogs and newswire , opinion retrieval ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["We show that a supervised approach to linking opinion clues to aspects is feasible , and that the extracted clues and aspects improve polarity and rating predictions .", "Aspect-oriented opinion mining aims to identify product aspects ( features of products ) about which opinion has been expressed in the text .", "We propose methods for acquiring a domain-specific opinion lexicon , linking opinion clues to product aspects , and predicting polarity and rating of reviews .", "We present an approach for aspect-oriented opinion mining from user reviews in Croatian ."]}
{"orig_sents": ["6", "0", "1", "3", "4", "2", "5"], "shuf_sents": ["Unlike general purpose retrieval engines , FAQ retrieval engines have to address the lexical gap between the query and the usually short answer .", "In this paper we describe the design and evaluation of a FAQ retrieval engine for Croatian .", "We train and evaluate on a FAQ test collection built specifically for this purpose .", "We frame the task as a binary classification problem , and train a model to classify each FAQ as either relevant or not relevant for a given query .", "We use a variety of semantic textual similarity features , including term overlap and vector space features .", "Our best-performing model reaches 0.47 of mean reciprocal rank , i.e. , on average ranks the relevant answer among the top two returned answers .", "Frequently asked questions ( FAQ ) are an efficient way of communicating domainspecific information to the users ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Within the proposed approach , we show how to treat some significant phenomena of the Russian language and also perform a brief evaluation of the parser implementation , known as DictaScope Syntax .", "This leads to accurate parses represented in a specific way , richer than constituency or dependency tree .", "It also allows reducing parsing time complexity .", "We present an approach for natural language parsing in which dependency and constituency parses are acquired simultaneously ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["We represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text .", "We describe GPKEX , a keyphrase extraction method based on genetic programming .", "We apply and evaluate GPKEX on Croatian newspaper articles .", "We show that GPKEX can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for Croatian ."]}
{"orig_sents": ["4", "0", "2", "6", "5", "3", "1"], "shuf_sents": ["The models stem from a new manually annotated SETIMES.HR corpus of Croatian , based on the SETimes parallel corpus .", "The SETIMES.HR corpus , its resulting models and test sets are all made freely available .", "We train models on Croatian text and evaluate them on samples of Croatian and Serbian from the SETimes corpus and the two Wikipedias .", "Results indicate that more complex methods of Croatian-toSerbian annotation projection are not required on such dataset sizes for these particular tasks .", "We investigate state-of-the-art statistical models for lemmatization and morphosyntactic tagging of Croatian and Serbian .", "Part of speech tagging accuracies reach 97.13 % and 96.46 % .", "Lemmatization accuracy for the two languages reaches 97.87 % and 96.30 % , while full morphosyntactic tagging accuracy using a 600-tag tagset peaks at 87.72 % and 85.56 % , respectively ."]}
{"orig_sents": ["0", "3", "5", "1", "2", "4"], "shuf_sents": ["We propose a language-independent word normalization method exemplified on modernizing historical Slovene words .", "In one , we use a lexicon of historical word ?", "contemporary word pairs and a list of contemporary words ; in the other , we only use a list of historical words and one of contemporary ones .", "Our method relies on character-based statistical machine translation and uses only shallow knowledge .", "We show that both methods produce significantly better results than the baseline .", "We present the relevant lexicons and two experiments ."]}
{"orig_sents": ["4", "6", "3", "0", "5", "1", "2", "7"], "shuf_sents": ["and DamerauLevenshtein distance between them is computed .", "sentence pairs .", "The experimental results show precision 0.81 and recall 0.8 , which allows the method to be used as additional data source in parallel corpora alignment .", "Sequences of POS tags for each sentence ( exactly , nouns , adjectives , verbs and pronouns ) are processed as ? words ?", "The present paper introduces approach to improve English-Russian sentence alignment , based on POS-tagging of automatically aligned ( by HunAlign ) source and target texts .", "This distance is then normalized by the length of the target sentence and is used as a threshold between supposedly mis-aligned and ? good ?", "The initial hypothesis is tested on a corpus of bitexts .", "At the same time , this leaves space for further improvement ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["In this paper we present a corpus-based approach to automatic identification of false friends for Slovene and Croatian , a pair of closely related languages .", "The presented approach works on non-parallel datasets , is knowledge-lean and language-independent , which makes it attractive for natural language processing tasks that often lack the lexical resources and can not afford to build them by hand .", "With the best performing setting we obtain very good average precision of 0.973 and 0.883 on different gold standards .", "By taking advantage of the lexical overlap between the two languages , we focus on measuring the difference in meaning between identically spelled words by using frequency and distributional information .", "We analyze the impact of corpora of different origin and size together with different association and similarity measures and compare them to a simple frequency-based baseline ."]}
{"orig_sents": ["3", "1", "4", "2", "0"], "shuf_sents": ["On this corpus , our CRF-based system achieves an overall F1-score of 87 % .", "In this work , we address the problem of NER in Estonian using supervised learning approach .", "For system training and evaluation purposes , we create a gold standard NER corpus .", "The task of Named Entity Recognition ( NER ) is to identify in text predefined units of information such as person names , organizations and locations .", "We explore common issues related to building a NER system such as the usage of language-agnostic and languagespecific features , the representation of named entity tags , the required corpus size and the need for linguistic tools ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We study various settings and combinations of the methods and present evaluation results on five corpora gathered from Twitter , centred around major events and known individuals .", "This paper reports on some experiments aiming at tuning a rule-based NER system designed for detecting names in Polish online news to the processing of targeted Twitter streams .", "In particular , one explores whether the performance of the baseline NER system can be improved through the incremental application of knowledge-poor methods for name matching and guessing ."]}
{"orig_sents": ["4", "3", "2", "0", "1"], "shuf_sents": ["The extensions include : new features , application of external knowledge and post-processing .", "For the partial evaluation the final model obtained 90.02 % recall with 91.30 % precision on the corpus of economic news .", "We also present several extensions to the binary model which give an improvement of the recall .", "We discuss to what extent the recall of NE recognition can be improved by reducing the space of NE categories .", "In the paper we discuss the problem of low recall for the named entity ( NE ) recognition task for Polish ."]}
{"orig_sents": ["0", "3", "5", "4", "2", "1"], "shuf_sents": ["This paper describes a plug-in component to extend the PULS information extraction framework to analyze Russian-language text .", "The approach described in the paper can be generalized to a range of heavilyinflected languages .", "The component for Russian analysis is described and its performance is evaluated on two news-analysis scenarios : epidemic surveillance and cross-border security .", "PULS is a comprehensive framework for information extraction ( IE ) that is used for analysis of news in several scenarios from English-language text and is primarily monolingual .", "Thus , the objective of the present work is to explore whether the base framework can be extended to cover additional languages with limited effort , and to leverage the preexisting PULS modules as far as possible , in order to accelerate the development process .", "Although monolinguality is recognized as a serious limitation , building an IE system for a new language from the bottom up is very labor-intensive ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["In this paper we present a semi-automatic approach for acqusition of lexico-syntactic knowledge for event extraction in two Slavic languages , namely Bulgarian and Czech .", "Moreover , an intervention from a language expert is envisaged on different steps in the learning procedure , which increases its accuracy , with respect to unsupervised methods for lexical and grammar learning .", "The method uses several weaklysupervised and unsupervised algorithms , based on distributional semantics ."]}
{"orig_sents": ["1", "4", "3", "5", "0", "2"], "shuf_sents": ["0.774 , and the extended method ( based on the data obtained through machine learning ) ?", "We propose a method for cross-language identification of semantic relations based on word similarity measurement and morphosemantic relations in WordNet .", "0.721 .", "Our experiments are based on Bulgarian-English parallel and comparable texts but the method is to a great extent language-independent and particularly suited to less-resourced languages , since it does not need parsed or semantically annotated data .", "We transfer these relations to pairs of derivationally unrelated words and train a model for automatic classification of new instances of ( morpho ) semantic relations in context based on the existing ones and the general semantic classes of collocated verb and noun senses .", "The application of the method leads to an increase in the number of discovered semantic relations by 58.35 % and performs relatively consistently , with a small decrease in precision between the baseline ( based on morphosemantic relations identified in wordnet ) ?"]}
{"orig_sents": ["0", "2", "4", "3", "1"], "shuf_sents": ["We propose a data-driven approach to enhance translation extraction from comparable corpora .", "As a consequence , the translated vectors are less noisy and richer , and allow for the extraction of higher quality lexicons compared to simpler methods .", "Instead of resorting to an external dictionary , we translate source vector features by using a cross-lingual Word Sense Disambiguation method .", "The translations found in the disambiguation output convey the sense of the features in the source vector , while the use of translation clusters permits to expand their translation with several variants .", "The candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature ."]}
{"orig_sents": ["6", "4", "0", "3", "1", "5", "2"], "shuf_sents": ["Moreover , we use a freely available word aligner : Anymalign ( Lardilleux et al , 2011 ) for constructing context vectors .", "Therefore , we do not require to translate context vectors by using a seed dictionary and improve the accuracy of low frequency word alignments that is weakness of statistical model by using Anymalign .", "The experimental results have demonstrated that our method for high-frequency words shows at least 76.3 and up to 87.2 % and for the lowfrequency words at least 43.3 % and up to 48.9 % within the top 20 ranking candidates , respectively .", "Unlike the previous works , we obtain context vectors via a pivot language .", "To do this , we bring in a bridge language named the pivot language and adopt information retrieval techniques combined with natural language processing techniques .", "In this paper , experiments have been conducted on two different language pairs that are bi-directional KoreanSpanish and Korean-French , respectively .", "This paper presents a simple and effective method for automatic bilingual lexicon extraction from less-known language pairs ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora .", "On two specialized French-English comparable corpora , empirical experimental results show that the proposed method consistently outperforms the standard approach .", "The aim of this process is to identify the translations that are more likely to give the best representation of words in the target language .", "We study of the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors .", "For this purpose , we augment the standard approach by a Word Sense Disambiguation process relying on a WordNet-based semantic similarity measure ."]}
{"orig_sents": ["4", "1", "5", "0", "2", "3"], "shuf_sents": ["In this paper , we present a study of some widelyused smoothing algorithms for language n-gram modeling ( Laplace , Good-Turing , Kneser-Ney ... ) .", "However , less attention has been given to it for bilingual lexicon extraction from comparable corpora .", "Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach ( Fung , 1998 ) traditionally used for bilingual lexicon extraction .", "We show that using smoothing as a preprocessing step of the standard approach increases its performance significantly .", "Smoothing is a central issue in language modeling and a prior step in different natural language processing ( NLP ) tasks .", "If a first work to improve the extraction of low frequency words showed significant improvement while using distance-based averaging ( Pekar et al , 2006 ) , no investigation of the many smoothing techniques has been carried out so far ."]}
{"orig_sents": ["1", "5", "2", "10", "8", "3", "4", "6", "7", "0", "9"], "shuf_sents": ["Experimental results on quasi ?", "Parallel sentences are crucial for statistical machine translation ( SMT ) .", "Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora .", "We extend a previous study that treats parallel sentence identification as a binary classification problem .", "Previous method of classifier training by the Cartesian product is not practical , because it differs from the real process of parallel sentence extraction .", "However , they are quite scarce for most language pairs , such as Chinese ? Japanese .", "We propose a novel classifier training method that simulates the real sentence extraction process .", "Furthermore , we use linguistic knowledge of Chinese character features .", "The task is significantly more difficult than the extraction from noisy parallel or comparable corpora .", "comparable corpora indicate that our proposed approach performs significantly better than the previous study .", "We extract Chinese ? Japanese parallel sentences from quasi ? comparable corpora , which are available in far larger quantities ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Besides describing the main modules integrated in the crawler ( dealing with page fetching , normalization , cleaning , text classification , de-duplication and document pair detection ) , we evaluate several of the system functionalities in an experiment for the acquisition of pairs of parallel documents in German and Italian for the `` Health & Safety at work '' domain .", "This paper discusses a modular and opensource focused crawler ( ILSP-FC ) for the automatic acquisition of domain-specific monolingual and bilingual corpora from the Web ."]}
{"orig_sents": ["4", "2", "0", "1", "3"], "shuf_sents": ["The data basis is the English Scientific Text Corpus ( SCITEX ) which covers a time range of roughly thirty years ( 1970/80s to early 2000s ) .", "In particular , we investigate the disciplinary diversification/relatedness of scientific research articles in terms of register .", "The focus is on selected scientific disciplines at the boundaries to computer science ( computational linguistics , bioinformatics , digital construction , microelectronics ) .", "Our results are relevant for research on multilingually comparable corpora as used in machine translation and related research , since they shed new light on the notion of ? comparablity ? .", "We present a study on linguistic contrast and commonality in English scientific discourse on the basis of a monolingually comparable corpus ."]}
{"orig_sents": ["1", "2", "0", "3"], "shuf_sents": ["The most important fact is that this approach does not need any domain specific corpus .", "In this article , we present an automated approach of extracting English-Bengali parallel fragments of text from comparable corpora created using Wikipedia documents .", "Our approach exploits the multilingualism of Wikipedia .", "We have been able to improve the BLEU score of an existing domain specific EnglishBengali machine translation system by 11.14 % ."]}
{"orig_sents": ["1", "2", "3", "4", "0"], "shuf_sents": ["Therefore , the corpus resources created , as well as our analysis results will find application in different research areas , such as translation studies , machine translation , and others .", "This paper presents a comparable translation corpus created to investigate translation variation phenomena in terms of contrasts between languages , text types and translation methods ( machine vs. computer-aided vs. human ) .", "These phenomena are reflected in linguistic features of translated texts belonging to different registers and produced with different translation methods .", "For their analysis , we combine methods derived from translation studies , language variation and machine translation , concentrating especially on textual and lexico-grammatical variation .", "To our knowledge , none of the existing corpora can provide comparable resources for a comprehensive analysis of variation across text types and translation methods ."]}
{"orig_sents": ["0", "1", "2", "3", "4"], "shuf_sents": ["c Elizabeth A. Lennon Winter Mason Jeffrey V. Nickerson Stevens Institute of Technology Center for Decision Technologies Castle Point on Hudson , Hoboken , NJ USA { ygenc , elennon , wmason , jnickerson } @ stevens.edu Abstract Tools and techniques that automate the inter-pretation of multilingual corpora are useful on many fronts ; scholars , as an example , could use such tools to more readily pinpoint relevant articles from journals in a wide vari-ety of languages .", "This work describes tech-niques to build and characterize ontologies using collaborative knowledge bases , e.g. , Wikipedia .", "These ontologies can then be used to search and classify texts .", "Originally devel-oped for monolingual corpora , we extend the approach to multilingual texts and test the methods with Mandarin scientific abstracts .", "The presented techniques provide a novel and efficient mechanism to obtain contextually rich ontologies and measure document rele-vancy within multilingual corpora ."]}
{"orig_sents": ["4", "6", "1", "0", "7", "5", "3", "2"], "shuf_sents": ["We apply our method on two language pairs : one that uses the same character set and another with a different script , English-French and EnglishChinese , respectively .", "Based on this hypothesis , we employ a Random Forest ( RF ) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples .", "Finally , we compare RF against Support Vector Machines and analyse our results .", "We evaluate RF against a state-of-the-art alignment method , GIZA++ , and we report a statistically significant improvement .", "We present a novel method to recognise semantic equivalents of biomedical terms in language pairs .", "Nonetheless , our method performs robustly on both cases .", "We hypothesise that biomedical term are formed by semantically similar textual units across languages .", "We show that English-French pairs of terms are highly transliterated in contrast to the EnglishChinese pairs ."]}
{"orig_sents": ["5", "6", "8", "9", "1", "4", "7", "2", "0", "3"], "shuf_sents": ["Experimental results show that , the proposed measure can capture differences in terms of opinions .", "To develop SCM , we need either to get or to build parallel sentiment corpora .", "Then we use the extracted parallel sentiment corpora to develop multilingual sentiment analysis system .", "The results also show that comparable articles variate in their objectivity and positivity .", "Because this kind of corpora are not available , we decided to build them .", "Multilingual sentiment analysis attracts increased attention as the massive growth of multilingual web contents .", "This conducts to study opinions across different languages by comparing the underlying messages written by different people having different opinions .", "For that , we propose a new method to automatically label parallel corpora with sentiment classes .", "In this paper , we propose Sentiment based Comparability Measures ( SCM ) to compare opinions in multilingual comparable articles without translating source/target into the same language .", "This will allow media trackers ( journalists ) to automatically detect public opinion split across huge multilingual web contents ."]}
{"orig_sents": ["6", "5", "2", "4", "3", "0", "1"], "shuf_sents": ["The SMT experiments , however , show that the extracted data is not refined enough to improve a strong in-domain SMT system .", "Nevertheless , it is good enough to boost the performance of an out-of-domain system trained on sizable amounts of data .", "The algorithm ranks the candidate sentence pairs by means of a customized metric , which combines different similarity criteria .", "The precision estimates show that the extracted sentence pairs are clearly semantically equivalent .", "Moreover , we limit the search space to a specific topical domain , since our final goal is to use the extracted data in a domain-specific Statistical Machine Translation ( SMT ) setting .", "This paper proposes a method for exploiting Wikipedia articles without worrying about the position of the sentences in the text .", "Previous attempts in extracting parallel data from Wikipedia were restricted by the monotonicity constraint of the alignment algorithm used for matching possible candidates ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Sentential paraphrases are extracted from a comparable corpus of temporally and topically related messages on Twitter which often express semantically identical information through distinct surface forms .", "We demonstrate the utility of this new resource on the task of paraphrasing and normalizing noisy text , showing improvement over several state-of-the-art paraphrase and normalization systems 1 .", "We present a new and unique paraphrase resource , which contains meaningpreserving transformations between informal user-generated text ."]}
{"orig_sents": ["4", "3", "1", "2", "0"], "shuf_sents": ["On a monolingual collection of less than 100 documents , the proposed approach assigns comparable documents to different comparable corpora with high confidence .", "These innovative characteristics are used to build a LSA vector-based representation of the texts .", "In accordance with this new reduced in dimensionality document space , an unsupervised machine learning algorithm gathers similar texts into comparable clusters .", "Its originality lies within the delineation of relevant comparability characteristics of similar documents in line with an established definition of comparable corpora .", "Focusing on a systematic Latent Semantic Analysis ( LSA ) and Machine Learning ( ML ) approach , this research contributes to the development of a methodology for the automatic compilation of comparable collections of documents ."]}
{"orig_sents": ["2", "1", "0", "3"], "shuf_sents": ["Enhanced algorithms are proposed to match more bilingual webpages following the credibility based on statistical analysis of the link relationship of the seed websites available .", "It extends from a previous algorithm that takes the number of bilingual URL pairs that a key ( i.e. , a URL pairing pattern ) can match as the objective function to search for the best set of keys yielding the greatest number of webpage pairs within targeted bilingual websites .", "This paper presents an efficient approach to finding more bilingual webpage pairs with high credibility via link analysis , using little prior knowledge or heuristics .", "With about 12,800 seed websites as test set , the enhanced algorithms improve precision over baseline by more than 5 % , from 94.06 % to 99.40 % , and hence find above 20 % more true bilingual URL pairs , illustrating that significantly more bilingual webpages with high credibility can be mined with the help of the link analysis ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["In this paper , we propose an explanation of this finding based on the notion of segmentation ambiguity .", "Cross-linguistic studies on unsupervised word segmentation have consistently shown that English is easier to segment than other languages .", "We suggest that segmentation ambiguity is linked to a trade-off between syllable structure complexity and word length distribution .", "We show that English has a very low segmentation ambiguity compared to Japanese and that this difference correlates with the segmentation performance in a unigram model ."]}
{"orig_sents": ["1", "0", "4", "3", "2"], "shuf_sents": ["However , there have been no computational proposals for how people might use another powerful learning mechanism : generalization from learned to analogous distinctions ( e.g. , from /b/ ? /p/ to /g/ ? /k/ ) .", "Computational work in the past decade has produced several models accounting for phonetic category learning from distributional and lexical cues .", "We present two sets of simulations that reproduce key features of human performance in behavioral experiments , and we discuss the model ? s implications and directions for future research .", "The model captures our proposal that linguistic knowledge includes the possibility that category types in a language ( such as voiced and voiceless ) can be shared across sound classes ( such as labial and velar ) , thus naturally leading to generalization .", "Here , we present a new simple model of generalization in phonetic category learning , formalized in a hierarchical Bayesian framework ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["Recent work in computational psycholinguistics shows that morpheme lexica can be acquired in an unsupervised manner from a corpus of words by selecting the lexicon that best balances productivity and reuse ( e.g .", "In this paper , we extend such work to the problem of acquiring non-concatenative morphology , proposing a simple model of morphology that can handle both concatenative and non-concatenative morphology and applying Bayesian inference on two datasets of Arabic and English verbs to acquire lexica .", "We show that our approach successfully extracts the non-contiguous triliteral root from Arabic verb stems .", "( 2009 ) and others ) .", "Goldwater et al ."]}
{"orig_sents": ["1", "5", "2", "4", "0", "3"], "shuf_sents": ["We take these results to indicate some of the strengths and the limitations of word and lexical class n-gram models as candidate representations of speakers ?", "We use a set of enriched n-gram models to track grammaticality judgements for different sorts of passive sentences in English .", "We test our models on classification tasks for different kinds of passive sentences .", "grammatical knowledge .", "Our experiments indicate that our n-gram models achieve high accuracy in identifying ill-formed passives in which ill-formedness depends on local relations within the n-gram frame , but they are far less successful in detecting non-local relations that produce unacceptability in other types of passive construction .", "We construct these models by specifying scoring functions to map the log probabilities ( logprobs ) of an n-gram model for a test set of sentences onto scores which depend on properties of the string related to the parameters of the model ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["To eliminate possible confounds due to surface structure , this paper introduces a processing model based on deep syntactic dependencies .", "Results on eye-tracking data indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface embeddings .", "Reading experiments using naturalistic stimuli have shown unanticipated facilitations for completing center embeddings when frequency effects are factored out ."]}
{"orig_sents": ["0", "1", "3", "5", "4", "2", "6"], "shuf_sents": ["There are few computational models of second language acquisition ( SLA ) .", "At the same time , many questions in the field of SLA remain unanswered .", "Our simulations replicate the expected SLA patterns as well as the two effects .", "In particular , SLA patterns are difficult to study due to the large amount of variation between human learners .", "We use the model to study general developmental patterns of SLA and two specific effects sometimes found in empirical studies : construction priming and a facilitatory effect of skewed frequencies in the input .", "We present a computational model of second language construction learning that allows manipulating specific parameters such as age of onset and amount of exposure .", "Our model can be used in further studies of various SLA phenomena ."]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["PLTAG is a psycholinguistically-motivated formalism which extends the standard TAG operations with a prediction and verification mechanism and has experimental support as a model of syntactic processing difficulty .", "We focus on the problem of formally modelling semantic role prediction in the context of an incremental parse and describe a flexible neo-Davidsonian formalism and composition procedure to accompany a PLTAG parse .", "To this end , we also provide a means of augmenting the PLTAG lexicon with semantic annotation .", "We augment an existing TAG-based incremental syntactic formalism , PLTAG , with a semantic component designed to support the simultaneous modeling effects of thematic fit as well as syntactic and semantic predictions .", "To illustrate this , we run through an experimentally-relevant model case , wherein the resolution of semantic role ambiguities influences the resolution of syntactic ambiguities and vice versa ."]}
{"orig_sents": ["2", "1", "4", "0", "3"], "shuf_sents": ["To the best of our knowledge , this is the first systematic evaluation of a wide range of DSM parameters in all possible combinations .", "The tasks at issue are ( i ) identification of consistent primes based on their semantic relatedness to the target and ( ii ) correlation of semantic relatedness with latency times .", "This paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments .", "An important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming .", "We also provide an evaluation of the impact of specific model parameters on the prediction of priming ."]}
{"orig_sents": ["2", "1", "4", "0", "3"], "shuf_sents": ["In a second experiment , we show that the quality of the representations of abstract words in LDA models can be improved by supplementing the training data with information on the physical properties of concrete concepts .", "By implementing both a vector space model and a Latent Dirichlet Allocation ( LDA ) Model , we explore the extent to which concreteness is reflected in the distributional patterns in corpora .", "An increasing body of empirical evidence suggests that concreteness is a fundamental dimension of semantic representation .", "We conclude by discussing the implications for computational systems and also for how concrete and abstract concepts are represented in the mind", "In one experiment , we show that that vector space models can be tailored to better model semantic domains of particular degrees of concreteness ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["Using the Penn Discourse Treebank , we investigate what information relevant to inferring discourse relations is conveyed by discourse connectives , and whether the specificity of discourse relations reflects general cognitive biases for establishing coherence .", "We also propose an approach to measure the effect of a discourse marker on sense identification according to the different levels of a relation sense hierarchy .", "Discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text .", "This will open a way to the computational modeling of discourse processing ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Working in an inherently incremental framework , Dynamic Syntax , we show how words can be associated with probabilistic procedures for the incremental projection of meaning , providing a grammar which can be used directly in incremental probabilistic parsing and generation .", "We test this on child-directed utterances from the CHILDES corpus , and show that it results in good coverage and semantic accuracy , without requiring annotation at the word level or any independent notion of syntax .", "We describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning ."]}
{"orig_sents": ["0", "6", "3", "2", "5", "1", "4"], "shuf_sents": ["Cultural heritage collections usually organise sets of items into exhibitions or guided tours .", "In order to address this , we conducted preliminary investigations to test whether Wikipedia can be used to automatically add background text for sequences of items .", "In this paper we characterise and analyse paths of items created by users of our online system .", "The PATHS project brings the idea of guided tours to digital library collections where a tool to create virtual paths are used to assist with navigation and provide guides on particular subjects and topics .", "In the future we would like to explore the automatic creation of full paths .", "The analysis highlights that most users spend time selecting items relevant to their chosen topic , but few users took time to add background information to the paths .", "These items are often accompanied by text that describes the theme and topic of the exhibition and provides background context and details of connections with other items ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We demonstrate that by using character overlap the performance of the machine translation process can be improved for this task .", "Language transformation can be defined as translating between diachronically distinct language variants .", "We investigate the transformation of Middle Dutch into Modern Dutch by means of machine translation ."]}
{"orig_sents": ["2", "5", "3", "4", "0", "1"], "shuf_sents": ["An application of the matching procedure for comparison of family trees is discussed .", "A visualization tool is described to present an interactive overview of comparison results .", "Differences between large-scale historical population archives and small decentralized databases can be used to improve data quality and record connectedness in both types of databases .", "A matching procedure is described to discover records from different databases referring to the same historical event .", "The problem of verification without reliable benchmark data is addressed by matching on a subset of record attributes and measuring support for the match using a different subset of attributes .", "A parser is developed to account for differences in syntax and data representation models ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["Cross-period ( diachronic ) thesaurus construction aims to enable potential users to search for modern terms and obtain semantically related terms from earlier periods in history .", "This is a complex task not previously addressed computationally .", "We demonstrate the empirical benefit of our scheme for a Jewish crossperiod thesaurus and evaluate its impact on recall and on the effectiveness of lexicographer manual effort .", "In this paper we introduce a semi-automatic iterative Query Expansion ( QE ) scheme for supporting cross-period thesaurus construction ."]}
{"orig_sents": ["3", "2", "1", "4", "0"], "shuf_sents": ["The intent is that they develop , not only an informed view as to how the data could be fruitfully analysed , but also how feasible it is to analyse it in that way .", "The tool can be used to build classifier cascades that decomposes tweet streams , and provide analysis of targeted conversations .", "Our approach supports collaborative construction of classifiers and associated gold standard data sets .", "We present an extension of the DUALIST tool that enables social scientists to engage directly with large Twitter datasets .", "A central concern is to provide an environment in which social science researchers can rapidly develop an informed sense of what the datasets look like ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["In our paper , we present a computational morphology for Old and Middle Hungarian used in two research projects that aim at creating morphologically annotated corpora of Old and Middle Hungarian .", "In addition , we present the web-based disambiguation tool used in the semi-automatic disambiguation of the annotations and the structured corpus query tool that has a unique but very useful feature of making corrections to the annotation in the query results possible ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["The focus of this paper in this overall pipeline , is identifying arguments in text : we present and empirically evaluate the hypothesis that verbal tense and mood are good indicators of arguments that have not been explored in the relevant literature .", "In this paper we describe an application of language technology to policy formulation , where it can support policy makers assess the acceptance of a yet-unpublished policy before the policy enters public consultation .", "One of the key concepts is that instead of relying on thematic similarity , we extract arguments expressed in support or opposition of positions that are general statements that are , themselves , consistent with the policy or not ."]}
{"orig_sents": ["0", "4", "3", "6", "5", "1", "2"], "shuf_sents": ["We develop a pipeline consisting of various text processing tools which is designed to assist political scientists in finding specific , complex concepts within large amounts of text .", "It interfaces an active learning algorithm which is complemented by the NLP pipeline to provide a rich feature selection .", "Political scientists are thus enabled to use their own intuitions to find custom concepts .", "It is of particular importance to find a ? common language ?", "Our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneficial assistance for the political scientists and new application challenges for NLP .", "Therefore , we use an interactive web-interface which is easily usable by non-experts .", "between the different disciplines ."]}
{"orig_sents": ["5", "4", "7", "1", "6", "0", "3", "2"], "shuf_sents": ["Human annotators show moderate to substantial agreement in their judgment of keywords .", "We carry out a quantitative and qualitative analysis of the keywords in the collection .", "We conclude that this is a promising approach to automate this time intensive task .", "Finally , we evaluate a learning to rank approach to extract and rank keyword candidates .", "They can serve as a shallow document summary and enable more efficient retrieval and aggregation of information .", "Manually assigned keywords provide a valuable means for accessing large document collections .", "Up to 80 % of the assigned keywords ( or a minor variation ) appear in the text itself .", "In this paper we investigate keywords in the context of the Dutch Folktale Database , a large collection of stories including fairy tales , jokes and urban legends ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We propose to bring together two kinds of linguistic resources ? interlinear glossed text ( IGT ) and a language-independent precision grammar resource ? to automatically create precision grammars in the context of language documentation .", "This paper takes the first steps in that direction by extracting major-constituent word order and case system properties from IGT for a diverse sample of languages ."]}
{"orig_sents": ["3", "1", "2", "4", "5", "0"], "shuf_sents": ["A first evaluation shows that our approach compares well with state-of-art approaches .", "Thus , we present a comparable corpus of historical German recipes and show how such a comparable text collection together with the application of innovative MT inspired strategies allow us ( i ) to address the word form normalization problem and ( ii ) to automatically generate a diachronic dictionary of spelling variants .", "Such a diachronic dictionary can be used both for spelling normalization and for extracting new ? translation ?", "In this paper , we argue that comparable collections of historical written resources can help overcoming typical challenges posed by heritage texts enhancing spelling normalization , POS-tagging and subsequent diachronic linguistic analyses .", "( word formation/change ) rules for diachronic spelling variants .", "Moreover , our approach can be applied virtually to any diachronic collection of texts regardless of the time span they represent ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We describe the various steps that lead to a running IE system : lexicalization of the labels of the thesaurus and semi-automatic generation of domain specific IE grammars , with their subsequent implementation in a finite state engine .", "Finally , we outline the actual field of application of the IE system : analysis of social media for recognition of relevant topics in the context of elections .", "We present current work dealing with the integration of a multilingual thesaurus for social sciences in a NLP framework for supporting Knowledge-Driven Information Extraction in the field of social sciences ."]}
{"orig_sents": ["2", "5", "1", "4", "6", "3", "0"], "shuf_sents": ["We then use qualitative analysis grounded in translation theory and real example outputs in order to address what makes literary translation particularly hard and the potential role of the machine in it .", "However , it also introduces additional challenges .", "Applying machine translation ( MT ) to literary texts involves the same domain shift challenges that arise for any sublanguage ( e.g .", "We use existing MT systems to translate samples of French literature into English .", "One focus in the discussion of translation theory in the humanities has been on the human translator ? s role in staying faithful to an original text versus adapting it to make it more familiar to readers .", "medical or scientific ) .", "In contrast to other domains , one objective in literary translation is to preserve the experience of reading a text when moving to the target language ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["In this paper we look at a task at border of natural language processing , historical linguistics and the study of language development , namely that of identifying the time when a text was written .", "We find that lexical features are the most helpful .", "We use machine learning classification using lexical , word ending and dictionary-based features , with linear support vector machines and random forests ."]}
{"orig_sents": ["7", "3", "1", "2", "0", "6", "5", "4"], "shuf_sents": ["We extend prior work in two ways .", "This paper presents our effort to deal with these problems .", "We describe our experiences with processing museum data extracted from two distinct sources , harmonizing this data and making its content accessible in natural language .", "Previous work on multilingual access to cultural heritage information has shown that at least two different problems must be dealt with when mapping from ontologies to natural language : ( 1 ) mapping multilingual metadata to interoperable knowledge sources ; ( 2 ) assigning multilingual knowledge to cultural data .", "The generation and retrieval system builds on W3C standards and is available for further research .", "Second , we describe how this multilingual system is exploited to form queries using the standard query language SPARQL .", "First , we present a grammar-based system that is designed to generate coherent texts from Semantic Web ontologies in 15 languages .", "As the amount of cultural data available on the Semantic Web is expanding , the demand of accessing this data in multiple languages is increasing ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Aiming at bringing together researchers and practitioners from the different multidisciplinary areas working in these directions , as well as at creating a brainstorming and discussion venue for Hybrid Translation approaches , the HyTra initiative was born .", "Such combinations typically involve the hybridization of different paradigms such as , for instance , the introduction of linguistic knowledge into statistical paradigms , the incorporation of data-driven components into rulebased paradigms , or the pre- and postprocessing of either sort of translation system outputs .", "This paper gives an overview of the Second Workshop on Hybrid Approaches to Translation ( HyTra 2013 ) concerning its motivation , contents and outcomes .", "A current increasing trend in machine translation is to combine data-driven and rule-based techniques ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["We explore the selection of training data for language models using perplexity .", "Finally , a method which combines surface forms and the linguistically motivated methods outperforms the baseline in all the scenarios , selecting data whose perplexity is between 3.49 % and 8.17 % ( depending on the corpus and language ) lower than that of the baseline .", "In four out of the six scenarios a linguistically motivated method outperforms the purely statistical state-of-theart approach .", "We introduce three novel models that make use of linguistic information and evaluate them on three different corpora and two languages ."]}
{"orig_sents": ["0", "5", "2", "3", "1", "4"], "shuf_sents": ["We have implemented a rule-based prototype of a Spanish-to-Cuzco Quechua MT system enhanced through the addition of statistical components .", "As the form of the subordinated verb depends heavily on the conjunction in the subordinated Spanish clause and the semantics of the main verb , we extracted this information from two treebanks and trained different classifiers on this data .", "The prototype has several rules that decide which verb form should be used in a given context .", "However , matching the context in order to apply the correct rule depends crucially on the parsing quality of the Spanish input .", "We tested the best classifier on a set of 4 texts , increasing the correct subordinated verb forms from 80 % to 89 % .", "The greatest difficulty during the translation process is to generate the correct Quechua verb form in subordinated clauses ."]}
{"orig_sents": ["0", "1", "5", "4", "3", "2"], "shuf_sents": ["Dependency parsers are almost ubiquitously evaluated on their accuracy scores , these scores say nothing of the complexity and usefulness of the resulting structures .", "The structures may have more complexity due to their coordination structure or attachment rules .", "UAS scores are more correlated to the NIST evaluation metric than to the BLEU Metric , however we see increases in both metrics .", "We show that parsers ?", "We show results from 7 individual parsers , including dependency and constituent parsers , and 3 ensemble parsing techniques with their overall effect on a Machine Translation system , Treex , for English to Czech translation .", "As dependency parses are basic structures in which other systems are built upon , it would seem more reasonable to judge these parsers down the NLP pipeline ."]}
{"orig_sents": ["2", "1", "3", "4", "0"], "shuf_sents": ["We show significant improvements in translation quality of sentences from news domain , when compared to state-of-the-art reordering methods .", "Reordering methods are effective , but need reliable parsers to extract the syntactic structure of the source sentences .", "Chinese and Japanese have a different sentence structure .", "However , Chinese has a loose word order , and Chinese parsers that extract the phrase structure do not perform well .", "We propose a framework where only POS tags and unlabeled dependency parse trees are necessary , and linguistic knowledge on structural difference can be encoded in the form of reordering rules ."]}
{"orig_sents": ["2", "0", "4", "5", "6", "1", "3"], "shuf_sents": ["We are proposing a rich set of rules for better reordering .", "We have used BLEU , NIST , multi-reference word error rate , multi-reference position independent error rate for judging the improvements .", "Reordering is pre-processing stage for Statistical Machine Translation ( SMT ) system where the words of the source sentence are reordered as per the syntax of the target language .", "We have exploited open source SMT toolkit MOSES to develop the system .", "The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase based SMT system .", "Reordering also helps the decoding process and hence improving the machine translation quality .", "We have observed significant improvements in the translation quality by using our approach over the baseline SMT ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Although automatic evaluation scores do not reliably reflect the improvement in all cases , human evaluation of our systems shows that readability and accuracy of the translations were improved both by reordering and applying richer models .", "However , if the languages are more distant from a grammatical point of view , the quality of translations is much behind the expectations , since the baseline translation system can not cope with long distance reordering of words and the mapping of word internal grammatical structures .", "In our paper , we present a method that tries to overcome these problems in the case of English-Hungarian translation by applying reordering rules prior to the translation process and by creating morpheme-based and factored models .", "Phrase-based statistical machine translation systems can generate translations of reasonable quality in the case of language pairs with similar structure and word order ."]}
{"orig_sents": ["0", "4", "6", "8", "3", "5", "1", "2", "7"], "shuf_sents": ["We explore the intersection of rule-based and statistical approaches in machine translation , with a particular focus on past and current work here at Microsoft Research .", "We describe some of our statistical and non-statistical attempts to incorporate linguistic insights into machine translation systems , showing what is currently working well , and what isn ? t .", "We also look at trade-offs in using linguistic knowledge ( ? rules ? )", "This led to huge improvements in translation quality as more and more data was consumed .", "Until about ten years ago , the only machine translation systems worth using were rule-based and linguistically-informed .", "By necessity , the pendulum is swinging towards the inclusion of linguistic features in MT systems .", "Along came statistical approaches , which use large corpora to directly guide translations toward expressions people would actually say .", "in pre- or post-processing by language pair , with a particular eye on the return on investment as training data increases in size .", "Rather than making local decisions when writing and conditioning rules , goodness of translation was modeled numerically and free parameters were selected to optimize that goodness ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["The model introduces a novel strategy for avoiding the pitfalls of premature pruning in chunking approaches , by incrementally splitting an ITGwhile using a second ITG to guide this search .", "In comparison to most current SMT approaches , the model learns a very parsimonious phrase translation lexicons that provide an obvious basis for generalization to abstract translation schemas .", "To do this , the model maintains internal consistency by avoiding use of mismatched or unrelated models , such as word alignments or probabilities from IBM models .", "We present a minimalist , unsupervised learning model that induces relatively clean phrasal inversion transduction grammars by employing the minimum description length principle to drive search over a space defined by two opposing extreme types of ITGs ."]}
{"orig_sents": ["0", "7", "8", "4", "2", "3", "5", "6", "1"], "shuf_sents": ["This paper presents a hybrid approach to the enhancement of English to Arabic statistical machine translation quality .", "We propose the scheme for recombining the segmented Arabic word , and describe their effect on translation .", "In this paper , to overcome these shortcomings , we describe our hybrid approach which integrates knowledge of the Arabic language into statistical machine translation .", "In this framework , we propose the use of a featured language model SFLM ( Sma ? li et al. , 2004 ) to be able to integrate syntactic and grammatical knowledge about each word .", "Statistical machine translation ( SMT ) engines often show poor syntax processing especially when the language used is morphologically rich such as Arabic .", "In this paper , we first discuss some challenges in translating from English to Arabic and we explore various techniques to improve performance on this task .", "We apply a morphological segmentation step for Arabic words and we present our hybrid approach by identifying morpho-syntactic class of each segmented word to build up our statistical feature language model .", "Machine Translation has been defined as the process that utilizes computer software to translate text from one natural language to another .", "Arabic , as a morphologically rich language , is a highly flexional language , in that the same root can lead to various forms according to its context ."]}
{"orig_sents": ["5", "2", "4", "1", "0", "3"], "shuf_sents": ["Since the proposed approach reduces the size of the phrase table , multi-tables are considered .", "Our experiments on Korean-Chinese show that our methods can improve the alignment and translation results .", "We utilize different types of POS tag to restructure source sentences and use an alignment-based reordering method to improve the alignment .", "The combination of all these methods together would get the best translation result .", "After applying the reordering method , we use two phrase tables in the decoding part to keep the translation performance .", "This paper presents the methods which are based on the part-of-speech ( POS ) and auto alignment information to improve the quality of machine translation result and the word alignment ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["Since the Tunisian revolution , Tunisian Dialect ( TD ) used in daily life , has became progressively used and represented in interviews , news and debate programs instead of Modern Standard Arabic ( MSA ) .", "This situation has important negative consequences for natural language processing ( NLP ) : since the spoken dialects are not officially written and do not have standard orthography , it is very costly to obtain adequate corpora to use for training NLP tools .", "So , we use explicit knowledge about the relation between TD and MSA .", "Furthermore , there are almost no parallel corpora involving TD and MSA .", "In this paper , we describe the creation of Tunisian dialect text corpus as well as a method for building a bilingual dictionary , in order to create language model for speech recognition system for the Tunisian Broadcast News ."]}
{"orig_sents": ["2", "1", "7", "5", "6", "0", "4", "3"], "shuf_sents": ["Chunks are aligned employing a bootstrapping approach by translating the source chunks into the target language using a baseline PB-SMT system and subsequently validating the target chunks using a fuzzy matching technique against the target corpus .", "The proposed hybrid alignment model provides most informative alignment links which are offered by both unsupervised and semi-supervised word alignment models .", "This paper proposes a hybrid word alignment model for Phrase-Based Statistical Machine translation ( PB-SMT ) .", "Our best system provided significant improvements over the baseline as measured by BLEU .", "All the experiments are carried out after single-tokenizing the multi-word NEs .", "The rule based aligner only aligns named entities ( NEs ) and chunks .", "The NEs are aligned through transliteration using a joint source-channel model .", "Two unsupervised word alignment models ( GIZA++ and Berkeley aligner ) and a rule based aligner are combined together ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We present initial results showing successful use of this system both in translating English to Spanish and Spanish to Guarani .", "We present initial work on an inexpensive approach for building largevocabulary lexical selection modules for hybrid RBMT systems by framing lexical selection as a sequence labeling problem .", "We submit that Maximum Entropy Markov Models ( MEMMs ) are a sensible formalism for this problem , due to their ability to take into account many features of the source text , and show how we can build a combination MEMM/HMM system that allows MT system implementors flexibility regarding which words have their lexical choices modeled with classifiers ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["With automatic evaluation , the weighted graph method yields an improvement of about +0.63 BLEU points , while the rulebased method scores about the same as the baseline .", "The first is based on hand-coded rules ; the second on weighted graphs derived from a large-scale pronunciation resource , with weights trained from a small bicorpus of domain language .", "On contrastive manual evaluation , both methods give highly significant improvements ( p < 0.0001 ) and score about equally when compared against each other .", "In the context of a hybrid French-toEnglish SMT system for translating online forum posts , we present two methods for addressing the common problem of homophone confusions in colloquial written language ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["This paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results , and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline MT system .", "Resource limitation is challenging for crossdomain adaption .", "The adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful , and the effect of using the selected pseudo bilingual corpus is significant ."]}
{"orig_sents": ["5", "3", "1", "0", "4", "2"], "shuf_sents": ["The article summarises implementation decisions , using the Greek-English language pair as a test case .", "Given the limited availability of resources for many languages , only a very small bilingual corpus is required , while language modelling is performed by sampling a large target language ( TL ) monolingual corpus .", "Finally , main error sources are identified and directions are described to improve this hybrid MT methodology .", "This methodology has been designed to facilitate rapid creation of MT systems for unconstrained language pairs , setting the lowest possible requirements on specialised resources and tools .", "Evaluation results are reported , for both objective and subjective metrics .", "The present article provides a comprehensive review of the work carried out on developing PRESEMT , a hybrid language-independent machine translation ( MT ) methodology ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["To improve the STSG model specificity we utilize a multi-level backoff model with additional syntactic annotations that allow for better discrimination over previous STSG formulations .", "We compare our approach to T3 ( Cohn and Lapata , 2009 ) , a recent STSG implementation , as well as two state-of-the-art phrase-based sentence simplifiers on a corpus of aligned sentences from English and Simple English Wikipedia .", "In this paper , we introduce a syntax-based sentence simplifier that models simplification using a probabilistic synchronous tree substitution grammar ( STSG ) .", "Our new approach performs significantly better than T3 , similarly to human simplifications for both simplicity and fluency , and better than the phrasebased simplifiers for most of the evaluation metrics ."]}
{"orig_sents": ["2", "1", "3", "4", "0"], "shuf_sents": ["We show the limits of the algorithm with respect to the language and domain of our data and suggest ways of circumventing them .", "We require parallel data to build a statistical machine translation ( SMT ) system that translates from German into Simple German .", "In this paper we report our experiments in creating a parallel corpus using German/Simple German documents from the web .", "Parallel data for SMT systems needs to be aligned at the sentence level .", "We applied an existing monolingual sentence alignment algorithm ."]}
{"orig_sents": ["5", "4", "1", "3", "0", "2"], "shuf_sents": ["We compare Support Vector Machines and the Margin-Infused Relaxed Algorithm measured by mean squared error .", "We demonstrate that reading level assessment is a discriminative problem that is best-suited for regression .", "We provide an analysis of which features are most predictive of a given level .", "Our baseline uses z-normalized shallow length features and TF-LOG weighted vectors on bag-of-words for Arabic , Dari , English , and Pashto .", "420 , Room 119 Monterey , CA 93944 , USAtamas.g.marius.civ @ mail.milAbstract In this paper , we introduce a new baseline for language-independent text difficulty assessment applied to the Interagency Language Roundtable ( ILR ) proficiency scale .", "ign Language Center , Bldg ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["The paper discusses the main issues regarding the reading skills and comprehension proficiency in written Bulgarian of people with communication difficulties , and deaf people , in particular .", "We consider several key components of text comprehension which pose a challenge for deaf readers and propose a rule-based system for automatic modification of Bulgarian texts intended to facilitate comprehension by deaf people , to assist education , etc .", "In order to demonstrate the benefits of such a system and to evaluate its performance , we have carried out a study among a group of deaf people who use Bulgarian Sign Language ( BulSL ) as their primary language ( primary BulSL users ) , which compares the comprehensibility of original texts and their modified versions .", "The results shows a considerable improvement in readability when using modified texts , but at the same time demonstrates that the level of comprehension is still low , and that a complex set of modifications will have to be implemented to attain satisfactory results ."]}
{"orig_sents": ["2", "0", "4", "3", "1"], "shuf_sents": ["In this research , we attempt to improve the readability of text by optimizing comma placements through integration of linguistic features of text and gaze features of readers .", "The experimental results show that our predictor reproduces the comma distribution in the Penn Chinese Treebank with 78.41 in F1-score and commas chosen by our filter smoothen certain gaze behaviors .", "Comma placements in Chinese text are relatively arbitrary although there are some syntactic guidelines for them .", "After that , we build a rule-based filter for categorizing commas in text according to their contribution to readability based on the analysis of gazes of people reading text with and without commas .", "We design a comma predictor for general Chinese text based on conditional random field models with linguistic features ."]}
{"orig_sents": ["3", "5", "1", "4", "6", "2", "0"], "shuf_sents": ["Finally , we report on generalization experiments showing that the features we used generalize well across different web sources .", "But how do the readability models and the features they are based on perform on real-world web texts ?", "At the same time , documents at a wide range of reading levels are identified and even among the Top-10 search results one finds documents at the lower levels , supporting the potential usefulness of readability ranking for the web .", "An increasing range of features is being used for automatic readability classification .", "In this paper , we want to take a step towards understanding this aspect on the basis of a broad range of lexical and syntactic features and several web datasets we collected .", "The impact of the features typically is evaluated using reference corpora containing graded reading material .", "Applying our models to web search results , we find that the average reading level of the retrieved web documents is relatively high ."]}
{"orig_sents": ["1", "5", "0", "3", "4", "2"], "shuf_sents": ["This paper addresses these shortcomings with a new corpus for evaluating a system ? s performance in identifying CWs .", "The task of identifying complex words ( CWs ) is important for lexical simplification , however it is often carried out with no evaluation of success .", "This paper describes the method used to produce the CW corpus and presents the results of evaluation , showing its validity .", "Simple Wikipedia edit histories were mined for instances of single word lexical simplifications .", "The corpus contains 731 sentences , each with one annotated CW .", "There is no basis for comparison of current techniques and , prior to this work , there has been no standard corpus or evaluation technique for the CW identification task ."]}
{"orig_sents": ["0", "4", "3", "1", "2"], "shuf_sents": ["In this paper we report the results of a pilot study of basing readability prediction on training data annotated with reading time .", "We create and evaluate a predictor using the binary classification problem ; the predictor identifies the better of two documents correctly with 68.55 % accuracy .", "We also report a comparison of predictors based on reading time and on readability scores .", "Instead of the subjective assessments of complexity , we use the more objective measure of reading time .", "Although reading time is known to be a good metric for predicting readability , previous work has mainly focused on annotating the training data with subjective readability scores usually on a 1 to 5 scale ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["We present three ways of inducing probability distributions on derivation trees produced by Minimalist Grammars , and give their maximum likelihood estimators .", "We argue that a parameterization based on locally normalized log-linear models balances competing requirements for modeling expressiveness and computational tractability ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["By imposing a partial order on the categories , the Adjoin operation can require that higher adjuncts precede lower adjuncts , but not vice versa , deriving order .", "In Minimalist Grammars ( MGs ) , it is straightforward to account for optionality or ordering , but not both .", "Adjuncts are characteristically optional , but many , such as adverbs and adjectives , are strictly ordered .", "I present an extension of MGs , MGs with Adjunction , which accounts for optionality and ordering simply by keeping track of two pieces of information at once : the original category of the adjoined-to phrase , and the category of the adjunct most recently adjoined ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["We study the complexity of uniform membership for Linear Context-Free Rewriting Systems , i.e. , the problem where we are given a string w and a grammar G and are asked whether w ?", "L ( G ) .", "In particular , we use parameterized complexity theory to investigate how the complexity depends on various parameters .", "While we focus primarily on rank and fan-out , derivation length is also considered ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["The strings have bounded but refinable granularity , suitable for analyzing ( im ) perfectivity , durativity , telicity , and various relations including branching .", "Timelines interpreting interval temporal logic formulas are segmented into strings which serve as semantic representations for tense and aspect ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns , including passing information between the relative clause and the modified noun phrase , as well as copying , combining , and discarding parts of the relative clause .", "This paper develops a compositional vector-based semantics of relative pronouns within a categorical framework .", "We develop two instantiations of the abstract semantics , one based on a truth-theoretic approach and one based on corpus statistics ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["vowel harmony patterns are studied in the context of subclasses of regular functions .", "Attested and ? pathological ?", "The analysis suggests that the computational complexity of phonology can be reduced from regular to weakly deterministic ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Second , these representations ( including the exponentially smaller ones ) describe actual formal languages which successfully model natural language phenomenon , notably in the subfield of phonology .", "This paper shows how factored finitestate representations of subregular language classes are identifiable in the limit from positive data by learners which are polytime iterative and optimal .", "These representations are motivated in two ways .", "First , the size of this representation for a given regular language can be exponentially smaller than the size of the minimal deterministic acceptor recognizing the language ."]}
{"orig_sents": ["4", "1", "2", "0", "3"], "shuf_sents": ["Specifically , we find that the low entropy of natural languages can allow us , with high probability , to bound the depth of the heuristic values expanded in the search .", "algorithm proposed in Corlett and Penn ( 2010 ) for deciphering letter-substitution ciphers .", "We argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language , and we develop a principled way of incorporating entropy into our complexity analysis .", "This leads to a novel probabilistic bound on search depth in these tasks .", "In this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the A ?"]}
{"orig_sents": ["5", "2", "6", "0", "4", "1", "3"], "shuf_sents": ["It can be shown that phrase pairs of interest to SMT form a sigma-algebra generated by components of such graphs .", "A by-product of this model is a derivation of probability mass functions for random partitions .", "However , no attention has been drawn to why this method is successful , other than empirical evidence .", "These are realized as cases of constrained , biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence .", "This construction is generalized by allowing segmented sentence pairs , which in turn gives rise to a phrase-based generative model .", "The consistency method has been established as the standard strategy for extracting high quality translation rules in statistical machine translation ( SMT ) .", "Using concepts from graph theory , we identify the relation between consistency and components of graphs that represent word-aligned sentence pairs ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["This document overviews the strategy , effort and aftermath of the MultiLing 2013 multilingual summarization data collection .", "We discuss the rationale behind the main decisions of the collection , the methodology used to generate the multilingual corpus , as well as challenges and problems faced per language .", "A second part , covering the remaining languages , is available as a distinct paper in the MultiLing 2013 proceedings .", "This paper overviews the work on Arabic , Chinese , English , Greek , and Romanian languages .", "We describe how the Data Contributors of MultiLing collected and generated a multilingual multi-document summarization corpus on 10 different languages : Arabic , Chinese , Czech , English , French , Greek , Hebrew , Hindi , Romanian and Spanish ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["This document overviews the strategy , effort and aftermath of the MultiLing 2013 multilingual summarization data collection .", "We describe how the Data Contributors of MultiLing collected and generated a multilingual multi-document summarization corpus on 10 different languages : Arabic , Chinese , Czech , English , French , Greek , Hebrew , Hindi , Romanian and Spanish .", "This paper overviews the work on Czech , Hebrew and Spanish languages .", "We discuss the rationale behind the main decisions of the collection , the methodology used to generate the multilingual corpus , as well as challenges and problems faced per language ."]}
{"orig_sents": ["4", "6", "7", "5", "2", "1", "3", "0"], "shuf_sents": ["This paper provides a brief description related to the data of both tasks , the evaluation methodology , as well as an overview of participation and corresponding results .", "An automatic evaluation task was also added to this year ? s set of tasks .", "The participating systems submitted over 15 runs , some providing summaries across all languages .", "The evaluation task meant to determine whether automatic measures of evaluation can function well in the multi-lingual domain .", "The MultiLing 2013 Workshop of ACL 2013 posed a multi-lingual , multidocument summarization task to the summarization community , aiming to quantify and measure the performance of multi-lingual , multi-document summarization systems across languages .", "The evaluation of the summaries was performed using automatic and manual processes .", "The task was to create a 240 ? 250 word summary from 10 news articles , describing a given topic .", "The texts of each topic were provided in 10 languages ( Arabic , Chinese , Czech , English , French , Greek , Hebrew , Hindi , Romanian , Spanish ) and each participant generated summaries for at least 2 languages ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["This report describes the pilot task , the dataset , the methods used to evaluate the submitted summaries , and the overall performance of each participant ? s system .", "The 2013 Association for Computational Linguistics MultiLing Pilot posed a task to measure the performance of multilingual , single-document , summarization systems using a dataset derived from many Wikipedias .", "The objective of the pilot was to assess automatic summarization of multilingual text documents outside the news domain and the potential of using Wikipedia articles for such research ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["This report provides a description of the methods applied in CIST system participating ACL MultiLing 2013 .", "hLDA topic model is adopted for multilingual multi-document modeling .", "Summarization is based on sentence extraction .", "Various features are combined to evaluate and extract candidate summary sentences ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["In this paper we present a linear model for the problem of text summarization , where a summary preserves the information coverage as much as possible in comparison to the original document set .", "We reduce the problem of finding the best summary to the problem of finding the point on a convex polytope closest to the given hyperplane , and solve it efficiently with the help of fractional linear programming .", "We supply here an overview of our system , titled POLY2 , that participated in the MultiLing contest at ACL 2013 ."]}
{"orig_sents": ["3", "6", "4", "8", "1", "2", "5", "7", "0"], "shuf_sents": ["The summariser produced best summaries in 6 from 10 considered languages according to the ROUGE-2 metric .", "Its results on the Multiling-2011 corpus were promising .", "The generated summaries were ranked first in several languages based on various metrics .", "The paper describes our participation in the Multi-document summarization task of Multiling-2013 .", "This year the corpus was extended by new three languages and another five topics , covering in total 15 topics in 10 languages .", "The summariser with minor changes was run on the updated 2013 corpus .", "The community initiative was born as a pilot task for the Text Analysis Conference in 2011 .", "Although we do not have the manual evaluation results yet the ROUGE-2 score indicates good results again .", "Our summariser is based on latent semantic analysis and it is in principle language independent ."]}
{"orig_sents": ["1", "4", "0", "3", "2", "6", "5"], "shuf_sents": ["The summarisers used word frequency lists and log likelihood calculations to generate single and multi document summaries .", "In this paper we show the results of our participation in the MultiLing 2013 summarisation tasks .", "We compare our results to other systems that participated in the same tracks on both Arabic and English languages .", "The single and multi summaries generated by our systems were evaluated by Arabic and English native speaker participants and by different automatic evaluation metrics , ROUGE , AutoSummENG , MeMoG and NPowER .", "We participated with single-document and multi-document corpus-based summarisers for both Arabic and English languages .", "Our Arabic multi-document summariser performed well in the human evaluation ranking second .", "Our single-document summarisers performed particularly well in the automatic evaluation with our English singledocument summariser performing better on average than the results of the other participants ."]}
{"orig_sents": ["3", "2", "0", "1", "5", "4"], "shuf_sents": ["The architecture includes language independent text processing modules , but also modules that are adapted for one language or another .", "In our experiments , the languages under consideration are Bulgarian , German , Greek , English , and Romanian .", "2013 .", "This paper describes the architecture of UAIC1 ? s Summarization system participating at MultiLing ?", "The output of the parsing process is used to extract general summaries .", "Our method exploits the cohesion and coherence properties of texts to build discourse structures ."]}
{"orig_sents": ["3", "6", "4", "0", "2", "1", "5"], "shuf_sents": ["Furthermore , this correspondence permits nonuniform semantic representations and more expressive composition operations than previous work .", "In these formulas , the meanings of words are represented by parameters that can be trained in a task-specific fashion .", "VSSP builds a CCG semantic parser respecting this correspondence ; this semantic parser parses text into lambda calculus formulas that evaluate to vector space representations .", "We present vector space semantic parsing ( VSSP ) , a framework for learning compositional models of vector space semantics .", "The complete correspondence is a direct consequence of minimal assumptions about the semantic representations of basic syntactic categories ( e.g. , nouns are vectors ) , and CCG ? s tight coupling of syntax and semantics .", "We present experiments using noun-verbnoun and adverb-adjective-noun phrases which demonstrate that VSSP can learn composition operations that RNN ( Socher et al , 2011 ) and MV-RNN ( Socher et al , 2012 ) can not .", "Our framework uses Combinatory Categorial Grammar ( CCG ) to define a correspondence between syntactic categories and semantic representations , which are vectors and functions on vectors ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["Tested on the WSJ23 , our method achieved a statistically significant improvement of 0.20 % on F-score ( 2 % error reduction ) and 0.95 % on exact match , compared with the state-ofthe-art Berkeley parser .", "This result shows that vector-based compositional semantics can be usefully applied in syntactic parsing , and demonstrates the benefits of combining the symbolic and connectionist approaches .", "In this paper , we address the problem of how to use semantics to improve syntactic parsing , by using a hybrid reranking method : a k-best list generated by a symbolic parser is reranked based on parsecorrectness scores given by a compositional , connectionist classifier .", "This classifier uses a recursive neural network to construct vector representations for phrases in a candidate parse tree in order to classify it as syntactically correct or not ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["In particular , we demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches for composing distributional semantic representations on an artificial task of verb sense disambiguation and a real-world application of judging event coreference .", "SDSM represents meaning as relation specific distributions over syntactic neighborhoods .", "In this paper we present a novel approach ( SDSM ) that incorporates structure in distributional semantics .", "We empirically show that the model can effectively represent the semantics of single words and provides significant advantages when dealing with phrasal units that involve word composition ."]}
{"orig_sents": ["0", "4", "3", "6", "1", "2", "5"], "shuf_sents": ["We present a letter-based encoding for words in continuous space language models .", "We show their influence in the task of machine translation using continuous space language models based on restricted Boltzmann machines .", "We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German .", "This way , similar words will automatically have a similar representation .", "We represent the words completely by letter n-grams instead of using the word index .", "Using our new approach a gain in BLEU score by up to 0.4 points can be achieved .", "With this we hope to better generalize to unknown or rare words and to also capture morphological information ."]}
{"orig_sents": ["0", "4", "1", "2", "3"], "shuf_sents": ["Classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors .", "( DRP ) .", "The core of the idea is straightforward : DRPs directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations .", "Results show that DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information .", "In this paper , we explore an alternative path to use syntax in feature spaces : the Distributed Representation ? Parsers ?"]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We present an evaluation of alternative cDSMs under truly comparable conditions .", "The linguistically motivated functional model of Baroni and Zamparelli ( 2010 ) and Coecke et al ( 2010 ) emerges as the winner in all our tests .", "In particular , we extend the idea of Baroni and Zamparelli ( 2010 ) and Guevara ( 2010 ) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature , so that all models can be tested under the same training conditions .", "In recent years , there has been widespread interest in compositional distributional semantic models ( cDSMs ) , that derive meaning representations for phrases from their parts ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We use the embedding directly to investigate sets of antonymic pairs , and indirectly to argue that function application in CVSMs requires not just vectors but two transformations ( corresponding to subject and object ) as well .", "We introduce a new 50-dimensional embedding obtained by spectral clustering of a graph describing the conceptual structure of the lexicon ."]}
{"orig_sents": ["0", "4", "2", "1", "5", "3"], "shuf_sents": ["This paper presents a comparative study of 5 different types of Word Space Models ( WSMs ) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions .", "the meaning of an expression is determined by the meaning of its constituents and their combination ?", "The study follows Biemann and Giesbrecht ( 2011 ) who attempted to find a list of expressions for which the compositionality assumption ?", "Our results are very promising and can be appreciated by those interested in WSMs , compositionality , and/or relevant evaluation methods .", "Many combinations of WSMs and measures have never been applied to the task before .", "does not hold ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In this paper , we address shortcomings in the ability of current models to capture logical operations such as negation .", "As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models .", "With the increasing empirical success of distributional models of compositional semantics , it is timely to consider the types of textual logic that such models are capable of capturing ."]}
{"orig_sents": ["3", "4", "1", "2", "0", "5"], "shuf_sents": ["To evaluate the method , a number of measures for word similarity are proposed , both contextual and non-contextual , including new measures based on optimal alignment of word senses .", "Storing and clustering context vectors can be expensive though .", "As an alternative , we introduce Multi-Sense Random Indexing , which performs on-the-fly ( incremental ) clustering .", "Most distributional models of word similarity represent a word type by a single vector of contextual features , even though , words commonly have more than one sense .", "The multiple senses can be captured by employing several vectors per word in a multi-prototype distributional model , prototypes that can be obtained by first constructing all the context vectors for the word and then clustering similar vectors to create sense vectors .", "Experimental results on the task of predicting semantic textual similarity do , however , not show a systematic difference between singleprototype and multi-prototype models ."]}
{"orig_sents": ["2", "1", "3", "0"], "shuf_sents": ["The model ? s estimation of the similarity of these pairs correlates well with human annotations , demonstrating a substantial improvement over several existing compositional approaches in both settings .", "This model reformulates earlier tensor-based approaches to vector space semantics as a top-down process , and provides efficient algorithms for transformation from natural language to vectors and from vectors to natural language .", "We present a novel compositional , generative model for vector space representations of meaning .", "We describe procedures for estimating the parameters of the model from positive examples of similar phrases , and from distributional representations , then use these procedures to obtain similarity judgments for a set of adjective-noun pairs ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["While words in documents are generally treated as discrete entities , they can be embedded in a Euclidean space which reflects an a priori notion of similarity between them .", "We report retrieval and clustering experiments in the case where the word-embeddings are computed from standard topic models showing significant improvements with respect to the original topic models .", "It consists in non-linearly mapping the wordembeddings in a higher-dimensional space and in aggregating them into a documentlevel representation .", "In such a case , a text document can be viewed as a bag-ofembedded-words ( BoEW ) : a set of realvalued vectors .", "We propose a novel document representation based on such continuous word embeddings ."]}
{"orig_sents": ["4", "2", "1", "6", "0", "3", "5"], "shuf_sents": ["A novel co-training task for the RNN , on subtree recognition , boosts performance , along with a scheme to consistently handle words that are not well-represented in the language model .", "In contrast to prior work , we fix neither the types of the questions nor the forms of the answers ; the system classifies tokens to match a substring chosen by the question ? s author .", "The RNN defines feature representations at every node of the parse trees of questions and supporting sentences , when applied recursively , starting with token vectors from a neural probabilistic language model .", "On our data set , we surpass an open source system epitomizing a classic ? pattern bootstrapping ?", "We develop a recursive neural network ( RNN ) to extract answers to arbitrary natural language questions from supporting sentences , by training on a crowdsourced data set ( to be released upon presentation ) .", "approach to question answering .", "Our classifier decides to follow each parse tree node of a support sentence or not , by classifying its RNN embedding together with those of its siblings and the root node of the question , until reaching the tokens it selects as the answer ."]}
{"orig_sents": ["5", "4", "3", "0", "1", "6", "2"], "shuf_sents": ["The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network .", "The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker .", "Without feature engineering or pretraining and with simple greedy decoding , the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment .", "We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality .", "Just as words combine to form the meaning of sentences , so do sentences combine to form the meaning of paragraphs , dialogues and general discourse .", "The compositionality of meaning extends beyond the single sentence .", "The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers ."]}
{"orig_sents": ["4", "1", "2", "0", "5", "3"], "shuf_sents": ["We analyzed the agreement for annotators using a state-ofthe-art segmentation similarity algorithm and compared annotations with a random baseline .", "In our annotation task , we asked speakers of English and Chinese to mark boundaries where they could construct the maximal concept using minimal words .", "We compared English data across genres ( news , literature , and policy ) .", "Our analysis includes an examination of phrase structure for annotated units using constituency parses .", "e for Infocomm Research , 1 Fusionpolis Way , Singaporejaw97 @ georgetown.edu { rembanchs , hli } @ i2r.a-star.edu.sgAbstract We present a new approach to dialogue processing in terms of ? meaning units ? .", "We found that annotators are able to identify meaning units systematically , even though they may disagree on the quantity and position of units ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["However , little has been written on the subject of where and when to encourage consistency .", "A number of approaches have been taken to improve lexical consistency in Statistical Machine Translation .", "I present an analysis of human authored translations , focussing on words belonging to different parts-of-speech across a number of different genres ."]}
{"orig_sents": ["0", "4", "3", "2", "1"], "shuf_sents": ["Explicit discourse connectives in a source language text are not always translated to comparable words or phrases in the target language .", "Work in progress aims to capture this natural implicitation of discourse connectives in current statistical machine translation models .", "In machine translation , this happens much less frequently ( up to 8 % only ) .", "Results show that discourse connectives are not translated into comparable forms ( or even any form at all ) , in up to 18 % of human reference translations from English to French or German .", "The paper provides a corpus analysis and a method for semi-automatic detection of such cases ."]}
{"orig_sents": ["1", "5", "2", "3", "4", "0"], "shuf_sents": ["While the phenomenon of the loss of associative texture has been theoretically predicted by translation scholars , we present a measure capable of quantifying the extent of this phenomenon .", "We present a suggestive finding regarding the loss of associative texture in the process of machine translation , using comparisons between ( a ) original and backtranslated texts , ( b ) reference and system translations , and ( c ) better and worse MT systems .", "a distribution of pointwise mutual information between all pairs of content word types in a text .", "We use the average of the distribution , which we term lexical tightness , as a single measure of the amount of association in a text .", "We show that the lexical tightness of humancomposed texts is higher than that of the machine translated materials ; human references are tighter than machine translations , and better MT systems produce lexically tighter translations .", "We represent the amount of association in a text using word association profile ?"]}
{"orig_sents": ["0", "4", "3", "2", "1"], "shuf_sents": ["The correct translation of verb tenses ensures that the temporal ordering of events in the source text is maintained in the target text .", "More importantly , manual evaluation shows that verb tense translation and verb choice are improved by respectively 9.7 % and 3.4 % ( absolute ) , leading to an overall improvement of verb translation of 17 % ( relative ) .", "The narrativity feature improves SMT by about 0.2 BLEU points when a factored SMT system is trained and tested on automatically labeled English-French data .", "The narrativity feature , which helps deciding which of the French past tenses is a correct translation of the English Simple Past , can be assigned with about 70 % accuracy ( F1 ) .", "This paper assesses the utility of automatically labeling English Simple Past verbs with a binary discursive feature , narrative vs. non-narrative , for statistical machine translation ( SMT ) into French ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["The paper presents machine translation experiments from English to Czech with a large amount of manually annotated discourse connectives .", "Error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful .", "The gold-standard discourse relation annotation leads to better translation performance in ranges of 4 ? 60 % for some ambiguous English connectives and helps to find correct syntactical constructs in Czech for less ambiguous connectives .", "Automatic scoring confirms the stability of the newly built discourseaware translation systems ."]}
{"orig_sents": ["3", "4", "1", "0", "2"], "shuf_sents": ["Features in the model take advantage of rich syntactic annotation TectoMT is based on , external tools for anaphoricity resolution , lexical co-occurrence frequencies measured on a large parallel corpus and gold coreference annotation .", "Armed with these observations , we design a discriminative translation model of it , which is then integrated into the TectoMT deep syntax MT framework .", "Even though the new model for it exhibits no improvement in terms of BLEU , manual evaluation shows that it outperforms the original solution in 8.5 % sentences containing it .", "We present a novel approach to the translation of the English personal pronoun it to Czech .", "We conduct a linguistic analysis on how the distinct categories of it are usually mapped to their Czech counterparts ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["We show experimentally that we can get competitive and relatively stable results when using a standard set of features , and that this framework also allows us to optimize documentlevel features , which can be used to model discourse phenomena .", "We extend the framework of sentence-level feature weight optimization to the document-level .", "This is an essential task for enabling future development of discourse-level statistical machine translation , as it allows easy integration of discourse features in the decoding process .", "We present an approach to feature weight optimization for document-level decoding ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["? a bilingual problem where typically one of the languages is unknown , and the other is the native language of the person solving the problem .", "This paper describes the process of composing problems that are suitable for competitions in linguistics .", "The process includes selecting phenomena , composing and arranging the data and assignments in order to illustrate the phenomena , and verifying the solvability and complexity of the problem .", "The type of problems described is ? Rosetta Stone ?"]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["We argue that corpus-based problems differ from traditional linguistic problems because they make it possible to represent language variation .", "Furthermore , they often require basic statistical thinking from the students .", "The paper is focused on self-contained linguistic problems based on text corpora .", "The practical value of using data obtained from text corpora for teaching linguistics through linguistic problems is shown ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Although deemed most desirable , the production of a problem set in several parallel versions and the verification of their equivalence is a time-consuming and errorprone task .", "This paper tells about the efforts to develop tools and methods which increase its efficiency and reliability .", "Multilinguality has been an essential feature of the International Linguistic Olympiad since its conception ."]}
{"orig_sents": ["3", "4", "0", "1", "2"], "shuf_sents": ["This paper describes how the competition is run ( with a regional First Round and a final National Round ) and the organisation of the competition ( a National Steering Committee and Local Organising Committees for each region ) and discusses the particular challenges faced by Australia ( timing of the competition and distance between the major population centres ) .", "One major factor in the growth and success of OzCLO has been the introduction of the online competition , allowing participation of students from rural and remote country areas .", "The organisation relies on the good-will and volunteer work of university and school staff but the strong interest among students and teachers shows that OzCLO is responding to a demand for linguistic challenges .", "The Australian Computational and Linguistics Olympiad ( OzCLO ) started in 2008 in only two locations and has since grown to a nationwide competition with almost 1500 high school students participating in 2013 .", "An Australian team has participated in the International Linguistics Olympiad ( ILO ) every year since 2009 ."]}
{"orig_sents": ["1", "4", "2", "0", "3"], "shuf_sents": ["The effort is put on promoting the interest of linguistics to students through fun material and good contact with teachers of languages .", "What is it that we want to achieve with our Linguistic Olympiads and how do the contests vary in different countries ?", "The contest involves not only a test but also lectures , school visits and teaching material .", "This presentation contains an overview of the Swedish version of Olympiads in Linguistics as well as some concrete examples of workshop material on linguistic problems for secondary school students .", "The Swedish Olympiad has been running for 7 years now and is primarily focused on public outreach - spreading linguistics to secondary school students ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We believe that the correspondence seminar is a great way to introduce talented high school students to linguistics .", "We evaluate specific payoffs of this way of teaching linguistics , and compare its nature to that of a linguistics olympiad .", "We present the concept of a correspondence seminar as a way to complement and support one-time contests , especially olympiads ."]}
{"orig_sents": ["5", "1", "0", "2", "4", "3"], "shuf_sents": ["Convolution kernels such as string kernels and tree kernels are widely used in Natural Language Processing ( NLP ) applications .", "We present how a perceptron ( in its dual form ) uses convolution kernels to learn to differentiate between two categories of objects .", "However , the baggage associated with learning the theory behind convolution kernels , which extends beyond graduate linear algebra , makes the adoption of this technology intrinsically difficult .", "By orchestrating such a choreography , we believe , we have obviated the need for people to posses advanced math background in order to appreciate the core ideas of using convolution kernels in a supervised learning setting .", "The main challenge in creating this choreography was that we were required to represent these mathematical equations at their meaning level before we could translate them into the language of movement .", "In this paper we present a choreography that explains the process of supervised machine learning ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We describe our experience in incorporating these processes at an undergraduate course on language information technology .", "Students collectively annotated the syntactic structures of a set of Classical Chinese poems ; the resulting treebank was put on a platform for corpus search and visualization ; finally , using this platform , students investigated research questions about the text of the treebank .", "Data-driven research in linguistics typically involves the processes of data annotation , data visualization and identification of relevant patterns ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["At the same time the forum campaign has been a source of a successful academic course which resulted in a closeknit student team , strong enough to implement the two-year research for the second forum on syntax , held in 2012 .", "The new forum of anaphora ( to be held in 2014 ) is now prepared mostly by students .", "The forum of 2010 started as a new initiative and was the first independent evaluation of morphology parsers for Russian in Russia .", "We present in the paper our experience of involving the students of the department of theoretical and computational linguistics of the Moscow State University into full-cycle activities of preparing and evaluating the results of the NLP Evaluation forums , held in 2010 and 2012 in Russia ."]}
{"orig_sents": ["3", "0", "4", "1", "2", "6", "5"], "shuf_sents": ["This web-based interactive visualization lets the user tune the probabilities of various shapes ? which grow and shrink accordingly ? by dragging sliders that correspond to feature weights .", "and ? Solve ?", "buttons .", "We present an open-source virtual manipulative for conditional log-linear models .", "The visualization displays a regularized training objective ; it supports gradient ascent by optionally displaying gradients on the sliders and providing ? Step ?", "Our website , http : //cs.jhu.edu/ ? jason/ tutorials/loglin/ , guides the user through a series of interactive lessons and provides auxiliary readings , explanations , practice problems and resources .", "The user can sample parameters and datasets of different sizes and compare their own parameters to the truth ."]}
{"orig_sents": ["3", "1", "2", "0"], "shuf_sents": ["Students this semester are pursuing NLP/ML projects , formulating their own tasks ( some of which are novel and presented towards the end of the paper ) , collecting and annotating data and building models for their task .", "We discuss the challenges we faced while incorporating NLP and ML to the curriculum followed by a presentation of how we met these challenges .", "The overall response ( of students ) to the inclusion of this new topic to the curriculum has been positive .", "In this paper we discuss our experience of teaching basic Natural Language Processing ( NLP ) and Machine Learning ( ML ) in an introductory course to Information Science ."]}
{"orig_sents": ["0", "2", "3", "1", "4", "5"], "shuf_sents": ["This paper describes a seminar course designed by IBM and Columbia University on the topic of Semantic Technologies , in particular as used in IBM WatsonTM ?", "against two human grand champions .", "a large scale Question Answering system which famously won at Jeopardy !", "R ?", "It was first offered at Columbia University during the 2013 spring semester , and will be offered at other institutions starting in the fall semester .", "We describe the course ? s first successful run and its unique features : a class centered around a specific industrial technology ; a large-scale class project which student teams can choose to participate in and which serves as the basis for an open source project that will continue to grow each time the course is offered ; publishable papers , demos and start-up ideas ; evidence that the course can be self-evaluating , which makes it potentially appropriate for an online setting ; and a unique model where a large company trains instructors and contributes to creating educational material at no charge to qualifying institutions ."]}
{"orig_sents": ["1", "0", "2", "3"], "shuf_sents": ["In this paper , we investigate the problem of online active learning within a new active domain adaptation setting : there are insufficient labeled data in both source and target domains , but it is cheaper to query labels in the source domain than in the target domain .", "Active learning and domain adaptation are both important tools for reducing labeling effort to learn a good supervised model in a target domain .", "Given a total budget , we develop two costsensitive online active learning methods , a multi-view uncertainty-based method and a multi-view disagreement-based method , to query the most informative instances from the two domains , aiming to learn a good prediction model in the target domain .", "Empirical studies on the tasks of cross-domain sentiment classification of Amazon product reviews demonstrate the efficacy of the proposed methods on reducing labeling cost ."]}
{"orig_sents": ["0", "5", "3", "2", "1", "4"], "shuf_sents": ["Within the natural language processing ( NLP ) community , active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies .", "Specifically , if the Kappa agreement between two models exceeds a threshold T ( where T > 0 ) , then the difference in F-measure performance between those models is bounded above by 4 ( 1 ? T ) T in all cases .", "Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented .", "The analysis has revealed three elements that are central to the success of the SP method : ( 1 ) bounds on Cohen ? s Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models ; ( 2 ) since the stop set does not have to be labeled , it can be made large in practice , helping to guarantee that the results transfer to previously unseen streams of examples at test/application time ; and ( 3 ) good ( low variance ) sample estimates of Kappa between successive models can be obtained .", "If precisionof the positive conjunction of the models is assumed to be p , then the bound can be tightened to 4 ( 1 ? T ) ( p+1 ) T .", "This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions ( SP ) ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["We investigate the source of this performance improvement and find that of the two types of significant co-occurrence - corpus-level and document-level , the concept of corpus level significance combined with the use of document counts in place of word counts is responsible for all the performance gains observed .", "The concept of document level significance is not helpful for PMI adaptation .", "By extensive experiments with a large number of publicly available datasets we show that the newly introduced measure performs better than other co-occurrence based measures and despite being resource-light , compares well with the best known resource-heavy distributional similarity and knowledge based word association measures .", "We design a new co-occurrence based word association measure by incorporating the concept of significant cooccurrence in the popular word association measure Pointwise Mutual Information ( PMI ) ."]}
{"orig_sents": ["4", "0", "3", "2", "1", "5", "6"], "shuf_sents": ["Our focus is on a lowresource learning setting , in which only a small amount of annotated word forms are available for model training , while unannotated word forms are available in abundance .", "Specifically , we employ conditional random fields , a popular discriminative log-linear model for segmentation .", "In contrast , we discuss 1 ) employing only the annotated data in a supervised manner , while entirely ignoring the unannotated data , and 2 ) directly learning to predict morph boundaries given their local sub-string contexts instead of learning the morph lexicons .", "The current state-of-art methods 1 ) exploit both the annotated and unannotated data in a semi-supervised manner , and 2 ) learn morph lexicons and subsequently uncover segmentations by generating the most likely morph sequences .", "We discuss data-driven morphological segmentation , in which word forms are segmented into morphs , the surface forms of morphemes .", "We present experiments on two data sets comprising five diverse languages .", "We show that the fully supervised boundary prediction approach outperforms the state-of-art semi-supervised morph lexicon approaches on all languages when using the same annotated data sets ."]}
{"orig_sents": ["3", "0", "1", "4", "2"], "shuf_sents": ["Our extension allows for any regularizer that is a convex , differentiable function of the appropriate marginals .", "We show that surprisingly , non-linearity of such regularization does not increase the complexity of learning , provided we use multiplicative updates of the structured exponentiated gradient algorithm .", "On sequential prediction tasks of handwriting recognition and part-ofspeech ( POS ) tagging , our method makes significant gains over strong baselines .", "We present a flexible formulation of semisupervised learning for structured models , which seamlessly incorporates graphbased and more general supervision by extending the posterior regularization ( PR ) framework .", "We illustrate the extended framework by learning conditional random fields ( CRFs ) with quadratic penalties arising from a graph Laplacian ."]}
{"orig_sents": ["3", "5", "1", "4", "0", "2"], "shuf_sents": ["We evaluate our training method with Noun Phrase Chunking , Text Chunking and Extended Named Entity Recognition .", "In the boosting , training samples that are incorrectly segmented or labeled have large weights .", "The experimental results show that our method achieves better accuracy than a semi-Markov perceptron and a semi-Markov Conditional Random Fields .", "This paper proposes a boosting algorithm that uses a semi-Markov perceptron .", "Such training samples are aggressively learned in the training of the semi-Markov perceptron because the weights are used as the learning ratios .", "The training algorithm repeats the training of a semi-Markov model and the update of the weights of training samples ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["This method is simple , efficient , and can be applied to a wide range of supervised sequence labeling tasks .", "Our experiments on a phoneme recognition task show that when equipped with informative feature functions , it performs significantly better than a supervised HMM and competitively with EM .", "We derive a spectral algorithm for learning the parameters of a refinement HMM .", "Like other spectral methods , it avoids the problem of local optima and provides a consistent estimate of the parameters ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["Our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-theart approaches .", "Experiments on corpora featuring human-generated compressions demonstrate a 13-15 % relative gain in 4gram accuracy over a well-studied language model-based compression system .", "We present a novel approach for discriminative sentence compression that unifies these notions and jointly produces sequential and syntactic representations for output text , leveraging a compact integer linear programming formulation to maintain structural integrity .", "Sentence compression techniques often assemble output sentences using fragments of lexical sequences such as ngrams or units of syntactic structure such as edges from a dependency tree representation ."]}
{"orig_sents": ["3", "4", "0", "1", "2"], "shuf_sents": ["The relations are established using information from automatic classifiers , i.e. , question category ( QC ) and focus classifiers ( FC ) and Named Entity Recognizers ( NER ) .", "This way ( i ) effective structural relational patterns can be automatically learned with kernel machines ; and ( ii ) structures are more invariant w.r.t .", "different domains , thus fostering adaptability .", "This paper proposes passage reranking models that ( i ) do not require manual feature engineering and ( ii ) greatly preserve accuracy , when changing application domain .", "Their main characteristic is the use of relational semantic structures representing questions and their answer passages ."]}
{"orig_sents": ["3", "2", "0", "1", "4"], "shuf_sents": ["Through extensive experiments on benchmark datasets , we find that even though a type-based VSM is effective for semantic composition , it is often outperformed by a VSM built using a combination of topic- and type-based statistics .", "We also introduce a new evaluation task wherein we predict the composed vector representation of a phrase from the brain activity of a human subject reading that phrase .", "In this paper , we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases .", "In most previous research on distributional semantics , Vector Space Models ( VSMs ) of words are built either from topical information ( e.g. , documents in which a word is present ) , or from syntactic/semantic types of words ( e.g. , dependency parse links of a word in sentences ) , but not both .", "We exploit a large syntactically parsed corpus of 16 billion tokens to build our VSMs , with vectors for both phrases and words , and make them publicly available ."]}
{"orig_sents": ["3", "0", "2", "1", "4"], "shuf_sents": ["First , we introduce a generative model of sentences , based on dependency trees and which takes into account homonymy .", "Second , we describe an efficient algorithm to perform inference and learning in this model .", "Our model can thus be seen as a generalization of Brown clustering .", "In this paper , we propose a new method for semantic class induction .", "Third , we apply our proposed method on two large datasets ( 108 tokens , 105 words types ) , and demonstrate that classes induced by our algorithm improve performance over Brown clustering on the task of semisupervised supersense tagging and named entity recognition ."]}
{"orig_sents": ["1", "5", "4", "2", "3", "0"], "shuf_sents": ["Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets , including a new dataset we introduce focused on rare words to complement existing ones in an interesting way .", "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks .", "This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes .", "We combine recursive neural networks ( RNNs ) , where each morpheme is a basic unit , with neural language models ( NLMs ) to consider contextual information in learning morphologicallyaware word representations .", "As a result , rare and complex words are often poorly estimated , and all unknown words are represented in a rather crude way using only one or a few vectors .", "However , common to most existing work , words are regarded as independent entities without any explicit relationship among morphologically related words being modeled ."]}
{"orig_sents": ["3", "2", "4", "5", "1", "0"], "shuf_sents": ["system on a composition-only task .", "Then we demonstrate the benefits of an ? unambiguous ?", "This paper provides evidence that the addition of a vector disambiguation step prior to the actual composition would be beneficial to the whole process , producing better composite representations .", "Most compositional-distributional models of meaning are based on ambiguous vector representations , where all the senses of a word are fused into the same vector .", "Furthermore , we relate this issue with the current evaluation practice , showing that disambiguation-based tasks can not reliably assess the quality of composition .", "Using a word sense disambiguation scheme based on the generic procedure of Sch ? tze ( 1998 ) , we first provide a proof of concept for the necessity of separating disambiguation from composition ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Empirical results on four datasets demonstrate the effectiveness of our extensions .", "Determining the stance expressed by an author from a post written for a two-sided debate in an online debate forum is a relatively new problem in opinion mining .", "We extend a state-of-the-art learningbased approach to debate stance classification by ( 1 ) inducing lexico-syntactic patterns based on syntactic dependencies and semantic frames that aim to capture the meaning of a sentence and provide a generalized representation of it ; and ( 2 ) improving the classification of a test post via a novel way of exploiting the information in other test posts with the same stance ."]}
{"orig_sents": ["1", "4", "3", "2", "0", "5"], "shuf_sents": ["We evaluate our system predicting held out facts , achieving 74.2 % accuracy and outperforming multiple baselines .", "Large databases of facts are prevalent in many applications .", "This work can be thought of as re-casting open domain information extraction : rather than growing a database of known facts , we smooth this data into a database in which any possible fact has membership with some confidence .", "In contrast to extending such a database , we present a system to query whether it contains an arbitrary fact .", "Such databases are accurate , but as they broaden their scope they become increasingly incomplete .", "We also evaluate the system as a commonsense filter for the ReVerb Open IE system , and as a method for answer validation in a Question Answering task ."]}
{"orig_sents": ["5", "3", "1", "2", "4", "0"], "shuf_sents": ["This should set the benchmark for future development of various NLP components in syntax and semantics , and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance .", "As a result , it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains .", "The completion of the OntoNotes corpus , a large-scale , multi-genre , multilingual corpus manually annotated with syntactic , semantic and discourse information , makes it possible to perform such an evaluation .", "Up till now , however , most of the evaluation has been done on monolithic corpora such as the Penn Treebank , the Proposition Bank .", "This paper presents an analysis of the performance of publicly available , state-of-the-art tools on all layers and languages in the OntoNotes v5.0 corpus .", "Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic , semantic and discourse analyzers , and they serve as training data as well as evaluation benchmarks ."]}
{"orig_sents": ["0", "1", "3", "4", "2"], "shuf_sents": ["Coreference resolution systems can benefit greatly from inclusion of global context , and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources .", "However , since alignment itself is a challenging task and is often noisy , existing systems either align conservatively , resulting in very few links , or combine the attributes of multiple candidates , leading to a conflation of entities .", "These forms of global context enables our system to improve classifier-based coreference by 1.09 B3 F1 points , and improve over the previous state-of-art by 0.41 points , thus introducing a new state-of-art result on the ACE 2004 data .", "Our approach instead performs joint inference between within-document coreference and entity linking , maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference .", "Further , we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["Previous incremental parsers have used monotonic state transitions .", "We evaluate the change in the context of a stateof-the-art system , and obtain a statistically significant improvement ( p < 0.001 ) on the English evaluation and 5/10 of the CoNLL languages .", "We show that a simple adjustment to the Arc-Eager transition system to relax its monotonicity constraints can improve accuracy , so long as the training data includes examples of mistakes for the nonmonotonic transitions to repair .", "However , transitions can be made to revise previous decisions quite naturally , based on further information ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper presents a collapsed variational Bayesian inference algorithm for PCFGs that has the advantages of two dominant Bayesian training algorithms for PCFGs , namely variational Bayesian inference and Markov chain Monte Carlo .", "In three kinds of experiments , we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using an order of magnitude less training time ; and outperforms the standard variational Bayesian inference and the EM algorithms with similar training time ."]}
{"orig_sents": ["3", "2", "0", "5", "1", "4"], "shuf_sents": ["We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages .", "Moreover , we investigate the semantic features captured by these embeddings through the proximity of word groupings .", "In this work , we train word embeddings for more than 100 languages using their corresponding Wikipedias .", "Distributed word representations ( word embeddings ) have recently contributed to competitive performance in language modeling and several NLP tasks .", "We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications .", "We find their performance to be competitive with near state-of-art methods in English , Danish and Swedish ."]}
{"orig_sents": ["3", "0", "5", "1", "4", "2"], "shuf_sents": ["The goal of this approach is to avoid the effort of acquiring and labeling new corpora to learn models when changing the language .", "These graphs represent a set of hypotheses ( a language ) that is the input to the statistical semantic decoder that provides the meaning of the sentence .", "They show the good behavior of the system , mainly when speech input is considered .", "In this work , we present an approach for multilingual portability of Spoken Language Understanding systems .", "Some experiments in a Spanish task evaluated with input French utterances and text are presented .", "The work presented in this paper is focused on the learning of a specific translator for the task and the mechanism of transmitting the information among the modules by means of graphs ."]}
{"orig_sents": ["2", "0", "1", "4", "5", "3"], "shuf_sents": ["However , inherent ambiguities in the pivot language ( s ) can lead to inadequate paraphrases .", "We propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language , i.e. , the language to be paraphrased .", "The use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases .", "This approach shows 62 % relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent .", "Text in the input language is annotated with ? senses ?", "in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment ."]}
{"orig_sents": ["2", "5", "3", "4", "0", "6", "1"], "shuf_sents": ["Unlike many previous work , our framework does not require any languagespecific knowledge for initialization .", "On a large-scale Wikipedia corpus , we demonstrate that our framework reliably extracts high-precision translation pairs on a wide variety of comparable data conditions .", "We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora .", "Intuitively , our approach works by converting a comparable document-aligned corpus into a parallel topic-aligned corpus , then learning word alignments using co-occurrence statistics .", "This topicaligned corpus is similar in structure to the sentence-aligned corpus frequently used in statistical machine translation , enabling us to exploit advances in word alignment research .", "Our approach is based on a novel combination of topic modeling and word alignment techniques .", "Furthermore , our framework attempts to handle polysemy by allowing multiple translation probability models for each word ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["Being able to detect such aspects represents an important subtask of aspect-based review mining systems , which aim at automatically generating structured summaries of customer opinions .", "For the best configuration , we find significant improvements over a state-of-the-art baseline method .", "We cast the task as a terminology extraction problem and examine the utility of varying term acquisition heuristics , filtering techniques , variant aggregation methods , and relevance measures .", "In this paper , we address the problem of identifying relevant product aspects in a collection of online customer reviews .", "We evaluate the different approaches on two distinct datasets ( hotel and camera reviews ) ."]}
{"orig_sents": ["4", "0", "1", "3", "2"], "shuf_sents": ["Some psycholinguistic theories attribute this lag to conceptual differences between the two classes , while others suggest that syntactic differences are responsible .", "Through computational experiments , we show that a probabilistic verb learning model exhibits the pattern of acquisition , even though there is no difference in the model in the difficulty of the semantic or syntactic properties of Belief vs .", "Our results point to the distributional properties of various verb classes as a potentially important , and heretofore unexplored , factor in the observed developmental lag of Belief verbs .", "Desire verbs .", "The acquisition of Belief verbs lags behind the acquisition of Desire verbs in children ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We also give an overview of the various approaches adopted by the participating teams , and present the evaluation results .", "In this paper , we give the task definition , present the data sets , and describe the evaluation metric and scorer used in the shared task .", "The CoNLL-2013 shared task was devoted to grammatical error correction ."]}
{"orig_sents": ["0", "3", "2", "4", "1"], "shuf_sents": ["The CoNLL-2013 shared task focuses on correcting grammatical errors in essays written by non-native learners of English .", "Out of 17 participating teams , our system is ranked first based on both the original annotation and on the revised annotation .", "The system consists of five components and targets five types of common grammatical mistakes made by English as Second Language writers .", "In this paper , we describe the University of Illinois system that participated in the shared task .", "We describe our underlying approach , which relates to our previous work , and describe the novel aspects of the system in more detail ."]}
{"orig_sents": ["1", "0", "3", "5", "4", "2"], "shuf_sents": ["30013 { maxis1718 , teer1990 , chiuhsunwen , joseph.yen , Joanne.boisson , wujc86 , jason.jschang } @ gmail.com Abstract Grammatical error correction has been an active research area in the field of Natural Language Processing .", "te of Information Systems and Applications + Department of Computer Science National Tsing Hua University HsinChu , Taiwan , R.O.C .", "We received an overall F-measure score of 0.325 , which put our system in second place among 17 systems evaluated .", "This paper describes the grammatical error correction system developed at NTHU in participation of the CoNLL-2013 Shared Task .", "Although more types of errors are involved that than last year ? s Shared Task , leading to more complicated problem this year , our system still obtain higher F-score as compared to last year .", "The system consists of four modules in a pipeline to correct errors related to determiners , prepositions , verb forms and noun number ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["As for subject-verb agreement errors , we show that the Treelet Language Model-based approach can correct errors in which the target verb is distant from its subject .", "Our system ranked fourth on the official run .", "This paper describes the Nara Institute of Science and Technology ( NAIST ) error correction system in the CoNLL 2013 Shared Task .", "We constructed three systems : a system based on the Treelet Language Model for verb form and subjectverb agreement errors ; a classifier trained on both learner and native corpora for noun number errors ; a statistical machine translation ( SMT ) -based model for preposition and determiner errors ."]}
{"orig_sents": ["7", "6", "3", "8", "0", "1", "4", "2", "5"], "shuf_sents": ["A language model based on the Google N-gram corpus was employed to select the best correction candidate from a confusion matrix .", "We also explored a graphbased label propagation approach to overcome the sparsity problem in training the model .", "The proposed model was evaluated on the test set consisting of 50 essays and with about 500 words in each essay .", "The process starts with spellchecking as a preprocessing step to correct any possible erroneous word .", "Finally , a number of deterministic rules were used to increase the precision and recall .", "Our system achieves the 5 th and 3 rd F1 scores on official test set among all 17 participating teams based on goldstandard edits before and after revision , respectively .", "A hybrid model is adopted for this special task .", "This paper describes the NLP2CT Grammatical Error Detection and Correction system for the CoNLL 2013 shared task , with a focus on the errors of article or determiner ( ArtOrDet ) , noun number ( Nn ) , preposition ( Prep ) , verb form ( Vform ) and subject-verb agreement ( SVA ) .", "We used a Maximum Entropy classifier together with manually rule-based filters to detect the grammatical errors in English ."]}
{"orig_sents": ["1", "3", "4", "0", "2", "5"], "shuf_sents": ["Our system obtains a precision , recall and F1 score of 0.27 , 0.1333 and 0.1785 , respectively , on the official test set .", "We present an approach to grammatical error correction for the CoNLL 2013 shared task based on a weighted tree-to-string transducer .", "On the revised annotations , the F1 score increases to 0.2505 .", "Rules for the transducer are extracted from the NUCLE training data .", "An n-gram language model is used to rerank k-best sentence lists generated by the transducer .", "Our system ranked 6th out of the participating teams on both the original and revised test set annotations ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Since the limited training data provided for the task was insufficient for training an effective SMT system , we also explored alternative ways of generating pairs of incorrect and correct sentences automatically from other existing learner corpora .", "This paper describes our use of phrasebased statistical machine translation ( PBSMT ) for the automatic correction of errors in learner text in our submission to the CoNLL 2013 Shared Task on Grammatical Error Correction .", "Our approach does not yield particularly high performance but reveals many problems that require careful attention when building SMT systems for error correction ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["We carried out feature engineering and we found that ( among others ) the f-structure of an LFG parser can provide very informative features for the machine learning system .", "We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks .", "We introduce here a participating system of the CoNLL-2013 Shared Task ? Grammatical Error Correction ? ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["This paper describes our system in the shared task of CoNLL-2013 .", "Our system achieves the F1 score of 17.13 % on the standard test set .", "We illustrate that grammatical error detection and correction can be transformed into a multiclass classification task and implemented as a single-model system regardless of various error types with the aid of maximum entropy modeling ."]}
{"orig_sents": ["4", "3", "1", "0", "2"], "shuf_sents": ["For subject-verb agreement correction , we propose a new rulebased system which utilizes dependency parse information and a set of conditional rules to ensure agreement of the verb group with its subject .", "For noun-number and determiner correction , we apply a classification approach using rich lexical and syntactic features .", "Our system obtained an F-score of 11.03 on the official test set using the M2 evaluation method ( the official evaluation method ) .", "Our system corrects three of the five error types specified for the shared task noun-number , determiner and subject-verb agreement errors .", "We describe our grammar correction system for the CoNLL-2013 shared task ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["This paper describes our submission for the CoNLL 2013 Shared Task , which aims to to improve the detection and correction of the five most common grammatical error types in English text written by non-native speakers .", "Our system concentrates only on two of them ; it employs machine learning classifiers for the ArtOrDet- , and a fully deterministic rule based workflow for the SVA error type ."]}
{"orig_sents": ["4", "2", "7", "5", "0", "3", "1", "6"], "shuf_sents": ["n-grams extracted by following the paths in dependency trees .", "Since it does not employ any additional resources or any sophisticated machine learning methods , it does not achieve high scores ( specifically , it has low recall ) but could be considered as a baseline system for the task .", "The system is based on the rule-based approach .", "The system is simple and was developed in a short period of time ( 1 month ) .", "We describe the system developed for the CoNLL-2013 shared task ? automatic English L2 grammar error correction .", "The system uses the syntactic information available in the training data : this information is represented as syntactic n-grams , i.e .", "On the other hand , it shows what can be obtained using a simple rule-based approach and presents a few situations where the rule-based approach can perform better than ML approach .", "It uses very few additional resources : a morphological analyzer and a list of 250 common uncountable nouns , along with the training data provided by the organizers ."]}
{"orig_sents": ["1", "6", "7", "0", "4", "2", "5", "3"], "shuf_sents": ["Trained on the Google Web 1T corpus , the first two classifiers determine the presence of a determiner or a preposition between all words in a text .", "We describe the ? TILB ?", "The fifth classifier is a general word predictor which is used to suggest noun and verb form corrections .", "We point out a number of obvious improvements to boost the scores obtained by the system .", "The second pair of classifiers determine which is the most likely correction of an occurring determiner or preposition .", "We report on the scores attained and errors corrected and missed .", "team entry for the CONLL-2013 Shared Task .", "Our system consists of five memory-based classifiers that generate correction suggestions for center positions in small text windows of two words to the left and to the right ."]}
{"orig_sents": ["4", "3", "1", "0", "2"], "shuf_sents": ["Grammatical correction is inherently difficult both to perform and to evaluate .", "Although the results on the shared task test set were poor , the approach may still be promising , as there are many aspects of the current implementation that could be optimised .", "As such , possible improvements to the evaluation are also discussed .", "The system was a provisional implementation of a beam search correction over a noisy channel model .", "We report on the TOR system that participated in the 2013 CoNLL shared task on grammatical correction ."]}
{"orig_sents": ["1", "3", "2", "0", "4", "5"], "shuf_sents": ["Preprocessing and post-processing procedures are employed to keep idiomatic phrases from being corrected .", "This paper presents a hybrid model for the CoNLL-2013 shared task which focuses on the problem of grammatical error correction .", "We correct these five types of errors in different modules where either machine learning based or rule-based methods are applied .", "This year ? s task includes determiner , preposition , noun number , verb form , and subject-verb agreement errors which is more comprehensive than previous error correction tasks .", "We achieved precision of 35.65 % , recall of 16.56 % , F1 of 22.61 % in the official evaluation and precision of 41.75 % , recall of 20.29 % , F1 of 27.3 % in the revised version .", "Some further comparisons employing different strategies are made in our experiments ."]}
{"orig_sents": ["2", "0", "1", "3", "4"], "shuf_sents": ["Error types covered by our system are article/determiner , preposition , and noun number agreement .", "This work is our first attempt on grammatical error correction research .", "This paper describes an English grammatical error correction system for CoNLL2013 shared task .", "In this work , we only focus on reimplementing the techniques presented before and optimizing the performance .", "As a result of the implementation , our system ? s final F1-score by m2 scorer is 0.1282 in our internal test set ."]}
{"orig_sents": ["1", "2", "5", "4", "0", "3"], "shuf_sents": ["We define translations between RST and DT preserving these interpretations , and introduce a similarity measure for discourse representations in these frameworks .", "Several discourse annotated corpora now exist for NLP .", "But they use different , not easily comparable annotation schemes : are the structures these schemes describe incompatible , incomparable , or do they share interpretations ?", "This will enable researchers to exploit different types of discourse annotated data for automated tasks .", "We offer a common language in which their structures can be defined and furnished a range of interpretations .", "In this paper , we relate three types of discourse annotation used in corpora or discourse parsing : ( i ) RST , ( ii ) SDRT , and ( iii ) dependency tree structures ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["The semantic structure is obtained with unsupervised Bayesian inference , using the Metropolis-Hastings sampling algorithm .", "The proposed model is akin to a standard semantic role labeling system , except that it is unsupervised , it does not rely on any syntactic information and it exploits concepts derived from a domain-specific ontology .", "It is evaluated both in terms of attachment accuracy and purity-collocation for clustering , and compared with strong baselines on the French MEDIA spoken-dialog corpus .", "This work proposes a generative model to infer latent semantic structures on top of manual speech transcriptions in a spoken dialog reservation task ."]}
{"orig_sents": ["2", "4", "7", "5", "3", "6", "0", "1"], "shuf_sents": ["The knowledge base is evaluated empirically .", "The results show that our metrics perform significantly better than the state-of-the-art on the task of detecting causal verbal events .", "The identification of causal relations between verbal events is important for achieving natural language understanding .", "More specifically , we propose a set of knowledge-rich metrics to learn the likelihood of causal relations between verbs .", "However , the problem has proven notoriously difficult since it is not clear which types of knowledge are necessary to solve this challenging problem close to human level performance .", "Since verbs play a very important role in causal relations , in this paper we harness , explore , and evaluate the predictive power of causal associations of verb-verb pairs .", "Employing these metrics , we automatically generate a knowledge base ( KBc ) which identifies three categories of verb pairs : Strongly Causal , Ambiguous , and Strongly Non-causal .", "Instead of employing a large set of features proved useful in other NLP tasks , we split the problem in smaller sub problems ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["An appealing methodology for natural language generation in dialogue systems is to train the system to match a target corpus .", "We show how users can provide such a corpus as a natural side effect of interacting with a prototype system , when the system uses mixed-initiative interaction and a reversible architecture to cover a domain familiar to users .", "We experiment with integrated problems of sentence planning and realization in a referential communication task .", "Our model learns general and context-sensitive patterns to choose descriptive content , vocabulary , syntax and function words , and improves string match with user utterances to 85.8 % from a handcrafted baseline of 54.4 % ."]}
{"orig_sents": ["2", "4", "0", "5", "1", "3"], "shuf_sents": ["Happily , an increasing amount of information and opinion exchange occur in natural dialogue in online forums , where people share their opinions about a vast range of topics .", "In this paper , we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic-independent way .", "Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available .", "Our results show that our theoretically motivated features achieve 66 % accuracy , an improvement over a unigram baseline of an absolute 6 % .", "This has impacted the dialogue research community ? s ability to develop better theories , as well as good off-the-shelf tools for dialogue processing .", "In particular we are interested in rejection in dialogue , also called disagreement and denial , where the size of available dialogue corpora , for the first time , offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue ."]}
{"orig_sents": ["3", "0", "1", "2", "4"], "shuf_sents": ["We do this by predicting friendship status in a dyad using a set of automatically harvested verbal and nonverbal features from videos of the interaction of students in a peer tutoring study .", "We propose a new computational model used to model friendship status in our data , based on a group sparse model ( GSM ) with L2,1 norm which is designed to accommodate the sparse and noisy properties of the multi-channel features .", "Our GSM model achieved the best overall performance compared to a non-sparse linear model ( NLM ) and a regular sparse linear model ( SLM ) , as well as outperforming human raters .", "In this paper we focus on modeling friendships between humans as a way of working towards technology that can initiate and sustain a lifelong relationship with users .", "Dyadic features , such as number and length of conversational turns and mutual gaze , in addition to low level features such as F0 and gaze at task , were found to be good predictors of friendship status ."]}
{"orig_sents": ["2", "6", "0", "1", "5", "3", "4"], "shuf_sents": ["In this paper , we examine the use of users ?", "intentions and debate structure for stance classification of the debate posts .", "Online debate forums provide a rich collection of differing opinions on various topics .", "intent at sentence level using its dependency parse and sentiWordNet and to build the intention structure of the post to identify its stance .", "To aid the task of classification , we define the health of the debate structure and show that maximizing its value leads to better stance classification accuracies .", "We propose a domain independent approach to capture users ?", "In dual-sided debates , users present their opinions or judge other ? s opinions to support their stance ."]}
{"orig_sents": ["1", "7", "0", "3", "5", "4", "2", "6"], "shuf_sents": ["As a first step , we propose a method for generating more specific questions than simple wh-questions to acquire the attributes , as such questions can narrow down the variation of the following user response and accordingly avoid possible speech recognition errors .", "Our aim is to acquire the attributes of concepts denoted by unknown words from users during dialogues .", "We evaluated distributions of the CMs by average errors from the reference .", "Specifically , we obtain an appropriately distributed confidence measure ( CM ) on the attributes to generate more specific questions .", "These are integrated to complement each other and used as the final CM .", "Two basic CMs are defined using ( 1 ) character and word distributions in the target database and ( 2 ) frequency of occurrence of restaurant attributes on Web pages .", "Results showed that the integrated CM outperformed the two basic CMs .", "A word unknown to spoken dialogue systems can appear in user utterances , and systems should be capable of acquiring information on it from the conversation partner as a kind of selflearning process ."]}
{"orig_sents": ["4", "2", "0", "3", "1"], "shuf_sents": ["In this paper , we have extended our previous graph-matching based approach to explicitly incorporate collaborative referring behaviors into the referential grounding algorithm .", "Our empirical results have shown that incorporating the most prevalent pattern of collaboration with our hypergraph-based approach significantly improves reference resolution in situated dialogue by an absolute gain of over 18 % .", "Humans and agents will need to make extra efforts by collaborating with each other to mediate a shared perceptual basis and to come to a mutual understanding of intended referents in the environment .", "In addition , hypergraph-based representations have been used to account for group descriptions that are likely to occur in spatial communications .", "In situated dialogue , because humans and agents have mismatched capabilities of perceiving the shared physical world , referential grounding becomes difficult ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["Using a corpus of news texts , our results show that it is possible to use discourse features ( based on Rhetorical Structure Theory ) for topic segmentation and that we outperform some well-known methods .", "In this paper , we describe novel methods for topic segmentation based on patterns of discourse organization ."]}
{"orig_sents": ["1", "2", "3", "0"], "shuf_sents": ["Then , we run simulations with this model and we compute confidence intervals for the mean of the expected return of the state-action couples .", "This paper presents a practical methodology for the integration of reinforcement learning during the design of a Spoken Dialogue System ( SDS ) .", "It proposes a method that enables SDS designers to know , in advance , the number of dialogues that their system will need in order to learn the value of each state-action couple .", "We ask the designer to provide a user model in a simple way ."]}
{"orig_sents": ["0", "2", "3", "1", "4"], "shuf_sents": ["Intelligent Tutoring Systems ( ITSs ) are now recognised as an interesting alternative for providing learning opportunities in various domains .", "In the dialogue case , engineering work can infer a precise state of the user by taking into account the uncertainty provided by the spoken understanding language module .", "The Reinforcement Learning ( RL ) approach has been shown reliable for finding efficient teaching strategies .", "However , similarly to other human-machine interaction systems such as spoken dialogue systems , ITSs suffer from a partial knowledge of the interlocutor ? s intentions .", "A model-free approach based on RL and Echo State Newtorks ( ESNs ) , which retrieves similar information , is proposed here for tutoring ."]}
{"orig_sents": ["2", "4", "0", "5", "1", "3"], "shuf_sents": ["They may also make irrational moves , i.e. , moves not consistent with their goals , to generate a variety of negotiation patterns .", "We evaluate the learned policies against hand-crafted SNs similar to the ones used for training but with the modification that these SNs no longer make irrational moves and thus are harder to beat .", "We use hand-crafted simulated negotiators ( SNs ) to train and evaluate dialogue policies for two-issue negotiation between two agents .", "The learned policies generally do as well as , or better than the hand-crafted SNs showing that RL can be successfully used for learning argumentation dialogue policies in twoissue negotiation scenarios .", "These SNs differ in their goals and in the use of strong and weak arguments to persuade their counterparts .", "Different versions of these SNs interact with each other to generate corpora for Reinforcement Learning ( RL ) of argumentation dialogue policies for each of the two agents ."]}
{"orig_sents": ["2", "1", "0"], "shuf_sents": ["Extensive empirical results , across different conversational modalities , demonstrate the effectiveness of our SVM-hmm model for dialogue act recognition in conversations .", "To this aim , we compare the results of SVM-multiclass and two structured predictors namely SVMhmm and CRF algorithms .", "In this work , we study the effectiveness of state-of-the-art , sophisticated supervised learning algorithms for dialogue act modeling across a comprehensive set of different spoken and written conversations including : emails , forums , meetings , and phone conversations ."]}
{"orig_sents": ["2", "3", "1", "0"], "shuf_sents": ["Correcting the hypotheses using the estimated model error increases performance by up to 4.1 % relative improvement in Unweighted Average Recall .", "Hence , we aim at tackling the task by estimating the error of the applied statistical classification algorithms in a two-stage approach .", "Determining the quality of an ongoing interaction in the field of Spoken Dialogue Systems is a hard task .", "While existing methods employing automatic estimation already achieve reasonable results , still there is a lot of room for improvement ."]}
{"orig_sents": ["0", "2", "3", "1"], "shuf_sents": ["SCXML was proposed as one description language for dialog control in the W3C Multimodal Architecture but lacks the facilities required for grounding and reasoning .", "Thereby bridging the gap between respective dialog modeling research and a standardized architecture to access and coordinate modalities .", "This prohibits the application of many dialog modeling techniques for multimodal applications following this W3C standard .", "By extending SCXML with a Prolog datamodel and scripting language , we enable those techniques to be employed again ."]}
{"orig_sents": ["2", "0", "4", "5", "3", "1", "6"], "shuf_sents": ["Localized error detection finds specific mis-recognized words in a user utterance .", "We improve over baseline results , where only ASRgenerated features are used , by constructing optimal feature sets for utterance and word mis-recognition prediction .", "We address the problem of localized error detection in Automatic Speech Recognition ( ASR ) output to support the generation of targeted clarifications in spoken dialogue systems .", "We extend and modify work presented in ( Stoyanchev et al. , 2012b ) by experimenting with a new set of features for predicting the likelihood of a local error in an ASR hypothesis on an unsifted version of the original dataset .", "Targeted clarifications , in contrast with generic ? please repeat/rephrase ?", "clarifications , target a specific mis-recognized word in an utterance ( Stoyanchev et al , 2012a ) and require accurate detection of such words .", "The f-measure for identifying incorrect utterances improves by 2.2 % and by 3.9 % for identifiying incorrect words ."]}
{"orig_sents": ["1", "5", "6", "3", "0", "7", "4", "2"], "shuf_sents": ["Our goal is to generate targeted clarification strategies for handling errors in spoken dialogue systems , when appropriate .", "We model human responses to speech recognition errors from a corpus of human clarification strategies .", "The same set of features predict the decision to ask a targeted question with accuracy of 74.6 % compared with the majority baseline of 71.8 % .1", "or ? please rephrase ? .", "A combination of linguistic features predicts a user ? s decision to continue or stop a dialogue with accuracy of 72.8 % over a majority baseline accuracy of 59.1 % .", "We employ learning techniques to study 1 ) the decision to either stop and ask a clarification question or to continue the dialogue without clarification , and 2 ) the decision to ask a targeted clarification question or a more generic question .", "Targeted clarification questions focus specifically on the part of an utterance that is misrecognized , in contrast with generic requests to ? please repeat ?", "Our experiments show that linguistic features , in particular the inferred part-ofspeech of a misrecognized word are predictive of human clarification decisions ."]}
{"orig_sents": ["2", "0", "1", "3"], "shuf_sents": ["Among other features , it integrates different human-computer interaction engines across multiple domains and communication styles such as command , question answering , task-oriented dialogue and chat-oriented dialogue .", "The platform accepts both speech and text as input modalities by either direct microphone/keyboard connections or by means of mobile device wireless connection .", "This demo paper describes our Artificial Intelligent Dialogue Agent ( AIDA ) , a dialogue management and orchestration platform under development at the Institute for Infocomm Research .", "The output interface , which is supported by a talking avatar , integrates speech and text along with other visual aids ."]}
{"orig_sents": ["1", "0", "4", "2", "3"], "shuf_sents": ["Four key scientific issues in the project are : embodiment , interaction paradigm , engagement and relationship .", "We summarize the status of an ongoing project to develop and evaluate a companion for isolated older adults .", "The system supports multiple activities , including discussing the weather , playing cards , telling stories , exercise coaching and video conferencing .", "A live , working demo system will be presented at the meeting .", "The system architecture is extensible and handles realtime behaviors ."]}
{"orig_sents": ["1", "0", "5", "3", "4", "2"], "shuf_sents": ["In contrast to existing mobile applications which treat these problems independently , our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads .", "We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system , question-answering ( QA ) , and geographic information system ( GIS ) technologies .", "of geographical entities .", "The new features include navigation based on visible landmarks , navigation adapted to the user ? s previous route knowledge , and tourist information pushing based on visible and proximal points-of-interest .", "The system also uses social media to infer ? popularity ?", "In this paper , we present the architecture and features of our latest system , extended from an earlier version which was built and evaluated with real users ( Janarthanam et al , 2013 ) ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["The Parlance system for interactive search processes dialogue at a microturn level , displaying dialogue phenomena that play a vital role in human spoken conversation .", "These dialogue phenomena include more natural turn-taking through rapid system responses , generation of backchannels , and user barge-ins .", "The Parlance demonstration system differentiates from other incremental systems in that it is data-driven with an infrastructure that scales well ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["Tools that better support the design , development and improvement of these types of applications are required .", "While natural language as an interaction modality is increasingly being accepted by users , remaining technological challenges still hinder its widespread employment .", "This demo presents a prototyping framework for Spoken Dialog System ( SDS ) design which combines existing language technology components for Automatic Speech Recognition ( ASR ) , Dialog Management ( DM ) , and Text-to-Speech Synthesis ( TTS ) with a multi-step component for Natural Language Understanding ( NLU ) ."]}
{"orig_sents": ["3", "2", "0", "1"], "shuf_sents": ["In this demo we present WebWOZ , a web-based WOZ prototyping platform that aims at supporting a variety of experimental settings and combinations of different language technology components .", "We argue that a generic and distributed platform such as WebWOZ can increase the usefulness of the WOZ method .", "However , software to support WOZ experimentation is often developed ad hoc for specific application scenarios .", "The Wizard of Oz ( WOZ ) method has been used for a variety of purposes in early-stage development of dialogue systems and language technology applications , from data collection , to experimentation , prototyping and evaluation ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["This setup has allowed us to study user reactions to the robot ? s conversational behaviour in order to get a better understanding of how to generate utterances in incremental dialogue systems .", "Furthermore , we show that the user ? s behaviour is affected by how pauses are realised in the robot ? s speech .", "We have analysed the participants ' subjective rating , task completion , verbal responses , gaze behaviour , drawing activity , and cognitive load .", "In this paper , we present a user study where a robot instructs a human on how to draw a route on a map , similar to a Map Task .", "The results show that users utilise the robot ? s gaze in order to disambiguate referring expressions and manage the flow of the interaction ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["We describe our model with an example , then establish that our model works well on nonsituated , telephony application-type utterances , show that it is effective in grounding language in a situated environment , and further show that it can make good use of embodied cues such as gaze and pointing in a fully multi-modal setting .", "In situated dialogue , speakers share time and space .", "We present a statistical model for understanding natural language that works incrementally ( i.e. , in real , shared time ) and is grounded ( i.e. , links to entities in the shared space ) ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["Haptic actions are rarely analyzed as fullfledged components of dialogue , but our data shows haptic actions are used to advance the state of the interaction .", "We report our experiments on recognizing Dialogue Acts in both offline and online modes .", "Our results show that multimodal features and the dialogue game aid in DA classification .", "We describe the annotation of a multimodal corpus that includes pointing gestures and haptic actions ( force exchanges ) ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["We explore the presence of indicators of psychological distress in the linguistic behavior of subjects in a corpus of semistructured virtual human interviews .", "At a more fine-grained level , we show that significant differences can also be found among features that represent subject behavior during specific moments in the dialogues .", "Finally , we present statistical classification results that suggest the potential for automatic assessment of psychological distress in individual interactions with a virtual human dialogue system .", "At the level of aggregate dialogue-level features , we identify several significant differences between subjects with depression and PTSD when compared to nondistressed subjects ."]}
{"orig_sents": ["1", "0", "2", "4", "5", "3"], "shuf_sents": ["In a complex task-oriented domain in which information is ex-changed via parallel , interleaved dialogue and task streams , effective dialogue management models should be able to make dialogue moves based on both the dialogue and the task context .", "Ha , Christopher M. Mitchell , Kristy Elizabeth Boyer , and James C. Lester Department of Computer Science North Carolina State University Raleigh , NC 27695 , USA { eha , cmmitch2 , keboyer , lester } @ ncsu.edu Abstract Learning dialogue management models poses significant challenges .", "This paper presents a data-driven ap-proach to learning dialogue management mod-els that determine when to make dialogue moves to assist users ?", "The results of an evaluation indicate the learned models are effective in predicting both the timing and the type of system dialogue moves .", "task completion activi-ties , as well as the type of dialogue move that should be selected for a given user interaction context .", "Combining features automatically ex-tracted from the dialogue and the task , we compare two alternate modeling approaches ."]}
{"orig_sents": ["1", "0", "3", "2", "4"], "shuf_sents": ["To handle dynamically changing domains , techniques will be needed to transfer and reuse existing dialogue policies and rapidly adapt them using a small number of dialogues in the new domain .", "Existing spoken dialogue systems are typically designed to operate in a static and well-defined domain , and are not well suited to tasks in which the concepts and values change dynamically .", "The paper shows that in the context of Gaussian process POMDP optimisation , a domain can be extended through a simple expansion of the kernel and then rapidly adapted .", "As a first step in this direction , this paper addresses the problem of automatically extending a dialogue system to include a new previously unseen concept ( or slot ) which can be then used as a search constraint in an information query .", "As well as being much quicker , adaptation rather than retraining from scratch is shown to avoid subjecting users to unacceptably poor performance during the learning stage ."]}
{"orig_sents": ["1", "3", "2", "4", "0"], "shuf_sents": ["The results indicate that the trained policy outperformed the hand-coded policy in terms of both subjective ( +18 % ) and objective ( +10.5 % ) task success .", "This paper describes a new approach to automatic learning of strategies for social multi-user human-robot interaction .", "The SSE is modelled as two connected Markov Decision Processes ( MDPs ) with action selection policies that are jointly optimised in interaction with a Multi-User Simulation Environment ( MUSE ) .", "Using the example of a robot bartender that tracks multiple customers , takes their orders , and serves drinks , we propose a model consisting of a Social State Recogniser ( SSR ) which processes audio-visual input and maintains a model of the social state , together with a Social Skills Executor ( SSE ) which takes social state updates from the SSR as input and generates robot responses as output .", "The SSR and SSE have been integrated in the robot bartender system and evaluated with human users in hand-coded and trained SSE policy variants ."]}
{"orig_sents": ["1", "6", "7", "2", "5", "3", "4", "0", "8"], "shuf_sents": ["However , the conversational dialog distracts more from driving than the command-based .", "Due to the mobile Internet revolution , people tend to browse the Web while driving their car which puts the driver ? s safety at risk .", "This paper reports from a very recent driving simulation study and its preliminary results which are conducted in order to compare different speech dialog strategies .", "Different GUIs are designed in order to support the respective dialog strategy the most and to evaluate the effect of the GUI on usability and driver distraction .", "The preliminary results show that the conversational speech dialog performs more efficient than the command-based dialog .", "The use of command-based and conversational SDS prototypes while driving is evaluated on usability and driving performance .", "Therefore , an intuitive and nondistractive in-car speech interface to the Web needs to be developed .", "Before developing a new speech dialog system in a new domain developers have to examine what the user ? s preferred interaction style is in order to use such a system .", "Furthermore , the results indicate that an SDS supported by a GUI is more efficient and better accepted by the user than without GUI ."]}
{"orig_sents": ["4", "0", "3", "6", "2", "8", "5", "7", "1"], "shuf_sents": ["Typically , such systems use a semantic parser to solve this problem .", "Our approach thus helps make a dialog agent more robust to user input and helps reduce number of turns required to detected intended tasks .", "We define task prediction as a classification problem , rather than ? parsing ?", "However , semantic parsers could fail if user utterances contain out-of-grammar words/phrases or if the semantics of uttered phrases did not match the parser ? s expectations .", "Goal-oriented dialog agents are expected to recognize user-intentions from an utterance and execute appropriate tasks .", "Our classifier uses semantic smoothing kernels that can encode information from knowledge bases such as Wordnet , NELL and Freebase.com .", "In this work , we have explored a more robust method of task prediction .", "Our experiments on two spoken language corpora show that augmenting semantic information from these knowledge bases gives about 30 % absolute improvement in task prediction over a parserbased method .", "and use semantic contexts to improve classification accuracy ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["We compare these models with previously proposed models as well as two human-level upper baselines .", "Our results show that the best performing models achieve close to human-level performance and require only surface text dialogue transcripts to train .", "We present virtual human dialogue models which primarily operate on the surface text level and can be extended to incorporate additional information state annotations such as topics or results from simpler models .", "The models are evaluated by collecting appropriateness judgments from human judges for responses generated for a set of fixed dialogue contexts ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["Differences in the time interval between stimulus question and response show that segmental reduction , intensity level , and the shape of the phrase-final rise all function as cues to turn-taking in conversation .", "Thus , the phonetics of turntaking goes beyond the traditional triad of duration , voice quality , and F0 level .", "Based on German production data from the ? Kiel Corpus of Spontaneous Speech ? , we conducted two perception experiments , using an innovative interactive task in which participants gave real oral responses to resynthesized question stimuli ."]}
{"orig_sents": ["0", "4", "2", "3", "1"], "shuf_sents": ["A fundamental problem in manual based gesture semantics reconstruction is the specification of preferred semantic concepts for gesture trajectories .", "This framework provides grounds for a detailed description of how to get at the semantic concept of circularity observed in the data .", "Based on a detailed example of a gesticulated circular trajectory , we present a data-driven approach that covers parts of the semantic reconstruction by making use of motion capturing ( mocap ) technology .", "In our FA3ME framework we use a complex event processing approach to analyse and annotate multi-modal events .", "This issue is complicated by problems human raters have annotating fast-paced three dimensional trajectories ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We have built a simulation framework that uses our incremental speech synthesis component to assemble in a timely manner complex commentary utterances .", "Even in cases where the incremental system overcommits temporally and requires a filled pause to wait for the upcoming event , the system is preferred over the baseline .", "In our evaluation , the resulting output is preferred over that from a baseline system that uses a simpler commenting strategy .", "In many environments ( e. g. sports commentary ) , situations incrementally unfold over time and often the future appearance of a relevant event can be predicted , but not in all its details or precise timing ."]}
{"orig_sents": ["2", "1", "6", "5", "4", "3", "0"], "shuf_sents": ["The results of experiments conducted to evaluate the proposed procedures show evidence of their acceptability and feeling of groupness .", "In three-participant conversations , the minimum unit for multiparty conversations , social imbalance , in which a participant is left behind in the current conversation , sometimes occurs .", "In this paper , we propose a framework for conversational robots that facilitates fourparticipant groups .", "We model and optimize these situations and procedures as a partially observable Markov decision process .", "During the procedures , a facilitator must be aware of both the presence of dominant participants leading the current conversation and the status of any participant that is left behind .", "Consequently , we present model procedures for obtaining conversational initiatives in incremental steps to engage such four-participant conversations .", "In such scenarios , a conversational robot has the potential to facilitate situations as the fourth participant ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We describe the interpersonal linguistic features that our analysis grammar can identify in uttered texts and present an inference procedure that strictly separates the semantic and pragmatic steps of utterance understanding , thereby meeting a higher degree of modularity , a prerequisite for extending robot functionality .", "Keywords : Dialogue System ; Dialogue Act ; Attitude ; Stance", "In this paper , we propose a novel approach to infer dialogue acts using the notion of tacit contracts ."]}
{"orig_sents": ["0", "2", "1", "3"], "shuf_sents": ["This study explores laughter distribution around topic changes in multiparty conversations .", "Shared laughter was significantly more frequent in the 15 seconds leading up to topic change in the informal conversations .", "The distribution of shared and solo laughter around topic changes was examined in corpora containing two types of spoken interaction ; meetings and informal conversation .", "A sample of informal conversations was then analysed by hand to gain further insight into links between laughter and topic change ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["One of our main results is that forum posts using hedges are more likely to get high ratings of their usefulness .", "We explore hedging in web forum conversations , which is interestingly different to hedging in academic articles , the main focus of recent automatic approaches to hedge detection .", "We also make a case for focusing annotation efforts on hedges that take the form of first-person epistemic phrases ."]}
{"orig_sents": ["0", "2", "1"], "shuf_sents": ["A challenge in dialogue act recognition is the mapping from noisy user inputs to dialogue acts .", "We report results based on the Let ? s Go dialogue corpora that show ( 1 ) that including ASR N-best information results in improved dialogue act recognition performance ( +7 % accuracy ) , and ( 2 ) that competitive results can be obtained from as early as the first system dialogue act , reducing the need to wait for subsequent system dialogue acts .", "In this paper we describe an approach for re-ranking dialogue act hypotheses based on Bayesian classifiers that incorporate dialogue history and Automatic Speech Recognition ( ASR ) N-best information ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["To investigate this question , we recorded ( with eye tracking and motion capture ) a corpus of interactions with a ( wizarded ) system .", "We find that with some initial calibration , a ? minimally invasive ? , stationary camera-based setting provides data of sufficient quality to support interaction .", "Can speaker gaze and speaker arm movements be used as a practical information source for naturalistic conversational human ? computer interfaces ?", "In this paper , we describe the recording , analysis infrastructure that we built for such studies , and analysis we performed on these data ."]}
{"orig_sents": ["1", "3", "2", "0"], "shuf_sents": ["Our open-domain conversational dialogue system outperforms retrieval-based conventional systems in chat experiments .", "Even though open-domain conversational dialogue systems are required in many fields , their development is complicated because of the flexibility and variety of user utterances .", "We propose a template-based approach that fills templates with the most salient words in a user utterance and with related words that are extracted using web-scale dependency structures gathered from Twitter .", "To address this flexibility , previous research on conversational dialogue systems has selected system utterances from web articles based on surface cohesion and shallow semantic coherence ; however , the generated utterances sometimes contain irrelevant sentences with respect to the input user utterance ."]}
{"orig_sents": ["4", "2", "3", "0", "1"], "shuf_sents": ["It also presents and evaluates an approach to auto-matically selecting features for an MDP state representation of this dialogue .", "The results suggest that the MDP formulation and the feature selection framework hold promise for learning effective turn-taking policies in task-oriented dialogue systems .", "In task-oriented dia-logue where the user can engage in task ac-tions in parallel with dialogue , unrestricted turn taking may be particularly important for dialogue success .", "This paper presents a novel Markov Decision Process ( MDP ) representa-tion of dialogue with unrestricted turn taking and a parallel task stream in order to automat-ically learn effective turn-taking policies for a tutorial dialogue system from a corpus .", "er M. Mitchell Kristy Elizabeth Boyer James C. Lester Department of Computer Science North Carolina State University Raleigh , NC , USA { cmmitch2 , keboyer , lester } @ ncsu.edu Abstract Learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance ."]}
{"orig_sents": ["3", "6", "5", "2", "4", "7", "8", "0", "1"], "shuf_sents": ["Dividing the ? garbage ?", "class into the set of subclasses by agglomerative hierarchical clustering we achieve about 9 % improvement of accuracy rate on the whole database .", "We have shown that the proposed approach is able to outperform existing methods on a large dataset and do not require morphological and stop-word filtering .", "Natural Language call routing remains a complex and challenging research area in machine intelligence and language understanding .", "In this paper we present a new formula for term relevance estimation , which is a modification of fuzzy rules relevance estimation for fuzzy classifier .", "The focus is on design of algorithm that combines supervised and unsupervised learning models in order to improve classification quality .", "This paper is in the area of classifying user utterances into different categories .", "Using this formula and only 300 frequent words for each class , we achieve an accuracy rate of 85.55 % on the database excluding the ? garbage ?", "class ( it includes utterances that can not be assigned to any useful class or that can be assigned to more than one class ) ."]}
{"orig_sents": ["6", "0", "2", "3", "4", "1", "5"], "shuf_sents": ["Our system interacts with users by recognizing what the users say , predicting the context , and following the users ?", "We used the conditional random field algorithm to extract 5W1H information , and constructed our counseling algorithm using a dialog strategy that was based on counseling techniques .", "feelings .", "For this interaction , our system follows three basic counseling techniques : paraphrasing , asking open questions , and reflecting feelings .", "To follow counseling techniques , we extracted 5W1H information and user emotions from user utterances , and we generated system utterances while using the counseling techniques .", "A total of 16 adults tested our system and rated it with a higher score as an interactive communicator compared with the baseline system .", "In this paper , we introduce our counseling dialog system ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The systems vary in how much they constrain the learner ? s answer : one system places no other constrain on the learner than that provided by the restricted domain and the dialog context ; the other provides the learner with an exercise whose solution is the expected answer .", "We present two dialogue systems for language learning which both restrict the dialog to a specific domain thereby promoting robustness and the learning of a given vocabulary .", "The first system uses supervised learning for simulating a human tutor whilst the second one uses natural language generation techniques to produce grammar exercises which guide the learner toward the expected answer ."]}
{"orig_sents": ["0", "1"], "shuf_sents": ["The demo shows Wikipedia-based opendomain information access dialogues with a talking humanoid robot .", "The robot uses face-tracking , nodding and gesturing to support interaction management and the presentation of information to the partner ."]}
{"orig_sents": ["1", "0"], "shuf_sents": ["We describe the components and the data that we plan to collect using the environment .", "We present a Wizard of Oz ( WoZ ) environment that was designed to build an artificial embodied intelligent tutoring system ( ITS ) that is capable of empathic conversations with school pupils aged between 10-13 ."]}
{"orig_sents": ["0", "1", "2", "3"], "shuf_sents": ["The demonstrator presents a test-bed for collecting data on human ? computer dialogue : a fully automated dialogue system that can perform Map Task with a user .", "In a first step , we have used the test-bed to collect human ? computer Map Task dialogue data , and have trained various data-driven models on it for detecting feedback response locations in the user ? s speech .", "One of the trained models has been tested in user interactions and was perceived better in comparison to a system using a random model .", "The demonstrator will exhibit three versions of the Map Task dialogue system ? each using a different trained data-driven model of Response Location Detection ."]}
{"orig_sents": ["3", "0", "2", "4", "1"], "shuf_sents": ["Such an agent needs to select actions based on not only instructions but also situations .", "It can be used as an experimental system for collecting human-robot interactions in dynamically changing situations .", "It is also expected to immediately react to the instructions .", "We demonstrate a robotic agent in a 3D virtual environment that understands human navigational instructions .", "Our agent incrementally understands spoken instructions and immediately controls a mobile robot based on the incremental understanding results and situation information such as the locations of obstacles and moving history ."]}
{"orig_sents": ["1", "0", "2"], "shuf_sents": ["The tool , called Roundtable , em-powers many different types of authors and varying team sizes to create flexible interac-tions by automating many editing workflows while limiting complexity and hiding architec-tural concerns .", "We present an online system that provides a complete web-based sandbox for creating , testing and publishing embodied conversation-al agents .", "Finished characters can be pub-lished directly to web servers , enabling highly interactive applications ."]}
{"orig_sents": ["0", "1", "3", "2"], "shuf_sents": ["We present a data-driven model for detecting suitable response locations in the user ? s speech .", "The model has been trained on human ? machine dialogue data and implemented and tested in a spoken dialogue system that can perform the Map Task with users .", "A subjective evaluation of the dialogue system suggests that interactions with a system using our trained model were perceived significantly better than those with a system using a model that made decisions at random .", "To our knowledge , this is the first example of a dialogue system that uses automatically extracted syntactic , prosodic and contextual features for online detection of response locations ."]}
{"orig_sents": ["1", "5", "4", "3", "0", "2"], "shuf_sents": ["This model has greater task success and efficiency than the standard approach when evaluated in a public spoken dialogue system .", "Barge-in enables the user to provide input during system speech , facilitating a more natural and efficient interaction .", "Index Terms : spoken dialogue systems , barge-in", "We propose and evaluate a barge-in processing method that uses a prediction strategy to continuously decide whether to pause , continue , or resume the prompt .", "Unfortunately , this approach performs poorly when used in challenging environments .", "Standard methods generally focus on singlestage barge-in detection , applying the dialogue policy irrespective of the barge-in context ."]}
{"orig_sents": ["4", "3", "5", "0", "6", "1", "2"], "shuf_sents": ["We then present speech recognition results for six different dialogue systems .", "We also show that there is an improvement over a previous generation of recognizers on some of these data sets .", "We also investigate language understanding ( NLU ) on the ASR output , and explore the relationship between ASR and NLU performance .", "We focus in particular on cloud based ASRs that recently have become available to the community .", "We present an analysis of several publicly available automatic speech recognizers ( ASRs ) in terms of their suitability for use in different types of dialogue systems .", "We include features of ASR systems and desiderata and requirements for different dialogue systems , taking into account the dialogue genre , type of user , and other features .", "The most interesting result is that different ASR systems perform best on the data sets ."]}
{"orig_sents": ["4", "0", "2", "1", "3"], "shuf_sents": ["Few studies , however , have reported the strengths and weaknesses of each method .", "In this paper , we present a set of techniques that build a robust dialog state tracker with high performance : wide-coverage and well-calibrated data selection , feature-rich discriminative model design , generalization improvement techniques and unsupervised prior adaptation .", "The Dialog State Tracking Challenge ( DSTC ) is designed to address this issue by comparing various methods on the same domain .", "The DSTC results show that the proposed method is superior to other systems on average on both the development and test datasets .", "For robust spoken conversational interaction , many dialog state tracking algorithms have been developed ."]}
{"orig_sents": ["4", "5", "3", "2", "0", "1"], "shuf_sents": ["The proposed method is evaluated in the Dialog State Tracking Challenge , where it achieves comparable performance in hypothesis accuracy to machine learning based systems .", "Consequently , with respect to different scenarios for the belief tracking problem , the potential superiority and weakness of machine learning approaches in general are investigated .", "The core insight is to maximise the amount of information directly gainable from an errorprone dialogue itself , so as to better lowerbound one ? s expectations on the performance of more advanced statistical techniques for the task .", "without requiring training data ) .", "This paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domainindependent rules , using basic probability operations .", "The rules apply to observed system actions and partially observable user acts , without using any knowledge obtained from external resources ( i.e ."]}
{"orig_sents": ["3", "1", "0", "6", "4", "5", "7", "2"], "shuf_sents": ["However , often there is a large corpus of mis-matched but related data ?", "When training a dialog state tracker , there is typically only a small corpus of well-matched dialog data available .", "Our main result is the finding that a simple method for multi-domain learning substantially improves performance in highly mis-matched conditions .", "Statistical approaches to dialog state tracking synthesize information across multiple turns in the dialog , overcoming some speech recognition errors .", "It would be desirable to use this related dialog data to supplement the small corpus of well-matched dialog data .", "This paper addresses this task as multi-domain learning , presenting 3 methods which synthesize data from different slots and different dialog systems .", "perhaps pertaining to different semantic concepts , or from a different dialog system .", "Since deploying a new dialog state tracker often changes the resulting dialogs in ways that are difficult to predict , we study how well each method generalizes to unseen distributions of dialog data ."]}
{"orig_sents": ["6", "2", "5", "4", "0", "1", "3"], "shuf_sents": ["It also provides a flexible mechanism for imposing relational constraints .", "To verify the effectiveness of the proposed method , we applied it to the Let ? s Go domain ( Raux et al , 2005 ) .", "Recent analyses , however , raised fundamental questions on the effectiveness of the generative formulation .", "The results show that the proposed model is superior to the baseline and generative model-based systems in accuracy , discrimination , and robustness to mismatches between training and test datasets .", "Unlike generative models , the proposed method affords the incorporation of features without having to consider dependencies between observations .", "In this paper , we present a structured discriminative model for dialog state tracking as an alternative .", "Many dialog state tracking algorithms have been limited to generative modeling due to the influence of the Partially Observable Markov Decision Process framework ."]}
{"orig_sents": ["3", "1", "0", "2"], "shuf_sents": ["Second , we present a generative model employing a simple dependency structure to achieve fast inference .", "First , we detail a novel discriminative dialogue state tracker which directly estimates slot-level beliefs using deterministic state transition probability distribution .", "The models are evaluated on the DSTC data , and both significantly outperform the baseline DSTC tracker .", "In this paper , we describe two dialogue state tracking models competing in the 2012 Dialogue State Tracking Challenge ( DSTC ) ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["In our approach we use discriminative general structured conditional random fields , instead of traditional generative directed graphic models , to incorporate arbitrary overlapping features .", "Our approach outperforms the simple 1-best tracking approach .", "This paper presents our approach to dialog state tracking for the Dialog State Tracking Challenge task ."]}
{"orig_sents": ["3", "4", "2", "0", "1"], "shuf_sents": ["We also report experimental results on a number of approaches to the models , and compare the overall performance of our tracker to other submitted trackers .", "An extended version of this paper is available as a technical report ( Kim et al , 2013 ) .", "We explain the main parts of our tracker : the observation model , the belief refinement model , and the belief transformation model .", "We describe our experience with engineering the dialog state tracker for the first Dialog State Tracking Challenge ( DSTC ) .", "Dialog trackers are one of the essential components of dialog systems which are used to infer the true user goal from the speech processing results ."]}
{"orig_sents": ["4", "3", "0", "1", "2"], "shuf_sents": ["Recent success in using deep learning for speech research motivates the Deep Neural Network approach presented here .", "The model parameters can be learnt by directly maximising the likelihood of the training data .", "The paper explores some aspects of the training , and the resulting tracker is found to perform competitively , particularly on a corpus of dialogs from a system not found in the training .", "The Dialogue State Tracking Challenge has allowed for such an analysis , comparing multiple belief tracking approaches on a shared task .", "While belief tracking is known to be important in allowing statistical dialog systems to manage dialogs in a highly robust manner , until recently little attention has been given to analysing the behaviour of belief tracking techniques ."]}
{"orig_sents": ["2", "3", "1", "0", "4", "5"], "shuf_sents": ["and compare results for both .", "one graph-based and one transition-based ?", "We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences .", "We take two popular dependency parsers ?", "Results show that using semisupervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy .", "We also try to use morphological information in a targeted way and fail to see any improvements ."]}
{"orig_sents": ["3", "2", "1", "0"], "shuf_sents": ["As expected , CASE is the single most important morphological feature , but virtually all available features bring some improvement , especially under the gold condition .", "We investigate the usefulness of different features and find that rich morphological features improve parsing accuracy significantly , by 7.5 percentage points with gold features and 5.6 points with predicted features .", "Using a greedy transition-based parser , we obtain a labeled attachment score of 74.7 with gold morphology and 68.1 with predicted morphology ( 77.8 and 72.8 unlabeled ) .", "We present the first statistical dependency parsing results for Lithuanian , a morphologically rich language in the Baltic branch of the Indo-European family ."]}
{"orig_sents": ["0", "4", "5", "1", "3", "2"], "shuf_sents": ["We investigate statistical dependency parsing of two closely related languages , Croatian and Serbian .", "we make use of the two available dependency treebanks of Croatian to produce state-of-the-art parsing models for both languages .", "We give insight into overall parser performance for Croatian and Serbian , impact of preprocessing for lemmas and morphosyntactic tags and influence of selected morphosyntactic features on parsing accuracy .", "We observe parsing accuracy on four test sets from two domains .", "As these two morphologically complex languages of relaxed word order are generally under-resourced ?", "with the topic of dependency parsing still largely unaddressed , especially for Serbian ?"]}
{"orig_sents": ["2", "1", "4", "3", "0"], "shuf_sents": ["We present the details of our general framework , our Arabic CTF-TM , and the setup and results of our experiments .", "NLP pipelines often suffer from error propagation , as errors committed in lower-level tasks cascade through the remainder of the processing pipeline .", "This paper describes cross-task flexible transition models ( CTF-TMs ) and demonstrates their effectiveness for Arabic natural language processing ( NLP ) .", "Our Arabic CTF-TM models tokenization , affix detection , affix labeling , partof-speech tagging , and dependency parsing , achieving state-of-the-art results .", "By allowing a flexible order of operations across and within multiple NLP tasks , a CTF-TM can mitigate both cross-task and within-task error propagation ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["This paper describes the LIGM-Alpage system for the SPMRL 2013 Shared Task .", "While the realistic scenario of predicting both MWEs and syntax has already been investigated for constituency parsing , the SPMRL 2013 shared task datasets offer the possibility to investigate it in the dependency framework .", "We obtain the best results for French , both for overall parsing and for MWE recognition , using a reparsing architecture that combines several parsers , with both pipeline architecture ( MWE recognition followed by parsing ) , and joint architecture ( MWE recognition performed by the parser ) .", "We only participated to the French part of the dependency parsing track , focusing on the realistic setting where the system is informed neither with gold tagging and morphology nor ( more importantly ) with gold grouping of tokens into multi-word expressions ( MWEs ) ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["The parser was also extended to handle ambiguous word lattices , with almost no loss w.r.t .", "The SPMRL 2013 shared task was the opportunity to develop and test , with promising results , a simple beam-based shift-reduce dependency parser on top of the tabular logic programming system DYALOG .", "We believe that this result is an interesting new one for shift-reduce parsing .", "disambiguated input , thanks to specific training , use of oracle segmentation , and large beams ."]}
{"orig_sents": ["0", "3", "1", "2"], "shuf_sents": ["The inclusion of morphological features provides very useful information that helps to enhance the results when parsing morphologically rich languages .", "In this paper , we present an extension of MaltOptimizer that explores , one by one and in combination , the features that are geared towards morphology .", "From our experiments in the context of the Shared Task on Parsing Morphologically Rich Languages , we extract an in-depth study that shows which features are actually useful for transition-based parsing and we provide competitive results , in a fast and simple way .", "MaltOptimizer is a tool , that given a data set , searches for the optimal parameters , parsing algorithm and optimal feature set achieving the best results that it can find for parsers trained with MaltParser ."]}
{"orig_sents": ["1", "2", "0"], "shuf_sents": ["Finally , the system will combine previously achieved parses using a voting approach .", "This paper presents a dependency parsing system , presented as BASQUE TEAM at the SPMRL ? 2013 Shared Task , based on the analysis of each morphological feature of the languages .", "Once the specific relevance of each morphological feature is calculated , this system uses the most significant of them to create a series of analyzers using two freely available and state of the art dependency parsers , MaltParser and Mate ."]}
{"orig_sents": ["0", "1", "5", "2", "4", "3"], "shuf_sents": ["We propose the use of the word categories and embeddings induced from raw text as auxiliary features in dependency parsing .", "To induce word features , we make use of contextual , morphologic and orthographic properties of the words .", "We generate morphologic and orthographic properties of word types in an unsupervised manner .", "The AI-KU system shows improvements for some of the languages it is trained on for the first Shared Task of Statistical Parsing of Morphologically Rich Languages .", "We use a co-occurrence model with these properties to embed words onto a 25dimensional unit sphere .", "To exploit the contextual information , we make use of substitute words , the most likely substitutes for target words , generated by using a statistical language model ."]}
{"orig_sents": ["2", "0", "1"], "shuf_sents": ["We participate in the Arabic Dependency parsing task for predicted POS tags and features .", "Our system is based on Marton et al ( 2013 ) .", "We describe the submission from the Columbia Arabic & Dialect Modeling group ( CADIM ) for the Shared Task at the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages ( SPMRL ? 2013 ) ."]}
{"orig_sents": ["0", "3", "4", "2", "1"], "shuf_sents": ["In this paper we use statistical dependency parsing techniques to detect NULL or Empty categories in the Hindi sentences .", "We have also discussed about shortcomings and difficulties in this approach and evaluated the performance of this approach on different Empty categories .", "In this approach we used a technique of introducing complex labels into the data to predict Empty categories in sentences .", "We have currently worked on Hindi dependency treebank which is released as part of COLINGMTPIL 2012 Workshop .", "Earlier Rule based approaches are employed to detect Empty heads for Hindi language but statistical learning for automatic prediction is not explored ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["We explore two state-of-the-art parsers , namely MSTParser andMaltParser , on the recently released Persian dependency treebank and establish some baselines for dependency parsing performance .", "Three sets of issues are addressed in our experiments : effects of using gold and automatically derived features , finding the best features for the parser , and a suitable way to alleviate the data sparsity problem .", "The final accuracy is 87.91 % and 88.37 % labeled attachment scores for MaltParser and MSTParser , respectively .", "This paper investigates the impact of different morphological and lexical information on data-driven dependency parsing of Persian , a morphologically rich language ."]}
{"orig_sents": ["1", "0", "3", "2"], "shuf_sents": ["Japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks , however , such expression is insufficient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure , which is required for practical applications such as syntactic reordering of machine translation .", "We present an empirical study on constructing a Japanese constituent parser , which can output function labels to deal with more detailed syntactic information .", "The evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers , and can output such function labels as the grammatical role of the argument and the type of adnominal phrases .", "We describe a preliminary effort on constructing a Japanese constituent parser by a Penn Treebank style treebank semi-automatically made from a dependency-based corpus ."]}
{"orig_sents": ["0", "1", "2"], "shuf_sents": ["This paper revisits the work of ( Malladi and Mannem , 2013 ) which focused on building a Statistical Morphological Analyzer ( SMA ) for Hindi and compares the performance of SMA with other existing statistical analyzer , Morfette .", "We shall evaluate SMA in various experiment scenarios and look at how it performs for unseen words .", "The later part of the paper presents the effect of the predicted morph features on dependency parsing and extends the work to other morphologically rich languages : Hindi and Telugu , without any language-specific engineering ."]}
{"orig_sents": ["2", "0", "3", "1"], "shuf_sents": ["We aim for this new framework to accommodate the highly agglutinative morphology of Turkish as well as to allow the annotation of unedited web data , and shape our decisions around these considerations .", "Secondly , we investigate alternative annotation schemes for coordination structures and present a better scheme ( nearly 11 % increase in recall scores ) than the one in Turkish Treebank ( Oflazer et al , 2003 ) for both parsing accuracies and compatibility for colloquial language .", "This paper presents our preliminary conclusions as part of an ongoing effort to construct a new dependency representation framework for Turkish .", "In this paper , we firstly describe a novel syntactic representation for morphosyntactic sub-word units ( namely inflectional groups ( IGs ) in Turkish ) which allows inter-IG relations to be discerned with perfect accuracy without having to hide lexical information ."]}
{"orig_sents": ["1", "3", "0", "2"], "shuf_sents": ["For both tracks we make significant improvements through high quality preprocessing and ( re ) ranking on top of strong baselines .", "This paper describes the IMS-SZEGED-CIS contribution to the SPMRL 2013 Shared Task .", "Our system came out first for both tracks .", "We participate in both the constituency and dependency tracks , and achieve state-of-theart for all languages ."]}
{"orig_sents": ["2", "3", "0", "1"], "shuf_sents": ["We report on the preparation of the data sets , on the proposed parsing scenarios , and on the evaluation metrics for parsing MRLs given different representation types .", "We present and analyze parsing results obtained by the task participants , and then provide an analysis and comparison of the parsers across languages and frameworks , reported for gold input as well as more realistic parsing scenarios .", "This paper reports on the first shared task on statistical parsing of morphologically rich languages ( MRLs ) .", "The task features data sets from nine languages , each available both in constituency and dependency annotation ."]}
{"orig_sents": ["0", "4", "1", "3", "2"], "shuf_sents": ["Traditional information retrieval models assume keyword-based queries and use unstructured document representations .", "We present a novel retrieval model that uses a structured event-based representation .", "Experimental results on two event-oriented test collections show significant improvements over state-ofthe-art keyword-based models .", "We structure queries and documents as graphs of event mentions and employ graph kernels to measure the query-document similarity .", "There is an abundance of event-centered texts ( e.g. , breaking news ) and event-oriented information needs that often involve structure that can not be expressed using keywords ."]}
{"orig_sents": ["1", "4", "2", "3", "0"], "shuf_sents": ["The visualization can be used as a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering , and is provided as an open source tool .", "We introduce an interactive visualization component for the JoBimText project .", "First we describe the underlying technology for computing a distributional thesaurus on words using bipartite graphs of words and context features , and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking .", "Then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization .", "JoBimText is an open source platform for large-scale distributional semantics based on graph representations ."]}
{"orig_sents": ["3", "0", "2", "1"], "shuf_sents": ["We present a semi-supervised approach to learn similarity between WordNet synsets using a graph based recursive similarity definition .", "Finally we discuss our method to derive coarse sense inventories at arbitrary granularities and show that the coarse-grained sense inventory obtained significantly boosts the disambiguation of nouns on standard test sets .", "We seed our framework with sense similarities of all the word-sense pairs , learnt using supervision on humanlabelled sense clusterings .", "WordNet , a widely used sense inventory for Word Sense Disambiguation ( WSD ) , is often too fine-grained for many Natural Language applications because of its narrow sense distinctions ."]}
{"orig_sents": ["5", "4", "3", "1", "2", "0"], "shuf_sents": ["Experiments show that our algorithm can handle the high-dimensional big data and outperform competing approximations in both domains .", "We show that the new objective function is convex and can be efficiently optimized by a stochastic-batch subgradient descent method .", "We applied our algorithm to two different domains ; semantic similarity of documents collected from the Web , and phenotype descriptions in genomic data .", "Our novel contributions include random projection that reduces dimensionality and a new objective function that regularizes intra-class and inter-class distances to handle a large number of classes .", "This paper presents algorithms to scale up learning of a Mahalanobis distance metric from a large data graph in a high dimensional space .", "Distance metric learning from high ( thousands or more ) dimensional data with hundreds or thousands of classes is intractable but in NLP and IR , high dimensionality is usually required to represent data points , such as in modeling semantic similarity ."]}
{"orig_sents": ["0", "3", "2", "1"], "shuf_sents": ["In this work , we propose a graph-based approach to computing similarities between words in an unsupervised manner , and take advantage of heterogeneous feature types in the process .", "High quality graphs are learned during training , and the proposed method outperforms experimental baselines .", "The graphs are connected through edges that link nodes in the feature graph to nodes in the word graph , the edge weights representing the importance of a particular feature for a particular word .", "The approach is based on the creation of two separate graphs , one for words and one for features of different types ( alignmentbased , orthographic , etc . ) ."]}
{"orig_sents": ["3", "0", "1", "2"], "shuf_sents": ["The framework is instantiated by the definition of term and context , which we derive from dependency parses in this work .", "Evaluating our approach on a standard data set for lexical substitution , we show substantial improvements over a strong non-contextualized baseline across all parts of speech .", "In contrast to comparable approaches , our framework defines an unsupervised generative method for similarity in context and does not rely on the existence of lexical resources as a source for candidate expansions .", "After recasting the computation of a distributional thesaurus in a graph-based framework for term similarity , we introduce a new contextualization method that generates , for each term occurrence in a text , a ranked list of terms that are semantically similar and compatible with the given context ."]}
{"orig_sents": ["5", "6", "4", "2", "8", "10", "9", "3", "7", "0", "1"], "shuf_sents": ["We propose a method called expected model rotation ( EMR ) that works well on not well-separated data which frequently occur as realistic data .", "Experimental results show that EMR can select seed sets that provide significantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets .", "The selected seed set affects accuracy , but how to select a good seed set is not yet clear .", "and labels the unlabeled instance in the seed set .", "set .", "Bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost .", "In bootstrapping , unlabeled instances can be harvested from the initial labeled ? seed ?", "Our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem .", "Thus , an ? iterative seeding ?", "Our framework iteratively selects the unlabeled instance that has the best ? goodness of seed ?", "framework is proposed for bootstrapping to reduce its labeling cost ."]}
{"orig_sents": ["1", "3", "0", "4", "2"], "shuf_sents": ["We use a word order graph , whose vertices , edges and double edges help determine structure-based match across texts .", "Review quality is determined by identifying the relevance of a review to a submission ( the article or paper the review was written for ) .", "Ours is a lexico-semantic approach , which predicts relevance with an accuracy of 66 % and f -measure of 0.67 .", "We identify relevance in terms of the semantic and syntactic similarities between two texts .", "We use WordNet to determine semantic relatedness ."]}
{"orig_sents": ["2", "4", "3", "6", "0", "5", "1"], "shuf_sents": ["For this purpose , we developed a graph-based text representation that makes the relations between textual units explicit .", "When applied to aviation investigation reports , our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes .", "Many organizations possess large collections of textual reports that document how a problem is solved or analysed , e.g .", "Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization .", "medical patient records , industrial accident reports , lawsuit records and investigation reports .", "This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers .", "In this article , we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand ."]}
{"orig_sents": ["0", "5", "3", "4", "1", "2", "7", "6"], "shuf_sents": ["Entity Resolution is the task of identifying which records in a database refer to the same entity .", "If a pair scores above a user-defined threshold , the records are presumed to represent the same entity .", "Finally , the clustering step turns the input records into clusters of records ( or profiles ) , where each cluster is uniquely associated with a single real-world entity .", "The blocking step groups records by shared properties to determine which pairs of records should be examined by the pairwise linker as potential duplicates .", "Next , the linkage step assigns a probability score to pairs of records inside each block .", "A standard machine learning pipeline for the entity resolution problem consists of three major components : blocking , pairwise linkage , and clustering .", "We demonstrate the viability of these algorithms for large data sets on a 50-node hadoop cluster .", "This paper describes the blocking and clustering strategies used to deploy a massive database of organization entities to power a major commercial People Search Engine ."]}
{"orig_sents": ["0", "3", "1", "4", "2"], "shuf_sents": ["This paper presents a system that performs skill extraction from text documents .", "We argue that the system can be practical for hiring and management of personnel in an organization .", "The system is based on first computing similarities between an input document and the texts of Wikipedia pages and then using a biased , hub-avoiding version of the Spreading Activation algorithm on the Wikipedia graph in order to associate the input document with skills .", "It outputs a list of professional skills that are relevant to a given input text .", "We make use of the texts and the hyperlink graph of Wikipedia , as well as a list of professional skills obtained from the LinkedIn social network ."]}
