extracting biomedical events from literature has attracted much recent attention. <eos> the bestperforming systems so far have been pipelines of simple subtask-specific local classifiers. <eos> a natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. <eos> we present three joint models of increasing complexity designed to overcome this problem. <eos> the first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. <eos> the second model captures correlations between events, while the third model ensures consistency between arguments of the same event. <eos> inference in these models is kept tractable through dual decomposition. <eos> the first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart. <eos> the third model yields the best results reported so far on the bionlp 2009 shared task, the bionlp 2011 genia task and the bionlp 2011 infectious diseases task.
online discussion forums are a valuable means for users to resolve specific information needs, both interactively for the participants and statically for users who search/browse over historical thread data. <eos> however, the complex structure of forum threads can make it difficult for users to extract relevant information. <eos> the discourse structure of web forum threads, in the form of labelled dependency relationships between posts, has the potential to greatly improve information access over web forum archives. <eos> in this paper, we present the task of parsing user forum threads to determine the labelled dependencies between posts. <eos> three methods, including a dependency parsing approach, are proposed to jointly classify the links ( relationships ) between posts and the dialogue act ( type ) of each link. <eos> the proposed methods significantly surpass an informed baseline. <eos> we also experiment with ? in situ ? <eos> classification of evolving threads, and establish that our best methods are able to perform equivalently well over partial threads as complete threads.
this paper describes an algorithm for exact decoding of phrase-based translation models, based on lagrangian relaxation. <eos> the method recovers exact solutions, with certificates of optimality, on over 99 % of test examples. <eos> the method is much more efficient than approaches based on linear programming ( lp ) or integer linear programming ( ilp ) solvers : these methods are not feasible for anything other than short sentences. <eos> we compare our method to moses ( koehn et al, 2007 ), and give precise estimates of the number and magnitude of search errors that moses makes.
minimum error rate training is a crucial component to many state-of-the-art nlp applications, such as machine translation and speech recognition. <eos> however, common evaluation functions such as bleu or word error rate are generally highly non-convex and thus prone to search errors. <eos> in this paper, we present lp-mert, an exact search algorithm for minimum error rate training that reaches the global optimum using a series of reductions to linear programming. <eos> given a set of n -best lists produced from s input sentences, this algorithm finds a linear model that is globally optimal with respect to this set. <eos> we find that this algorithm is polynomial in n and in the size of the model, but exponential in s. we present extensions of this work that let us scale to reasonably large tuning sets ( e.g., one thousand sentences ), by either searching only promising regions of the parameter space, or by using a variant of lp-mert that relies on a beam-search approximation. <eos> experimental results show improvements over the standard och algorithm.
we describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. <eos> our approach is based on a model that locally mixes between supervised models from the helper languages. <eos> parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. <eos> we obtain state-of-theart performance for two tasks of structure prediction : unsupervised part-of-speech tagging and unsupervised dependency parsing.
we present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. <eos> we first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. <eos> we then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. <eos> unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. <eos> the projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
we propose a method to improve the accuracy of parsing bilingual texts ( bitexts ) with the help of statistical machine translation ( smt ) systems. <eos> previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. <eos> instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. <eos> however, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. <eos> to overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. <eos> the experimental results show that our new parsers significantly outperform state-of-theart baselines. <eos> moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. <eos> especially notable is that our approach can be used in a purely monolingual setting with the help of smt.
we present a novel approach to data-oriented parsing ( dop ). <eos> like other dop models, our parser utilizes syntactic fragments of arbitrary size from a treebank to analyze new sentences, but, crucially, it uses only those which are encountered at least twice. <eos> this criterion allows us to work with a relatively small but representative set of fragments, which can be employed as the symbolic backbone of several probabilistic generative models. <eos> for parsing we define a transform-backtransform approach that allows us to use standard pcfg technology, making our results easily replicable. <eos> according to standard parseval metrics, our best model is on par with many state-ofthe-art parsers, while offering some complementary benefits : a simple generative probability model, and an explicit representation of the larger units of grammar.
we present a method that paraphrases a given sentence by first generating candidate paraphrases and then ranking ( or classifying ) them. <eos> the candidates are generated by applying existing paraphrasing rules extracted from parallel corpora. <eos> the ranking component considers not only the overall quality of the rules that produced each candidate, but also the extent to which they preserve grammaticality and meaning in the particular context of the input sentence, as well as the degree to which the candidate differs from the input. <eos> we experimented with both a maximum entropy classifier and an svr ranker. <eos> experimental results show that incorporating features from an existing paraphrase recognizer in the ranking component improves performance, and that our overall method compares well against a state of the art paraphrase generator, when paraphrasing rules apply to the input sentences. <eos> we also propose a new methodology to evaluate the ranking components of generate-and-rank paraphrase generators, which evaluates them across different combinations of weights for grammaticality, meaning preservation, and diversity. <eos> the paper is accompanied by a paraphrasing dataset we constructed for evaluations of this kind.
we present a novel approach for automatic collocation error correction in learner english which is based on paraphrases extracted from parallel corpora. <eos> our key assumption is that collocation errors are often caused by semantic similarity in the first language ( l1language ) of the writer. <eos> an analysis of a large corpus of annotated learner english confirms this assumption. <eos> we evaluate our approach on real-world learner data and show that l1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and wordnet synonyms.
class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. <eos> yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. <eos> we propose two graph-theoretic methods ( centrality and regularization ), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. <eos> we carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. <eos> we conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.
this paper presents a model that extends semantic role labeling. <eos> existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. <eos> however, sentences express relations via other linguistic phenomena as well. <eos> furthermore, these phenomena interact with each other, thus restricting the structures they articulate. <eos> in this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. <eos> the scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. <eos> the key strength of our model is that we use existing structure predictors as black boxes. <eos> by enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.
this paper presents a domain-assisted approach to organize various aspects of a product into a hierarchy by integrating domain knowledge ( e.g., the product specifications ), as well as consumer reviews. <eos> based on the derived hierarchy, we generate a hierarchical organization of consumer reviews on various product aspects and aggregate consumer opinions on these aspects. <eos> with such organization, user can easily grasp the overview of consumer reviews. <eos> furthermore, we apply the hierarchy to the task of implicit aspect identification which aims to infer implicit aspects of the reviews that do not explicitly express those aspects but actually comment on them. <eos> the experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach.
we introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. <eos> our method learns vector space representations for multi-word phrases. <eos> in sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. <eos> we also evaluate the model ? s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. <eos> the dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. <eos> our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.
polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. <eos> this paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. <eos> firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on rhetorical structure theory ( rst ). <eos> then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations ( ssrs ). <eos> finally, an unsupervised method was adopted to generate, weigh and filter new ssrs without cue phrases for recognizing discourse relations. <eos> experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.
we present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. <eos> thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. <eos> for example, combining an adverb ( e.g., ? very ? ) <eos> with a positive polar adjective ( e.g., ? good ? ) <eos> produces a phrase ( ? very good ? ) <eos> with increased polarity over the adjective alone. <eos> inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. <eos> although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition ( rudolph and giesbrecht, 2010 ), training such models has to be done carefully : the optimization is nonconvex and requires a good initial starting point. <eos> this paper presents the first such algorithm for learning a matrix-space model for semantic composition. <eos> in the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.
we propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. <eos> we apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. <eos> we use a corpus of weakly-labeled reference reorderings to guide parser training. <eos> our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.
when translating among languages that differ substantially in word order, machine translation ( mt ) systems benefit from syntactic preordering ? an approach that uses features from a syntactic parse to permute source words into a target-language-like order. <eos> this paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. <eos> these induced parses are used to preorder source sentences. <eos> we demonstrate that our induced parser is effective : it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. <eos> these results show that the syntactic structure which is relevant to mt pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.
due to its explicit modeling of the grammaticality of the output via target-side syntax, the string-to-tree model has been shown to be one of the most successful syntax-based translation models. <eos> however, a major limitation of this model is that it does not utilize any useful syntactic information on the source side. <eos> in this paper, we analyze the difficulties of incorporating source syntax in a string-totree model. <eos> we then propose a new way to use the source syntax in a fuzzy manner, both in source syntactic annotation and in rule matching. <eos> we further explore three algorithms in rule matching : 0-1 matching, likelihood matching, and deep similarity matching. <eos> our method not only guarantees grammatical output with an explicit target tree, but also enables the system to choose the proper translation rules via fuzzy use of the source syntax. <eos> our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system.
dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. <eos> however, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. <eos> in our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. <eos> the head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. <eos> large-scale experiments show that our model performs well on long distance reordering, and outperforms the stateof-the-art constituency-to-string model ( +1.47 bleu on average ) and hierarchical phrasebased model ( +0.46 bleu on average ) on two chinese-english nist test sets without resort to phrases or parse forest. <eos> for the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.
real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them ? <eos> we present a bayesian method for measuring how well a topic model fits a corpus. <eos> our approach is based on posterior predictive checking, a method for diagnosing bayesian models in user-defined ways. <eos> our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.
dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. <eos> however, in cases where lightweight decompositions are not readily available ( e.g., due to the presence of rich features or logical constraints ), the original subgradient algorithm is inefficient. <eos> we sidestep that difficulty by adopting an augmented lagrangian method that accelerates model consensus by regularizing towards the averaged votes. <eos> we show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.
we exploit sketch techniques, especially the count-min sketch, a memory, and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself. <eos> these methods use hashing to deal with massive amounts of streaming text. <eos> we apply countmin sketch to approximate word pair counts and exhibit their effectiveness on three important nlp tasks. <eos> our experiments demonstrate that on all of the three tasks, we get performance comparable to exact word pair counts setting and state-of-the-art system. <eos> our method scales to 49 gb of unzipped web data using bounded space of 2 billion counters ( 8 gb memory ).
latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. <eos> in order for people to use such models, however, they must trust them. <eos> unfortunately, typical dimensionality reduction methods for text, such as latent dirichlet al location, often produce low-dimensional subspaces ( topics ) that are obviously flawed to human domain experts. <eos> the contributions of this paper are threefold : ( 1 ) an analysis of the ways in which topics can be flawed ; ( 2 ) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data ; ( 3 ) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the national institutes of health ( nih ).
argumentative zoning ( az ) ? <eos> analysis of the argumentative structure of a scientific paper ? <eos> has proved useful for a number of information access tasks. <eos> current approaches to az rely on supervised machine learning ( ml ). <eos> requiring large amounts of annotated data, these approaches are expensive to develop and port to different domains and tasks. <eos> a potential solution to this problem is to use weaklysupervised ml instead. <eos> we investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple az classes. <eos> our best classifier based on the combination of active learning and selftraining outperforms our best supervised classifier, yielding a high accuracy of 81 % when using just 10 % of the labeled data. <eos> this result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of az across different information access tasks.
this paper presents a new algorithm for linear text segmentation. <eos> it is an adaptation of affinity propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. <eos> affinity propagation for segmentation, or aps, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres ? <eos> data points which best describe all other data points within the segment. <eos> aps iteratively passes messages in a cyclic factor graph, until convergence. <eos> each iteration works with information on all available similarities, resulting in highquality results. <eos> aps scales linearly for realistic segmentation tasks. <eos> we derive the algorithm from the original affinity propagation formulation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. <eos> the results suggest that aps performs on par with or outperforms these two very competitive baselines.
this paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. <eos> while it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. <eos> we show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality.
this paper introduces a psycholinguistic model of sentence processing which combines a hidden markov model noun phrase chunker with a co-reference classifier. <eos> both models are fully incremental and generative, giving probabilities of lexical elements conditional upon linguistic structure. <eos> this allows us to compute the information theoretic measure of surprisal, which is known to correlate with human processing effort. <eos> we evaluate our surprisal predictions on the dundee corpus of eye-movement data show that our model achieve a better fit with human reading times than a syntax-only model which does not have access to co-reference information.
we present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. <eos> the objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. <eos> we introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. <eos> our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.
in this paper, we consider the problem of unsupervised morphological analysis from a new angle. <eos> past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand. <eos> we propose instead to treat morphological analysis as a structured prediction problem, where languages with labeled data serve as training examples for unlabeled languages, without the assumption of parallel data. <eos> we define a universal morphological feature space in which every language and its morphological analysis reside. <eos> we develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language. <eos> we apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, mdlbased approach. <eos> our analysis indicates that accuracy continues to improve substantially as the number of training languages increases.
log-linear parsing models are often trained by optimizing likelihood, but we would prefer to optimise for a task-specific metric like fmeasure. <eos> softmax-margin is a convex objective for such models that minimises a bound on expected risk for a given loss function, but its na ? <eos> ? ve application requires the loss to decompose over the predicted structure, which is not true of f-measure. <eos> we use softmaxmargin to optimise a log-linear ccg parser for a variety of loss functions, and demonstrate a novel dynamic programming algorithm that enables us to use it with f-measure, leading to substantial gains in accuracy on ccgbank. <eos> when we embed our loss-trained parser into a larger model that includes supertagging features incorporated via belief propagation, we obtain further improvements and achieve a labelled/unlabelled dependency f-measure of 89.3 % /94.0 % on gold part-of-speech tags, and 87.2 % /92.8 % on automatic part-of-speech tags, the best reported results for this task.
we present a system for the large scale induction of cognate groups. <eos> our model explains the evolution of cognates as a sequence of mutations and innovations along a phylogeny. <eos> on the task of identifying cognates from over 21,000 words in 218 different languages from the oceanic language family, our model achieves a cluster purity score over 91 %, while maintaining pairwise recall over 62 %.
we explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. <eos> these sentences may be selected with simple cross-entropy based methods, of which we present three. <eos> as these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. <eos> these subcorpora ? <eos> 1 % the size of the original ? <eos> can then used to train small domain-adapted statistical machine translation ( smt ) systems which outperform systems trained on the entire corpus. <eos> performance is further improved when we use these domain-adapted models in combination with a true in-domain model. <eos> the results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.
we investigate the differences between language models compiled from original target-language texts and those compiled from texts manually translated to the target language. <eos> corroborating established observations of translation studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better. <eos> furthermore, translated texts yield better language models for statistical machine translation than original texts.
many machine translation evaluation metrics have been proposed after the seminal bleu metric, and many among them have been found to consistently outperform bleu, demonstrated by their better correlations with human judgment. <eos> it has long been the hope that by tuning machine translation systems against these new generation metrics, advances in automatic machine translation evaluation can lead directly to advances in automatic machine translation. <eos> however, to date there has been no unambiguous report that these new metrics can improve a state-of-theart machine translation system over its bleutuned baseline. <eos> in this paper, we demonstrate that tuning joshua, a hierarchical phrase-based statistical machine translation system, with the tesla metrics results in significantly better humanjudged translation quality than the bleutuned baseline. <eos> tesla-m in particular is simple and performs well in practice on large datasets. <eos> we release all our implementation under an open source license. <eos> it is our hope that this work will encourage the machine translation community to finally move away from bleu as the unquestioned default and to consider the new generation metrics when tuning their systems.
methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks, making cross-experimental evaluation opaque. <eos> this paper develops a robust procedure for cross-experimental evaluation, based on deterministic unificationbased operations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. <eos> we demonstrate that, for different conversions of the penn treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground.
in order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the english wikipedia. <eos> we construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.
text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. <eos> most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. <eos> this paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. <eos> we describe how such a grammar can be induced from wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. <eos> we show experimentally that our method creates simplifications that significantly reduce the reading difficulty of the input, while maintaining grammaticality and preserving its meaning.
conversations provide rich opportunities for interactive, continuous learning. <eos> when something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. <eos> in this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. <eos> we demonstrate learning without any explicit annotation of the meanings of user utterances. <eos> instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. <eos> this loss drives the overall learning approach, which induces a weighted ccg grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. <eos> experiments on darpa communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.
we investigate an important and challenging problem in summary generation, i.e., evolutionary trans-temporal summarization ( etts ), which generates news timelines from massive data on the internet. <eos> etts greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. <eos> given the collection of time-stamped web documents related to the evolving news, etts aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. <eos> existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. <eos> we propose to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sentence dependencies, and present a novel combination. <eos> we develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. <eos> evaluation results in rouge metrics indicate the effectiveness of the proposed approach based on trans-temporal information.
we propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. <eos> the input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. <eos> as predicting actions from still images directly is unreliable, we use a language model trained from the english gigaword corpus to obtain their estimates ; together with probabilities of co-located nouns, scenes and prepositions. <eos> we use these estimates as parameters on a hmm that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. <eos> experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.
automatically produced texts ( e.g. <eos> translations or summaries ) are usually evaluated with n-gram based measures such as bleu or rouge, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes. <eos> in this paper we first present an indepth analysis of the state of the art in order to clarify this issue. <eos> after this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies. <eos> these properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process. <eos> in addition, the greater the heterogeneity of the measures ( which is measurable ) the higher their combined reliability. <eos> these results support the use of heterogeneous measures in order to consolidate text evaluation results.
the text analysis conference ( tac ) ranks summarization systems by their average score over a collection of document sets. <eos> we investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems.
we present a quasi-synchronous dependency grammar ( smith and eisner, 2006 ) for machine translation in which the leaves of the tree are phrases rather than words as in previous work ( gimpel and smith, 2009 ). <eos> this formulation allows us to combine structural components of phrase-based and syntax-based mt in a single model. <eos> we describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. <eos> for decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. <eos> we demonstrate performance improvements for chinese-english and urduenglish translation over a phrase-based baseline. <eos> we also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.
preordering of source side sentences has proved to be useful in improving statistical machine translation. <eos> most work has used a parser in the source language along with rules to map the source language word order into the target language word order. <eos> the requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. <eos> instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. <eos> our model learns pairwise costs of a word immediately preceding another word. <eos> we use the lin-kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. <eos> we show gains in translation performance based on our reordering model for translating from hindi to english, urdu to english ( with a public dataset ), and english to hindi. <eos> for english to hindi we show that our technique achieves better performance than a method that uses rules applied to the source side english parse.
we propose an algorithm allowing to efficiently retrieve example treelets in a parsed tree database in order to allow on-the-fly extraction of syntactic translation rules. <eos> we also propose improvements of this algorithm allowing several kinds of flexible matchings.
this paper presents a generative model for the automatic discovery of relations between entities in electronic medical records. <eos> the model discovers relation instances and their types by determining which context tokens express the relation. <eos> additionally, the valid semantic classes for each type of relation are determined. <eos> we show that the model produces clusters of relation trigger words which better correspond with manually annotated relations than several existing clustering techniques. <eos> the discovered relations reveal some of the implicit semantic structure present in patient records.
we consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. <eos> we show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. <eos> more specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the path ranking algorithm ( lao and cohen, 2010b ). <eos> we apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by nell, a never-ending language learner ( carlson et al, 2010 ). <eos> this new system improves significantly over nell ? s earlier horn-clause learning and inference method : it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.
this paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. <eos> hot in hot summer denoting the attribute temperature, rather than taste. <eos> we formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. <eos> the vectors incorporate latent semantic information obtained from two variants of lda topic models. <eos> our lda models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of lda models. <eos> for the first time, we extend the attribute selection task to a new data set with more than 200 classes. <eos> we observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. <eos> again, the lda models outperform the vsm baseline.
in this paper, we propose a novel topic model based on incorporating dictionary definitions. <eos> traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. <eos> they infer topics only by observing surface word co-occurrence. <eos> however, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. <eos> exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. <eos> we exploit wordnet as a lexical resource for sense definitions. <eos> we show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.
we present an automatic method which leverages word lengthening to adapt a sentiment lexicon specifically for twitter and similar social messaging networks. <eos> the contributions of the paper are as follows. <eos> first, we call attention to lengthening as a widespread phenomenon in microblogs and social messaging, and demonstrate the importance of handling it correctly. <eos> we then show that lengthening is strongly associated with subjectivity and sentiment. <eos> finally, we present an automatic method which leverages this association to detect domain-specific sentiment- and emotionbearing words. <eos> we evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses. <eos> our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes.
in recent years, the amount of user-generated opinionated texts ( e.g., reviews, user comments ) continues to grow at a rapid speed : featured news stories on a major event easily attract thousands of user comments on a popular online news service. <eos> how to consume subjective information of this volume becomes an interesting and important research question. <eos> in contrast to previous work on review analysis that tried to filter or summarize information for a generic average user, we explore a different direction of enabling personalized recommendation of such information. <eos> for each user, our task is to rank the comments associated with a given article according to personalized user preference ( i.e., whether the user is likely to like or dislike the comment ). <eos> to this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. <eos> our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.
we present a data-driven approach to generating responses to twitter status posts, based on phrase-based statistical machine translation. <eos> we find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment can not be further decomposed. <eos> after addressing these challenges, we compare approaches based on smt and information retrieval in a human evaluation. <eos> we show that smt outperforms ir on this task, and its output is preferred over actual human responses in 15 % of cases. <eos> as far as we are aware, this is the first work to investigate the use of phrase-based smt to directly translate a linguistic stimulus into an appropriate response.
we consider the problem of predicting measurable responses to scientific articles based primarily on their text content. <eos> specifically, we consider papers in two fields ( economics and computational linguistics ) and make predictions about downloads and within-community citations. <eos> our approach is based on generalized linear models, allowing interpretability ; a novel extension that captures first-order temporal effects is also presented. <eos> we demonstrate that text features significantly improve accuracy of predictions over metadata features like authors, topical categories, and publication venues.
a key factor of high quality word segmentation for japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. <eos> although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. <eos> to supplement a morphological dictionary with these resources, we propose a new task of japanese noun phrase segmentation. <eos> we apply non-parametric bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. <eos> for inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. <eos> experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer.
we present an inference algorithm that organizes observed words ( tokens ) into structured inflectional paradigms ( types ). <eos> it also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles ( grammar ) that generalize to wholly unobserved words. <eos> our bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. <eos> it assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms ( string tuples ). <eos> each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. <eos> these assumptions naturally lead to an elegant empirical bayes inference procedure that exploits monte carlo em, belief propagation, and dynamic programming. <eos> given 50 ? 100 seed paradigms, adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10 %.
in this paper, we describe a novel approach to cascaded learning and inference on sequences. <eos> we propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. <eos> in this model, inference on sequences is modeled as cascaded decision. <eos> however, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. <eos> it is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. <eos> we also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. <eos> one of the dynamic programming algorithms resembles back propagation algorithm for multilayer feed-forward neural networks. <eos> the other is a generalized version of the forwardbackward algorithm. <eos> we also report experiments of cascaded part-of-speech tagging and chunking of english sentences and show effectiveness of the proposed method.
in this paper we present a fully unsupervised syntactic class induction system formulated as a bayesian multinomial mixture model, where each word type is constrained to belong to a single class. <eos> by using a mixture model rather than a sequence model ( e.g., hmm ), we are able to easily add multiple kinds of features, including those at both the type level ( morphology features ) and token level ( context and alignment features, the latter from parallel corpora ). <eos> using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. <eos> using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.
responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds ( ncs ), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. <eos> we employ bootstrapping and web statistics, and utilize the relationship between ncs and paraphrasing patterns to jointly extract ncs and such patterns in multiple alternating iterations. <eos> in evaluation, we found that having one compound noun fixed yields both a higher number of semantically interpreted ncs and improved accuracy due to stronger semantic restrictions.
in the last few years, the interest of the research community in micro-blogs and social media services, such as twitter, is growing exponentially. <eos> yet, so far not much attention has been paid on a key characteristic of microblogs : the high level of information redundancy. <eos> the aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. <eos> we cast redundancy in the framework of textual entailment recognition. <eos> we also provide quantitative evidence on the pervasiveness of redundancy in twitter, and describe a dataset of redundancy-annotated tweets. <eos> finally, we present a general purpose system for identifying redundant tweets. <eos> an extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.
we address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. <eos> our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. <eos> in line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to : i ) tackle the scarcity of data available to train and evaluate systems, and ii ) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. <eos> we show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. <eos> the resulting dataset, obtained from a pipeline of different jobs routed to amazon mechanical turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in english, italian and german.
metaphor is ubiquitous in text, even in highly technical text. <eos> correct inference about textual entailment requires computers to distinguish the literal and metaphorical senses of a word. <eos> past work has treated this problem as a classical word sense disambiguation task. <eos> in this paper, we take a new approach, based on research in cognitive linguistics that views metaphor as a method for transferring knowledge from a familiar, well-understood, or concrete domain to an unfamiliar, less understood, or more abstract domain. <eos> this view leads to the hypothesis that metaphorical word usage is correlated with the degree of abstractness of the word ? s context. <eos> we introduce an algorithm that uses this hypothesis to classify a word sense in a given context as either literal ( denotative ) or metaphorical ( connotative ). <eos> we evaluate this algorithm with a set of adjectivenoun phrases ( e.g., in dark comedy, the adjective dark is used metaphorically ; in dark hair, it is used literally ) and with the trofi ( trope finder ) example base of literal and nonliteral usage for fifty verbs. <eos> we achieve state-of-theart performance on both datasets.
decision trees have been applied to a variety of nlp tasks, including language modeling, for their ability to handle a variety of attributes and sparse context space. <eos> moreover, forests ( collections of decision trees ) have been shown to substantially outperform individual decision trees. <eos> in this work, we investigate methods for combining trees in a forest, as well as methods for diversifying trees for the task of syntactic language modeling. <eos> we show that our tree interpolation technique outperforms the standard method used in the literature, and that, on this particular task, restricting tree contexts in a principled way produces smaller and better forests, with the best achieving an 8 % relative reduction in word error rate over an n-gram baseline.
augmented and alternative communication ( aac ) devices enable users with certain communication disabilities to participate in everyday conversations. <eos> such devices often rely on statistical language models to improve text entry by offering word predictions. <eos> these predictions can be improved if the language model is trained on data that closely reflects the style of the users ? <eos> intended communications. <eos> unfortunately, there is no large dataset consisting of genuine aac messages. <eos> in this paper we demonstrate how we can crowdsource the creation of a large set of fictional aac messages. <eos> we show that these messages model conversational aac better than the currently used datasets based on telephone conversations or newswire text. <eos> we leverage our crowdsourced messages to intelligently select sentences from much larger sets of twitter, blog and usenet data. <eos> compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of aac-like communications by 60 ? <eos> 82 % relative. <eos> this translated to a potential keystroke savings in a predictive keyboard interface of 5 ? 11 %.
the last decade has seen many interesting applications of question answering ( qa ) technology. <eos> the jeopardy ! <eos> quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. <eos> in this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of jeopardy ! <eos> definition questions. <eos> our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. <eos> such classifiers are also used in the state-of-the-art qa pipeline constituting watson, the ibm jeopardy ! <eos> system. <eos> our experiments measuring their impact on watson show enhancements in qa accuracy and a consequent increase in the amount of money earned in game-based evaluation.
multiword expressions ( mwe ), a known nuisance for both linguistics and nlp, blur the lines between syntax and semantics. <eos> previous work onmwe identification has relied primarily on surface statistics, which perform poorly for longer mwes and can not model discontinuous expressions. <eos> to address these problems, we show that even the simplest parsing models can effectively identify mwes of arbitrary length, and that tree substitution grammars achieve the best results. <eos> our experiments show a 36.4 % f1 absolute improvement for french over an n-gram surface statistics baseline, currently the predominant method for mwe identification. <eos> our models are useful for several nlp tasks in which mwe pre-grouping has improved accuracy.
we present the first algorithms to automatically identify explicit discourse connectives and the relations they signal for arabic text. <eos> first we show that, for arabic news, most adjacent sentences are connected via explicit connectives in contrast to english, making the treatment of explicit discourse connectives for arabic highly important. <eos> we also show that explicit arabic discourse connectives are far more ambiguous than english ones, making their treatment challenging. <eos> in the second part of the paper, we present supervised algorithms to address automatic discourse connective identification and discourse relation recognition. <eos> our connective identifier based on gold standard syntactic features achieves almost human performance. <eos> in addition, an identifier based solely on simple lexical and automatically derived morphological and pos features performs with high reliability, essential for languages that do not have high-quality parsers yet. <eos> our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation.
this research studies the text genre of message board forums, which contain a mixture of expository sentences that present factual information and conversational sentences that include communicative acts between the writer and readers. <eos> our goal is to create sentence classifiers that can identify whether a sentence contains a speech act, and can recognize sentences containing four different speech act classes : commissives, directives, expressives, and representatives. <eos> we conduct experiments using a wide variety of features, including lexical and syntactic features, speech act word lists from external resources, and domain-specific semantic class features. <eos> we evaluate our results on a collection of message board posts in the domain of veterinary medicine.
information-oriented document labeling is a special document multi-labeling task where the target labels refer to a specific information instead of the topic of the whole document. <eos> these kind of tasks are usually solved by looking up indicator phrases and analyzing their local context to filter false positive matches. <eos> here, we introduce an approach for machine learning local content shifters which detects irrelevant local contexts using just the original document-level training labels. <eos> we handle content shifters in general, instead of learning a particular language phenomenon detector ( e.g. <eos> negation or hedging ) and form a single system for document labeling and content shift detection. <eos> our empirical results achieved 24 % error reduction ? <eos> compared to supervised baseline methods ? <eos> on three document labeling tasks.
in this paper, we present a new ranking scheme, collaborative ranking ( cr ). <eos> in contrast to traditional non-collaborative ranking scheme which solely relies on the strengths of isolated queries and one stand-alone ranking algorithm, the new scheme integrates the strengths from multiple collaborators of a query and the strengths from multiple ranking algorithms. <eos> we elaborate three specific forms of collaborative ranking, namely, micro collaborative ranking ( micr ), macro collaborative ranking ( macr ) and micro-macro collaborative ranking ( mimacr ). <eos> experiments on entity linking task show that our proposed scheme is indeed effective and promising.
disambiguating named entities in naturallanguage text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as dbpedia or yago. <eos> this paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. <eos> it unifies prior approaches into a comprehensive framework that combines three measures : the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. <eos> the method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. <eos> experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.
most nlp systems use tokenization as part of preprocessing. <eos> generally, tokenizers are based on simple heuristics and do not recognize multi-word units ( mwus ) like hot dog or black hole unless a precompiled list of mwus is available. <eos> in this paper, we propose a new cascaded model for detecting mwus of arbitrary length for tokenization, focusing on noun phrases in the physics domain. <eos> we adopt a classification approach because ? <eos> unlike other work on mwus ? <eos> tokenization requires a completely automatic approach. <eos> we achieve an accuracy of 68 % for recognizing non-compositional mwus and show that our mwu recognizer improves retrieval performance when used as part of an information retrieval system.
in this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. <eos> we use both local contexts and global world knowledge to expand query language models. <eos> we place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. <eos> our experiments on the tac-kbp 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. <eos> compared with the official results from kbp 2010 participants, our system shows competitive performance.
we address the task of automatic discovery of information extraction template from a given text collection. <eos> our approach clusters candidate slot fillers to identify meaningful template slots. <eos> we propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. <eos> empirical results suggest that the proposed prior can bring substantial improvements to our task as compared to a k-means baseline and a gaussian mixture model baseline. <eos> specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates.
this paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns ( e.g. <eos> ? x causes y ? <eos> for causal relations ) but instead learns a combination of indirect evidence for the target relation ? <eos> semantic word classes and partial patterns. <eos> this method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large japaneseweb corpus ? <eos> in extreme cases, patterns that occur only once in the entire corpus. <eos> such patterns are beyond the reach of current pattern based methods. <eos> we show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. <eos> this ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance.
we propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts. <eos> the architecture combines various linguistically-motivated classification features in a bayesian network. <eos> we introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the bayesian network models. <eos> our methodology is almost entirely unsupervised and completely languageindependent ; it relies on few language resources and is thus suitable for a large number of languages. <eos> furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic constructions. <eos> we demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines.
an a-c bilingual dictionary can be inferred by merging a-b and b-c dictionaries using b as pivot. <eos> however, polysemous pivot words often produce wrong translation candidates. <eos> this paper analyzes two methods for pruning wrong candidates : one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. <eos> as both methods depend exclusively on easily available resources, they are well suited to less resourced languages. <eos> we studied whether these two techniques complement each other given that they are based on different paradigms. <eos> we also researched combining them by looking for the best adequacy depending on various application scenarios.
long-distance reordering remains one of the biggest challenges facing machine translation. <eos> we derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrasebased model. <eos> our approach significantly improves chinese ? english machine translation on a large-scale task by 0.84 bleu points on average. <eos> moreover, when we switch the tuning function from bleu to the lrscore which promotes reordering, we observe total improvements of 1.21 bleu, 1.30 lrscore and 3.36 ter over the baseline. <eos> on average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering.
part-of-speech language modeling is commonly used as a component in statistical machine translation systems, but there is mixed evidence that its usage leads to significant improvements. <eos> we argue that its limited effectiveness is due to the lack of lexicalization. <eos> we introduce a new approach that builds a separate local language model for each word and part-of-speech pair. <eos> the resulting models lead to more context-sensitive probability distributions and we also exploit the fact that different local models are used to estimate the language model probability of each word during decoding. <eos> our approach is evaluated for arabic- and chinese-to-english translation. <eos> we show that it leads to statistically significant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model.
although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. <eos> we propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. <eos> our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. <eos> with millions of features trained on 519k sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 bleu over the baseline system on the nist chinese-english test sets.
models of word alignment built as sequences of links have limited expressive power, but are easy to decode. <eos> word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. <eos> we propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. <eos> then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. <eos> improved alignment performance is shown for all nine language pairs tested. <eos> the improved alignments also improved translation quality from chinese to english and english to italian.
state of the art tree structures prediction techniques rely on bottom-up decoding. <eos> these approaches allow the use of context-free features and bottom-up features. <eos> we discuss the limitations of mainstream techniques in solving common natural language processing tasks. <eos> then we devise a new framework that goes beyond bottom-up decoding, and that allows a better integration of contextual features. <eos> furthermore we design a system that addresses these issues and we test it on hierarchical machine translation, a well known tree structure prediction problem. <eos> the structure of the proposed system allows the incorporation of non-bottom-up features and relies on a more sophisticated decoding approach. <eos> we show that the proposed approach can find better translations using a smaller portion of the search space.
statistical machine translation systems are usually trained on a large amount of bilingual sentence pairs and translate one sentence at a time, ignoring document-level information. <eos> in this paper, we propose a cache-based approach to document-level translation. <eos> since caches mainly depend on relevant data to supervise subsequent decisions, it is critical to fill the caches with highly-relevant data of a reasonable size. <eos> in this paper, we present three kinds of caches to store relevant document-level information : 1 ) a dynamic cache, which stores bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document ; 2 ) a static cache, which stores relevant bilingual phrase pairs extracted from similar bilingual document pairs ( i.e. <eos> source documents similar to the test document and their corresponding target documents ) in the training parallel corpus ; 3 ) a topic cache, which stores the target-side topic words related with the test document in the source-side. <eos> in particular, three new features are designed to explore various kinds of document-level information in above three kinds of caches. <eos> evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in blue score over moses. <eos> especially, detailed analysis and discussion are presented to give new insights to document-level translation.
discriminative training for machine translation has been well studied in the recent past. <eos> a limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training. <eos> we present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough ? reverse ? <eos> translation system. <eos> intuitively, our method strives to ensure that probabilistic ? round-trip ? <eos> translation from a targetlanguage sentence to the source-language and back will have low expected loss. <eos> theoretically, this may be justified as ( discriminatively ) minimizing an imputed empirical risk. <eos> empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both iwslt and nist tasks.
mapping documents into an interlingual representation can help bridge the language barrier of cross-lingual corpora. <eos> many existing approaches are based on word co-occurrences extracted from aligned training data, represented as a covariance matrix. <eos> in theory, such a covariance matrix should represent semantic equivalence, and should be highly sparse. <eos> unfortunately, the presence of noise leads to dense covariance matrices which in turn leads to suboptimal document representations. <eos> in this paper, we explore techniques to recover the desired sparsity in covariance matrices in two ways. <eos> first, we explore word association measures and bilingual dictionaries to weigh the word pairs. <eos> later, we explore different selection strategies to remove the noisy pairs based on the association scores. <eos> our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs.
we discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. <eos> this setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. <eos> we tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. <eos> we then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. <eos> the homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. <eos> we then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path. <eos> our derivations are based on a compact geometric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters. <eos> we demonstrate the usability of our approach by applying the problem to zipfian distributions over a large alphabet.
in active dual supervision, not only informative examples but also features are selected for labeling to build a high quality classifier with low cost. <eos> however, how to measure the informativeness for both examples and feature on the same scale has not been well solved. <eos> in this paper, we propose a non-negative matrix factorization based approach to address this issue. <eos> we first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes. <eos> then by making use of the reconstruction error, we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled. <eos> empirical results demonstrate the effectiveness of our proposed methods.
word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in english, and it is beneficial for various nlp applications to split such noun compounds. <eos> in the case of japanese, noun compounds made up of katakana words ( i.e., transliterated foreign words ) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. <eos> to overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. <eos> experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the web in our case, and then using that information for constructing splitting models.
this paper investigates improving supervised word segmentation accuracy with unlabeled data. <eos> both large-scale in-domain data and small-scale document text are considered. <eos> we present a unified solution to include features derived from unlabeled data to a discriminative learning model. <eos> for the large-scale data, we derive string statistics from gigaword to assist a character-based segmenter. <eos> in addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary ( oov ) words which appear more than once inside a document. <eos> novel features1 result in relative error reductions of 13.8 % and 15.4 % in terms of f-score and the recall of oov words respectively.
metonymic language is a pervasive phenomenon. <eos> metonymic type shifting, or argument type coercion, results in a selectional restriction violation where the argument ? s semantic class differs from the class the predicate expects. <eos> in this paper we present an unsupervised method that learns the selectional restriction of arguments and enables the detection of argument coercion. <eos> this method also generates an enhanced probabilistic resolution of logical metonymies. <eos> the experimental results indicate substantial improvements the detection of coercions and the ranking of metonymic interpretations.
we re-investigate the rationale for and the effectiveness of adopting the notions of depth and density in wordnet-based semantic similarity measures. <eos> we show that the intuition for including these notions in wordnet-based similarity measures does not always stand up to empirical examination. <eos> in particular, the traditional definitions of depth and density as ordinal integer values in the hierarchical structure of wordnet does not always correlate with human judgment of lexical semantic similarity, which imposes strong limitations on their contribution to an accurate similarity measure. <eos> we thus propose several novel definitions of depth and density, which yield significant improvement in degree of correlation with similarity. <eos> when used in wordnet-based semantic similarity measures, the new definitions consistently improve performance on a task of correlating with human judgment.
this paper presents a novel method for the computation of word meaning in context. <eos> we make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. <eos> the factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. <eos> the evaluation on a lexical substitution task ? <eos> carried out for both english and french ? <eos> indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.
most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature. <eos> also natural language processing ( nlp ) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification. <eos> we introduce a new clustering method called hierarchical graph factorization clustering ( hgfc ) and extend it so that it is optimal for the task. <eos> our results show that hgfc outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from verbnet, and that it yields state-of-the-art performance also on a flat test set. <eos> we demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.
a central topic in natural language processing is the design of lexical and syntactic features suitable for the target application. <eos> in this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. <eos> we define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. <eos> the experiments with such kernels for question classification show an unprecedented results, e.g. <eos> 41 % of error reduction of the former state-of-the-art. <eos> additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.
this paper investigates novel methods for incorporating syntactic information in probabilistic latent variable models of lexical choice and contextual similarity. <eos> the resulting models capture the effects of context on the interpretation of a word and in particular its effect on the appropriateness of replacing that word with a potentially related one. <eos> evaluating our techniques on two datasets, we report performance above the prior state of the art for estimating sentence similarity and ranking lexical substitutes.
lexical co-occurrence is an important cue for detecting word associations. <eos> we propose a new measure of word association based on a new notion of statistical significance for lexical co-occurrences. <eos> existing measures typically rely on global unigram frequencies to determine expected co-occurrence counts. <eos> instead, we focus only on documents that contain both terms ( of a candidate word-pair ) and ask if the distribution of the observed spans of the word-pair resembles that under a random null model. <eos> this would imply that the words in the pair are not related strongly enough for one word to influence placement of the other. <eos> however, if the words are found to occur closer together than explainable by the null model, then we hypothesize a more direct association between the words. <eos> through extensive empirical evaluation on most of the publicly available benchmark data sets, we show the advantages of our measure over existing co-occurrence measures.
an entity in a dialogue may be old, new, or mediated/inferrable with respect to the hearer ? s beliefs. <eos> knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation. <eos> we address the under-investigated problem of automatically determining the information status of discourse entities. <eos> specifically, we extend nissim ? s ( 2006 ) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution. <eos> experimental results on a set of switchboard dialogues reveal that ( 1 ) incorporating our proposed features into nissim ? s feature set enables our system to achieve stateof-the-art performance on information-status classification, and ( 2 ) the resulting information can be used to improve the performance of learning-based coreference resolvers.
traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. <eos> we propose semantic features using word senses for a supervised document-level sentiment classifier. <eos> to highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where wordnet senses of the words are used as features. <eos> in addition, we highlight the benefit of senses by presenting a part-ofspeech-wise effect on sentiment classification. <eos> finally, we show that even if a wsd engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. <eos> since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on wordnet to address the problem of not finding a sense in the training corpus. <eos> we perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. <eos> the results show promising improvement with respect to the baseline.
in this paper, we introduce a connotation lexicon, a new type of lexicon that lists words with connotative polarity, i.e., words with positive connotation ( e.g., award, promotion ) and words with negative connotation ( e.g., cancer, war ). <eos> connotation lexicons differ from much studied sentiment lexicons : the latter concerns words that express sentiment, while the former concerns words that evoke or associate with a specific polarity of sentiment. <eos> understanding the connotation of words would seem to require common sense and world knowledge. <eos> however, we demonstrate that much of the connotative polarity of words can be inferred from natural language text in a nearly unsupervised manner. <eos> the key linguistic insight behind our approach is selectional preference of connotative predicates. <eos> we present graphbased algorithms using pagerank and hits that collectively learn connotation lexicon together with connotative predicates. <eos> our empirical study demonstrates that the resulting connotation lexicon is of great value for sentiment analysis complementing existing sentiment lexicons.
reranking models have been successfully applied to many tasks of natural language processing. <eos> however, there are two aspects of this approach that need a deeper investigation : ( i ) assessment of hypotheses generated for reranking at classification phase : baseline models generate a list of hypotheses and these are used for reranking without any assessment ; ( ii ) detection of cases where reranking models provide a worst result : the best hypothesis provided by the reranking model is assumed to be always the best result. <eos> in some cases the reranking model provides an incorrect hypothesis while the baseline best hypothesis is correct, especially when baseline models are accurate. <eos> in this paper we propose solutions for these two aspects : ( i ) a semantic inconsistency metric to select possibly more correct n-best hypotheses, from a large set generated by an slu basiline model. <eos> the selected hypotheses are reranked applying a state-of-the-art model based on partial tree kernels, which encode slu hypotheses in support vector machines with complex structured features ; ( ii ) finally, we apply a decision strategy, based on confidence values, to select the final hypothesis between the first ranked hypothesis provided by the baseline slu model and the first ranked hypothesis provided by the re-ranker. <eos> we show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate conditional random field model. <eos> we evaluate our approach on the french media corpus. <eos> the results show significant improvements with respect to current state-of-the-art and previous re-ranking models.
a re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. <eos> two pass strategies have become popular in a number of recognition tasks such as asr ( automatic speech recognition ), mt ( machine translation ) and ocr ( optical character recognition ). <eos> the first pass typically applies a weak language model ( n-grams ) to a lattice and the second pass applies a stronger language model to n best lists. <eos> the stronger language model is intended to capture more longdistance dependencies. <eos> the proposed method uses rnn-lm ( recurrent neural network language model ), which is a long span lm, to rescore word lattices in the second pass. <eos> a hill climbing method ( iterative decoding ) is proposed to search over islands of confusability in the word lattice. <eos> an evaluation based on broadcast news shows speedups of 20 over basic n best re-scoring, and word error rate reduction of 8 % ( relative ) on a highly competitive setup.
we propose an efficient way to train maximum entropy language models ( melm ) and neural network language models ( nnlm ). <eos> the advantage of the proposed method comes from a more robust and efficient subsampling technique. <eos> the original multi-class language modeling problem is transformed into a set of binary problems where each binary classifier predicts whether or not a particular word will occur. <eos> we show that the binarized model is as powerful as the standard model and allows us to aggressively subsample negative training examples without sacrificing predictive performance. <eos> empirical results show that we can train melm and nnlm at 1 % ? <eos> 5 % of the standard complexity with no loss in performance.
in this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. <eos> we first develop an event-aspect lda model to cluster sentences into aspects. <eos> we then use extended lexrank algorithm to rank the sentences in each cluster. <eos> we use integer linear programming for sentence selection. <eos> key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. <eos> also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. <eos> we compare our method with four baseline methods. <eos> quantitative evaluation based on rouge metric demonstrates the effectiveness and advantages of our method.
machine-produced text often lacks grammaticality and fluency. <eos> this paper studies grammaticality improvement using a syntax-based algorithm based on ccg. <eos> the goal of the search problem is to find an optimal parse tree among all that can be constructed through selection and ordering of the input words. <eos> the search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. <eos> in a standard word ordering task, our system gives a bleu score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system.
traditional computational approaches to referring expression generation operate in a deliberate manner, choosing the attributes to be included on the basis of their ability to distinguish the intended referent from its distractors. <eos> however, work in psycholinguistics suggests that speakers align their referring expressions with those used previously in the discourse, implying less deliberate choice and more subconscious reuse. <eos> this raises the question as to which is a more accurate characterisation of what people do. <eos> using a corpus of dialogues containing 16,358 referring expressions, we explore this question via the generation of subsequent references in shared visual scenes. <eos> we use a machine learning approach to referring expression generation and demonstrate that incorporating features that correspond to the computational tradition does not match human referring behaviour as well as using features corresponding to the process of alignment. <eos> the results support the view that the traditional model of referring expression generation that is widely assumed in work on natural language generation may not in fact be correct ; our analysis may also help explain the oft-observed redundancy found in humanproduced referring expressions.
previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. <eos> however, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. <eos> we extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. <eos> we discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. <eos> we illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.
part-of-speech ( pos ) is an indispensable feature in dependency parsing. <eos> current research usually models pos tagging and dependency parsing independently. <eos> this may suffer from error propagation problem. <eos> our experiments show that parsing accuracy drops by about 6 % when using automatic pos tags instead of gold ones. <eos> to solve this issue, this paper proposes a solution by jointly optimizing pos tagging and dependency parsing in a unique model. <eos> we design several joint models and their corresponding decoding algorithms to incorporate different feature sets. <eos> we further present an effective pruning strategy to reduce the search space of candidate pos tags, leading to significant improvement of parsing speed. <eos> experimental results on chinese penn treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5 %. <eos> detailed analysis shows that the joint method is able to choose such pos tags that are more helpful and discriminative from parsing viewpoint. <eos> this is the fundamental reason of parsing accuracy improvement.
we propose a relaxed correspondence assumption for cross-lingual projection of constituent syntax, which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse. <eos> such a relaxed assumption fundamentally tolerates the syntactic non-isomorphism between languages, and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. <eos> based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. <eos> experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers.
the computation of logical form has been proposed as an intermediate step in the translation of sentences to logic. <eos> logical form encodes the resolution of scope ambiguities. <eos> in this paper, we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form, called abstract syntax trees ( asts ). <eos> the main step in computing asts is to order scope-taking operators. <eos> a learning model for ranking is adapted for this ordering. <eos> we design features by studying the problem of comparing the scope of one operator to another. <eos> the scope comparisons are used to compute asts, with an f-score of 90.6 % on the set of ordering decisons.
the notion of infix probability has been introduced in the literature as a generalization of the notion of prefix ( or initial substring ) probability, motivated by applications in speech recognition and word error correction. <eos> for the case where a probabilistic context-free grammar is used as language model, methods for the computation of infix probabilities have been presented in the literature, based on various simplifying assumptions. <eos> here we present a solution that applies to the problem in its full generality.
this paper develops a framework for syntactic dependency parse correction. <eos> dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. <eos> we use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. <eos> the parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. <eos> our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by several representative state-of-the-art dependency parsers for french.
we describe a generative model for nonprojective dependency parsing based on a simplified version of a transition system that has recently appeared in the literature. <eos> we then develop a dynamic programming parsing algorithm for our model, and derive an insideoutside algorithm that can be used for unsupervised learning of non-projective dependency trees.
this paper introduces chart inference ( ci ), an algorithm for deriving a ccg category for an unknown word from a partial parse chart. <eos> it is shown to be faster and more precise than a baseline brute-force method, and to achieve wider coverage than a rule-based system. <eos> in addition, we show the application of ci to a domain adaptation task for question words, which are largely missing in the penn treebank. <eos> when used in combination with self-training, ci increases the precision of the baseline statccg parser over subjectextraction questions by 50 %. <eos> an error analysis shows that ci contributes to the increase by expanding the number of category types available to the parser, while self-training adjusts the counts.
dependency parsers are critical components within many nlp systems. <eos> however, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. <eos> furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. <eos> in this paper, we present a new dependency-tree conversion of the penn treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. <eos> we explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. <eos> the parser performs well when evaluated on the standard test section of the penn treebank, outperforming several popular open source dependency parsers ; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93 % accuracy.
we present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives. <eos> in its simplest form, lateen em alternates between the two objectives of ordinary ? soft ? <eos> and ? hard ? <eos> expectation maximization ( em ) algorithms. <eos> switching objectives when stuck can help escape local optima. <eos> we find that applying a single such alternation already yields state-of-the-art results for english dependency grammar induction. <eos> more elaborate lateen strategies track both objectives, with each validating the moves proposed by the other. <eos> disagreements can signal earlier opportunities to switch or terminate, saving iterations. <eos> de-emphasizing fixed points in these ways eliminates some guesswork from tuning em. <eos> an evaluation against a suite of unsupervised dependency parsing tasks, for a variety of languages, showed that lateen strategies significantly speed up training of both em algorithms, and improve accuracy for hard em.
we show that categories induced by unsupervised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction. <eos> unlike classic clustering algorithms, our method allows a word to have different tags in different contexts. <eos> in an ablative analysis, we first demonstrate that this context-dependence is crucial to the superior performance of gold tags ? <eos> requiring a word to always have the same part-ofspeech significantly degrades the performance of manual tags in grammar induction, eliminating the advantage that human annotation has over unsupervised tags. <eos> we then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise, to allow words to be tagged differently in different contexts. <eos> with these new induced tags as input, our state-ofthe-art dependency grammar inducer achieves 59.1 % directed accuracy on section 23 ( all sentences ) of the wall street journal ( wsj ) corpus ? <eos> 0.7 % higher than using gold tags.
we propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale ngram model for spelling correction. <eos> the syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. <eos> experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4 % over the current state-of-the-art. <eos> the word co-occurrence information shows potential but only improves overall accuracy slightly.
accurate prediction of demographic attributes from social media and other informal online content is valuable for marketing, personalization, and legal investigation. <eos> this paper describes the construction of a large, multilingual dataset labeled with gender, and investigates statistical models for determining the gender of uncharacterized twitter users. <eos> we explore several different classifier types on this dataset. <eos> we show the degree to which classifier accuracy varies based on tweet volumes as well as when various kinds of profile metadata are included in the models. <eos> we also perform a large-scale human assessment using amazon mechanical turk. <eos> our methods significantly out-perform both baseline models and almost all humans on the same task.
information published in online stock investment message boards, and more recently in stock microblogs, is considered highly valuable by many investors. <eos> previous work focused on aggregation of sentiment from all users. <eos> however, in this work we show that it is beneficial to distinguish expert users from non-experts. <eos> we propose a general framework for identifying expert investors, and use it as a basis for several models that predict stock rise from stock microblogging messages ( stock tweets ). <eos> in particular, we present two methods that combine expert identification and per-user unsupervised learning. <eos> these methods were shown to achieve relatively high precision in predicting stock rise, and significantly outperform our baseline. <eos> in addition, our work provides an in-depth analysis of the content and potential usefulness of stock tweets.
in this paper we present a method for unsupervised semantic role induction which we formalize as a graph partitioning problem. <eos> argument instances of a verb are represented as vertices in a graph whose edge weights quantify their role-semantic similarity. <eos> graph partitioning is realized with an algorithm that iteratively assigns vertices to clusters based on the cluster assignments of neighboring vertices. <eos> our method is algorithmically and conceptually simple, especially with respect to how problem-specific knowledge is incorporated into the model. <eos> experimental results on the conll 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of f1 whilst attaining significantly higher cluster purity.
based on analysis of on-line review corpus we observe that most sentences have complicated opinion structures and they can not be well represented by existing methods, such as frame-based and feature-based ones. <eos> in this work, we propose a novel graph-based representation for sentence level sentiment. <eos> an integer linear programming-based structural learning method is then introduced to produce the graph representations of input sentences. <eos> experimental evaluations on a manually labeled chinese corpus demonstrate the effectiveness of the proposed approach.
most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. <eos> however, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. <eos> hence we investigate an important and challenging problem in summary generation, i.e., interactive personalized summarization ( ips ), which generates summaries in an interactive and personalized manner. <eos> given the source documents, ips captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. <eos> we develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. <eos> evaluation results in rouge metrics indicate the comparable performance between ips and the best competing system but ips produces summaries with much more user satisfaction according to evaluator ratings. <eos> besides, low rouge consistency among these user preferred summaries indicates the existence of personalization.
we offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking ( herbrich et al, 1999 ). <eos> unlike the popular mert algorithm ( och, 2003 ), our pairwise ranking optimization ( pro ) method is not limited to a handful of parameters and can easily handle systems with thousands of features. <eos> moreover, unlike recent approaches built upon the mira algorithm of crammer and singer ( 2003 ) ( watanabe et al, 2007 ; chiang et al, 2008b ), pro is easy to implement. <eos> it uses off-the-shelf linear binary classifier software and can be built on top of an existing mert framework in a matter of hours. <eos> we establish pro ? s scalability and effectiveness by comparing it to mert and mira and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.
this paper compares several translation representations for a synchronous context-free grammar parse including cfgs/hypergraphs, finite-state automata ( fsa ), and pushdown automata ( pda ). <eos> the representation choice is shown to determine the form and complexity of target lm intersection and shortest-path algorithms that follow. <eos> intersection, shortest path, fsa expansion and rtn replacement algorithms are presented for pdas. <eos> chinese-toenglish translation experiments using hifst and hipdt, fsa and pda-based decoders, are presented using admissible ( or exact ) search, possible for hifst with compact scfg rulesets and hipdt with compact lms. <eos> for large rulesets with large lms, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance.
modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. <eos> we implement the abstract categorical model of coecke et al ( 2010 ) using data from the bnc and evaluate it. <eos> the implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. <eos> the evaluation is based on the word disambiguation task developed by mitchell and lapata ( 2008 ) for intransitive sentences, and on a similar new experiment designed for transitive sentences. <eos> our model matches the results of its competitors in the first experiment, and betters them in the second. <eos> the general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
context-dependent word similarity can be measured over multiple cross-cutting dimensions. <eos> for example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. <eos> both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. <eos> towards this end, we develop a novel model, multi-view mixture ( mvm ), that represents words as multiple overlapping clusterings. <eos> mvm finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to latent dirichlet allocation. <eos> intuitively, this constraint favors feature partitions that have coherent topical semantics. <eos> furthermore, mvm uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. <eos> through a series of experiments, we demonstrate the utility of mvm as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as latent dirichlet allocation.
it is often assumed that ? grounded ? <eos> learning tasks are beyond the scope of grammatical inference techniques. <eos> in this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in kim and mooney ( 2010 ) can be reduced to a probabilistic context-free grammar learning task in a way that gives state of the art results. <eos> we further show that additionally letting our model learn the language ? s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1
this paper describes a novel approach to the semantic relation detection problem. <eos> instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. <eos> specifically, we detect a new semantic relation by projecting the new relation ? s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. <eos> first, we construct a large relation repository of more than 7,000 relations from wikipedia. <eos> second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. <eos> similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. <eos> third, we integrate the relation topics in a kernel function, and use it together with svm to construct detectors for new relations. <eos> the experimental results on wikipedia and ace data have confirmed that backgroundknowledge-based topics generated from the wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.
we report on empirical results in extreme extraction. <eos> it is extreme in that ( 1 ) from receipt of the ontology specifying the target concepts and relations, development is limited to one week and that ( 2 ) relatively little training data is assumed. <eos> we are able to surpass human recall and achieve an f1 of 0.51 on a question-answering task with less than 50 hours of effort using a hybrid approach that mixes active learning, bootstrapping, and limited ( 5 hours ) manual rule writing. <eos> we compare the performance of three systems : extraction with handwritten rules, bootstrapped extraction, and a combination. <eos> we show that while the recall of the handwritten rules surpasses that of the learned system, the learned system is able to improve the overall recall and f1.
traditional approaches to relation extraction from text require manually defining the relations to be extracted. <eos> we propose here an approach to automatically discovering relevant relations, given a large text corpus plus an initial ontology defining hundreds of noun categories ( e.g., athlete, musician, instrument ). <eos> our approach discovers frequently stated relations between pairs of these categories, using a two step process. <eos> for each pair of categories ( e.g., musician and instrument ) it first coclusters the text contexts that connect known instances of the two categories, generating a candidate relation for each resulting cluster. <eos> it then applies a trained classifier to determine which of these candidate relations is semantically valid. <eos> our experiments apply this to a text corpus containing approximately 200 million web pages and an ontology containing 122 categories from the nell system [ carlson et al, 2010b ], producing a set of 781 proposed candidate relations, approximately half of which are semantically valid. <eos> we conclude this is a useful approach to semi-automatic extension of the ontology for large-scale information extraction systems such as nell.
we explore unsupervised approaches to relation extraction between two named entities ; for instance, the semantic bornin relation between a person and location entity. <eos> concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. <eos> the output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. <eos> our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. <eos> we examine effectiveness of our approach via multiple evaluations and demonstrate 12 % error reduction in precision over a state-of-the-art weakly supervised baseline.
this paper describes dualist, an active learning annotation paradigm which solicits and learns from labels on both features ( e.g., words ) and instances ( e.g., documents ). <eos> we present a novel semi-supervised training algorithm developed for this setting, which is ( 1 ) fast enough to support real-time interactive speeds, and ( 2 ) at least as accurate as preexisting methods for learning with mixed feature and instance labels. <eos> human annotators in user studies were able to produce near-stateof-the-art classifiers ? on several corpora in a variety of application domains ? with only a few minutes of effort.
we propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of eisner ? s generative model. <eos> in our framework, we define two kinds of generative model for reranking. <eos> one is learned from training data offline and the other from a forest generated by a baseline parser on the fly. <eos> the final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. <eos> in order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and viterbi algorithms. <eos> experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches.
we present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions. <eos> the primary example is the extension of a standard supervised parsing objective function with additional loss-functions, either based on intrinsic parsing quality or task-specific extrinsic measures of quality. <eos> our empirical results show how this approach performs for two dependency parsing algorithms ( graph-based and transition-based parsing ) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation.
linear models have enjoyed great success in structured prediction in nlp. <eos> while a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. <eos> common approaches employ ad hoc filtering or l1regularization ; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. <eos> we fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them. <eos> experiments on three tasks ( chunking, entity recognition, and dependency parsing ) show gains in performance, compactness, and model interpretability.
we consider the problem of learning factored probabilistic ccg grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. <eos> traditional ccg lexicons list lexical items that pair words and phrases with syntactic and semantic content. <eos> such lexicons can be inefficient when words appear repeatedly with closely related lexical content. <eos> in this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. <eos> we also present an algorithm for learning factored ccg lexicons, along with a probabilistic parse-selection model. <eos> evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.
people tweet more than 100 million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. <eos> the performance of standard nlp tools is severely degraded on tweets. <eos> this paper addresses this issue by re-building the nlp pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. <eos> our novel t-ner system doubles f1 score compared with the stanford ner system. <eos> t-ner leverages the redundancy inherent in tweets to achieve this performance, using labeledlda to exploit freebase dictionaries as a source of distant supervision. <eos> labeledlda outperforms cotraining, increasing f1 by 25 % over ten common entity types. <eos> our nlp tools are available at : http : // github.com/aritter/twitter_nlp
open information extraction ( ie ) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. <eos> this paper shows that the output of state-ofthe-art open ie systems is rife with uninformative and incoherent extractions. <eos> to overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. <eos> we implemented the constraints in the reverb open ie system, which more than doubles the area under the precision-recall curve relative to previous extractors such as textrunner and woepos. <eos> more than 30 % of reverb ? s extractions are at precision 0.8 or higher ? <eos> compared to virtually none for earlier systems. <eos> the paper concludes with a detailed analysis of reverb ? s errors, suggesting directions for future work.1
supervised classification needs large amounts of annotated training data that is expensive to create. <eos> two approaches that reduce the cost of annotation are active learning and crowdsourcing. <eos> however, these two approaches have not been combined successfully to date. <eos> we evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.
we present a named entity recognition ( ner ) system for extracting product attributes and values from listing titles. <eos> information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. <eos> in this work, we combine supervised ner with bootstrapping to expand the seed list, and output normalized results. <eos> focusing on listings from ebay ? s clothing and shoes categories, our bootstrapped ner system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. <eos> among the top 300 new brands predicted, our system achieves 90.33 % precision. <eos> to output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.
aki sachiko maskawa mizuki morita the university of tokyo the university of tokyo national institute of jst presto biomedical innovation tokyo, japan tokyo, japan osaka, japan eiji.aramaki @ gmail.com sachiko.maskawa @ gmail.com morita.mizuki @ gmail.com abstract with the recent rise in popularity and scale of social media, a growing need exists for systems that can extract useful information from huge amounts of data. <eos> we address the issue of detecting influenza epidemics. <eos> first, the proposed system extracts influen-za related tweets using twitter api. <eos> then, only tweets that mention actual influenza patients are extracted by the support vector machine ( svm ) based classifier. <eos> the ex-periment results demonstrate the feasibility of the proposed approach ( 0.89 correlation to the gold standard ). <eos> especially at the out-break and early spread ( early epidemic stage ), the proposed method shows high correlation ( 0.97 correlation ), which out-performs the state-of-the-art methods. <eos> this paper describes that twitter texts reflect the real world, and that nlp techniques can be applied to extract only tweets that contain useful information.
it is popular for users in web 2.0 era to freely annotate online resources with tags. <eos> to ease the annotation process, it has been great interest in automatic tag suggestion. <eos> we propose a method to suggest tags according to the text description of a resource. <eos> by considering both the description and tags of a given resource as summaries to the resource written in two languages, we adopt word alignment models in statistical machine translation to bridge their vocabulary gap. <eos> based on the translation probabilities between the words in descriptions and the tags estimated on a large set of description-tags pairs, we build a word trigger method ( wtm ) to suggest tags according to the words in a resource description. <eos> experiments on real world datasets show that wtm is effective and robust compared with other methods. <eos> moreover, wtm is relatively simple and efficient, which is practical for web applications.
a rumor is commonly defined as a statement whose true value is unverifiable. <eos> rumors may spread misinformation ( false information ) or disinformation ( deliberately false information ) on a network of people. <eos> identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority. <eos> in this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features : content-based, network-based, and microblog-specific memes for correctly identifying rumors. <eos> moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. <eos> we perform our experiments on more than 10,000 manually annotated tweets collected from twitter and show how our retrieval model achieves more than 0.95 in mean average precision ( map ). <eos> finally, we believe that our dataset is the first large-scale dataset on rumor detection. <eos> it can open new dimensions in analyzing online misinformation and other aspects of microblog conversations.
attempts to profile authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features. <eos> drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent influenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identification. <eos> we take two types of parse substructure as features ? <eos> horizontal slices of trees, and the more general feature schemas from discriminative parse reranking ? and show that using this kind of syntactic feature results in an accuracy score in classification of seven native languages of around 80 %, an error reduction of more than 30 %.
this paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. <eos> the approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. <eos> sentences can then be generated based on such grammar rules with a log-linear model. <eos> to acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. <eos> experiments on benchmark datasets for both english and chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.
in this work we propose methods to label probabilistic synchronous context-free grammar ( pscfg ) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. <eos> the proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. <eos> our models improve translation quality over the single generic label approach of chiang ( 2005 ) and perform on par with the syntactically motivated approach from zollmann and venugopal ( 2006 ) on the nist large chineseto-english translation task. <eos> these results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.
in this work, we tackle the task of machine translation ( mt ) without parallel training data. <eos> we frame the mt problem as a decipherment task, treating the foreign text as a cipher for english and present novel methods for training translation models from nonparallel text.
in the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. <eos> given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. <eos> in order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. <eos> therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. <eos> extensive experiments involving large-scale english-tojapanese translation revealed a significant improvement of 1.8 points in bleu score, as compared with a strong forest-to-string baseline system.
active learning ( al ) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. <eos> while various simulation studies for a number of nlp tasks have shown that al works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. <eos> this paper presents a thorough evaluation of the impact of annotation noise on al and shows that systematic noise resulting from biased coder decisions can seriously harm the al process. <eos> we present a method to filter out inconsistent annotations during al and show that this makes al far more robust when applied to noisy data.
this paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. <eos> given a misspelled word, the system finds words in a dictionary, which are most ? similar ? <eos> to the misspelled word. <eos> the paper proposes a probabilistic approach to the task, which is both accurate and efficient. <eos> the approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. <eos> the log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. <eos> the learning method employs the criterion in candidate generation as loss function. <eos> the retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. <eos> experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.
we consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. <eos> one way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. <eos> such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. <eos> the danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. <eos> we introduce a constraint enforcing that marginal distributions of each cluster ( i.e., each latent variable ) do not vary significantly across domains. <eos> we show that this constraint is effective on the sentiment classification task ( pang et al, 2002 ), resulting in scores similar to the ones obtained by the structural correspondence methods ( blitzer et al, 2007 ) without the need to engineer auxiliary tasks.
we describe an exact decoding algorithm for syntax-based statistical translation. <eos> the approach uses lagrangian relaxation to decompose the decoding problem into tractable subproblems, thereby avoiding exhaustive dynamic programming. <eos> the method recovers exact solutions, with certificates of optimality, on over 97 % of test examples ; it has comparable speed to state-of-the-art decoders.
we propose methods for estimating the probability that an entity from an entity database is associated with a web search query. <eos> association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. <eos> smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. <eos> a large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. <eos> the association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining queryproduct associations through web search session analysis. <eos> experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9 % of general web queries with 94 % precision.
searching documents that are similar to a query document is an important component in modern information retrieval. <eos> some existing hashing methods can be used for efficient document similarity search. <eos> however, unsupervised hashing methods can not incorporate prior knowledge for better hashing. <eos> although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. <eos> this paper proposes a novel ( semi- ) supervised hashing method named semi-supervised simhash ( s3h ) for high-dimensional data similarity search. <eos> the basic idea of s3h is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. <eos> we evaluate our method with several state-of-the-art methods on two large datasets. <eos> all the results show that our method gets the best performance.
marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. <eos> due to their brevity and idiosyncratic structure, search queries pose a challenge to existing nlp tools. <eos> to address this challenge, we propose a probabilistic approach for performing joint query annotation. <eos> first, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. <eos> then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. <eos> we evaluate our method using a range of queries extracted from a web search log. <eos> experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.
we propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. <eos> query weighting is a key step in ranking model adaptation. <eos> as the learning object of ranking algorithms is divided by query instances, we argue that it ? s more reasonable to conduct importance weighting at query level than document level. <eos> we present two query weighting schemes. <eos> the first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. <eos> this method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. <eos> the second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. <eos> adaptation experiments on letor3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.
joint sentiment-topic ( jst ) model was previously proposed to detect sentiment and topic simultaneously from text. <eos> the only supervision required by jst model learning is domain-independent polarity word priors. <eos> in this paper, we modify the jst model by incorporating word polarity priors through modifying the topic-word dirichlet priors. <eos> we study the polarity-bearing topics extracted by jst and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classifiers learned from augmented feature representation achieve the state-of-the-art performance of 95 % on the movie review data and an average of 90 % on the multi-domain sentiment dataset. <eos> furthermore, using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classification, our proposed approach performs either better or comparably compared to previous approaches. <eos> nevertheless, our approach is much simpler and does not require difficult parameter tuning.
we describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. <eos> we automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. <eos> the created thesaurus is then used to expand feature vectors to train a binary classifier. <eos> unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. <eos> our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing amazon user reviews for different types of products.
unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of nlp tasks. <eos> we present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term ? document information as well as rich sentiment content. <eos> the proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. <eos> we instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents ( e.g. <eos> star ratings ). <eos> we evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. <eos> we also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.
sentiment analysis on twitter data has attracted much attention recently. <eos> in this paper, we focus on target-dependent twitter sentiment classification ; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. <eos> here the query serves as the target of the sentiments. <eos> the state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. <eos> moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment ; they ignore its context ( i.e., related tweets ). <eos> however, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. <eos> in this paper, we propose to improve target-dependent twitter sentiment classification by 1 ) incorporating target-dependent features ; and 2 ) taking related tweets into consideration. <eos> according to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.
it has been widely recognized that one of the most difficult and intriguing problems in natural language processing ( nlp ) is how to cope with idiosyncratic multiword expressions. <eos> this paper presents an overview of the comprehensive dictionary ( jdmwe ) of japanese multiword expressions. <eos> the jdmwe is characterized by a large notational, syntactic, and semantic diversity of contained expressions as well as a detailed description of their syntactic functions, structures, and flexibilities. <eos> the dictionary contains about 104,000 expressions, potentially 750,000 expressions. <eos> this paper shows that the jdmwe ? s validity can be supported by comparing the dictionary with a large-scale japanese n-gram frequency dataset, namely the ldc2009t08, generated by google inc. ( kudo et al 2009 ).
we describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or mmgs. <eos> mmgs typically involve groups of players ( 5-30 ) who control their avatars1, perform various activities ( questing, competing, fighting, etc. ) <eos> and communicate via chat or speech using assumed screen names. <eos> we collected a corpus of 48 group quests in second life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. <eos> multiple levels of coordinated annotation of this corpus ( dialogue, movements, touch, gaze, wear, etc ) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. <eos> the annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously.
we demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ? english as a second or other language ? <eos> ( esol ) examination scripts. <eos> in particular, we use rank preference learning to explicitly model the grade relationships between scripts. <eos> a number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. <eos> a comparison between regression and rank preference models further supports our method. <eos> experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. <eos> finally, using a set of ? outlier ? <eos> texts, we test the validity of our model and identify cases where the model ? s scores diverge from that of a human examiner.
a lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. <eos> we address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. <eos> the highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. <eos> in addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
this paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed markov random field paradigm. <eos> the composite language model has been trained by performing a convergent n-best list approximate em algorithm that has linear time complexity and a followup em algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. <eos> the large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the bleu score and ? readability ? <eos> when applied to the task of re-ranking the n-best list from a state-of-theart parsing-based machine translation system.
state-of-the-art statistical machine translation ( mt ) systems have made significant progress towards producing user-acceptable translation output. <eos> however, there is still no efficient way for mt systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. <eos> we propose a novel framework to predict wordlevel and sentence-level mt errors with a large number of novel features. <eos> experimental results show that the mt error prediction accuracy is increased from 69.1 to 72.2 in f-score. <eos> the pearson correlation between the proposed confidence measure and the human-targeted translation edit rate ( hter ) is 0.6. <eos> improvements between 0.4 and 0.9 ter reduction are obtained with the n-best list reranking task using the proposed confidence measure. <eos> also, we present a visualization prototype of mt errors at the word and sentence levels with the objective to improve post-editor productivity.
we introduce a novel semi-automated metric, meant, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as hter but at much lower labor cost. <eos> as machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented mt evaluation metrics such as bleu, which fail to properly evaluate adequacy, become more apparent. <eos> but more accurate, nonautomatic adequacy-oriented mt evaluation metrics like hter are highly labor-intensive, which bottlenecks the evaluation cycle. <eos> we first show that when using untrained monolingual readers to annotate semantic roles in mt output, the non-automatic version of the metric hmeant achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to bleu at only 0.20, and equal to the far more expensive hter. <eos> we then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80 % as closely correlated as hter despite an even lower labor cost for the evaluation procedure. <eos> the results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than hter.
this paper presents an exponential model for translation into highly inflected languages which can be scaled to very large datasets. <eos> as in other recent proposals, it predicts targetside phrases and can be conditioned on sourceside context. <eos> however, crucially for the task of modeling morphological generalizations, it estimates feature parameters from the entire training set rather than as a collection of separate classifiers. <eos> we apply it to english-czech translation, using a variety of features capturing potential predictors for case, number, and gender, and one of the largest publicly available parallel data sets. <eos> we also describe generation and modeling of inflected forms unobserved in training data and decoding procedures for a model with non-local target-side feature dependencies.
we introduce a novel bayesian approach for deciphering complex substitution ciphers. <eos> our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. <eos> bayesian inference is performed on our model using an efficient sampling technique. <eos> we evaluate the quality of the bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100 % accurate decipherments. <eos> the new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous zodiac-408 cipher in a fully automated fashion, which has never been done before.
topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. <eos> however, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. <eos> in this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent dirichlet alocation ( lda ) by adding constraints that enforce that sets of words must appear together in the same topic. <eos> we incorporate these constraints interactively by selectively removing elements in the state of a markov chain used for inference ; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.
n -gram language models are a major resource bottleneck in machine translation. <eos> in this paper, we present several language model implementations that are both highly compact and fast to query. <eos> our fastest implementation is as fast as the widely used srilm while requiring only 25 % of the storage. <eos> our most compact representation can store all 4 billion n-grams and associated counts for the google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. <eos> we also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models ( and srilm ) by up to 300 %.
this paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. <eos> our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. <eos> our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. <eos> our method operates in the monte-carlo search framework, and learns both text analysis and game strategies based only on environment feedback. <eos> we apply our approach to the complex strategy game civilization ii using the official game manual as the text guide. <eos> our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27 % absolute improvement and winning over 78 % of games when playing against the builtin ai of civilization ii. <eos> 1
information retrieval ( ir ) and figurative language processing ( flp ) could scarcely be more different in their treatment of language and meaning. <eos> ir views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text ? s potential relevance than its potential meaning. <eos> in contrast, flp views language as a system of unstable signs that can be used to talk about the world in creative new ways. <eos> there is another key difference : ir is practical, scalable and robust, and in daily use by millions of casual users. <eos> flp is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. <eos> this paper thus presents a mutually beneficial hybrid of ir and flp, one that enriches ir with new operators to enable the non-literal retrieval of creative expressions, and which also transplants flp into a robust, scalable framework in which practical applications of linguistic creativity can be implemented.
this paper proposes the use of local histograms ( lh ) over character n-grams for authorship attribution ( aa ). <eos> lhs are enriched histogram representations that preserve sequential information in documents ; they have been successfully used for text categorization and document visualization using word histograms. <eos> in this work we explore the suitability of lhs over n-grams at the character-level for aa. <eos> we show that lhs are particularly helpful for aa, because they provide useful information for uncovering, to some extent, the writing style of authors. <eos> we report experimental results in aa data sets that confirm that lhs over character n-grams are more helpful for aa than the usual global histograms, yielding results far superior to state of the art approaches. <eos> we found that lhs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. <eos> our results motivate further research on the use of lhs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.
consumers increasingly rate, review and research products online ( jansen, 2010 ; litvin et al, 2008 ). <eos> consequently, websites containing consumer reviews are becoming targets of opinion spam. <eos> while recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam ? fictitious opinions that have been deliberately written to sound authentic. <eos> integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90 % accurate on our gold-standard opinion spam dataset. <eos> based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.
most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. <eos> we present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. <eos> we rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. <eos> experiments on multiple data sets show that the proposed approach ( 1 ) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44 % -8.12 % ; ( 2 ) outperforms two standard approaches for leveraging unlabeled data ; and ( 3 ) produces ( albeit smaller ) performance gains when employing pseudo-parallel data from machine translation engines.
this paper presents a pilot study of opinion summarization on conversations. <eos> we create a corpus containing extractive and abstractive summaries of speaker ? s opinion towards a given topic using 88 telephone conversations. <eos> we adopt two methods to perform extractive summarization. <eos> the first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. <eos> the second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. <eos> our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. <eos> in particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.
we present disputant relation-based method for classifying news articles on contentious issues. <eos> we observe that the disputants of a contention are an important feature for understanding the discourse. <eos> it performs unsupervised classification on news articles based on disputant relations, and helps readers intuitively view the articles through the opponent-based frame. <eos> the readers can attain balanced understanding on the contention, free from a specific biased view. <eos> we applied a modified version of hits algorithm and an svm classifier trained with pseudo-relevant data for article analysis.
we present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets. <eos> our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties. <eos> this approach directly enables discovery of highly rated or inconsistent properties of a product. <eos> our model admits an efficient variational meanfield inference algorithm which can be parallelized and run on large snippet collections. <eos> we evaluate our model on a large corpus of snippets from yelp reviews to assess property and attribute prediction. <eos> we demonstrate that it outperforms applicable baselines by a considerable margin.
the challenges of named entities recognition ( ner ) for tweets lie in the insufficient information in a tweet and the unavailability of training data. <eos> we propose to combine a k-nearest neighbors ( knn ) classifier with a linear conditional random fields ( crf ) model under a semi-supervised learning framework to tackle these challenges. <eos> the knn based classifier conducts pre-labeling to collect global coarse evidence across tweets while the crf model conducts sequential labeling to capture fine-grained information encoded in a tweet. <eos> the semi-supervised learning plus the gazetteers alleviate the lack of training data. <eos> extensive experiments show the advantages of our method over the baselines as well as the effectiveness of knn and semisupervised learning.
twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for nlp. <eos> in this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. <eos> our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. <eos> both word similarity and context are then exploited to select the most probable correction candidate for the word. <eos> the proposed method doesn ? t require any annotations, and achieves state-of-the-art performance over an sms corpus and a novel dataset based on twitter.
summarizing and analyzing twitter content is an important and challenging task. <eos> in this paper, we propose to extract topical keyphrases as one way to summarize twitter. <eos> we propose a context-sensitive topical pagerank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. <eos> we evaluate our proposed methods on a large twitter data set. <eos> experiments show that these methods are very effective for topical keyphrase extraction.
we present a novel method for record extraction from social streams such as twitter. <eos> unlike typical extraction setups, these environments are characterized by short, one sentence messages with heavily colloquial speech. <eos> to further complicate matters, individual messages may not express the full relation to be uncovered, as is often assumed in extraction tasks. <eos> we develop a graphical model that addresses these problems by learning a latent set of records and a record-message alignment simultaneously ; the output of our model is a set of canonical records, the values of which are consistent with aligned messages. <eos> we demonstrate that our approach is able to accurately induce event records from twitter messages, evaluated against events from a local city guide. <eos> our method achieves significant error reduction over baseline methods.1
grapheme-to-phoneme conversion ( g2p ) of names is an important and challenging problem. <eos> the correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. <eos> we investigate the problem of using transliterations to correct errors produced by state-of-the-art g2p systems. <eos> we present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. <eos> our experiments demonstrate significant accuracy improvements when re-ranking is applied to n-best lists generated by three different g2p programs.
we introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by brown et al ( 1993 ). <eos> in our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. <eos> however, unlike previous work on discriminative modeling of word alignment ( which also permits the use of arbitrary features ), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. <eos> using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.
unsupervised word alignment is most often modeled as a markov process that generates a sentence f conditioned on its translation e. a similar model generating e from f will make different alignment predictions. <eos> statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. <eos> this paper presents a graphical model that embeds two directional aligners into a single model. <eos> inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. <eos> our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. <eos> the resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.
we propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. <eos> in contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. <eos> we conduct experiments on data sets from the news 2010 shared task on transliteration mining and achieve an f-measure of up to 92 %, outperforming most of the semi-supervised systems that were submitted. <eos> we also apply our method to english/hindi and english/arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. <eos> finally, we integrate the transliteration module into the giza++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.
efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more nlp applications leverage syntactic analyses. <eos> we review prior methods for pruning and then present a new framework that unifies their strengths into a single approach. <eos> using a log linear model, we learn the optimal beam-search pruning parameters for each cyk chart cell, effectively predicting the most promising areas of the model space to explore. <eos> we demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the charniak and berkeley parsers, by empirically comparing our parser to the berkeley parser using the same grammar and under identical operating conditions.
we study the problem of finding the best headdriven parsing strategy for linear contextfree rewriting system productions. <eos> a headdriven strategy must begin with a specified righthand-side nonterminal ( the head ) and add the remaining nonterminals one at a time in any order. <eos> we show that it is np-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing.
we present a method for the computation of prefix probabilities for synchronous contextfree grammars. <eos> our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.
via an oracle experiment, we show that the upper bound on accuracy of a ccg parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. <eos> inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. <eos> to overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. <eos> on ccgbank we achieve a labelled dependency f-measure of 88.8 % on gold pos tags, and 86.7 % on automatic part-of-speeoch tags, the best reported results for this task.
we learn a joint model of sentence extraction and compression for multi-document summarization. <eos> our model scores candidate summaries according to a combined linear model whose features factor over ( 1 ) the n-gram types in the summary and ( 2 ) the compressions used. <eos> we train the model using a marginbased objective whose loss captures end summary quality. <eos> because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. <eos> inference in our model can be cast as an ilp and thereby solved in reasonable time ; we also present a fast approximation scheme which achieves similar performance. <eos> our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both rouge and pyramid, without a drop in judged linguistic quality. <eos> we achieve the highest published rouge results to date on the tac 2008 data set.
extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. <eos> we present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. <eos> based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. <eos> although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 rouge on the duc-07 test set, roughly in the range of state-of-the-art supervised models.
in citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. <eos> previous work on this problem focused almost exclusively on its extraction aspect ( i.e. <eos> selecting a representative set of citation sentences that highlight the contribution of the target paper ). <eos> meanwhile, the fluency of the produced summaries has been mostly ignored. <eos> for example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. <eos> this resulted in noisy and confusing summaries. <eos> in this work, we present an approach for producing readable and cohesive citation-based summaries. <eos> our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency.
we design a class of submodular functions meant for document summarization tasks. <eos> these functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. <eos> critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. <eos> when evaluated on duc 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. <eos> lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.
we present a simple semi-supervised relation extraction system with large-scale word clustering. <eos> we focus on systematically exploring the effectiveness of different cluster-based features. <eos> we also propose several statistical methods for selecting clusters at an appropriate level of granularity. <eos> when training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system.
we present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. <eos> our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. <eos> these constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. <eos> we capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. <eos> across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. <eos> furthermore, we find that a small set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. <eos> 1
information extraction ( ie ) holds the promise of generating a large-scale knowledge base from the web ? s natural language text. <eos> knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. <eos> recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint ? <eos> for example they can not extract the pair founded ( jobs, apple ) and ceo-of ( jobs, apple ). <eos> this paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. <eos> we apply our model to learn extractors for ny times text using weak supervision from freebase. <eos> experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.
in this paper, we observe that there exists a second dimension to the relation extraction ( re ) problem that is orthogonal to the relation type dimension. <eos> we show that most of these second dimensional structures are relatively constrained and not difficult to identify. <eos> we propose a novel algorithmic approach to re that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. <eos> in the real re problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. <eos> we show that this re framework provides significant improvement in re performance.
recent work on bilingual word sense disambiguation ( wsd ) has shown that a resource deprived language ( l1 ) can benefit from the annotation work done in a resource rich language ( l2 ) via parameter projection. <eos> however, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. <eos> instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. <eos> we then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of l1 is used to annotate the untagged data of l2 and vice versa using parameter projection. <eos> the untagged instances of l1 and l2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. <eos> our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using hindi ( l1 ) and marathi ( l2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.
resolving polysemy and synonymy is required for high-quality information extraction. <eos> we present conceptresolver, a component for the never-ending language learner ( nell ) ( carlson et al, 2010 ) that handles both phenomena by identifying the latent concepts that noun phrases refer to. <eos> conceptresolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data. <eos> domain knowledge ( the ontology ) guides concept creation by defining a set of possible semantic types for concepts. <eos> word sense induction is performed by inferring a set of semantic types for each noun phrase. <eos> synonym detection exploits redundant information to train several domain-specific synonym classifiers in a semi-supervised fashion. <eos> when conceptresolver is run on nell ? s knowledge base, 87 % of the word senses it creates correspond to real-world concepts, and 85 % of noun phrases that it suggests refer to the same concept are indeed synonyms.
negation is present in all human languages and it is used to reverse the polarity of part of statements that are otherwise affirmative by default. <eos> a negated statement often carries positive implicit meaning, but to pinpoint the positive part from the negative part is rather difficult. <eos> this paper aims at thoroughly representing the semantics of negation by revealing implicit positive meaning. <eos> the proposed representation relies on focus of negation detection. <eos> for this, new annotation over propbank and a learning algorithm are proposed.
compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. <eos> in this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. <eos> in tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. <eos> on two standard semantic parsing benchmarks ( geo and jobs ), our system obtains the highest published accuracies, despite requiring no annotated logical forms.
we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. <eos> our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ), making it applicable to a wide array of resource-poor languages. <eos> we use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model ( bergkirkpatrick et al, 2010 ). <eos> across eight european languages, our approach results in an average absolute improvement of 10.4 % over a state-of-the-art baseline, and 16.7 % over vanilla hidden markov models induced with the expectation maximization algorithm.
extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. <eos> in this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. <eos> we model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. <eos> we apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released ( schoenmackers et al, 2010 ). <eos> our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs.
this paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. <eos> bottom-up and topdown parsers typically require a completed string as input. <eos> this requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. <eos> incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. <eos> we give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the moses phrase-based translation system. <eos> we present empirical results on a constrained urdu-english translation task that demonstrate a significant bleu score improvement and a large decrease in perplexity.
we present an unsupervised model for joint phrase alignment and extraction using nonparametric bayesian methods and inversion transduction grammars ( itgs ). <eos> the key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. <eos> this allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. <eos> experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.
while it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. <eos> the key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. <eos> these approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. <eos> we propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. <eos> our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. <eos> we obtain statistically significant improvements across 4 different language pairs with english as source, mounting up to +1.92 bleu for chinese as target.
community-based question answer ( q & a ) has become an important issue due to the popularity of q & a archives on the web. <eos> this paper is concerned with the problem of question retrieval. <eos> question retrieval in q & a archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. <eos> in this paper, we propose a novel phrase-based translation model for question retrieval. <eos> compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation. <eos> experiments conducted on real q & a data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.
dependency parsing is a central nlp task. <eos> in this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. <eos> we show that for three leading unsupervised parsers ( klein and manning, 2004 ; cohen and smith, 2009 ; spitkovsky et al, 2010a ), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. <eos> these parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. <eos> therefore, the standard evaluation does not provide a true indication of algorithm quality. <eos> we present a new measure, neutral edge direction ( ned ), and show that it greatly reduces this undesired phenomenon.
we develop a general dynamic programming technique for the tabulation of transition-based dependency parsers, and apply it to obtain novel, polynomial-time algorithms for parsing with the arc-standard and arc-eager models. <eos> we also show how to reverse our technique to obtain new transition-based dependency parsers from existing tabular methods. <eos> additionally, we provide a detailed discussion of the conditions under which the feature models commonly used in transition-based parsing can be integrated into our algorithms.
ccgs are directly compatible with binarybranching bottom-up parsing algorithms, in particular cky and shift-reduce algorithms. <eos> while the chart-based approach has been the dominant approach for ccg, the shift-reduce method has been little explored. <eos> in this paper, we develop a shift-reduce ccg parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based c & c parser. <eos> we study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to c & c. <eos> considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the ccg lexical categories which form the shift actions, this is a surprising result.
counts from large corpora ( like the web ) can be powerful syntactic cues. <eos> past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb pp attachments and noun compound bracketings. <eos> in this work, we first present a method for generating web count features that address the full range of syntactic attachments. <eos> these features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. <eos> we then integrate our features into full-scale dependency and constituent parsers. <eos> we show relative error reductions of 7.0 % over the second-order dependency parser of mcdonald and pereira ( 2006 ), 9.2 % over the constituent parser of petrov et al. <eos> ( 2006 ), and 3.4 % over a non-local constituent reranker.
unrehearsed spoken language often contains disfluencies. <eos> in order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. <eos> operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisychannel model. <eos> we show that language models trained on large amounts of non-speech data improve performance more than a language model trained on a more modest amount of speech data, and that optimising f-score rather than log loss improves disfluency detection performance. <eos> our approach uses a log-linear reranker, operating on the top n analyses of a noisy channel model. <eos> we use large language models, introduce new features into this reranker and examine different optimisation strategies. <eos> we obtain a disfluency detection f-scores of 0.838 which improves upon the current state-of-theart.
large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. <eos> hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems ; new words can then be represented by combinations of subword units. <eos> previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. <eos> we propose a probabilistic model to learn the subword lexicon optimized for a given task. <eos> we consider the task of out of vocabulary ( oov ) word detection, which relies on output from a hybrid model. <eos> a hybrid model with our learned sub-word lexicon reduces error by 6.3 % and 7.6 % ( absolute ) at a 5 % false alarm rate on an english broadcast news and mit lectures task respectively.
this paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence. <eos> our goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods. <eos> on human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2.
in this paper, we adopt an n-best rescoring scheme using pitch-accent patterns to improve automatic speech recognition ( asr ) performance. <eos> the pitch-accent model is decoupled from the main asr system, thus allowing us to develop it independently. <eos> n-best hypotheses from recognizers are rescored by additional scores that measure the correlation of the pitch-accent patterns between the acoustic signal and lexical cues. <eos> to test the robustness of our algorithm, we use two different data sets and recognition setups : the first one is english radio news data that has pitch accent labels, but the recognizer is trained from a small amount of data and has high error rate ; the second one is english broadcast news data using a state-of-the-art sri recognizer. <eos> our experimental results demonstrate that our approach is able to reduce word error rate relatively by about 3 %. <eos> this gain is consistent across the two different tests, showing promising future directions of incorporating prosodic information to improve speech recognition.
the automatic coding of clinical documents is an important task for today ? s healthcare providers. <eos> though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. <eos> we propose a lexically-triggered hidden markov model ( lt-hmm ) that leverages these phrases to improve coding accuracy. <eos> the lt-hmm works in two stages : first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. <eos> next, a discriminative hmm selects the best subset of codes to assign to the document by tagging candidates as present or absent. <eos> by confirming codes proposed by a dictionary, the lt-hmm can share features across codes, enabling strong performance even on rare codes. <eos> in fact, we are able to recover codes that do not occur in the training set at all. <eos> our approach achieves the best ever performance on the 2007medical nlp challenge test set, with an f-measure of 89.84.
in this work we address the task of computerassisted assessment of short student answers. <eos> we combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. <eos> we also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.
we investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. <eos> our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. <eos> through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as aim, sms text messaging, myspace and facebook first became popular, enable accurate age prediction. <eos> we also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. <eos> our best results allow for 81.57 % accuracy.
sociolinguists have long argued that social context influences language use in all manner of ways, resulting in lects 1. <eos> this paper explores a text classification problem we will call lect modeling, an example of what has been termed computational sociolinguistics. <eos> in particular, we use machine learning techniques to identify social power relationships between members of a social network, based purely on the content of their interpersonal communication. <eos> we rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. <eos> we then apply support vector machines to model the social power lects representing superior-subordinate communication in the enron email corpus. <eos> our results validate the treatment of lect modeling as a text classification problem ? <eos> albeit a hard one ? <eos> and constitute a case for future research in computational sociolinguistics.
in this paper, we present an unsupervised framework that bootstraps a complete coreference resolution ( core ) system from word associations mined from a large unlabeled corpus. <eos> we show that word associations are useful for core ? <eos> e.g., the strong association between obama and president is an indicator of likely coreference. <eos> association information has so far not been used in core because it is sparse and difficult to learn from small labeled corpora. <eos> since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. <eos> in a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. <eos> we show that this unsupervised system has better core performance than other learning approaches that do not use manually labeled data.
cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. <eos> for large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. <eos> to solve the problem we propose two ideas : ( a ) a distributed inference technique that uses parallelism to enable large scale processing, and ( b ) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. <eos> to evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in web pages by selecting link anchors referring to wikipedia entities. <eos> we show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy ( with error reduction of 38 % ) on this large dataset, demonstrating the scalability of our approach.
we present an ilp-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by denis and baldridge ( 2007 ), but revises it and extends it into a three-way ilp problem also incorporating subject detection. <eos> we show that this new model outperforms several baselines and competing models, as well as a direct translation of the denis / baldridge model, for both italian and japanese zero anaphora. <eos> we incorporate our model in complete anaphoric resolvers for both italian and japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for explicitly realized anaphors.
while world knowledge has been shown to improve learning-based coreference resolvers, the improvements were typically obtained by incorporating world knowledge into a fairly weak baseline resolver. <eos> hence, it is not clear whether these benefits can carry over to a stronger baseline. <eos> moreover, since there has been no attempt to apply different sources of world knowledge in combination to coreference resolution, it is not clear whether they offer complementary benefits to a resolver. <eos> we systematically compare commonly-used and under-investigated sources of world knowledge for coreference resolution by applying them to two learning-based coreference models and evaluating them on documents annotated with two different annotation schemes.
the local multi bottom-up tree transducer is introduced and related to the ( non-contiguous ) synchronous tree sequence substitution grammar. <eos> it is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus. <eos> finally, the problem of non-preservation of regularity is addressed. <eos> three properties that ensure preservation are introduced, and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled.
tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. <eos> forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. <eos> we propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. <eos> provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. <eos> for the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. <eos> combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in nist 2008 english-to-chinese track ( 1.3 bleu points over a phrase-based system, 0.8 bleu points over a hierarchical phrase-based system ). <eos> consistent and significant gains are also shown in wmt 2010 in the english to german, french, spanish and czech tracks.
we propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. <eos> we transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. <eos> in particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. <eos> we learn the structural preferences from the data in a generative framework. <eos> the syntax-based translation system integrating the proposed techniques outperforms the best arabic-english unconstrained system in nist08 evaluations by 1.3 absolute bleu, which is statistically significant.
most statistical machine translation systems rely on composed rules ( rules that can be formed out of smaller rules in the grammar ). <eos> though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. <eos> here, we take the opposite approach, where we only use minimal rules ( those that can not be formed out of other rules ), and instead rely on a rule markov model of the derivation history to capture dependencies between minimal rules. <eos> large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality ( measured using bleu ) as composed rules.
in this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. <eos> we develop a novel hidden markov model incorporating sophisticated smoothing using a hierarchical pitman-yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. <eos> central to our approach is a new type-based sampling algorithm for hierarchical pitman-yor models in which we track fractional table counts. <eos> in an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.
arabic handwriting recognition ( hr ) is a challenging problem due to arabic ? s connected letter forms, consonantal diacritics and rich morphology. <eos> in this paper we isolate the task of identification of erroneous words in hr from the task of producing corrections for these words. <eos> we consider a variety of linguistic ( morphological and syntactic ) and non-linguistic features to automatically identify these errors. <eos> our best approach achieves a roughly ? 15 % absolute increase in f-score over a simple but reasonable baseline. <eos> a detailed error analysis shows that linguistic features, such as lemma ( i.e., citation form ) models, help improve hr-error detection precisely where we expect them to : semantically incoherent error words.
most previous studies of morphological disambiguation and dependency parsing have been pursued independently. <eos> morphological taggers operate on n-grams and do not take into account syntactic relations ; parsers use the ? pipeline ? <eos> approach, assuming that morphological information has been separately obtained. <eos> however, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. <eos> in this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. <eos> in evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.
this paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. <eos> the model extends hidden semi-markov chain models by using factored output nodes and special structures for its conditional probability distributions. <eos> it relies on morpho-syntactic and lexical source-side information ( part-of-speech, morphological segmentation ) while learning a morpheme segmentation over the target language. <eos> our model outperforms a competitive word alignment system in alignment quality. <eos> used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three arabic and hebrew datasets.
spelling correction for keyword-search queries is challenging in restricted domains such as personal email ( or desktop ) search, due to the scarcity of query logs, and due to the specialized nature of the domain. <eos> for that task, this paper presents an algorithm that is based on statistics from the corpus data ( rather than the query log ). <eos> this algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability ( e.g., email subject vs. email body ), and can handle complex spelling errors like splitting and merging of words. <eos> an experimental study shows the superiority of the algorithm over existing alternatives in the email domain.
we present a novel approach to grammatical error correction based on alternating structure optimization. <eos> as part of our work, we introduce the nus corpus of learner english ( nucle ), a fully annotated one million words corpus of learner english available for research purposes. <eos> we conduct an extensive evaluation for article and preposition errors using various feature sets. <eos> our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. <eos> our approach also outperforms two commercial grammar checking software packages.
we consider the problem of correcting errors made by english as a second language ( esl ) writers and address two issues that are essential to making progress in esl error correction - algorithm selection and model adaptation to the first language of the esl learner. <eos> a variety of learning algorithms have been applied to correct esl mistakes, but often comparisons were made between incomparable data sets. <eos> we conduct an extensive, fair comparison of four popular learning methods for the task, reversing conclusions from earlier evaluations. <eos> our results hold for different training sets, genres, and feature sets. <eos> a second key issue in esl error correction is the adaptation of a model to the first language of the writer. <eos> errors made by non-native speakers exhibit certain regularities and, as we show, models perform much better when they use knowledge about error patterns of the nonnative writers. <eos> we propose a novel way to adapt a learned algorithm to the first language of the writer that is both cheaper to implement and performs better than other adaptation methods.
automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance. <eos> current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the corrections to a limited scope. <eos> we introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. <eos> we show how to use the em algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model. <eos> this frees us from the burden of acquiring a large corpora of corrected sentences. <eos> we also present a cheap and efficient way to provide automated evaluation results for grammar corrections by using bleu and meteor, in contrast to the commonly used manual evaluations.
linking entities with knowledge base ( entity linking ) is a key issue in bridging the textual data with the structural knowledge base. <eos> due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities. <eos> in this paper, we propose a generative probabilistic model, called entitymention model, which can leverage heterogenous entity knowledge ( including popularity knowledge, name knowledge and context knowledge ) for the entity linking task. <eos> in our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document p ( e ), the distribution of possible names of a specific entity p ( s|e ), and the distribution of possible contexts of a specific entity p ( c|e ). <eos> to find the referent entity of a name mention, our method combines the evidences from all the three distributions p ( e ), p ( s|e ) and p ( c|e ). <eos> experimental results show that our method can significantly outperform the traditional methods.
we investigate automatic geolocation ( i.e. <eos> identification of the location, expressed as latitude/longitude coordinates ) of documents. <eos> geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. <eos> we describe several simple supervised methods for document geolocation using only the document ? s raw text as evidence. <eos> all of our methods predict locations in the context of geodesic grids of varying degrees of resolution. <eos> we evaluate the methods on geotagged wikipedia articles and twitter feeds. <eos> for wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. <eos> twitter geolocation is more challenging : we obtain a median error of 479 km, an improvement on previous results for the dataset.
we use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition ( ner ) from news text to web queries. <eos> the key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. <eos> we achieve strong gains in ner performance on news, in-domain and out-of-domain, and on web queries.
standard algorithms for template-based information extraction ( ie ) require predefined template schemas, and often labeled data, to learn to extract their slot fillers ( e.g., an embassy is the target of a bombing template ). <eos> this paper describes an approach to template-based ie that removes this requirement and performs extraction without knowing the template structure in advance. <eos> our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events ( e.g., bombings include detonate, set off, and destroy events ) associated with semantic roles. <eos> we also solve the standard ie task, using the induced syntactic patterns to extract role fillers from specific documents. <eos> we evaluate on the muc-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an f1 score of.40, approaching the performance of algorithms that require full knowledge of the templates.
argumentation schemes are structures or templates for various kinds of arguments. <eos> given the text of an argument with premises and conclusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. <eos> we achieve accuracies of 63 ? 91 % in one-against-others classification and 80 ? 94 % in pairwise classification ( baseline = 50 % in both cases ).
we present a novel model to represent and assess the discourse coherence of text. <eos> our model assumes that coherent text implicitly favors certain types of discourse relation transitions. <eos> we implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a permuted ordering of its sentences. <eos> the experimental results demonstrate that our model is able to significantly outperform the state-ofthe-art coherence model by barzilay and lapata ( 2005 ), reducing the error rate of the previous approach by an average of 29 % over three data sets against human upper bounds. <eos> we further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73 % when the features from both models are combined for the task.
this paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of german. <eos> we investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. <eos> its degree of underspecification. <eos> we extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. <eos> this allows us to study the interaction of voice and word order alternations in realistic german corpus data. <eos> we show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5 % accuracy ( over a baseline of 82.5 % ) in the prediction of the original voice.
we present a novel computational formulation of speaker authority in discourse. <eos> this notion, which focuses on how speakers position themselves relative to each other in discourse, is first developed into a reliable coding scheme ( 0.71 agreement between human annotators ). <eos> we also provide a computational model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with integer linear programming. <eos> we show that this constrained model ? s analyses of speaker authority correlates very strongly with expert human judgments ( r2 coefficient of 0.947 ).
one of the major challenges facing statistical machine translation is how to model differences in word order between languages. <eos> although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. <eos> most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. <eos> this paper presents a novel metric, the lrscore, which explicitly measures the quality of word order by using permutation distance metrics. <eos> we show that the metric is more consistent with human judgements than other metrics, including the bleu score. <eos> we also show that the lrscore can successfully be used as the objective function when training translation model parameters. <eos> training with the lrscore leads to output which is preferred by humans. <eos> moreover, the translations incur no penalty in terms of bleu scores.
this paper proposes a novel reordering model for statistical machine translation ( smt ) by means of modeling the translation orders of the source language collocations. <eos> the model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. <eos> during decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. <eos> the experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 bleu score over the baseline methods.
we present a novel machine translation model which models translation by a linear sequence of operations. <eos> in contrast to the ? n-gram ? <eos> model, this sequence includes not only translation but also reordering operations. <eos> key ideas of our model are ( i ) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and ( ii ) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based mt. <eos> we observe statistically significant improvements in bleu over moses for german-to-english and spanish-to-english tasks, and comparable results for a french-to-english task.
a system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. <eos> under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. <eos> here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. <eos> we also present a behavioral experiment confirming the key empirical predictions of the theory.
when designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. <eos> moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. <eos> the order in which phenomena are treated may therefore have a major impact on the resulting grammar. <eos> this paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. <eos> i argue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. <eos> the idea is illustrated through a comparative study of auxiliary structures in hpsg-based grammars for german and dutch. <eos> auxiliaries form a central phenomenon of german and dutch and are likely to influence many components of the grammar. <eos> this study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.
we consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing ? the unsupervised version of text chunking. <eos> we show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, seginer ? s ( 2007 ) ccl. <eos> these finite-state models are combined in a cascade to produce more general ( full-sentence ) constituent structures ; doing so outperforms ccl by a wide margin in unlabeled parseval scores for english, german and chinese. <eos> finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in ccl.
we propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the web. <eos> we observe that a huge number of concepts are defined in web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. <eos> we show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. <eos> experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 ? <eos> 108 web documents with a precision rate of about 94 %.
we analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. <eos> using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. <eos> we analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. <eos> we show how different summaries use various phrasal information units ( i.e., nuggets ) to express the same atomic semantic units, called factoids. <eos> finally, we present a ranker that employs distributional similarities to build a network of words, and captures the diversity of perspectives by detecting communities in this network. <eos> our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.
in this work, we present a novel approach to the generation task of ordering prenominal modifiers. <eos> we take a maximum entropy reranking approach to the problem which admits arbitrary features on a permutation of modifiers, exploiting hundreds of thousands of features in total. <eos> we compare our error rates to the state-of-the-art and to a strong google ngram count baseline. <eos> we attain a maximum error reduction of 69.8 % and average error reduction across all test sets of 59.1 % compared to the state-of-the-art and a maximum error reduction of 68.4 % and average error reduction across all test sets of 41.8 % compared to our google n-gram count baseline.
in this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. <eos> we present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. <eos> the method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. <eos> by combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. <eos> evaluation on the conll 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.
event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. <eos> the state-of-the-art research on the task is transductive inference ( e.g. <eos> cross-event inference ). <eos> in this paper, we propose a new method of event extraction by well using cross-entity inference. <eos> in contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. <eos> we adopt this inference method to improve the traditional sentence-level event extraction system. <eos> experiments show that we can get 8.6 % gain in trigger ( event ) identification, and more than 11.8 % gain for argument ( role ) classification in ace event extraction.
the goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. <eos> we propose a multilayered event extraction architecture that progressively ? zooms in ? <eos> on relevant information. <eos> our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. <eos> these modules are organized as a pipeline to gradually zero in on event-related information. <eos> we present results on the muc-4 event extraction data set and show that this model performs better than previous systems.
in this paper we give an overview of the knowledge base population ( kbp ) track at the 2010 text analysis conference. <eos> the main goal of kbp is to promote research in discovering facts about entities and augmenting a knowledge base ( kb ) with these facts. <eos> this is done through two tasks, entity linking ? <eos> linking names in context to entities in the kb ? <eos> and slot filling ? <eos> adding information about an entity to the kb. <eos> a large source collection of newswire and web documents is provided from which systems are to discover information. <eos> attributes ( ? slots ? ) <eos> derived from wikipedia infoboxes are used to create the reference kb. <eos> in this paper we provide an overview of the techniques which can serve as a basis for a good kbp system, lay out the remaining challenges by comparison with traditional information extraction ( ie ) and question answering ( qa ) tasks, and provide some suggestions to address these challenges.
this paper focuses on mining the hyponymy ( or is-a ) relation from large-scale, open-domain web documents. <eos> a nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. <eos> based on the model, we design a set of evidence combination and propagation algorithms. <eos> these significantly improve the result quality of existing approaches. <eos> experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20 % performance improvement in terms of p @ 5, map and r-precision.
this paper presents a supervised pronoun anaphora resolution system based on factorial hidden markov models ( fhmms ). <eos> the basic idea is that the hidden states of fhmms are an explicit short-term memory with an antecedent buffer containing recently described referents. <eos> thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. <eos> a system implementing this model is evaluated on the ace corpus with promising performance.
we evaluate several popular models of local discourse coherence for domain and task generality by applying them to chat disentanglement. <eos> using experiments on synthetic multiparty conversations, we show that most models transfer well from text to dialogue. <eos> coherence models improve results overall when good parses and topic models are available, and on a constrained task for real chat data.
dialogue act classification is a central chal-lenge for dialogue systems. <eos> although the im-portance of emotion in human dialogue is widely recognized, most dialogue act classifi-cation models make limited or no use of affec-tive channels in dialogue act classification. <eos> this paper presents a novel affect-enriched dialogue act classifier for task-oriented dia-logue that models facial expressions of users, in particular, facial expressions related to con-fusion. <eos> the findings indicate that the affect-enriched classifiers perform significantly bet-ter for distinguishing user requests for feed-back and grounding dialogue acts within textual dialogue. <eos> the results point to ways in which dialogue systems can effectively lever-age affective channels to improve dialogue act classification.
we develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of web search queries. <eos> extracting meaning from short text segments is difficult as there is little semantic redundancy between terms ; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. <eos> furthermore search queries lack explicit syntax often used to determine intent in question answering. <eos> in this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class pcfg. <eos> this class-label correlation ( clc ) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. <eos> we demonstrate its performance in terms of ( 1 ) its predicted label accuracy on polysemous queries and ( 2 ) its ability to accurately chunk queries into base constituents.
the availability of learner corpora, especially those which have been manually error-tagged or shallow-parsed, is still limited. <eos> this means that researchers do not have a common development and test set for natural language processing of learner english such as for grammatical error detection. <eos> given this background, we created a novel learner corpus that was manually error-tagged and shallowparsed. <eos> this corpus is available for research and educational purposes on the web. <eos> in this paper, we describe it in detail together with its data-collection method and annotation schemes. <eos> another contribution of this paper is that we take the first step toward evaluating the performance of existing postagging/chunking techniques on learner corpora using the created corpus. <eos> these contributions will facilitate further research in related areas such as grammatical error detection and automated essay scoring.
naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. <eos> we demonstrate a variety of mechanisms that increase the translation quality to near professional levels. <eos> specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. <eos> we propose a set of features that model both the translations and the translators, such as country of residence, lm perplexity of the translation, edit rate from the other translations, and ( optionally ) calibration against professional translators. <eos> using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. <eos> we recreate the nist 2009 urdu-toenglish evaluation set with mechanical turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. <eos> the total cost is more than an order of magnitude lower than professional translation.
in many natural language applications, there is a need to enrich syntactical parse trees. <eos> we present a statistical tree annotator augmenting nodes with additional information. <eos> the annotator is generic and can be applied to a variety of applications. <eos> we report 3 such applications in this paper : predicting function tags ; predicting null elements ; and predicting whether a tree constituent is projectable in machine translation. <eos> our function tag prediction system outperforms significantly published results.
we present a discriminative learning method to improve the consistency of translations in phrase-based statistical machine translation ( smt ) systems. <eos> our method is inspired by translation memory ( tm ) systems which are widely used by human translators in industrial settings. <eos> we constrain the translation of an input sentence using the most similar ? translation example ? <eos> retrieved from the tm. <eos> differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. <eos> we observe that using this method can benefit the smt system by not only producing consistent translations, but also improved translation outputs. <eos> we report a 0.9 point improvement in terms of bleu score on english ? chinese technical documents.
the state-of-the-art system combination method for machine translation ( mt ) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. <eos> we introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. <eos> the forest is generated using syntactic consensus among parsed hypotheses : first, mt outputs are parsed. <eos> second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. <eos> third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. <eos> the new hypothesis is produced by searching the best derivation in the forest. <eos> experimental results on the wmt10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.
this paper presents hypothesis mixture decoding ( hm decoding ), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. <eos> hm decoding involves two decoding stages : first, each component system decodes independently, with the explored search space kept for use in the next step ; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the hm decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. <eos> few assumptions are made by our approach about the underlying component systems, enabling us to leverage smt models based on arbitrary paradigms. <eos> we compare our approach with several related techniques, and demonstrate significant bleu improvements in large-scale chinese-to-english translation tasks.
we present minimum bayes-risk system combination, a method that integrates consensus decoding and system combination into a unified multi-system minimum bayes-risk ( mbr ) technique. <eos> unlike other mbr methods that re-rank translations of a single smt system, mbr system combination uses the mbr decision rule and a linear combination of the component systems ? <eos> probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary. <eos> we introduce expected bleu, an approximation to the bleu score that allows to efficiently apply mbr in these conditions. <eos> mbr system combination is a general method that is independent of specific smt models, enabling us to combine systems with heterogeneous structure. <eos> experiments show that our approach bring significant improvements to single-system-based mbr decoding and achieves comparable results to different state-of-the-art system combination methods.
we introduce synchronous tree adjoining grammars ( tag ) into tree-to-string translation, which converts a source tree to a target string. <eos> without reconstructing tag derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned treebank-style trees. <eos> as tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. <eos> less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 bleu over the baseline system only allowing for tree substitution on nist chineseenglish test sets.
in this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation : a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. <eos> we integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. <eos> our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 bleu point over a competitive baseline.
we propose a novel approach to translating from a morphologically complex language. <eos> unlike previous research, which has targeted word inflections and concatenations, we focus on the pairwise relationship between morphologically related words, which we treat as potential paraphrases and handle using paraphrasing techniques at the word, phrase, and sentence level. <eos> an important advantage of this framework is that it can cope with derivational morphology, which has so far remained largely beyond the capabilities of statistical machine translation systems. <eos> our experiments translating from malay, whose morphology is mostly derivational, into english show significant improvements over rivaling approaches based on five automatic evaluation measures ( for 320,000 sentence pairs ; 9.5 million english word tokens ).
we propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. <eos> in a hidden semimarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. <eos> agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. <eos> expanding the state space to include ? gappy phrases ? <eos> ( such as french ne ? <eos> pas ) makes the alignment space more symmetric ; thus, it allows agreement between discontinuous alignments. <eos> the resulting system shows substantial improvements in both alignment quality and translation quality over word-based hidden markov models, while maintaining asymptotically equivalent runtime.
while it is has often been observed that the product of translation is somehow different than non-translated text, scholars have emphasized two distinct bases for such differences. <eos> some have noted interference from the source language spilling over into translation in a source-language-specific way, while others have noted general effects of the process of translation that are independent of source language. <eos> using a series of text categorization experiments, we show that both these effects exist and that, moreover, there is a continuum between them. <eos> there are many effects of translation that are consistent among texts translated from a given source language, some of which are consistent even among texts translated from families of source languages. <eos> significantly, we find that even for widely unrelated source languages and multiple genres, differences between translated texts and non-translated texts are sufficient for a learned classifier to accurately determine if a given text is translated or original.
we present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. <eos> we incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. <eos> we test our hypothesis on different pairs of languages and corpora. <eos> we obtain very high f-measure between 80 % and 98 % for recognizing and extracting correct translations for rare terms ( from 1 to 5 occurrences ). <eos> moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a f-measure of 77 % for the classification of chinese-english translations using a training corpus of spanish-french. <eos> our method is therefore even potentially applicable to low resources languages without training data.
this paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. <eos> we claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. <eos> we experiment with a phrasal matching method in order to : i ) build a system portable across languages, and ii ) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. <eos> results achieved on an english-spanish corpus obtained from the rte3 dataset support our claim, with an overall accuracy above average scores reported by rte participants on monolingual data. <eos> finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.
resolving coordination ambiguity is a classic hard problem. <eos> this paper looks at coordination disambiguation in complex noun phrases ( nps ). <eos> parsers trained on the penn treebank are reporting impressive numbers these days, but they don ? t do very well on this problem ( 79 % ). <eos> we explore systems trained using three types of corpora : ( 1 ) annotated ( e.g. <eos> the penn treebank ), ( 2 ) bitexts ( e.g. <eos> europarl ), and ( 3 ) unannotated monolingual ( e.g. <eos> google n-grams ). <eos> size matters : ( 1 ) is a million words, ( 2 ) is potentially billions of words and ( 3 ) is potentially trillions of words. <eos> the unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. <eos> the bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. <eos> we train separate classifiers with monolingual and bilingual features and iteratively improve them via co-training. <eos> the co-trained classifier achieves close to 96 % accuracy on treebank data and makes 20 % fewer errors than a supervised system trained with treebank annotations.
we propose a novel unsupervised method for separating out distinct authorial components of a document. <eos> in particular, we show that, given a book artificially ? munged ? <eos> from two thematically similar biblical books, we can separate out the two constituent books almost perfectly. <eos> this allows us to automatically recapitulate many conclusions reached by bible scholars over centuries of research. <eos> one of the key elements of our method is exploitation of differences in synonym choice by different authors.
we present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. <eos> using aggregate demographic statistics about the authors ? <eos> geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. <eos> by imposing a composite `1, ? <eos> regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. <eos> we perform two regression studies. <eos> first, we use term frequencies to predict demographic attributes ; our method identifies a compact set of words that are strongly associated with author demographics. <eos> next, we conjoin demographic attributes into features, which we use to predict term frequencies. <eos> the composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.
disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. <eos> the comprehensiveness of wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. <eos> disambiguation to wikipedia is similar to a traditional word sense disambiguation task, but distinct in that the wikipedia link structure provides additional information about which disambiguations are compatible. <eos> in this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document ( which we call ? global ? <eos> approaches ), and compare them to more traditional ( local ) approaches. <eos> we show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.
the large combined search space of joint word segmentation and part-of-speech ( pos ) tagging makes efficient decoding very hard. <eos> as a result, effective high order features representing rich contexts are inconvenient to use. <eos> in this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. <eos> our solution is a two step process. <eos> first, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and pos information. <eos> second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with pos tags by a fine-grained sub-word tagger. <eos> the coarse-to-fine search scheme is efficient, while in the sub-word tagging step rich contextual features can be approximately derived. <eos> evaluation on the penn chinese treebank shows that our model yields improvements over the best system reported in the literature.
translating compounds is an important problem in machine translation. <eos> since many compounds have not been observed during training, they pose a challenge for translation systems. <eos> previous decompounding methods have often been restricted to a small set of languages as they can not deal with more complex compound forming processes. <eos> we present a novel and unsupervised method to learn the compound parts and morphological operations needed to split compounds into their compound parts. <eos> the method uses a bilingual corpus to learn the morphological operations required to split a compound into its parts. <eos> furthermore, monolingual corpora are used to learn and filter the set of compound part candidates. <eos> we evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach.
there are several tasks where is preferable not responding than responding incorrectly. <eos> this idea is not new, but despite several previous attempts there isn ? t a commonly accepted measure to assess non-response. <eos> we study here an extension of accuracy measure with this feature and a very easy to understand interpretation. <eos> the measure proposed ( c @ 1 ) has a good balance of discrimination power, stability and sensitivity properties. <eos> we show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. <eos> this measure is well suited for tasks such as reading comprehension tests, where multiple choices per question are given, but only one is correct.
in this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users ? <eos> information needs. <eos> our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. <eos> we show that translation model can be effectively utilized to predict the information need given only the user ? s query question. <eos> experiments show that the proposed information need prediction approach can improve the performance of question recommendation.
we describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. <eos> our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. <eos> we construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. <eos> the label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15 % absolute improvement in frame identification accuracy and over 13 % absolute improvement in full frame-semantic parsing f1 score on a blind test set, over a state-of-the-art supervised baseline.
we propose a non-parametric bayesian model for unsupervised semantic parsing. <eos> following poon and domingos ( 2009 ), we consider a semantic parsing setting where the goal is to ( 1 ) decompose the syntactic dependency tree of a sentence into fragments, ( 2 ) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and ( 3 ) predict predicate-argument relations between the fragments. <eos> we use hierarchical pitmanyor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. <eos> we develop a modification of the metropolishastings split-merge sampler, resulting in an efficient inference algorithm for the model. <eos> the method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.
this paper presents an unsupervised method for deriving inference axioms by composing semantic relations. <eos> the method is independent of any particular relation inventory. <eos> it relies on describing semantic relations using primitives and manipulating these primitives according to an algebra. <eos> the method was tested using a set of eight semantic relations yielding 78 inference axioms which were evaluated over propbank.
learning by reading ( lbr ) aims at enabling machines to acquire knowledge from and reason about textual input. <eos> this requires knowledge about the domain structure ( such as entities, classes, and actions ) in order to do inference. <eos> we present a method to infer this implicit knowledge from unlabeled text. <eos> unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. <eos> from a corpus of 1.4m sentences, we learn about 250k simple propositions about american football in the form of predicateargument structures like ? quarterbacks throw passes to receivers ?. <eos> using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. <eos> human subjects judged up to 96.6 % of the resulting propositions to be sensible. <eos> the classes and probabilistic model can be used in textual enrichment to improve the performance of lbr end-to-end systems.
in this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. <eos> the induction step and the disambiguation step are based on the same principle : words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. <eos> the intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions ; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. <eos> the model is evaluated on the semeval-2010 word sense induction and disambiguation task, on which it reaches stateof-the-art results.
current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. <eos> this supervision bottleneck is one of the major difficulties in scaling up semantic parsing. <eos> we argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. <eos> the algorithm takes a self training approach driven by confidence estimation. <eos> evaluated over geoquery, a standard dataset for this task, our system achieved 66 % accuracy, compared to 80 % of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.
in this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. <eos> the important aspects are identified according to two observations : ( a ) the important aspects of a product are usually commented by a large number of consumers ; and ( b ) consumers ? <eos> opinions on the important aspects greatly influence their overall opinions on the product. <eos> in particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers ? <eos> opinions on these aspects via a sentiment classifier. <eos> we then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers ? <eos> opinions given to each aspect on their overall opinions. <eos> the experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. <eos> we further apply the aspect ranking results to the application of documentlevel sentiment classification, and improve the performance significantly.
this paper explores approaches to sentiment classification of u.s. congressional floordebate transcripts. <eos> collective classification techniques are used to take advantage of the informal citation structure present in the debates. <eos> we use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. <eos> our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.
building on earlier work that integrates different factors in language modeling, we view ( i ) backing off to a shorter history and ( ii ) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation. <eos> this view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events. <eos> we construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models.
topic models have been successfully applied to many document analysis tasks to discover topics embedded in text. <eos> however, existing topic models generally can not capture the latent topical structures in documents. <eos> since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks. <eos> in this work, we propose a new topic model, structural topic model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order markov chain. <eos> experiment results show that the proposed structural topic model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering.
we propose a method for automatically labelling topics learned via lda topic models. <eos> we generate our label candidate set from the top-ranking topic terms, titles of wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the wikipedia article titles. <eos> we rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. <eos> our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.
cross-language document summarization is defined as the task of producing a summary in a target language ( e.g. <eos> chinese ) for a set of documents in a source language ( e.g. <eos> english ). <eos> existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language. <eos> in this study, we propose to use the bilingual information from both the source and translated documents for this task. <eos> two summarization methods ( simfusion and corank ) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. <eos> experimental results on the duc2001 dataset with manually translated reference chinese summaries show the effectiveness of the proposed methods.
in this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. <eos> conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. <eos> this paper extends previous work to wordto-word selectional preferences by using webscale data. <eos> experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. <eos> there is no data like more data, performance improves log-linearly with the number of parameters ( unique n-grams ). <eos> more importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.
it is well known that parsing accuracy suffers when a model is applied to out-of-domain data. <eos> it is also known that the most beneficial data to parse a given domain is data that matches the domain ( sekine, 1997 ; gildea, 2001 ). <eos> hence, an important task is to select appropriate domains. <eos> however, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. <eos> as more and more data becomes available, automatic ways to select data that is beneficial for a new ( unknown ) target domain are becoming attractive. <eos> this paper evaluates various ways to automatically acquire related training data for a given test set. <eos> the results show that an unsupervised technique based on topic models is effective ? <eos> it outperforms random data selection on both languages examined, english and dutch. <eos> moreover, the technique works better than manually assigned labels gathered from meta-data that is available for english.
we present a systematic comparison and combination of two orthogonal techniques for efficient parsing of combinatory categorial grammar ( ccg ). <eos> first we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser ? s search space using a separate sequence model. <eos> next we consider several variants on a*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like ccg. <eos> in addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of a* parsing on the more realistic but more stringent metric of cpu time. <eos> by itself, a* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for ccg this does not always correspond to improvements in cpu time over a cky baseline. <eos> combining a* with adaptive supertagging decreases cpu time by 15 % for our best model.
we explore the contribution of morphological features ? <eos> both lexical and inflectional ? <eos> to dependency parsing of arabic, a morphologically rich language. <eos> using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. <eos> we further contrast the contribution of form-based and functional features, and show that functional gender and number ( e.g., ? broken plurals ? ) <eos> and the related rationality feature improve over form-based features. <eos> it is the first time functional morphological features are used for arabic nlp.
recent work has shown how a parallel corpus can be leveraged to build syntactic parser for a target language by projecting automatic source parse onto the target sentence using word alignments. <eos> the projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers. <eos> in this paper, we present a greedy non-directional parsing algorithm which doesn ? t need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. <eos> our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for bulgarian, spanish and hindi. <eos> it also gave a significant improvement over previously reported results for bulgarian and set a benchmark for hindi.
the role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels ( e.g., brazilian cities, business centers, hilly sites ) extracted from web documents for various instances ( e.g., rio de janeiro ). <eos> the co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. <eos> experiments over evaluation sets of instances associated with web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts.
text mining and data harvesting algorithms have become popular in the computational linguistics community. <eos> they employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process ( or both ) in a recursive cycle, using data learned in one step to generate more seeds for the next. <eos> they therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. <eos> the results of computational network analysis, especially from the world wide web, are thus applicable. <eos> surprisingly, these results have not yet been broadly introduced into the computational linguistics community. <eos> in this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.
nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a ? crime ? <eos> event can cause a ? investigation ? <eos> event, which can lead to an ? arrest ? <eos> event. <eos> however, most current approaches address event extraction with highly local models that extract each event and argument independently. <eos> we propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. <eos> this provides a simple framework that captures global properties of both nested and flat event structures. <eos> we explore a rich feature space that models both the events to be parsed and context from the original supporting text. <eos> our approach obtains competitive results in the extraction of biomedical events from the bionlp ? 09 shared task with a f1 score of 53.5 % in development and 48.6 % in testing.
the automatic extraction of comparative information is an important text mining problem and an area of increasing interest. <eos> in this paper, we study how to build a korean comparison mining system. <eos> our work is composed of two consecutive tasks : 1 ) classifying comparative sentences into different types and 2 ) mining comparative entities and predicates. <eos> we perform various experiments to find relevant features and learning techniques. <eos> as a result, we achieve outstanding performance enough for practical use.
in this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks. <eos> we prove that the semiring allows for exact encoding of backoff models with epsilon transitions. <eos> this allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit ( on-line ) failure transition representations. <eos> we present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection.
active learning ( al ) is typically initialized with a small seed of examples selected randomly. <eos> however, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. <eos> our contribution is twofold : ( 1 ) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and ( 2 ) we use this technique for seeding al and demonstrate that it leads to a higher learning rate. <eos> the evaluation is conducted in the context of word sense disambiguation.
we propose a generative model based on temporal restricted boltzmann machines for transition based dependency parsing. <eos> the parse tree is built incrementally using a shiftreduce parse and an rbm is used to model each decision step. <eos> the rbm at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. <eos> our parser achieves labeled and unlabeled attachment scores of 88.72 % and 91.65 % respectively, which compare well with similar previous models and the state-of-the-art.
we describe a novel mechanism called reservoir counting for application in online locality sensitive hashing. <eos> this technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.
we investigate the empirical behavior of ngram discounts within and across domains. <eos> when a language model is trained and evaluated on two corpora from exactly the same domain, discounts are roughly constant, matching the assumptions of modified kneser-ney lms. <eos> however, when training and test corpora diverge, the empirical discount grows essentially as a linear function of the n-gram count. <eos> we adapt a kneser-ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified kneser-ney and jelinek-mercer baselines.
in bootstrapping ( seed set expansion ), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. <eos> in this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to pantel and pennacchiotti ? s espresso bootstrapping algorithm. <eos> the idea is to select seeds and create a stop list using the rankings of instances and patterns computed by kleinberg ? s hits algorithm. <eos> experimental results on a variation of the lexical sample task show the effectiveness of our method.
the written form of arabic, modern standard arabic ( msa ), differs quite a bit from the spoken dialects of arabic, which are the true ? native ? <eos> languages of arabic speakers used in daily life. <eos> however, due to msa ? s prevalence in written form, almost all arabic datasets have predominantly msa content. <eos> we present the arabic online commentary dataset, a 52m-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level ( and dialect itself ) in each sentence of the dataset. <eos> so far, we have labeled 108k sentences, 41 % of which as having dialectal content. <eos> we also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation.
we address the problem of part-of-speech tagging for english data from the popular microblogging service twitter. <eos> we develop a tagset, annotate data, develop features, and report tagging results nearing 90 % accuracy. <eos> the data and tools have been made available to the research community with the goal of enabling richer text analysis of twitter and related social media data sets.
this paper introduces a new training set condensation technique designed for mixtures of labeled and unlabeled data. <eos> it finds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classification accuracy. <eos> we evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the wall street journal data set.
transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. <eos> however, a single model can not deal with different words from different language origins, e.g., ? get ? <eos> in ? piaget ? <eos> and ? target. ? <eos> li et al. <eos> ( 2007 ) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly. <eos> this model, however, requires an explicitly tagged training set with language origins. <eos> we propose a novel method which models language origins as latent classes. <eos> the parameters are learned from a set of transliterated word pairs via the em algorithm. <eos> the experimental results of the transliteration task of western names to japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.
beginning with goldsmith ( 1976 ), the phonological tier has a long history in phonological theory to describe non-local phenomena. <eos> this paper defines a class of formal languages, the tier-based strictly local languages, which begin to describe such phenomena. <eos> then this class is located within the subregular hierarchy ( mcnaughton and papert, 1971 ). <eos> it is found that these languages contain the strictly local languages, are star-free, are incomparable with other known sub-star-free classes, and have other interesting properties.
we investigate authorship attribution using classifiers based on frame semantics. <eos> the purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. <eos> our results suggest ( i ) that frame-based classifiers are usable for author attribution of both translated and untranslated texts ; ( ii ) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but ( iii ) perform as well as, or superior to the baseline classifiers on translated texts ; ( iv ) that ? contrary to current belief ? na ? ve classifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.
most text message normalization approaches are based on supervised learning and rely on human labeled training data. <eos> in addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. <eos> in this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. <eos> our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. <eos> to avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level alignment for model training. <eos> experiments on both twitter and sms messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker ( absolute accuracy gain of 21.69 % and 18.16 % over jazzy spell checker on the two test sets respectively ).
this paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation. <eos>
community-based knowledge forums, such as wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. <eos> most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in wikipedia. <eos> in this paper, we explore more linguistically motivated approaches to vandalism detection. <eos> in particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. <eos> experimental results suggest that ( 1 ) statistical models give evidence to unique language styles in vandalism, and that ( 2 ) deep syntactic patterns based on probabilistic context free grammars ( pcfg ) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams.
humor identification is a hard natural language understanding problem. <eos> we identify a subproblem ? <eos> the ? that ? s what she said ? <eos> problem ? <eos> with two distinguishing characteristics : ( 1 ) use of nouns that are euphemisms for sexually explicit nouns and ( 2 ) structure common in the erotic domain. <eos> we address this problem in a classification approach that includes features that model those two characteristics. <eos> experiments on web data demonstrate that our approach improves precision by 12 % over baseline techniques that use only word-based features.
individual utterances often serve multiple communicative purposes in dialogue. <eos> we present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. <eos> our approach results in significantly increased understanding of user intent, compared to two strong baselines.
we investigate systems that identify opinion expressions and assigns polarities to the extracted expressions. <eos> in particular, we demonstrate the benefit of integrating opinion extraction and polarity classification into a joint model using features reflecting the global polarity structure. <eos> the model is trained using large-margin structured prediction methods. <eos> the system is evaluated on the mpqa opinion corpus, where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classification. <eos> the results show an improvement of between 10 and 15 absolute points in f-measure.
this opinion paper discusses subjective natural language problems in terms of their motivations, applications, characterizations, and implications. <eos> it argues that such problems deserve increased attention because of their potential to challenge the status of theoretical understanding, problem-solving methods, and evaluation techniques in computational linguistics. <eos> the author supports a more holistic approach to such problems ; a view that extends beyond opinion mining or sentiment analysis.
in conversation, when speech is followed by a backchannel, evidence of continued engagement by one ? s dialogue partner, that speech displays a combination of cues that appear to signal to one ? s interlocutor that a backchannel is appropriate. <eos> we term these cues backchannel-preceding cues ( bpc ) s, and examine the columbia games corpus for evidence of entrainment on such cues. <eos> entrainment, the phenomenon of dialogue partners becoming more similar to each other, is widely believed to be crucial to conversation quality and success. <eos> our results show that speaking partners entrain on bpcs ; that is, they tend to use similar sets of bpcs ; this similarity increases over the course of a dialogue ; and this similarity is associated with measures of dialogue coordination and task success.
we investigate the use of textual internet conversations for detecting questions in spoken conversations. <eos> we compare the text-trained model with models trained on manuallylabeled, domain-matched spoken utterances with and without prosodic features. <eos> overall, the text-trained model achieves over 90 % of the performance ( measured in area under the curve ) of the domain-matched model including prosodic features, but does especially poorly on declarative questions. <eos> we describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation.
we extend the popular entity grid representation for local coherence modeling. <eos> the grid abstracts away information about the entities it models ; we add discourse prominence, named entity type and coreference features to distinguish between important and unimportant entities. <eos> we improve the best result for wsj document discrimination by 6 %.
this article presents the main points in the creation of the french timebank ( bittar, 2010 ), a reference corpus annotated according to the iso-timeml standard for temporal annotation. <eos> a number of improvements were made to the markup language to deal with linguistic phenomena not yet covered by iso-timeml, including cross-language modifications and others specific to french. <eos> an automatic preannotation system was used to speed up the annotation process. <eos> a preliminary evaluation of the methodology adopted for this project yields positive results in terms of data quality and annotation time.
web search is an information-seeking activity. <eos> often times, this amounts to a user seeking answers to a question. <eos> however, queries, which encode user ? s information need, are typically not expressed as full-length natural language sentences ? <eos> in particular, as questions. <eos> rather, they consist of one or more text fragments. <eos> as humans become more searchengine-savvy, do natural-language questions still have a role to play in web search ? <eos> through a systematic, large-scale study, we find to our surprise that as time goes by, web users are more likely to use questions to express their search intent.
previous work on quantifier scope annotation focuses on scoping sentences with only two quantified noun phrases ( nps ), where the quan-tifiers are restricted to a predefined list. <eos> it also ignores negation, modal/logical operators, and other sentential adverbials. <eos> we present a com-prehensive scope annotation scheme. <eos> we anno-tate the scope interaction between all scopal terms in the sentence from quantifiers to scopal adverbials, without putting any restriction on the number of scopal terms in a sentence. <eos> in ad-dition, all nps, explicitly quantified or not, with no restriction on the type of quantification, are investigated for possible scope interactions.
mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. <eos> previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. <eos> in this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. <eos> we first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. <eos> since the candidate alignments are noisy, we develop a robust learning algorithm to learn the interlingual representation. <eos> we show that bilingual dictionaries generalize to different domains better : our approach gives better performance than either a word by word translation method or canonical correlation analysis ( cca ) trained on a different domain.
this work introduces am-fm, a semantic framework for machine translation evaluation. <eos> based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. <eos> the metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. <eos> comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks ( overall quality assessment and comparative ranking ) over a large collection of human evaluations involving five european languages. <eos> finally, the main pros and cons of the proposed framework are discussed along with future research directions.
word is usually adopted as the smallest unit in most tasks of chinese language processing. <eos> however, for automatic evaluation of the quality of chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. <eos> so far, there has been no detailed study to compare the correlations of these two approaches with human assessment. <eos> in this paper, we compare word-level metrics with characterlevel metrics on the submitted output of english-to-chinese translation systems in the iwslt ? 08 ct-ec and nist ? 08 ec tasks. <eos> our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. <eos> our analysis suggests several key reasons behind this finding.
word alignment is a central problem in statistical machine translation ( smt ). <eos> in recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. <eos> the objective of this work is to explore the performance limit of supervised alignment under the current smt paradigm. <eos> our experiments used a manually aligned chineseenglish corpus with 280k words recently released by the linguistic data consortium ( ldc ). <eos> we treated the human alignment as the oracle of supervised alignment. <eos> the result is surprising : the gain of human alignment over a state of the art unsupervised method ( giza++ ) is less than 1 point in bleu. <eos> furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions.
we cast the word alignment problem as maximizing a submodular function under matroid constraints. <eos> our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. <eos> we show that submodularity naturally arises when modeling word fertility. <eos> experiments on the english-french hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.
in statistical machine translation, a researcher seeks to determine whether some innovation ( e.g., a new feature, model, or inference algorithm ) improves translation quality in comparison to a baseline system. <eos> to answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. <eos> in this paper, we consider how to make such experiments more statistically reliable. <eos> we provide a systematic analysis of the effects of optimizer instability ? an extraneous variable that is seldom controlled for ? on experimental outcomes, and make recommendations for reporting results more accurately.
in this work, we compare the translation performance of word alignments obtained via bayesian inference to those obtained via expectation-maximization ( em ). <eos> we propose a gibbs sampler for fully bayesian inference in ibm model 1, integrating over all possible parameter values in finding the alignment distribution. <eos> we show that bayesian inference outperforms em in all of the tested language pairs, domains and data set sizes, by up to 2.99 bleu points. <eos> we also show that the proposed method effectively addresses the well-known rare word problem in em-estimated models ; and at the same time induces a much smaller dictionary of bilingual word-pairs.
transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. <eos> in this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. <eos> in the standard penn treebank setup, our novel features improve attachment score form 91.4 % to 92.9 %, giving the best results so far for transitionbased parsing and rivaling the best results overall. <eos> for the chinese treebank, they give a signficant improvement of the state of the art. <eos> an open source release of our parser is freely available.
an attractive property of attribute-value grammars is their reversibility. <eos> attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. <eos> we propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking.
graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins. <eos> state-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree ; they label tokens with roles such as root, leaf, or attaches-tothe-left, and then filter arcs accordingly. <eos> because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others. <eos> we integrate the various pointwise decisions as latent variables in a single arc-level svm classifier. <eos> this novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length. <eos> our system filters 32 % more arcs than the independently-trained classifiers, without reducing filtering speed. <eos> this leads to faster parsing with no reduction in accuracy.
we propose a model that incorporates an insertion operator in bayesian tree substitution grammars ( btsg ). <eos> tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than btsg. <eos> the experimental parsing results show that our model outperforms a standard pcfg and btsg for a small dataset. <eos> for a large dataset, our model obtains comparable results to btsg, making the number of grammar rules much smaller than with btsg.
we present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. <eos> this method outperforms the best published method we are aware of on english and a recently published method on chinese.
in this paper, we show that local features computed from the derivations of tree substitution grammars ? <eos> such as the identify of particular fragments, and a count of large and small fragments ? <eos> are useful in binary grammatical classification tasks. <eos> such features outperform n-gram features and various model scores by a wide margin. <eos> although they fall short of the performance of the hand-crafted feature set of charniak and johnson ( 2005 ) developed for parse tree reranking, they do so with an order of magnitude fewer features. <eos> furthermore, since the tsgs employed are learned in a bayesian setting, the use of their derivations can be viewed as the automatic discovery of tree patterns useful for classification. <eos> on the bllip dataset, we achieve an accuracy of 89.9 % in discriminating between grammatical text and samples from an n-gram language model.
we propose a new method for query-oriented extractive multi-document summarization. <eos> to enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. <eos> we then formulate the summarization problem as a maximum coverage problem with knapsack constraints based on word pairs rather than single words. <eos> our experiments with the ntcir aclia question answering test collections show that our method achieves a pyramid f3-score of up to 0.313, a 36 % improvement over a baseline using maximal marginal relevance.
in this paper, we argue that ordering prenominal modifiers ? <eos> typically pursued as a supervised modeling task ? <eos> is particularly wellsuited to semi-supervised approaches. <eos> by relying on automatic parses to extract noun phrases, we can scale up the training data by orders of magnitude. <eos> this minimizes the predominant issue of data sparsity that has informed most previous approaches. <eos> we compare several recent approaches, and find improvements from additional training data across the board ; however, none outperform a simple n-gram model.
this short paper introduces an implemented and evaluated monolingual text-to-text generation system. <eos> the system takes monologue and transforms it to two-participant dialogue. <eos> after briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.
in this paper, we address the problem of optimizing the style of textual content to make it more suitable to being listened to by a user as opposed to being read. <eos> we study the differences between the written style and the audio style by consulting the linguistics and journalism literatures. <eos> guided by this study, we suggest a number of linguistic features to distinguish between the two styles. <eos> we show the correctness of our features and the impact of style transformation on the user experience through statistical analysis, a style classification task, and a user study.
the task of aligning corresponding phrases across two related sentences is an important component of approaches for natural language problems such as textual inference, paraphrase detection and text-to-text generation. <eos> in this work, we examine a state-of-the-art structured prediction model for the alignment task which uses a phrase-based representation and is forced to decode alignments using an approximate search approach. <eos> we propose instead a straightforward exact decoding technique based on integer linear programming that yields order-of-magnitude improvements in decoding speed. <eos> this ilp-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model.
annotating training data for event extraction is tedious and labor-intensive. <eos> most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. <eos> in this paper, we present a novel self-training strategy, which uses information retrieval ( ir ) to collect a cluster of related documents as the resource for bootstrapping. <eos> also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. <eos> we compare this approach to self-training on a normal newswire corpus and show that ir can provide a better corpus for bootstrapping and that global inference can further improve instance selection. <eos> we obtain gains of 1.7 % in trigger labeling and 2.3 % in role labeling through ir and an additional 1.1 % in trigger labeling and 1.3 % in role labeling by applying global inference.
state-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. <eos> unfortunately, their use introduces a substantial amount of supervised knowledge. <eos> we present the relation guided bootstrapping ( rgb ) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. <eos> this removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.
we explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains : we generate additional training examples by substituting temporal expression words with potential synonyms. <eos> we explore using synonyms both from wordnet and from the latent words language model ( lwlm ), which predicts synonyms in context using an unsupervised approach. <eos> we evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from tempeval 2010, reuters and wikipedia. <eos> we find that the lwlm provides substan-tial improvements on the reuters corpus, and smaller improvements on the wikipedia corpus. <eos> we find that wordnet alne never improves performance, though intersecting the examples from the lwlm and wordnet provides more stable results for wikipedia.
in this paper, we extend distant supervision ( ds ) based on wikipedia for relation extraction ( re ) by considering ( i ) relations defined in external repositories, e.g. <eos> yago, and ( ii ) any subset of wikipedia documents. <eos> we show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. <eos> our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful f1 of 74.29 % on a manually annotated test set : this highly improves the state-of-art in re using ds. <eos> additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document.
as an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. <eos> most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. <eos> sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. <eos> we explore whether coreference can improve the learning process. <eos> that is, if the algorithm considered examples such as his sister, would accuracy be improved ? <eos> with coreference, we see on average a 2-fold increase in f-score. <eos> despite using potentially errorful machine coreference, we see significant increase in recall on all relations. <eos> precision increases in four cases and decreases in six.
we present an approach of expanding parallel corpora for machine translation. <eos> by utilizing semantic role labeling ( srl ) on one side of the language pair, we extract srl substitution rules from existing parallel corpus. <eos> the rules are then used for generating new sentence pairs. <eos> an svm classifier is built to filter the generated sentence pairs. <eos> the filtered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with baseline models. <eos> experimental results on chineseenglish machine translation tasks show an average improvement of 0.45 bleu and 1.22 ter points across 5 different nist test sets.
broad-coverage semantic annotations for training statistical learners are only available for a handful of languages. <eos> previous approaches to cross-lingual transfer of semantic annotations have addressed this problem with encouraging results on a small scale. <eos> in this paper, we scale up previous efforts by using an automatic approach to semantic annotation that does not rely on a semantic ontology for the target language. <eos> moreover, we improve the quality of the transferred semantic annotations by using a joint syntacticsemantic parser that learns the correlations between syntax and semantics of the target language and smooths out the errors from automatic transfer. <eos> we reach a labelled f-measure for predicates and arguments of only 4 % and 9 % points, respectively, lower than the upper bound from manual annotations.
this paper presents a new approach to detecting and tracking changes in word meaning by visually modeling and representing diachronic development in word contexts. <eos> previous studies have shown that computational models are capable of clustering and disambiguating senses, a more recent trend investigates whether changes in word meaning can be tracked by automatic methods. <eos> the aim of our study is to offer a new instrument for investigating the diachronic development of word senses in a way that allows for a better understanding of the nature of semantic change in general. <eos> for this purpose we combine techniques from the field of visual analytics with unsupervised methods from natural language processing, allowing for an interactive visual exploration of semantic change.
kim ellen riloff st ? phane m. meystre school of computing school of computing department of biomedical informatics university of utah university of utah university of utah salt lake city, ut salt lake city, ut salt lake city, ut youngjun @ cs.utah.edu riloff @ cs.utah.edu stephane.meystre @ hsc.utah.edu abstract we present an nlp system that classifies the assertion type of medical problems in clinical notes used for the fourth i2b2/va challenge. <eos> our classifier uses a variety of linguistic fea-tures, including lexical, syntactic, lexico-syntactic, and contextual features. <eos> to overcome an extremely unbalanced distribution of asser-tion types in the data set, we focused our efforts on adding features specifically to improve the performance of minority classes. <eos> as a result, our system reached 94.17 % micro-averaged and 79.76 % macro-averaged f1-measures, and showed substantial recall gains on the minority classes.
this paper describes a set of exploratory experiments for a multilingual classificationbased approach to word sense disambiguation. <eos> instead of using a predefined monolingual sense-inventory such as wordnet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus. <eos> we built five classifiers with english as an input language and translations in the five supported languages ( viz. <eos> french, dutch, italian, spanish and german ) as classification output. <eos> the feature vectors incorporate both the more traditional local context features, as well as binary bag-of-words features that are extracted from the aligned translations. <eos> our results show that the parasense multilingual wsd system shows very competitive results compared to the best systems that were evaluated on the semeval-2010 cross-lingual word sense disambiguation task for all five target languages.
we present a preliminary study on unsupervised preposition sense disambiguation ( psd ), comparing different models and training techniques ( em, map-em with l0 norm, bayesian inference using gibbs sampling ). <eos> to our knowledge, this is the first attempt at unsupervised preposition sense disambiguation. <eos> our best accuracy reaches 56 %, a significant improvement ( at p <.001 ) of 16 % over the most-frequent-sense baseline.
understanding language requires both linguistic knowledge and knowledge about how the world works, also known as common-sense knowledge. <eos> we attempt to characterize the kinds of common-sense knowledge most often involved in recognizing textual entailments. <eos> we identify 20 categories of common-sense knowledge that are prevalent in textual entailment, many of which have received scarce attention from researchers building collections of knowledge.
in many computational linguistic scenarios, training labels are subjectives making it necessary to acquire the opinions of multiple annotators/experts, which is referred to as ? wisdom of crowds ?. <eos> in this paper, we propose a new approach for modeling wisdom of crowds based on the latent mixture of discriminative experts ( lmde ) model that can automatically learn the prototypical patterns and hidden dynamic among different experts. <eos> experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations.
for 20 years, information extraction has focused on facts expressed in text. <eos> in contrast, this paper is a snapshot of research in progress on inferring properties and relationships among participants in dialogs, even though these properties/relationships need not be expressed as facts. <eos> for instance, can a machine detect that someone is attempting to persuade another to action or to change beliefs or is asserting their credibility ? <eos> we report results on both english and arabic discussion forums.
annotated corpora are essential for almost all nlp applications. <eos> whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. <eos> with this work we want to draw attention to this fact. <eos> additionally, we try to estimate the amount of errors and propose a method for their automatic correction. <eos> whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. <eos> at last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary.
we present an enriched version of the penn arabic treebank ( maamouri et al, 2004 ), where latent features necessary for modeling morpho-syntactic agreement in arabic are manually annotated. <eos> we describe our process for efficient annotation, and present the first quantitative analysis of arabic morphosyntactic phenomena.
broad coverage lexicons for the english language have traditionally been handmade. <eos> this approach, while accurate, requires too much human labor. <eos> furthermore, resources contain gaps in coverage, contain specific types of information, or are incompatible with other resources. <eos> we believe that the state of open-license technology is such that a comprehensive syntactic lexicon can be automatically compiled. <eos> this paper describes the creation of such a lexicon, nu-lex, an open-license feature-based lexicon for general purpose parsing that combines wordnet, verbnet, and wiktionary and contains over 100,000 words. <eos> nu-lex was integrated into a bottom up chart parser. <eos> we ran the parser through three sets of sentences, 50 sentences total, from the simple english wikipedia and compared its performance to the same parser using comlex. <eos> both parsers performed almost equally with nu-lex finding all lex-items for 50 % of the sentences and comlex succeeding for 52 %. <eos> furthermore, nulex ? s shortcomings primarily fell into two categories, suggesting future research directions.
colour is a key component in the successful dissemination of information. <eos> since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complemented with the use of appropriate colours in information visualization and product marketing. <eos> yet, there is no comprehensive resource that captures concept ? colour associations. <eos> we present a method to create a large word ? colour association lexicon by crowdsourcing. <eos> a wordchoice question was used to obtain sense-level annotations and to ensure data quality. <eos> we focus especially on abstract concepts and emotions to show that even they tend to have strong colour associations. <eos> thus, using the right colours can not only improve semantic coherence, but also inspire the desired emotional response.
we present conditional random fields based approaches for detecting agreement/disagreement between speakers in english broadcast conversation shows. <eos> we develop annotation approaches for a variety of linguistic phenomena. <eos> various lexical, structural, durational, and prosodic features are explored. <eos> we compare the performance when using features extracted from automatically generated annotations against that when using human annotations. <eos> we investigate the efficacy of adding prosodic features on top of lexical, structural, and durational features. <eos> since the training data is highly imbalanced, we explore two sampling approaches, random downsampling and ensemble downsampling. <eos> overall, our approach achieves 79.2 % ( precision ), 50.5 % ( recall ), 61.7 % ( f1 ) for agreement detection and 69.2 % ( precision ), 46.9 % ( recall ), and 55.9 % ( f1 ) for disagreement detection, on the english broadcast conversation data.
word alignment has an exponentially large search space, which often makes exact inference infeasible. <eos> recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. <eos> however, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. <eos> in this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. <eos> we also propose a variant of the grammar which eliminates those ambiguities. <eos> our grammar shows advantages over previous grammars in both synthetic and real-world experiments.
there are a number of systems that use a syntax-based reordering step prior to phrasebased statistical mt. <eos> an early work proposing this idea showed improved translation performance, but subsequent work has had mixed results. <eos> speculations as to cause have suggested the parser, the data, or other factors. <eos> we systematically investigate possible factors to give an initial answer to the question : under what conditions does this use of syntax help psmt ?
in interactive machine translation ( imt ), a human expert is integrated into the core of a machine translation ( mt ) system. <eos> the human expert interacts with the imt system by partially correcting the errors of the system ? s output. <eos> then, the system proposes a new solution. <eos> this process is repeated until the output meets the desired quality. <eos> in this scenario, the interaction is typically performed using the keyboard and the mouse. <eos> in this work, we present an alternative modality to interact within imt systems by writing on a tactile display or using an electronic pen. <eos> an on-line handwritten text recognition ( htr ) system has been specifically designed to operate with imt systems. <eos> our htr system improves previous approaches in two main aspects. <eos> first, htr decoding is tightly coupled with the imt system. <eos> second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based ibm models. <eos> the proposed system achieves an important boost in performance with respect to previous work.
in this paper, we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. <eos> in order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. <eos> we show that the tunable ter-plus metric from machine translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources.
we present an scfg binarization algorithm that combines the strengths of early terminal matching on the source language side and early language model integration on the target language side. <eos> we also examine how different strategies of target-side terminal attachment during binarization can significantly affect translation quality.
we show that unseen words account for a large part of the translation error when moving to new domains. <eos> using an extension of a recent approach to mining translations from comparable corpora ( haghighi et al, 2008 ), we are able to find translations for otherwise oov terms. <eos> we show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality ( between 0.5 and 1.5 bleu points ) on four domains and two language pairs.
we discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. <eos> we examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. <eos> we also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases.
to address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation ( sdg ) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. <eos> experiments on chinese-english translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. <eos> our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models.
in this paper we present a novel discriminative mixture model for statistical machine translation ( smt ). <eos> we model the feature space with a log-linear combination of multiple mixture components. <eos> each component contains a large set of features trained in a maximumentropy framework. <eos> all features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. <eos> this approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for smt. <eos> it is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. <eos> the proposed approach improves the translation performance significantly on a large-scale arabic-to-english mt task.
one problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as japanese-english. <eos> in this paper, we propose a method of imposing reordering constraints using document-level context. <eos> as the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. <eos> given a source sentence, zones which cover the noun phrases are used as reordering constraints. <eos> then, in decoding, reorderings which violate the zones are restricted. <eos> experiment results for patent translation tasks show a significant improvement of 1.20 % bleu points in japaneseenglish translation and 1.41 % bleu points in english-japanese translation.
language models based on word surface forms only are unable to benefit from available linguistic knowledge, and tend to suffer from poor estimates for rare features. <eos> we propose an approach to overcome these two limitations. <eos> we use factored features that can flexibly capture linguistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. <eos> finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative language modeling.
the language model ( lm ) is a critical component in most statistical machine translation ( smt ) systems, serving to establish a probability distribution over the hypothesis space. <eos> most smt systems use a static lm, independent of the source language input. <eos> while previous work has shown that adapting lms based on the input improves smt performance, none of the techniques has thus far been shown to be feasible for on-line systems. <eos> in this paper, we develop a novel measure of cross-lingual similarity for biasing the lm based on the test input. <eos> we also illustrate an efficient on-line implementation that supports integration with on-line smt systems by transferring much of the computational load off-line. <eos> our approach yields significant reductions in target perplexity compared to the static lm, as well as consistent improvements in smt performance across language pairs ( english-dari and english-pashto ).
in most statistical machine translation systems, the phrase/rule extraction algorithm uses alignments in the 1-best form, which might contain spurious alignment points. <eos> the usage of weighted alignment matrices that encode all possible alignments has been shown to generate better phrase tables for phrase-based systems. <eos> we propose two algorithms to generate the well known msd reordering model using weighted alignment matrices. <eos> experiments on the iwslt 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular msd models of 0.4 bleu points in the btec french to english test set, and of 1.5 bleu points in the dialog chinese to english test set.
we introduce two simple improvements to the lexical weighting features of koehn, och, and marcu ( 2003 ) for machine translation : one which smooths the probability of translating word f to word e by simplifying english morphology, and one which conditions it on the kind of training data that f and e co-occurred in. <eos> these new variations lead to improvements of up to +0.8 bleu, with an average improvement of +0.6 bleu across two language pairs, two genres, and two translation systems.
contrary to popular belief, we show that the optimal parameters for ibm model 1 are not unique. <eos> we demonstrate that, for a large class of words, ibm model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. <eos> we study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.
in contrast to many languages ( like russian or french ), modern english does not distinguish formal and informal ( ? t/v ? ) <eos> address overtly, for example by pronoun choice. <eos> we describe an ongoing study which investigates to what degree the t/v distinction is recoverable in english text, and with what textual features it correlates. <eos> our findings are : ( a ) human raters can label english utterances as t or v fairly well, given sufficient context ; ( b ), lexical cues can predict t/v almost at human level.
we study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. <eos> we introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. <eos> our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.
a topic model outputs a set of multinomial distributions over words for each topic. <eos> in this paper, we investigate the value of bilingual topic models, i.e., a bilingual latent dirichlet allocation model for finding translations of terms in comparable corpora without using any linguistic resources. <eos> experiments on a document-aligned english-italian wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. <eos> the best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.
chinese pinyin input method is very important for chinese language information processing. <eos> users may make errors when they are typing in chinese words. <eos> in this paper, we are concerned with the reasons that cause the errors. <eos> inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. <eos> in addition, we present a comparative analysis of the data to achieve a better understanding of users ? <eos> input behaviors. <eos> comparisons with english typos suggest that some language-specific properties result in a part of chinese input errors.
common approaches to assessing document quality look at shallow aspects, such as grammar and vocabulary. <eos> for many real-world applications, deeper notions of quality are needed. <eos> this work represents a first step in a project aimed at developing computational methods for deep assessment of quality in the domain of intelligence reports. <eos> we present an automated system for ranking intelligence reports with regard to coverage of relevant material. <eos> the system employs methodologies from the field of automatic summarization, and achieves performance on a par with human judges, even in the absence of the underlying information sources.
we present a method for lexical simplification. <eos> simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. <eos> our method is unsupervised. <eos> furthermore, it does not require any alignment or correspondence among the complex and simple corpora. <eos> we evaluate the simplification according to three criteria : preservation of grammaticality, preservation of meaning, and degree of simplification. <eos> results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.
identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers. <eos> as a first step towards enhancing existing peerreview systems with new functionality based on helpfulness detection, we examine whether standard product review analysis techniques also apply to our new context of peer reviews. <eos> in addition, we investigate the utility of incorporating additional specialized features tailored to peer review. <eos> our preliminary results show that the structural features, review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews, while peer-review specific auxiliary features can further improve helpfulness prediction.
this paper presents an original approach to semi-supervised learning of personal name ethnicity from typed graphs of morphophonemic features and first/last-name co-occurrence statistics. <eos> we frame this as a general solution to an inference problem over typed graphs where the edges represent labeled relations between features that are parameterized by the edge types. <eos> we propose a framework for parameter estimation on different constructions of typed graphs for this problem using a gradient-free optimization method based on grid search. <eos> results on both in-domain and out-of-domain data show significant gains over 30 % accuracy improvement using the techniques presented in the paper.
the number of users on twitter has drastically increased in the past years. <eos> however, twitter does not have an effective user grouping mechanism. <eos> therefore tweets from other users can quickly overrun and become inconvenient to read. <eos> in this paper, we propose methods to help users group the people they follow using their provided seeding users. <eos> two sources of information are used to build sub-systems : textural information captured by the tweets sent by users, and social connections among users. <eos> we also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. <eos> our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.
we present a class-based language model that clusters rare words of similar morphology together. <eos> the model improves the prediction of words after histories containing outof-vocabulary words. <eos> the morphological features used are obtained without the use of labeled data. <eos> the perplexity improvement compared to a state of the art kneser-ney model is 4 % overall and 81 % on unknown histories.
we present a pointwise approach to japanese morphological analysis ( ma ) that ignores structure information during learning and tagging. <eos> despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for japanese ma, and achieves accuracy similar to that of structured predictors using the same feature set. <eos> we also find that the method is both robust to outof-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning.
machine transliteration is defined as automatic phonetic translation of names across languages. <eos> in this paper, we propose synchronous adaptor grammar, a novel nonparametric bayesian learning approach, for machine transliteration. <eos> this model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. <eos> the proposed model outperforms the state-of-the-art embased model in the english to chinese transliteration task.
several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. <eos> however, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the minimum description length ( mdl ) principle. <eos> therefore, it is necessary to generate a set of candidate segmentations and select between them according to the mdl principle. <eos> we evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the bootstrapped voting experts algorithm consistently outperforms other methods when paired with mdl.
paraphrase generation is an important task that has received a great deal of interest recently. <eos> proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of nlp tools to more complex approaches that rely on numerous language-dependent resources. <eos> despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches. <eos> this paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses. <eos> our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy.
this paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. <eos> the method first identifies each sense of a word in the dictionary to its corresponding category. <eos> we used a text classification technique to select appropriate senses for each domain. <eos> then, senses were scored by computing the rank scores. <eos> we used markov random walk ( mrw ) model. <eos> the method was tested on english and japanese resources, wordnet 3.0 and edr japanese dictionary. <eos> for evaluation of the method, we compared english results with the subject field codes ( sfc ) resources. <eos> we also compared each english and japanese results to the first sense heuristics in the wsd task. <eos> these results suggest that identification of domain-specific senses ( idss ) may actually be of benefit.
recognizing entailment at the lexical level is an important and commonly-addressed component in textual inference. <eos> yet, this task has been mostly approached by simplified heuristic methods. <eos> this paper proposes an initial probabilistic modeling framework for lexical entailment, with suitable em-based parameter estimation. <eos> our model considers prominent entailment factors, including differences in lexical-resources reliability and the impacts of transitivity and multiple evidence. <eos> evaluations show that the proposed model outperforms most prior systems while pointing at required future improvements.
we investigate the expression of opinions about human entities in user-generated content ( ugc ). <eos> a set of 2,800 online news comments ( 8,000 sentences ) was manually annotated, following a rich annotation scheme designed for this purpose. <eos> we conclude that the challenge in performing opinion mining in such type of content is correctly identifying the positive opinions, because ( i ) they are much less frequent than negative opinions and ( ii ) they are particularly exposed to verbal irony. <eos> we also show that the recognition of human targets poses additional challenges on mining opinions from ugc, since they are frequently mentioned by pronouns, definite descriptions and nicknames.
identifying domain-dependent opinion words is a key problem in opinion mining and has been studied by several researchers. <eos> however, existing work has been focused on adjectives and to some extent verbs. <eos> limited work has been done on nouns and noun phrases. <eos> in our work, we used the feature-based opinion mining model, and we found that in some domains nouns and noun phrases that indicate product features may also imply opinions. <eos> in many such cases, these nouns are not subjective but objective. <eos> their involved sentences are also objective sentences and imply positive or negative opinions. <eos> identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. <eos> to the best of our knowledge, this problem has not been studied in the literature. <eos> this paper proposes a method to deal with the problem. <eos> experimental results based on real-life datasets show promising results.
sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. <eos> we report on a method for constructing a corpus of sarcastic twitter messages in which determination of the sarcasm of each message has been made by its author. <eos> we use this reliable corpus to compare sarcastic utterances in twitter to utterances that express positive or negative attitudes without sarcasm. <eos> we investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. <eos> perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.
although subjectivity and sentiment analysis ( ssa ) has been witnessing a flurry of novel research, there are few attempts to build ssa systems for morphologically-rich languages ( mrl ). <eos> in the current study, we report efforts to partially fill this gap. <eos> we present a newly developed manually annotated corpus of modern standard arabic ( msa ) together with a new polarity lexicon.the corpus is a collection of newswire documents annotated on the sentence level. <eos> we also describe an automatic ssa tagging system that exploits the annotated data. <eos> we investigate the impact of different levels of preprocessing settings on the ssa classification task. <eos> we show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance.
we present a method for identifying the positive or negative semantic orientation of foreign words. <eos> identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. <eos> identifying the semantic orientation of english words has been extensively studied in literature. <eos> most of this work assumes the existence of resources ( e.g. <eos> wordnet, seeds, etc ) that do not exist in foreign languages. <eos> in this work, we describe a method based on constructing a multilingual network connecting english and foreign words. <eos> we use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. <eos> the method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.
recently, hierarchical text classification has become an active research topic. <eos> the essential idea is that the descendant classes can share the information of the ancestor classes in a predefined taxonomy. <eos> in this paper, we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively. <eos> then, we propose a variant passive-aggressive ( pa ) algorithm for hierarchical text classification with latent concepts. <eos> experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms.
in this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. <eos> first, partial sentence trees are proposed to represent a speech recognition output sentence. <eos> semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. <eos> the constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. <eos> the proposed approach is implemented and evaluated in a mandarin spoken dialogue system for tour-guiding service. <eos> combined with scores derived from the asr recognition probability and the dialogue history, the proposed approach achieves 84.3 % detection accuracy, an absolute improvement of 34.7 % over the baseline of the semantic slot-based method with 49.6 % detection accuracy.
there are several theories regarding what influences prominence assignment in english noun-noun compounds. <eos> we have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories : the informativeness theory and the semantic composition theory. <eos> the evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.
verbal feedback is an important information source in establishing interactional rapport. <eos> however, predicting verbal feedback across languages is challenging due to languagespecific differences, inter-speaker variation, and the relative sparseness and optionality of verbal feedback. <eos> in this paper, we employ an approach combining classifier weighting and smote algorithm oversampling to improve verbal feedback prediction in arabic, english, and spanish dyadic conversations. <eos> this approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. <eos> analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration.
in the face of sparsity, statistical models are often interpolated with lower order ( backoff ) models, particularly in language modeling. <eos> in this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective. <eos> we show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context ( such as decision tree models ), this relation is generally not satisfied. <eos> based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.
we present a novel probabilistic classifier, which scales well to problems that involve a large number of classes and require training on large datasets. <eos> a prominent example of such a problem is language modeling. <eos> our classifier is based on the assumption that each feature is associated with a predictive strength, which quantifies how well the feature can predict the class by itself. <eos> the predictions of individual features can then be combined according to their predictive strength, resulting in a model, whose parameters can be reliably and efficiently estimated. <eos> we show that a generative language model based on our classifier consistently matches modified kneser-ney smoothing and can outperform it if sufficiently rich features are incorporated.
we describe a method for disambiguating chinese commas that is central to chinese sentence segmentation. <eos> chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. <eos> trained and tested on data derived from the chinese treebank, our model achieves a classification accuracy of close to 90 % overall, which translates to an f1 score of 70 % for detecting commas that signal sentence boundaries.
this paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. <eos> we use unsupervised data to generate informative ? condensed feature representations ? <eos> from the original feature set used in supervised nlp systems. <eos> the main contribution of our method is that it can offer dense and low-dimensional feature spaces for nlp tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. <eos> our method matches the results of current state-of-the-art systems with very few features, i.e., f-score 90.72 with 344 features for conll-2003 ner data, and uas 93.55 with 12.5k features for dependency parsing data derived from ptb-iii.
statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. <eos> to compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. <eos> the standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. <eos> we present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a hidden markov model and are thereby deemphasized in the term frequency statistics. <eos> this approach is compared to both a stopword-list and pos-tagging approach and our method demonstrates improved coverage on the duc 2006 and tac 2010 datasets using the rouge metric.
comparative news summarization aims to highlight the commonalities and differences between two comparable news topics. <eos> in this study, we propose a novel approach to generating comparative news summaries. <eos> we formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics. <eos> we consider semantic-related cross-topic concept pairs as comparative evidences, and consider topic-related concepts as representative evidences. <eos> the optimization problem is addressed by using a linear programming model. <eos> the experimental results demonstrate the effectiveness of our proposed model.
surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. <eos> we therefore propose the joint optimisation of content selection and surface realisation using hierarchical reinforcement learning ( hrl ). <eos> to this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. <eos> it is based on a generation space in the form of a hidden markov model ( hmm ). <eos> results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.
in this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of referring expressions generation ( reg ). <eos> to enable comparison between different-sized training sets, a systematic training method was developed. <eos> the results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance.
in this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. <eos> we introduce a new data set that pairs english wikipedia with simple english wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. <eos> the data contains the full range of simplification operations including rewording, reordering, insertion and deletion. <eos> we provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simplification.
we investigate the relevance of hierarchical topic models to represent the content of web gists. <eos> we focus our attention on dmoz, a popular web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. <eos> our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. <eos> our second approach is fully bayesian and derived from the more general model, hierarchical lda. <eos> we evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.
we present a novel pruning method for context-free parsing that increases efficiency by disallowing phrase-level unary productions in cky chart cells spanning a single word. <eos> our work is orthogonal to recent work on ? closing ? <eos> chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned. <eos> we show that a simple discriminative classifier can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions. <eos> eliminating these unary productions from the search can have a large impact on downstream processing, depending on implementation details of the search. <eos> we apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-fine, agenda, and beam-search pruning.
we consider a very simple, yet effective, approach to cross language adaptation of dependency parsers. <eos> we first remove lexical items from the treebanks and map part-of-speech tags into a common tagset. <eos> we then train a language model on tag sequences in otherwise unlabeled target data and rank labeled source data by perplexity per word of tag sequences from less similar to most similar to the target. <eos> we then train our target language parser on the most similar data points in the source labeled data. <eos> the strategy achieves much better results than a non-adapted baseline and stateof-the-art unsupervised dependency parsing, and results are comparable to more complex projection-based cross language adaptation algorithms.
this paper suggests two ways of improving transition-based, non-projective dependency parsing. <eos> first, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. <eos> second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. <eos> the new addition to the algorithm shows a clear advantage in parsing speed. <eos> the bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.
this work introduces a new approach to checking treebank consistency. <eos> derivation trees based on a variant of tree adjoining grammar are used to compare the annotation of word sequences based on their structural similarity. <eos> this overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison. <eos> we report on the result of applying this approach to the penn arabic treebank and how this approach leads to high precision of error detection.
this paper presents the introduction of wordnet semantic classes in a dependency parser, obtaining improvements on the full penn treebank for the first time. <eos> we tried different combinations of some basic semantic classes and word sense disambiguation algorithms. <eos> our experiments show that selecting the adequate combination of semantic features on development data is key for success. <eos> given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements.
we experiment with extending a lattice parsing methodology for parsing hebrew ( goldberg and tsarfaty, 2008 ; golderg et al, 2009 ) to make use of a stronger syntactic model : the pcfg-la berkeley parser. <eos> we show that the methodology is very effective : using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented hebrew text with an f-score of almost 80 %, an error reduction of over 20 % over the best previous result for this task. <eos> this result indicates that lattice parsing with the berkeley parser is an effective methodology for parsing over uncertain inputs.
we combine multiple word representations based on semantic clusters extracted from the ( brown et al, 1992 ) algorithm and syntactic clusters obtained from the berkeley parser ( petrov et al, 2006 ) in order to improve discriminative dependency parsing in the mstparser framework ( mcdonald et al, 2005 ). <eos> we also provide an ensemble method for combining diverse cluster-based models. <eos> the two contributions together significantly improves unlabeled dependency accuracy from 90.82 % to 92.13 %.
for the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. <eos> experiments on two chinese treebanks show that our approach improves conversion accuracy by 1.31 % over a strong baseline.
we investigate full-scale shortest-derivation parsing ( sdp ), wherein the parser selects an analysis built from the fewest number of training fragments. <eos> shortest derivation parsing exhibits an unusual range of behaviors. <eos> at one extreme, in the fully unpruned case, it is neither fast nor accurate. <eos> at the other extreme, when pruned with a coarse unlexicalized pcfg, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. <eos> our analysis includes an investigation of tie-breaking and associated dynamic programs. <eos> at its best, our parser achieves an accuracy of 87 % f1 on the english wsj task with minimal annotation, and 90 % f1 with richer annotation.
this paper proposes three modules based on latent topics of documents for alleviating ? semantic drift ? <eos> in bootstrapping entity set expansion. <eos> these new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. <eos> in this study, we model latent topics with lda ( latent dirichlet allocation ) in an unsupervised way. <eos> experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2 % depending on the domain.
in this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. <eos> instead of combining word alignments of different models ( xiang et al, 2010 ), we try to combine word alignments over multiple monolingually motivated word segmentation. <eos> our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. <eos> our combination algorithm is simple, efficient, and easy to implement. <eos> in the chinese-english experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations.
in summarization, sentence ordering is conducted to enhance summary readability by accommodating text coherence. <eos> we propose a grouping-based ordering framework that integrates local and global coherence concerns. <eos> summary sentences are grouped before ordering is applied on two levels : group-level and sentence-level. <eos> different algorithms for grouping and ordering are discussed. <eos> the preliminary results on single-document news datasets demonstrate the advantage of our method over a widely accepted method.
in this thesis proposal i present my thesis work, about pre- and postprocessing for statistical machine translation, mainly into germanic languages. <eos> i focus my work on four areas : compounding, definite noun phrases, reordering, and error correction. <eos> initial results are positive within all four areas, and there are promising possibilities for extending these approaches. <eos> in addition i also focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed.
named entity disambiguation is the task of linking an entity mention in a text to the correct real-world referent predefined in a knowledge base, and is a crucial subtask in many areas like information retrieval or topic detection and tracking. <eos> named entity disambiguation is challenging because entity mentions can be ambiguous and an entity can be referenced by different surface forms. <eos> we present an approach that exploits wikipedia relations between entities co-occurring with the ambiguous form to derive a range of novel features for classifying candidate referents. <eos> we find that our features improve disambiguation results significantly over a strong popularity baseline, and are especially suitable for recognizing entities not contained in the knowledge base. <eos> our system achieves state-of-the-art results on the tac-kbp 2009 dataset.
this paper describes a method for automatically extracting and classifying multiword expressions ( mwes ) for urdu on the basis of a relatively small unannotated corpus ( around 8.12 million tokens ). <eos> the mwes are extracted by an unsupervised method and classified into two distinct classes, namely locations and person names. <eos> the classification is based on simple heuristics that take the co-occurrence of mwes with distinct postpositions into account. <eos> the resulting classes are evaluated against a hand-annotated gold standard and achieve an f-score of 0.5 and 0.746 for locations and persons, respectively. <eos> a target application is the urdu pargram grammar, where mwes are needed to generate a more precise syntactic and semantic analysis.
recently, several latent topic analysis methods such as lsi, plsi, and lda have been widely used for text analysis. <eos> however, those methods basically assign topics to words, but do not account for the events in a document. <eos> with this background, in this paper, we propose a latent topic extracting method which assigns topics to events. <eos> we also show that our proposed method is useful to generate a document summary based on a latent topic.
in this paper i present a master ? s thesis proposal in syntax-based statistical machine translation. <eos> i propose to build discriminative smt models using both tree-to-string and tree-to-tree approaches. <eos> translation and language models will be represented mainly through the use of tree automata and tree transducers. <eos> these formalisms have important representational properties that makes them well-suited for syntax modeling. <eos> i also present an experiment plan to evaluate these models through the use of a parallel corpus written in english and brazilian portuguese.
we present consentcanvas, a system which structures and ? texturizes ? <eos> end-user license agreement ( eula ) documents to be more readable. <eos> the system aims to help users better understand the terms under which they are providing their informed consent. <eos> consentcanvas receives unstructured text documents as input and uses unsupervised natural language processing methods to embellish the source document using a linked stylesheet. <eos> unlike similar usable security projects which employ summarization techniques, our system preserves the contents of the source document, minimizing the cognitive and legal burden for both the end user and the licensor. <eos> our system does not require a corpus for training.
temporal ? contrastive discourse connectives ( although, while, since, etc. ) <eos> signal various types of relations between clauses such as temporal, contrast, concession and cause. <eos> they are often ambiguous and therefore difficult to translate from one language to another. <eos> we discuss several new and translation-oriented experiments for the disambiguation of a specific subset of discourse connectives in order to correct some of the translation errors made by current statistical machine translation systems.
sentiment analysis is one of the hot demanding research areas since last few decades. <eos> although a formidable amount of research has been done but still the existing reported solutions or available systems are far from perfect or to meet the satisfaction level of end user 's. <eos> the main issue may be there are many conceptual rules that govern sentiment, and there are even more clues ( possibly unlimited ) that can convey these concepts from realization to verbalization of a human being. <eos> human psychology directly relates to the unrevealed clues ; govern the sentiment realization of us. <eos> human psychology relates many things like social psychology, culture, pragmatics and many more endless intelligent aspects of civilization. <eos> proper incorporation of human psychology into computational sentiment knowledge representation may solve the problem. <eos> psychosentiwordnet is an extension over sentiwordnet that holds human psychological knowledge and sentiment knowledge simultaneously.
this paper describes a backtracking strategy for an incremental deterministic transitionbased parser for hpsg. <eos> the method could theoretically be implemented on any other transition-based parser with some adjustments. <eos> in this paper, the algorithm is evaluated on cuteforce, an efficient deterministic shiftreduce hpsg parser. <eos> the backtracking strategy may serve to improve existing parsers, or to assess if a deterministic parser would benefit from backtracking as a strategy to improve parsing.
relation extraction in documents allows the detection of how entities being discussed in a document are related to one another ( e.g. <eos> partof ). <eos> this paper presents an analysis of a relation extraction system based on prior work but applied to the j.d. <eos> power and associates sentiment corpus to examine how the system works on documents from a range of social media. <eos> the results are examined on three different subsets of the jdpa corpus, showing that the system performs much worse on documents from certain sources. <eos> the proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs.
flat noun phrase structure was, up until recently, the standard in annotation for the penn treebanks. <eos> with the recent addition of internal noun phrase annotation, dependency parsing and applications down the nlp pipeline are likely affected. <eos> some machine translation systems, such as tectomt, use deep syntax as a language transfer layer. <eos> it is proposed that changes to the noun phrase dependency parse will have a cascading effect down the nlp pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. <eos> this paper examines this noun phrase structure ? s effect on dependency parsing, in english, with a maximum spanning tree parser and shows a 2.43 %, 0.23 bleu score, improvement for english to czech machine translation.
we propose a framework for generating an abstractive summary from a semantic model of a multimodal document. <eos> we discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. <eos> to this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.
sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. <eos> in this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. <eos> using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. <eos> our results show that 3-grams and dependencies perform best in this task ; they outperform the sentence splitting, science lexicon and negation based features.
allophonic rules are responsible for the great variety in phoneme realizations. <eos> infants can not reliably infer abstract word representations without knowledge of their native allophonic grammar. <eos> we explore the hypothesis that some properties of infants ? <eos> input, referred to as indicators, are correlated with allophony. <eos> first, we provide an extensive evaluation of individual indicators that rely on distributional or lexical information. <eos> then, we present a first evaluation of the combination of indicators of different types, considering both logical and numerical combinations schemes. <eos> though distributional and lexical indicators are not redundant, straightforward combinations do not outperform individual indicators.
most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. <eos> this research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. <eos> results show that while there was variation between subjects, three features were significant turn-yielding cues overall. <eos> in addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated.
we consider the problem of predicting which words a student will click in a vocabulary learning system. <eos> often a language learner will find value in the ability to look up the meaning of an unknown word while reading an electronic document by clicking the word. <eos> highlighting words likely to be unknown to a reader is attractive due to drawing his or her attention to it and indicating that information is available. <eos> however, this option is usually done manually in vocabulary systems and online encyclopedias such as wikipedia. <eos> furthurmore, it is never on a per-user basis. <eos> this paper presents an automated way of highlighting words likely to be unknown to the specific user. <eos> we present related work in search engine ranking, a description of the study used to collect click data, the experiment we performed using the random forest machine learning algorithm and finish with a discussion of future work.
turkish is an agglutinative language with complex morphological structures, therefore using only word forms is not enough for many computational tasks. <eos> in this paper we analyze the effect of morphology in a named entity recognition system for turkish. <eos> we start with the standard word-level representation and incrementally explore the effect of capturing syntactic and contextual properties of tokens. <eos> furthermore, we also explore a new representation in which roots and morphological features are represented as separate tokens instead of representing only words as tokens. <eos> using syntactic and contextual properties with the new representation provide an 7.6 % relative improvement over the baseline.
in my thesis, i propose to build a system that would enable extraction of social interactions from texts. <eos> to date i have defined a comprehensive set of social events and built a preliminary system that extracts social events from news articles. <eos> i plan to improve the performance of my current system by incorporating semantic information. <eos> using domain adaptation techniques, i propose to apply my system to a wide range of genres. <eos> by extracting linguistic constructs relevant to social interactions, i will be able to empirically analyze different kinds of linguistic constructs that people use to express social interactions. <eos> lastly, i will attempt to make convolution kernels more scalable and interpretable.
arabic language is a morphologically complex language. <eos> affixes and clitics are regularly attached to stems which make direct comparison between words not practical. <eos> in this paper we propose a new automatic headline generation technique that utilizes character cross-correlation to extract best headlines and to overcome the arabic language complex morphology. <eos> the system that uses character cross-correlation achieves rouge-l score of 0.19384 while the exact word matching scores only 0.17252 for the same set of documents.
one of the major problems of k-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. <eos> we address this issue by using feature hashing ( weinberger et al., 2009 ), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. <eos> our analysis gives theoretical motivation and justification for applying feature hashing to kmeans, by showing how much will the objective ofk-means be ( additively ) distorted. <eos> furthermore, to empirically verify our method, we experimented on a document clustering task.
hindi-punjabi being closely related language pair ( goyal v. and lehal g.s., 2008 ), hybrid machine translation approach has been used for developing hindi to punjabi machine translation system. <eos> non-availability of lexical resources, spelling variations in the source language text, source text ambiguous words, named entity recognition and collocations are the major challenges faced while developing this syetm. <eos> the key activities involved during translation process are preprocessing, translation engine and post processing. <eos> lookup algorithms, pattern matching algorithms etc formed the basis for solving these issues. <eos> the system accuracy has been evaluated using intelligibility test, accuracy test and bleu score. <eos> the hybrid syatem is found to perform better than the constituent systems. <eos> keywords : machine translation, computational linguistics, natural language processing, hindi, punjabi. <eos> translate hindi to punjabi, closely related languages.
we describe a novel application for structured search in scientific digital libraries. <eos> the acl anthology searchbench is meant to become a publicly available research tool to query the content of the acl anthology. <eos> the application provides search in both its bibliographic metadata and semantically analyzed full textual content. <eos> by combining these two features, very efficient and focused queries are possible. <eos> at the same time, the application serves as a showcase for the recent progress in natural language processing ( nlp ) research and language technology. <eos> the system currently indexes the textual content of 7,500 anthology papers from 2002 ? 2009 with predicateargument-like semantic structures. <eos> it also provides useful search filters based on bibliographic metadata. <eos> it will be extended to provide the full anthology content and enhanced functionality based on further nlp techniques.
large lexical resources, such as corpora and databases of web ngrams, are a rich source of pre-fabricated phrases that can be reused in many different contexts. <eos> however, one must be careful in how these resources are used, and noted writers such as george orwell have argued that the use of canned phrases encourages sloppy thinking and results in poor communication. <eos> nonetheless, while orwell prized home-made phrases over the readymade variety, there is a vibrant movement in modern art which shifts artistic creation from the production of novel artifacts to the clever reuse of readymades or objets trouv ? s. <eos> we describe here a system that makes creative reuse of the linguistic readymades in the google ngrams. <eos> our system, the jigsaw bard, thus owes more to marcel duchamp than to george orwell. <eos> we demonstrate how textual readymades can be identified and harvested on a large scale, and used to drive a modest form of linguistic creativity.
we present a mobile touchable application for online topic graph extraction and exploration of web content. <eos> the system has been implemented for operation on an ipad. <eos> the topic graph is constructed from n web snippets which are determined by a standard search engine. <eos> we consider the extraction of a topic graph as a specific empirical collocation extraction task where collocations are extracted between chunks. <eos> our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. <eos> an initial user evaluation shows that this system is especially helpful for finding new interesting information on topics about which the user has only a vague idea or even no idea at all.
we introduce a new method for learning to detect grammatical errors in learner ? s writing and provide suggestions. <eos> the method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words, function words, and parts-of-speech ( e.g., ? play ~ role in ving ? <eos> and ? look forward to ving ? ). <eos> at runtime, the given passage submitted by the learner is matched using an extended levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions. <eos> we present a prototype implementation of the proposed method, edit, that can handle a broad range of errors. <eos> promising results are illustrated with three common types of errors in nonnative writing.
event related potentials ( erp ) corresponding to stimuli in electroencephalography ( eeg ) can be used to detect the intent of a person for brain computer interfaces ( bci ). <eos> this paradigm is widely used to build letter-byletter text input systems using bci. <eos> nevertheless using a bci-typewriter depending only on eeg responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. <eos> hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. <eos> in this demonstration we will present a bci system for typing that integrates a stochastic language model with erp classification to achieve speedups, via the rapid serial visual presentation ( rsvp ) paradigm.
this paper presents engkoo 1, a system for exploring and learning language. <eos> it is built primarily by mining translation knowledge from billions of web pages - using the internet to catch language in motion. <eos> currently engkoo is built for chinese users who are learning english ; however the technology itself is language independent and can be extended in the future. <eos> at a system level, engkoo is an application platform that supports a multitude of nlp technologies such as cross language retrieval, alignment, sentence classification, and statistical machine translation. <eos> the data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. <eos> specifically, web pages that contain both chinese and english are discovered and analyzed for parallelism, extracted and formulated into clear term definitions and sample sentences. <eos> this approach allows us to build perhaps the world ? s largest lexicon linking both chinese and english together - at the same time covering the most up-to-date terms as captured by the net.
sentiment analysis is one of the hot demanding research areas since last few decades. <eos> although a formidable amount of research have been done, the existing reported solutions or available systems are still far from perfect or do not meet the satisfaction level of end users ?. <eos> the main issue is the various conceptual rules that govern sentiment and there are even more clues ( possibly unlimited ) that can convey these concepts from realization to verbalization of a human being. <eos> human psychology directly relates to the unrevealed clues and governs the sentiment realization of us. <eos> human psychology relates many things like social psychology, culture, pragmatics and many more endless intelligent aspects of civilization. <eos> proper incorporation of human psychology into computational sentiment knowledge representation may solve the problem. <eos> in the present paper we propose a template based online interactive gaming technology, called dr sentiment to automatically create the psychosentiwordnet involving internet population. <eos> the psychosentiwordnet is an extension of sentiwordnet that presently holds human psychological knowledge on a few aspects along with sentiment knowledge.
we present blast, an open source tool for error analysis of machine translation ( mt ) output. <eos> we believe that error analysis, i.e., to identify and classify mt errors, should be an integral part of mt development, since it gives a qualitative view, which is not obtained by standard evaluation methods. <eos> blast can aid mt researchers and users in this process, by providing an easy-to-use graphical user interface. <eos> it is designed to be flexible, and can be used with any mt system, language pair, and error typology. <eos> the annotation task can be aided by highlighting similarities with a reference translation.
virtual instructors can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. <eos> in this paper we present a novel algorithm for rapidly prototyping virtual instructors from human-human corpora without manual annotation. <eos> automatically prototyping full-fledged dialogue systems from corpora is far from being a reality nowadays. <eos> our algorithm is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual world. <eos> we evaluate a virtual instructor, generated using this algorithm, with human users. <eos> we compare our results both with human instructors and rule-based virtual instructors hand-coded for the same task.
state-of-the-art machine translation ( mt ) systems are still far from being perfect. <eos> an alternative is the so-called interactive machine translation ( imt ) framework, where the knowledge of a human translator is combined with the mt system. <eos> we present a statistical imt system able to learn from user feedback by means of the application of online learning techniques. <eos> these techniques allow the mt system to update the parameters of the underlying models in real time. <eos> according to empirical results, our system outperforms the results of conventional imt systems. <eos> to the best of our knowledge, this online learning capability has never been provided by previous imt systems. <eos> our imt system is implemented in c++, javascript, and actionscript ; and is publicly available on the web.
we present wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. <eos> wikulu is implemented as an extensible architecture which transparently integrates natural language processing ( nlp ) techniques with wikis. <eos> it is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of nlp algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. <eos> additionally, we show how wikulu can be applied for visually analyzing the results of nlp algorithms, educational purposes, and enabling semantic wikis.
the automatic content linking device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the internet. <eos> the documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. <eos> results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation.
macaon is a tool suite for standard nlp tasks developed for french. <eos> macaon has been designed to process both human-produced text and highly ambiguous word-lattices produced by nlp tools. <eos> macaon is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through xml files. <eos> in addition, exchange protocols with external tools are easily definable. <eos> macaon is a fast, modular and open tool, distributed under gnu public license.
this paper describes dico ii+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a ? speech cursor ? <eos> which enables menu navigation as well as browsing long list using haptic input and spoken output.
we present an open-source toolkit which allows ( i ) to reconstruct past states of wikipedia, and ( ii ) to efficiently access the edit history of wikipedia articles. <eos> reconstructing past states of wikipedia is a prerequisite for reproducing previous experimental work based on wikipedia. <eos> beyond that, the edit history of wikipedia articles has been shown to be a valuable knowledge source for nlp, but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data. <eos> by using a dedicated storage format, our toolkit massively decreases the data volume to less than 2 % of the original size, and at the same time provides an easy-to-use interface to access the revision data. <eos> the language-independent design allows to process any language represented in wikipedia. <eos> we expect this work to consolidate nlp research using wikipedia in general, and to foster research making use of the knowledge encoded in wikipedia ? s edit history.
we introduce a new publicly available tool that implements efficient indexing and retrieval of large n-gram datasets, such as the web1t 5-gram corpus. <eos> our tool indexes the entire web1t dataset with an index size of only 100 mb and performs a retrieval of any n-gram with a single disk access. <eos> with an increased index size of 420 mb and duplicate data, it also allows users to issue wild card queries provided that the wild cards in the query are contiguous. <eos> furthermore, we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the web1t 5gram corpus ( yuret, 2008 ). <eos> we demonstrate the effectiveness of our tool and the smoothing algorithms on the english lexical substitution task by a simple implementation that gives considerable improvement over a basic language model.
emerging text-intensive enterprise applications such as social analytics and semantic search pose new challenges of scalability and usability to information extraction ( ie ) systems. <eos> this paper presents systemt, a declarative ie system that addresses these challenges and has been deployed in a wide range of enterprise applications. <eos> systemt facilitates the development of high quality complex annotators by providing a highly expressive language and an advanced development environment. <eos> it also includes a cost-based optimizer and a high-performance, flexible runtime with minimummemory footprint. <eos> we present systemt as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable ie systems.
in this demo, we present scisumm, an interactive multi-document summarization system for scientific articles. <eos> the document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. <eos> at the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. <eos> this analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. <eos> scisumm is currently built over the 2008 acl anthology, however the generalizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. <eos> evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system ( mead ).
in this paper we present clairlib, an opensource toolkit for natural language processing, information retrieval, and network analysis. <eos> clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. <eos> it has a command-line interface, a graphical interface, and a documented api. <eos> clairlib is compatible with all the common platforms and operating systems. <eos> in addition to its own functionality, it provides interfaces to external software and corpora. <eos> clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos.
social networking and micro-blogging sites are stores of opinion-bearing content created by human users. <eos> we describe c-feel-it, a system which can tap opinion content in posts ( called tweets ) from the micro-blogging website, twitter. <eos> this web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. <eos> we present a qualitative evaluation of this system based on a human-annotated tweet corpus.
this paper presents a system to summarize a microblog post and its responses with the goal to provide readers a more constructive and concise set of information for efficient digestion. <eos> we introduce a novel two-phase summarization scheme. <eos> in the first phase, the post plus its responses are classified into four categories based on the intention, interrogation, sharing, discussion and chat. <eos> for each type of post, in the second phase, we exploit different strategies, including opinion analysis, response pair identification, and response relevancy detection, to summarize and highlight critical information to display. <eos> this system provides an alternative thinking about machinesummarization : by utilizing ai approaches, computers are capable of constructing deeper and more user-friendly abstraction.
ace for rapid natural language processing development in uima balaji r. soundrarajan, thomas ginter, scott l. duvall va salt lake city health care system and university of utah balaji @ cs.utah.edu, { thomas.ginter, scott.duvall } @ utah.edu abstract this demonstration presents the annotation librarian, an application programming inter-face that supports rapid development of natu-ral language processing ( nlp ) projects built in apache unstructured information man-agement architecture ( uima ). <eos> the flexibility of uima to support all types of unstructured data ? <eos> images, audio, and text ? <eos> increases the complexity of some of the most common nlp development tasks. <eos> the annotation librarian interface handles these common functions and allows the creation and management of anno-tations by mirroring java methods used to manipulate strings. <eos> the familiar syntax and nlp-centric design allows developers to adopt and rapidly develop nlp algorithms in uima. <eos> the general functionality of the inter-face is described in relation to the use cases that necessitated its creation.
we present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. <eos> our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. <eos> during inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. <eos> we also automatically refine the syntactic categories given in our coarsely tagged input. <eos> across six languages our approach outperforms state-of-theart unsupervised methods by a significant margin.1
this paper presents an update semantic for dialogue acts, defined in terms of combinations of very simple ? elementary update functions ?. <eos> this approach allows fine-grained distinctions to be made between related types of dialogue acts, and relations like entailment and exclusion between dialogue acts to be established. <eos> the approach is applied to dialogue act representations as defined in the dialogue act markup language ( diaml ), part of the recently proposed iso standard 24617-2 for dialogue act annotation.
we present a method for training a statistical model for mapping natural language sentences to semantic expressions. <eos> the semantics are expressions of an underspecified logical form that has properties making it particularly suitable for statistical mapping from text. <eos> an encoding of the semantic expressions into dependency trees with automatically generated labels allows application of existing methods for statistical dependency parsing to the mapping task ( without the need for separate traditional dependency labels or parts of speech ). <eos> the encoding also results in a natural per-word semantic-mapping accuracy measure. <eos> we report on the results of training and testing statistical models for mapping sentences of the penn treebank into the semantic expressions, for which per-word semantic mapping accuracy ranges between 79 % and 86 % depending on the experimental conditions. <eos> the particular choice of algorithms used also means that our trained mapping is deterministic ( in the sense of deterministic parsing ), paving the way for large-scale text-to-semantic mapping.
this paper explores the role played by a multilingual feature representation for the task of word sense disambiguation. <eos> we translate the context of an ambiguous word in multiple languages, and show through experiments on standard datasets that by using a multilingual vector space we can obtain error rate reductions of up to 25 %, as compared to a monolingual classifier.
we present a system to translate natural language sentences to formulas in a formal or a knowledge representation language. <eos> our system uses two inverse ? -calculus operators and using them can take as input the semantic representation of some words, phrases and sentences and from that derive the semantic representation of other words and phrases. <eos> our inverse ? <eos> operator works on many formal languages including first order logic, database query languages and answer set programming. <eos> our system uses a syntactic combinatorial categorial parser to parse natural language sentences and also to construct the semantic meaning of the sentences as directed by their parsing. <eos> the same parser is used for both. <eos> in addition to the inverse ? -calculus operators, our system uses a notion of generalization to learn semantic representation of words from the semantic representation of other words that are of the same category. <eos> together with this, we use an existing statistical learning approach to assign weights to deal with multiple meanings of words. <eos> our system produces improved results on standard corpora on natural language interfaces for robot command and control and database queries.
this paper presents a model to compose semantic relations. <eos> the model is independent of any particular set of relations and uses an extended definition for semantic relations. <eos> this extended definition includes restrictions on the domain and range of relations and utilizes semantic primitives to characterize them. <eos> primitives capture elementary properties between the arguments of a relation. <eos> an algebra for composing semantic primitives is used to automatically identify the resulting relation of composing a pair of compatible relations. <eos> inference axioms are obtained. <eos> axioms take as input a pair of semantic relations and output a new, previously ignored relation. <eos> the usefulness of this proposed model is shown using propbank relations. <eos> eight inference axioms are obtained and their accuracy and productivity are evaluated. <eos> the model offers an unsupervised way of accurately extracting additional semantics from text.
abduction is a method for finding the best explanation for observations. <eos> arguably the most advanced approach to abduction, especially for natural language processing, is weighted abduction, which uses logical formulas with costs to guide inference. <eos> but it has no clear probabilistic semantics. <eos> in this paper we propose an approach that implements weighted abduction in markov logic, which uses weighted first-order formulas to represent probabilistic knowledge, pointing toward a sound probabilistic semantics for weighted abduction. <eos> application to a series of challenge problems shows the power and coverage of our approach.
entailment pairs are sentence pairs of a premise and a hypothesis, where the premise textually entails the hypothesis. <eos> such sentence pairs are important for the development of textual entailment systems. <eos> in this paper, we take a closer look at a prominent strategy for their automatic acquisition from newspaper corpora, pairing first sentences of articles with their titles. <eos> we propose a simple logistic regression model that incorporates and extends this heuristic and investigate its robustness across three languages and three domains. <eos> we manage to identify two predictors which predict entailment pairs with a fairly high accuracy across all languages. <eos> however, we find that robustness across domains within a language is more difficult to achieve.
first-order logic provides a powerful and flexible mechanism for representing natural language semantics. <eos> however, it is an open question of how best to integrate it with uncertain, probabilistic knowledge, for example regarding word meaning. <eos> this paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of statistical relational ai. <eos> specifically, we show how discourse representation structures can be combined with distributional models for word meaning inside a markov logic network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context.
this paper presents a novel approach to semantic role annotation implementing an entailmentbased view of the concept of semantic role. <eos> i propose to represent arguments of predicates with grammatically relevant primitive properties entailed by the semantics of predicates. <eos> such meaning components generalise over a range of semantic relations which humans tend to express systematically through language. <eos> in a preliminary study, i show that we can model linguistic knowledge at a general, principled syntax-semantics interface by incorporating a layer of skeletal, entailment-based representation of word meaning in large-scale corpus annotation.
this article introduces and evaluates an approach to semantic compositionality in computational linguistics based on the combination of distributional semantics and supervised machine learning. <eos> in brief, distributional semantic spaces containing representations for complex constructions such as adjective-noun and verb-noun pairs, as well as for their constituent parts, are built. <eos> these representations are then used as feature vectors in a supervised learning model using multivariate multiple regression. <eos> in particular, the distributional semantic representations of the constituents are used to predict those of the complex structures. <eos> this approach outperforms the rivals in a series of experiments with adjective-noun pairs extracted from the bnc. <eos> in a second experimental setting based on verb-noun pairs, a comparatively much lower performance was obtained by all the models ; however, the proposed approach gives the best results in combination with a random indexing semantic space.
we present the first approach to learning the durations of events without annotated training data, employing web query patterns to infer duration distributions. <eos> for example, we learn that ? war ? <eos> lasts years or decades, while ? look ? <eos> lasts seconds or minutes. <eos> learning aspectual information is an important goal for computational semantics and duration information may help enable rich document understanding. <eos> we first describe and improve a supervised baseline that relies on event duration annotations. <eos> we then show how web queries for linguistic patterns can help learn the duration of events without labeled data, producing fine-grained duration judgments that surpass the supervised system. <eos> we evaluate on the timebank duration corpus, and also investigate how an event ? s participants ( arguments ) effect its duration using a corpus collected through amazon ? s mechanical turk. <eos> we make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events.
this paper proposes a framework for representing cross-lingual/interlingual lexical semantic correspondences that are expected to be recovered through a series of on-demand/on-the-fly invocations of a lexical semantic matching process. <eos> one of the central notions of the proposed framework is a pseudo synset, which is introduced to represent a cross-lingual/multilingual lexical concept, jointly denoted by word senses in more than one language. <eos> another important ingredient of the proposed framework is a framework for semantifying bilingual lexical resource entries. <eos> this is a necessary substep when associating and representing corresponding lexical concepts in different languages by using bilingual lexical resources. <eos> based on these devices, this paper further discusses possible extensions to the iso standard lexical markup framework ( lmf ). <eos> these extensions would enable recovered correspondences to be organized as a dynamic secondary language resource, while keeping the existing primary language resources intact.
we present the results of several machine learning tasks that exploit explicit spatial language to classify rhetorical relations and the spatial information of narrative events. <eos> three corpora are annotated with figure and ground ( granularity ) relationships, mereotopologically classified verbs and prepositions, and frames of reference. <eos> for rhetorical relations, na ? <eos> ? ve bayesian models achieve 84.90 % and 57.87 % accuracy in classifying narration and background / elaboration relations respectively ( 16 % and 23 % above baseline ). <eos> for the spatial information of narrative events, k* models achieve 55.68 % average accuracy ( 12 % above baseline ) for all spatial information types. <eos> this result is boosted to 71.85 % ( 28 % above baseline ) when inertial spatial reference and text sequence information are considered. <eos> overall, spatial information is shown to be central to narrative discourse structure and prediction tasks.
measures of similarity have traditionally focused on computing the semantic relatedness between pairs of words and texts. <eos> in this paper, we construct an evaluation framework to quantify cross-modal semantic relationships that exist between arbitrary pairs of words and images. <eos> we study the effectiveness of a corpus-based approach to automatically derive the semantic relatedness between words and images, and perform empirical evaluations by measuring its correlation with human annotators.
we describe the methodology for constructing axioms defining event-related words, anchored in core theories of change of state and causality. <eos> we first derive from wordnet senses a smaller set of abstract, general ? supersenses ?. <eos> we encode axioms for these, and we test them on textual entailment pairs. <eos> we look at two specific examples in detail to illustrate both the power of the method and the holes in the knowledge base that it exposes. <eos> then we address the problem of holes more systematically, asking, for example, what kinds of ? pairwise interactions ? <eos> are possible for core theory predicates like change and cause.1
we propose a method to automatically alignwordnet synsets andwikipedia articles to obtain a sense inventory of higher coverage and quality. <eos> for eachwordnet synset, we first extract a set of wikipedia articles as alignment candidates ; in a second step, we determine which article ( if any ) is a valid alignment, i.e. <eos> is about the same sense or concept. <eos> in this paper, we go significantly beyond stateof-the-art word overlap approaches, and apply a threshold-based personalized pagerank method for the disambiguation step. <eos> we show that wordnet synsets can be aligned to wikipedia articles with a performance of up to 0.78 f1-measure based on a comprehensive, well-balanced reference dataset consisting of 1,815 manually annotated sense alignment candidates. <eos> the fully-aligned resource as well as the reference dataset is publicly available.1
in the recognizing textual entailment ( rte ) task, sentence pairs are classified into one of three semantic relations : entailment, contradiction or unknown. <eos> while we find some sentence pairs hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one another depending on a specific situation. <eos> these partial contradiction sentence pairs contain useful information for opinion mining and other such tasks, but it is difficult for internet users to access this knowledge because current frameworks do not differentiate between full contradictions and partial contradictions. <eos> in this paper, under current approaches to semantic relation recognition, we define a new semantic relation known as confinement in order to recognize this useful information. <eos> this information is classified as either contradiction or entailment. <eos> we provide a series of semantic templates to recognize confinement relations in web texts, and then implement a system for recognizing confinement between sentence pairs. <eos> we show that our proposed system can obtains a f-score of 61 % for recognizing confinement in japanese-language web texts, and it outperforms a baseline which does not use a manually compiled list of lexico-syntactic patterns to instantiate the semantic templates.
this paper presents a discourse processing framework based on weighted abduction. <eos> we elaborate on ideas described in hobbs et al ( 1993 ) and implement the abductive inference procedure in a system called mini-tacitus. <eos> particular attention is paid to constructing a large and reliable knowledge base for supporting inferences. <eos> for this purpose we exploit such lexical-semantic resources as wordnet and framenet. <eos> we test the proposed procedure and the obtained knowledge base on the recognizing textual entailment task using the data sets from the rte-2 challenge for evaluation. <eos> in addition, we provide an evaluation of the semantic role labeling produced by the system taking the frame-annotated corpus for textual entailment as a gold standard.
this paper presents a machine learning-based approach to the incremental understanding of dialogue utterances, with a focus on the recognition of their communicative functions. <eos> a token-based approach combining the use of local classifiers, which exploit local utterance features, and global classifiers which use the outputs of local classifiers applied to previous and subsequent tokens, is shown to result in excellent dialogue act recognition scores for unsegmented spoken dialogue. <eos> this can be seen as a significant step forward towards the development of fully incremental, on-line methods for computing the meaning of utterances in spoken dialogue.
we use data from a virtual world game for automated learning of words and grammatical constructions and their meanings. <eos> the language data are an integral part of the social interaction in the game and consist of chat dialogue, which is only constrained by the cultural context, as set by the nature of the provided virtual environment. <eos> building on previous work, where we extracted a vocabulary for concrete objects in the game by making use of the non-linguistic context, we now target np/dp grammar, in particular determiners. <eos> we assume that we have captured the meanings of a set of determiners if we can predict which determiner will be used in a particular context. <eos> to this end we train a classifier that predicts the choice of a determiner on the basis of features from the linguistic and non-linguistic context.
we consider the problem of distinguishing polysemous from homonymous nouns. <eos> this distinction is often taken for granted, but is seldom operationalized in the shape of an empirical model. <eos> we present a first step towards such a model, based on wordnet augmented with ontological classes provided by corelex. <eos> this model provides a polysemy index for each noun which ( a ), accurately distinguishes between polysemy and homonymy ; ( b ), supports the analysis that polysemy can be grounded in the frequency of the meaning shifts shown by nouns ; and ( c ), improves a regression model that predicts when the ? one-sense-per-discourse ? <eos> hypothesis fails.
wordnet is extensively used as a major lexical resource in nlp. <eos> however, its quality is far from perfect, and this alters the results of applications using it. <eos> we propose here to complement previous efforts for ? cleaning up ? <eos> the top-level of its taxonomy with semi-automatic methods based on the detection of errors at the lower levels. <eos> the methods we propose test the coherence of two sources of knowledge, exploiting ontological principles and semantic constraints.
distributed models of semantics assume that word meanings can be discovered from ? the company they keep. ? <eos> many such approaches learn semantics from large corpora, with each document considered to be unstructured bags of words, ignoring syntax and compositionality within a document. <eos> in contrast, this paper proposes a structured vectorial semantic framework, in which semantic vectors are defined and composed in syntactic context. <eos> as such, syntax and semantics are fully interactive ; composition of semantic vectors necessarily produces a hypothetical syntactic parse. <eos> evaluations show that using relationally-clustered headwords as a semantic space in this framework improves on a syntax-only model in perplexity and parsing accuracy.
this paper reports on an exploratory investigation as to whether classes of urdu n-v complex predicates can be identified on the basis syntactic patterns and lexical choices associated with the n-v complex predicates. <eos> working with data from a pos annotated corpus, we show that choices with respect to the number of arguments, case marking on subjects and which light verbs are felicitous with which nouns depend heavily on the semantics of the noun in the n-v complex predicate. <eos> this initial work represents an important step towards identifying semantic criteria relevant for complex predicate formation. <eos> identifying the semantic criteria and being able to systematically code them in turn represents a first step towards building up a lexical resource for nouns as part of developing natural language processing tools for the underresourced south asian language urdu.
in this paper we describe discuss, a dialogue move taxonomy layered over semantic representations. <eos> we designed this scheme to enable development of computational models of tutorial dialogues and to provide an intermediate representation suitable for question and tutorial act generation. <eos> as such, discuss captures semantic and pragmatic elements across four dimensions : dialogue act, rhetorical form, predicate type, semantic roles. <eos> together these dimensions provide a summary of an utterance ? s propositional content and how it may change the underlying information state of the conversation. <eos> this taxonomy builds on previous work in both general dialogue act taxonomies as well as work in tutorial act and tutorial question categorization. <eos> the types and values found within our taxonomy are based on preliminary observations and on-going annotation from our corpus of multimodal tutorial dialogues for elementary school science education.
semantic change has mostly been studied by historical linguists and typically at the scale of centuries. <eos> here we study semantic change at a finer-grained level, the decade, making use of recent newspaper corpora. <eos> we detect semantic change candidates by observing context shifts which can be triggered by topic salience or may be independent from it. <eos> to discriminate these phenomena with accuracy, we combine variation filters with a series of indices which enable building a coherent and flexible semantic change detection model. <eos> the indices include widely adaptable tools such as frequency counts, co-occurrence patterns and networks, ranks, as well as model-specific items such as a variability and cohesion measure and graphical representations. <eos> the research uses acom, a co-occurrence based geometrical model, which is an extension of the semantic atlas. <eos> compared to other models of semantic representation, it allows for extremely detailed analysis and provides insight as to how connotational drift processes unfold.
in the textual entailment community, a shared effort towards a deeper understanding of the core phenomena involved in textual inference is recently arose. <eos> to analyse how the common intuition that decomposing te would allow a better comprehension of the problem from both a linguistic and a computational viewpoint, we propose a definition for strong component-based te, where each component is in itself a complete te system, able to address a te task on a specific phenomenon in isolation. <eos> we review the literature according to our definition, trying to position relevant work as more or less close to our idea of strong component-based te. <eos> several dimensions of the problem are discussed : i ) the implementation of system components to address specific inference types, ii ) the analysis of the phenomena relevant to component-based te, and iii ) the development of evaluation methodologies to assess te systems capabilities to address single phenomena in a pair.
the question of how to compose meaning in distributional representations of meaning has recently been recognised as a central issue in computational linguistics. <eos> in this paper we describe three general and powerful tools that can be used to describe composition in distributional semantics : quotient algebras, learning of finite dimensional algebras, and the construction of algebras from semigroups.
question classifiers are used within question answering to predict the expected answer type for a given question. <eos> this paper describes the first steps towards applying a similar methodology to identifying question classes in dialogue contexts, beginning with a study of questions drawn from the enron email corpus. <eos> human-annotated data is used as a gold standard for assessing the output from an existing, open-source question classifier ( qa-sys ). <eos> problem areas are identified and potential solutions discussed.
the paper presents an ongoing research that aims at owl ontology authoring and verbalizationusing a deterministic controlled natural language ( cnl ) that would be as natural and intuitive aspossible. <eos> moreover, we focus on a multilingual cnl interface to owl by considering both highlyanalytical and highly synthetic languages ( namely, english and latvian ). <eos> we propose a flexible two-level translation approach that is enabled by the grammatical framework and that has allowed us todevelop a more natural, but still predictable multilingual cnl on top of the widely used attemptocontrolled english ( its subset for owl, ace-owl ). <eos> this has also allowed us to exploit the readilyavailable ace parser and verbalizer not only for the modified and extended version of ace-owl, but also for the corresponding controlled latvian.
in this paper, we describe the baseball announcers ? <eos> language linked with general annotation of meaningful events ( ballgame ) project ? <eos> a text corpus for research in computional semantics. <eos> we collected pitch-by-pitch event data for a sample of baseball games and used this data to build an annotated corpus composed of transcripts of radio broadcasts of these games. <eos> our annotation links text from the broadcast to events in a formal representation of the semantics of the baseball game. <eos> we describe our corpus model, the annotation tool used to create the corpus, and conclude by discussing applications of this corpus in semantics research and natural language processing.
in this paper we present some features of an architecture for the translation ( italian ? <eos> italian sign language ) that performs syntactic analysis, semantic interpretation and generation. <eos> such architecture relies on an ontology that has been used to encode the domain of weather forecasts as well as information on language as part of the world knowledge. <eos> we present some general issues of the ontological semantic interpretation and discuss the analysis of ordinal numbers.
in this paper we summarize existing work on the recently introduced task of processing the scope of negation and modality cues ; we analyse the scope model that existing systems can process, which is mainly the model reflected in the annotations of the biomedical corpus on which the systems have been trained ; and we point out aspects of the scope finding task that would be different based on observations from a corpus from a different domain and nature. <eos>
in the effort of building a verb lexicon classifying the most used verbs in arabic and providing information about their syntax and semantics ( mousser, 2010 ), the problem of classes over-generation arises because of the overt morphology of arabic, which codes not only agreement and inflection relations but also semantic information related to thematic arity or other semantic information like ? intensity ?, ? pretension ?, etc. <eos> the hierarchical structure of verb classes and the inheritance relation between their subparts expels derived verbs from the main class, although they share most of its properties. <eos> in this article we present a way to adapt the verb class approach to a language with a productive ( verb ) morphology by introducing sibling classes.
this paper discusses the phenomenon of granularity in natural language1. <eos> by ? granularity ? <eos> we mean the level of detail of description of an event or object. <eos> humans can seamlessly shift their granularity perspective while reading or understanding a text. <eos> to emulate this mechanism, we describe a set of features that identify the levels of granularity in text, and empirically verify this feature set using a human annotation study for granularity identification. <eos> this theory is the foundation for any system that can learn the ( global ) behavior of event descriptions from ( local ) behavior descriptions. <eos> this is the first research initiative, to our knowledge, for identifying granularity shifts in natural language descriptions.
this paper describes recent work on the dyndial project ? <eos> towards incremental semantic interpretation in dialogue. <eos> we outline our domain-general grammar-based approach, using a variant of dynamic syntax integrated with type theory with records and a davidsonian event-based semantics. <eos> we describe a java-based implementation of the parser, used within the jindigo framework to produce an incremental dialogue system capable of handling inherently incremental phenomena such as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
the multimodal interface language formalism ( mmil ) has been selected as the high level semantic ( hls ) formalism for annotating the french media dialogue corpus. <eos> this corpus is composed of human-machine dialogues in the domain of hotel reservation and tourist information. <eos> utterances in dialogues have been previously annotated with a concept-value flat semantics for studying and evaluating spoken language understanding modules in dialogue systems. <eos> we are now interested in investigating the use of more complex representations to improve the understanding capability. <eos> the mmil intermediate language is a high level semantic formalism that bears relevant linguistic information, from syntax up to discourse. <eos> this representation should increase the expressivity of the current annotation though at the expense of the annotation process complexity. <eos> in this paper we present our first attempt in defining the annotation guidelines for the hls annotation of the media corpus and its effect on the annotation process itself, revealed by annotators ? <eos> disagreements due to the different levels of hierarchy and the granularity of the features defined in mmil.
wordseye is a system for automatically converting natural language text into 3d scenes representing the meaning of that text. <eos> at the core of wordseye is the scenario-based lexical knowledge resource ( sblr ), a unified knowledge base and representational system for expressing lexical and real-world knowledge needed to depict scenes from text. <eos> to enrich a portion of the sblr, we need to fill out some contextual information about its objects, including information about their typical parts, typical locations and typical objects located near them. <eos> this paper explores our proposed methodology to achieve this goal. <eos> first we try to collect some semantic information by using amazon ? s mechanical turk ( amt ). <eos> then, we manually filter and classify the collected data and finally, we compare the manual results with the output of some automatic filtration techniques which use several wordnet similarity and corpus association measures.
we introduce a novel approach to measuring semantic relatedness of terms based on an automatically generated, large-scale semantic network. <eos> we present promising first results that indicate potential competitiveness with approaches based on manually created resources.
we propose a biomedical event extraction system, hvs-bioevent, which employs the hidden vector state ( hvs ) model for semantic parsing. <eos> biomedical events extraction needs to deal with complex events consisting of embedded or hierarchical relations among proteins, events, and their textual triggers. <eos> in hvs-bioevent, we further propose novel machine learning approaches for event trigger word identification, and for biomedical events extraction from the hvs parse results. <eos> our proposed system achieves an f-score of 49.57 % on the corpus used in the bionlp ? 09 shared task, which is only two points lower than the best performing system by uturku. <eos> nevertheless, hvsbioevent outperforms uturku on the extraction of complex event types. <eos> the results suggest that the hvs model with the hierarchical hidden state structure is indeed more suitable for complex event extraction since it can naturally model embedded structural context in sentences.
the extraction of protein-protein interactions ( ppis ) reported in scientific publications is one of the most studied topics in text mining in the life sciences, as such algorithms can substantially decrease the effort for databases curators. <eos> the currently best methods for this task are based on analyzing the dependency tree ( dt ) representation of sentences. <eos> many approaches exploit only topological features and thus do not yet fully exploit the information contained in dts. <eos> we show that incorporating the grammatical information encoded in the types of the dependencies in dts noticeably improves extraction performance by using a pattern matching approach. <eos> we automatically infer a large set of linguistic patterns using only information about interacting proteins. <eos> patterns are then refined based on shallow linguistic features and the semantics of dependency types. <eos> together, these lead to a total improvement of 17.2 percent points in f1, as evaluated on five publicly available ppi corpora. <eos> more than half of that improvement is gained by properly handling dependency types. <eos> our method provides a general framework for building task-specific relationship extraction methods that do not require annotated training data. <eos> furthermore, our observations offer methods to improve upon relation extraction approaches.
entailment detection systems are generally designed to work either on single words, relations or full sentences. <eos> we propose a new task ? <eos> detecting entailment between dependency graph fragments of any type ? <eos> which relaxes these restrictions and leads to much wider entailment discovery. <eos> an unsupervised framework is described that uses intrinsic similarity, multi-level extrinsic similarity and the detection of negation and hedged language to assign a confidence score to entailment relations between two fragments. <eos> the final system achieves 84.1 % average precision on a data set of entailment examples from the biomedical domain.
accurate phenotype mapping will play an important role in facilitating phenome-wide association studies ( phewas ), and potentially in other phenomics based studies. <eos> the phewas approach investigates the association between genetic variation and an extensive range of phenotypes in a high-throughput manner to better understand the impact of genetic variations on multiple phenotypes. <eos> herein we define the phenotype mapping problem posed by phewas analyses, discuss the challenges, and present a machine-learning solution. <eos> our key ideas include the use of weighted jaccard features and term augmentation by dictionary lookup. <eos> when compared to string similarity metric-based features, our approach improves the f-score from 0.59 to 0.73. <eos> with augmentation we show further improvement in f-score to 0.89. <eos> for terms not covered by the dictionary, we use transitive closure inference and reach an f-score of 0.91, close to a level sufficient for practical use. <eos> we also show that our model generalizes well to phenotypes not used in our training dataset.
in comparative genomics, functional annotations are transferred from one organism to another relying on sequence similarity. <eos> with more than 20 million citations in pubmed, text mining provides the ideal tool for generating additional large-scale homology-based predictions. <eos> to this end, we have refined a recent dataset of biomolecular events extracted from text, and integrated these predictions with records from public gene databases. <eos> accounting for lexical variation of gene symbols, we have implemented a disambiguation algorithm that uniquely links the arguments of 11.2 million biomolecular events to well-defined gene families, providing interesting opportunities for query expansion and hypothesis generation. <eos> the resulting mysql database, including all 19.2 million original events as well as their homology-based variants, is publicly available at http : //bionlp.utu.fi/.
a simple and accurate method for assigning broad semantic classes to text strings is presented. <eos> the method is to map text strings to terms in ontologies based on a pipeline of exact matches, normalized strings, headword matching, and stemming headwords. <eos> the results of three experiments evaluating the technique are given. <eos> five semantic classes are evaluated against the craft corpus of full-text journal articles. <eos> twenty semantic classes are evaluated against the corresponding full ontologies, i.e. <eos> by reflexive matching. <eos> one semantic class is evaluated against a structured test suite. <eos> precision, recall, and f-measure on the corpus when evaluating against only the ontologies in the corpus is micro-averaged 67.06/78.49/72.32 and macro-averaged 69.84/83.12/75.31. <eos> accuracy on the corpus when evaluating against all twenty semantic classes ranges from 77.12 % to 95.73 %. <eos> reflexive matching is generally successful, but reveals a small number of errors in the implementation. <eos> evaluation with the structured test suite reveals a number of characteristics of the performance of the approach.
traditionally, automated triage of papers is performed using lexical ( unigram, bigram, and sometimes trigram ) features. <eos> this paper explores the use of information extraction ( ie ) techniques to create richer linguistic features than traditional bag-of-words models. <eos> our classifier includes lexico-syntactic patterns and more-complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. <eos> our experimental results show that the ie-based features can improve performance over unigram and bigram features alone. <eos> we present intrinsic evaluation results of full-text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the mouse genome informatics ( mgi ) system at the jackson laboratories. <eos> we also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge curation at mgi.
medical entity recognition is a crucial step towards efficient medical texts analysis. <eos> in this paper we present and compare three methods based on domain-knowledge and machine-learning techniques. <eos> we study two research directions through these approaches : ( i ) a first direction where noun phrases are extracted in a first step with a chunker before the final classification step and ( ii ) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. <eos> each of the presented approaches is tested on a standard corpus of clinical texts. <eos> the obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance.
named entity recognition ( ner ) is an important first step for bionlp tasks, e.g., gene normalization and event extraction. <eos> employing supervised machine learning techniques for achieving high performance recent ner systems require a manually annotated corpus in which every mention of the desired semantic types in a text is annotated. <eos> however, great amounts of human effort is necessary to build and maintain an annotated corpus. <eos> this study explores a method to build a high-performance ner without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of semantic types and with huge amount of unannotated texts. <eos> we underscore the effectiveness of our approach by comparing the performance of ners trained on an automatically acquired training data and on a manually annotated corpus.
semantic role labeling ( srl ) plays a key role in many nlp applications. <eos> the development of srl systems for the biomedical domain is frustrated by the lack of large domainspecific corpora that are labeled with semantic roles. <eos> corpus development has been very expensive and time-consuming. <eos> in this paper we propose a method for building frame-based corpus on the basis of domain knowledge provided by ontologies. <eos> we believe that ontologies, as a structured and semantic representation of domain knowledge, can instruct and ease the tasks in building the corpora. <eos> in the paper we present a corpus built by using the method. <eos> we compared it to bioframenet, and examined the gaps between the semantic classification of the target words in the domainspecific corpus and in framenet and propbank/verbnet.
one of the reasons for which the resolution of coreferences has remained a challenging information extraction task, especially in the biomedical domain, is the lack of training data in the form of annotated corpora. <eos> in order to address this issue, we developed the hanapin corpus. <eos> it consists of full-text articles from biochemistry literature, covering entities of several semantic types : chemical compounds, drug targets ( e.g., proteins, enzymes, cell lines, pathogens ), diseases, organisms and drug effects. <eos> all of the coreferring expressions pertaining to these semantic types were annotated based on the annotation scheme that we developed. <eos> we observed four general types of coreferences in the corpus : sortal, pronominal, abbreviation and numerical. <eos> using the masi distance metric, we obtained 84 % in computing the inter-annotator agreement in terms of krippendorff ? s alpha. <eos> consisting of 20 full-text, open-access articles, the corpus will enable other researchers to use it as a resource for their own coreference resolution methodologies.
the paper discuses problems in annotating a corpus containing polish clinical data with low level linguistic information. <eos> we propose an approach to tokenization and automatic morphologic annotation of data that uses existing programs combined with a set of domain specific rules and vocabulary. <eos> finally we present the results of manual verification of the annotation for a subset of data.
we present a bootstrapping approach to infer new proteins, locations and protein-location pairs by combining uniprot seed proteinlocation pairs with dependency paths from a large collection of text. <eos> of the top 20 system proposed protein-location pairs, 18 were in uniprot or supported by online evidence. <eos> interestingly, 3 of the top 20 locations identified by the system were in the uniprot description, but missing from the formal ontology.
the construction of pathways is a major focus of present-day biology. <eos> typical pathways involve large numbers of entities of various types whose associations are represented as reactions involving arbitrary numbers of reactants, outputs and modifiers. <eos> until recently, few information extraction approaches were capable of resolving the level of detail in text required to support the annotation of such pathway representations. <eos> we argue that event representations of the type popularized by the bionlp shared task are potentially applicable for pathway annotation support. <eos> as a step toward realizing this possibility, we study the mapping from a formal pathway representation to the event representation in order to identify remaining challenges in event extraction for pathway annotation support. <eos> following initial analysis, we present a detailed study of protein association and dissociation reactions, proposing a new event class and representation for the latter and, as a step toward its automatic extraction, introduce a manually annotated resource incorporating the type among a total of nearly 1300 annotated event instances. <eos> as a further practical contribution, we introduce the first pathway-to-event conversion software for sbml/celldesigner pathways and discuss the opportunities arising from the ability to convert the substantial existing pathway resources to events.
protein modifications, in particular posttranslational modifications, have a central role in bringing about the full repertoire of protein functions, and the identification of specific protein modifications is important for understanding biological systems. <eos> this task presents a number of opportunities for the automatic support of manual curation efforts. <eos> however, the sheer number of different types of protein modifications is a daunting challenge for automatic extraction that has so far not been met in full, with most studies focusing on single modifications or a few prominent ones. <eos> in this work, aim to meet this challenge : we analyse protein modification types through ontologies, databases, and literature and introduce a corpus of 360 abstracts manually annotated in the bionlp shared task event representation for over 4500 mentions of proteins and 1000 statements of modification events of nearly 40 different types. <eos> we argue that together with existing resources, this corpus provides sufficient coverage of modification types to make effectively exhaustive extraction of protein modifications from text feasible.
kernel methods are considered the most effective techniques for various relation extraction ( re ) tasks as they provide higher accuracy than other approaches. <eos> in this paper, we introduce new dependency tree ( dt ) kernels for re by improving on previously proposed dependency tree structures. <eos> these are further enhanced to design more effective approaches that we call mildly extended dependency tree ( medt ) kernels. <eos> the empirical results on the protein-protein interaction ( ppi ) extraction task on the aimed corpus show that tree kernels based on our proposed dt structures achieve higher accuracy than previously proposed dt and phrase structure tree ( pst ) kernels.
increasingly, as full-text scientific papers are becoming available, scientific queries have shifted from looking for facts to looking for arguments. <eos> researchers want to know when their colleagues are proposing theories, outlining evidentiary relations, or explaining discrepancies. <eos> we show here that sentence-level annotation with the cisp schema adapts well to a corpus of biomedical articles, and we present preliminary results arguing that the cisp schema is uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence.
in this study we investigate the merits of fast approximate string matching to address challenges relating to spelling variants and to utilise large-scale lexical resources for semantic class disambiguation. <eos> we integrate string matching results into machine learning-based disambiguation through the use of a novel set of features that represent the distance of a given textual span to the closest match in each of a collection of lexical resources. <eos> we collect lexical resources for a multitude of semantic categories from a variety of biomedical domain sources. <eos> the combined resources, containing more than twenty million lexical items, are queried using a recently proposed fast and efficient approximate string matching algorithm that allows us to query large resources without severely impacting system performance. <eos> we evaluate our results on six corpora representing a variety of disambiguation tasks. <eos> while the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. <eos> we suggest possible explanations and future research directions. <eos> our lexical resources and implementation are made freely available for research purposes at : http : //github.com/ninjin/ simsem
we present an end-to-end system that processes narrative clinical records, constructs timelines for the medical histories of patients, and visualizes the results. <eos> this work is motivated by real clinical records and our general approach is based on deep semantic natural language understanding.
suppose we have a large collection of documents most of which are unlabeled. <eos> suppose further that we have a small subset of these documents which represent a particular class of documents we are interested in, i.e. <eos> these are labeled as positive examples. <eos> we may have reason to believe that there are more of these positive class documents in our large unlabeled collection. <eos> what data mining techniques could help us find these unlabeled positive examples ? <eos> here we examine machine learning strategies designed to solve this problem. <eos> we find that a proper choice of machine learning method as well as training strategies can give substantial improvement in retrieving, from the large collection, data enriched with positive examples. <eos> we illustrate the principles with a real example consisting of multiword umls phrases among a much larger collection of phrases from medline.
this paper presents our preliminary work on adaptation of parsing technology toward natural language query processing for biomedical domain. <eos> we built a small treebank of natural language queries, and tested a state-of-theart parser, the results of which revealed that a parser trained on wall-street-journal articles and medline abstracts did not work well on query sentences. <eos> we then experimented an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. <eos> despite the small scale of the experiments, the results are encouraging, enlightening the direction for effective improvement.
liang donia scott school of computer science, school of informatics, the university of manchester, oxford road, the university of sussex, falmer, manchester, m13 9pl, uk brighton, bn1 9qh, uk fennie.liang @ cs.man.ac.uk d.r.scott @ sussex.ac.uk robert stevens alan rector school of computer science, school of computer science, the university of manchester, oxford road, the university of manchester, oxford road, manchester, m13 9pl, uk manchester, m13 9pl, uk robert.stevens @ cs.man.ac.uk rector @ cs.man.ac.uk abstract ontology authoring is a specialised task requiring amongst other things a deep knowledge of the ontology language being used. <eos> understanding and reusing ontologies can thus be difficult for domain experts, who tend not to be ontology experts. <eos> to address this problem, we have developed a natural language generation system for transforming the axioms that form the definitions of ontology classes into natural language paragraphs. <eos> our method relies on deploying ontology axioms into a top-level rhetorical structure theory schema. <eos> axioms are ordered and structured with specific rhetorical relations under rhetorical structure trees. <eos> we describe here an implementation that focuses on a sub-module of snomed ct. with some refinements on articles and layout, the resulting paragraphs are fluent and coherent, offering a way for subject specialists to understand an ontology ? s content without need to understand its logical representation.
word sense disambiguation ( wsd ) is an intermediate task within information retrieval and information extraction, attempting to select the proper sense of ambiguous words. <eos> due to the scarcity of training data, semi-supervised learning, which profits from seed annotated examples and a large set of unlabeled data, are worth researching. <eos> we present preliminary results of two semi-supervised learning algorithms on biomedical word sense disambiguation. <eos> both methods add relevant unlabeled examples to the training set, and optimal parameters are similar for each ambiguous word.
we present medstractplus, a resource for mining relations from the medline bibliographic database. <eos> it was built on the remains of medstract, a previously created resource that included a bio-relation server and an acronym database. <eos> medstractplus uses simple and scalable natural language processing modules to structure text and is designed with reusability and extendibility in mind.
thai traditional medicine ( ttm ) has a long history in thailand and is nowadays considered an effective alternative approach to the modern medicine. <eos> one of the main knowledge in thai traditional medicine is the use of various types of herbs to form medicines. <eos> our main goal is to bridge the gap between the traditional knowledge and the modern biomedical knowledge. <eos> using text mining and visualization techniques, some implicit relations from one source could be used to verify and enhance the knowledge discovery in another source. <eos> in this paper, we present our ongoing work, thaiherbminer, a thai herbal medicine mining and visualizing tool. <eos> thaiherbminer applies text mining to extract some salient relations from a collection of pubmed articles related to thai herbs. <eos> the extracted relations can be browsed and viewed using information visualization. <eos> our proposed tool can also recommend a list of herbs which have similar medical properties.
the connection between part-of-speech ( pos ) categories and morphological properties is well-documented in linguistics but underutilized in text processing systems. <eos> this paper proposes a novel model for morphological segmentation that is driven by this connection. <eos> our model learns that words with common affixes are likely to be in the same syntactic category and uses learned syntactic categories to refine the segmentation boundaries of words. <eos> our results demonstrate that incorporating pos categorization yields substantial performance gains on morphological segmentation of arabic. <eos> 1
we show how punctuation can be used to improve unsupervised dependency parsing. <eos> our linguistic analysis confirms the strong connection between english punctuation and phrase boundaries in the penn treebank. <eos> however, approaches that naively include punctuation marks in the grammar ( as if they were words ) do not perform well with klein and manning ? s dependency model with valence ( dmv ). <eos> instead, we split a sentence at punctuation and impose parsing restrictions over its fragments. <eos> our grammar inducer is trained on the wall street journal ( wsj ) and achieves 59.5 % accuracy out-of-domain ( brown sentences with 100 or fewer words ), more than 6 % higher than the previous best results. <eos> further evaluation, using the 2006/7 conll sets, reveals that punctuation aids grammar induction in 17 of 18 languages, for an overall average net gain of 1.3 %. <eos> some of this improvement is from training, but more than half is from parsing with induced constraints, in inference. <eos> punctuation-aware decoding works with existing ( even already-trained ) parsing models and always increased accuracy in our experiments.
while many computational models have been created to explore how children might learn to segment words, the focus has largely been on achieving higher levels of performance and exploring cues suggested by artificial learning experiments. <eos> we propose a broader focus that includes designing models that display properties of infants ? <eos> performance as they begin to segment words. <eos> we develop an efficient bootstrapping online learner with this focus in mind, and evaluate it on child-directed speech. <eos> in addition to attaining a high level of performance, this model predicts the error patterns seen in infants learning to segment words.
during language acquisition, children learn to segment speech into phonemes, syllables, morphemes, and words. <eos> we examine word segmentation specifically, and explore the possibility that children might have generalpurpose chunking mechanisms to perform word segmentation. <eos> the voting experts ( ve ) and bootstrapped voting experts ( bve ) algorithms serve as computational models of this chunking ability. <eos> ve finds chunks by searching for a particular information-theoretic signature : low internal entropy and high boundary entropy. <eos> bve adds to ve the ability to incorporate information about word boundaries previously found by the algorithm into future segmentations. <eos> we evaluate the general chunking model on phonemicallyencoded corpora of child-directed speech, and show that it is consistent with empirical results in the developmental literature. <eos> we argue that it offers a parsimonious alternative to specialpurpose linguistic models.
we develop an approach to biomedical event extraction using a search-based structured prediction framework, searn, which converts the task into cost-sensitive classification tasks whose models are learned jointly. <eos> we show that searn improves on a simple yet strong pipeline by 8.6 points in f-score on the bionlp 2009 shared task, while achieving the best reported performance by a joint inference method. <eos> additionally, we consider the issue of cost estimation during learning and present an approach called focused costing that improves improves efficiency and predictive accuracy.
automatic extraction of opinion holders and targets ( together referred to as opinion entities ) is an important subtask of sentiment analysis. <eos> in this work, we attempt to accurately extract opinion entities from urdu newswire. <eos> due to the lack of resources required for training role labelers and dependency parsers ( as in english ) for urdu, a more robust approach based on ( i ) generating candidate word sequences corresponding to opinion entities, and ( ii ) subsequently disambiguating these sequences as opinion holders or targets is presented. <eos> detecting the boundaries of such candidate sequences in urdu is very different than in english since in urdu, grammatical categories such as tense, gender and case are captured in word inflections. <eos> in this work, we exploit the morphological inflections associated with nouns and verbs to correctly identify sequence boundaries. <eos> different levels of information that capture context are encoded to train standard linear and sequence kernels. <eos> to this end the best performance obtained for opinion entity detection for urdu sentiment analysis is 58.06 % f-score using sequence kernels and 61.55 % f-score using a combination of sequence and linear kernels.
crisis-affected populations are often able to maintain digital communications but in a sudden-onset crisis any aid organizations will have the least free resources to process such communications. <eos> information that aid agencies can actually act on, ? actionable ? <eos> information, will be sparse so there is great potential to ( semi ) automatically identify actionable communications. <eos> however, there are hurdles as the languages spoken will often be underresourced, have orthographic variation, and the precise definition of ? actionable ? <eos> will be response-specific and evolving. <eos> we present a novel system that addresses this, drawing on 40,000 emergency text messages sent in haiti following the january 12, 2010 earthquake, predominantly in haitian kreyol. <eos> we show that keyword/ngram-based models using streaming maxent achieve up to f=0.21 accuracy. <eos> further, we find current state-ofthe-art subword models increase this substantially to f=0.33 accuracy, while modeling the spatial, temporal, topic and source contexts of the messages can increase this to a very accurate f=0.86 over direct text messages and f=0.90-0.97 over social media, making it a viable strategy for message prioritization.
sociolinguistic theories ( e.g., lakoff ( 1973 ) ) postulate that women ? s language styles differ from that of men. <eos> in this paper, we explore statistical techniques that can learn to identify the gender of authors in modern english text, such as web blogs and scientific papers. <eos> although recent work has shown the efficacy of statistical approaches to gender attribution, we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier. <eos> our work is the first that consciously avoids gender bias in topics, thereby providing stronger evidence to gender-specific styles in language beyond topic. <eos> in addition, our comparative study provides new insights into robustness of various stylometric techniques across topic and genre.
subjectivity word sense disambiguation ( swsd ) is automatically determining which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. <eos> swsd has been shown to improve the performance of contextual opinion analysis, but only on a small scale and using manually developed integration rules. <eos> in this paper, we scale up the integration of swsd into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. <eos> further, by improving the method for integrating swsd into contextual opinion analysis, even greater benefits from swsd are achieved than in previous work. <eos> we thus more firmly demonstrate the potential of swsd to improve contextual opinion analysis.
we present a computational model of language learning via a sequence of interactions between a teacher and a learner. <eos> experiments learning limited sublanguages of 10 natural languages show that the learner achieves a high level of performance after a reasonable number of interactions, the teacher can produce meaning-preserving corrections of the learner ? s utterances, and the learner can detect them. <eos> the learner does not treat corrections specially ; nonetheless in several cases, significantly fewer interactions are needed by a learner interacting with a correcting teacher than with a non-correcting teacher.
feature feedback is an alternative to instance labeling when seeking supervision from human experts. <eos> combination of instance and feature feedback has been shown to reduce the total annotation cost for supervised learning. <eos> however, learning problems may not benefit equally from feature feedback. <eos> it is well understood that the benefit from feature feedback reduces as the amount of training data increases. <eos> we show that other characteristics such as domain, instance granularity, feature space, instance selection strategy and proportion of relevant text, have a significant effect on benefit from feature feedback. <eos> we estimate the maximum benefit feature feedback may provide ; our estimate does not depend on how the feedback is solicited and incorporated into the model. <eos> we extend the complexity measures proposed in the literature and propose some new ones to categorize learning problems, and find that they are strong indicators of the benefit from feature feedback.
in this paper we present ulisse, an unsupervised linguistically ? driven algorithm to select reliable parses from the output of a dependency parser. <eos> different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains. <eos> in all cases, ulisse appears to outperform the baseline algorithms.
finding the right representation for words is critical for building accurate nlp systems when domain-specific labeled data for the task is scarce. <eos> this paper investigates language model representations, in which language models trained on unlabeled corpora are used to generate real-valued feature vectors for words. <eos> we investigate ngram models and probabilistic graphical models, including a novel lattice-structured markov random field. <eos> experiments indicate that language model representations outperform traditional representations, and that graphical model representations outperform ngram models, especially on sparse and polysemous words.
keyphrase extraction aims to select a set of terms from a document as a short summary of the document. <eos> most methods extract keyphrases according to their statistical properties in the given document. <eos> appropriate keyphrases, however, are not always statistically significant or even do not appear in the given document. <eos> this makes a large vocabulary gap between a document and its keyphrases. <eos> in this paper, we consider that a document and its keyphrases both describe the same object but are written in two different languages. <eos> by regarding keyphrase extraction as a problem of translating from the language of documents to the language of keyphrases, we use word alignment models in statistical machine translation to learn translation probabilities between the words in documents and the words in keyphrases. <eos> according to the translation model, we suggest keyphrases given a new document. <eos> the suggested keyphrases are not necessarily statistically frequent in the document, which indicates that our method is more flexible and reliable. <eos> experiments on news articles demonstrate that our method outperforms existing unsupervised methods on precision, recall and f-measure.
in this paper, we introduce a knowledge-based method to disambiguate biomedical acronyms using second-order co-occurrence vectors. <eos> we create these vectors using information about a long-form obtained from the unified medical language system and medline. <eos> we evaluate this method on a dataset of 18 acronyms found in biomedical text. <eos> our method achieves an overall accuracy of 89 %. <eos> the results show that using second-order features provide a distinct representation of the long-form and potentially enhances automated disambiguation.
the first step in graph-based semi-supervised classification is to construct a graph from input data. <eos> while the k-nearest neighbor graphs have been the de facto standard method of graph construction, this paper advocates using the less well-known mutual k-nearest neighbor graphs for high-dimensional natural language data. <eos> to compare the performance of these two graph construction methods, we run semi-supervised classification methods on both graphs in word sense disambiguation and document classification tasks. <eos> the experimental results show that the mutual k-nearest neighbor graphs, if combined with maximum spanning trees, consistently outperform the knearest neighbor graphs. <eos> we attribute better performance of the mutual k-nearest neighbor graph to its being more resistive to making hub vertices. <eos> the mutual k-nearest neighbor graphs also perform equally well or even better in comparison to the state-of-the-art b-matching graph construction, despite their lower computational complexity.
in this paper we present methods for automatically acquiring training examples for the task of entity extraction. <eos> experimental evidence show that : ( 1 ) our methods compete with a current heavily supervised state-of-the-art system, within 0.04 absolute mean average precision ; and ( 2 ) our model significantly outperforms other supervised and unsupervised baselines by between 0.15 and 0.30 in absolute mean average precision.
this paper makes two contributions to the area of single-word based word alignment for bilingual sentence pairs. <eos> firstly, it integrates the ? <eos> seemingly rather different ? <eos> works of ( bodrumlu et al, 2009 ) and the standard probabilistic ones into a single framework. <eos> secondly, we present two algorithms to optimize the arising task. <eos> the first is an iterative scheme similar to viterbi training, able to handle large tasks. <eos> the second is based on the inexact solution of an integer program. <eos> while it can handle only small corpora, it allows more insight into the quality of the model and the performance of the iterative scheme. <eos> finally, we present an alternative way to handle prior dictionary knowledge and discuss connections to computing ibm-3 viterbi alignments.
the problem of authorship attribution ? <eos> attributing texts to their original authors ? <eos> has been an active research area since the end of the 19th century, attracting increased interest in the last decade. <eos> most of the work on authorship attribution focuses on scenarios with only a few candidate authors, but recently considered cases with tens to thousands of candidate authors were found to be much more challenging. <eos> in this paper, we propose ways of employing latent dirichlet allocation in authorship attribution. <eos> we show that our approach yields state-of-the-art performance for both a few and many candidate authors, in cases where these authors wrote enough texts to be modelled effectively.
we describe the extension and objective evaluation of a network1 of semantically related noun senses ( or concepts ) that has been automatically acquired by analyzing lexical cooccurrence in wikipedia. <eos> the acquisition process makes no use of the metadata or links that have been manually built into the encyclopedia, and nouns in the network are automatically disambiguated to their corresponding noun senses without supervision. <eos> for this task, we use the noun sense inventory of wordnet 3.0. <eos> thus, this work can be conceived of as augmenting the wordnet noun ontologywith unweighted, undirected relatedto edges between synsets. <eos> our network contains 208,832 such edges. <eos> we evaluate our network ? s performance on a word sense disambiguation ( wsd ) task and show : a ) the network is competitive with wordnet when used as a stand-alone knowledge source for two wsd algorithms ; b ) combining our network with wordnet achieves disambiguation results that exceed the performance of either resource individually ; and c ) our network outperforms a similar resource that has been automatically derived from semantic annotations in the wikipedia corpus.
we investigate the use of semi-supervised learning ( ssl ) in opinion detection both in sparse data situations and for domain adaptation. <eos> we show that co-training reaches the best results in an in-domain setting with small labeled data sets, with a maximum absolute gain of 33.5 %. <eos> for domain transfer, we show that self-training gains an absolute improvement in labeling accuracy for blog data of 16 % over the supervised approach with target domain training data.
we propose a normalized-cut model for the problem of aligning a known hierarchical browsing structure, e.g., electronic slides of lecture recordings, with the sequential transcripts of the corresponding spoken documents, with the aim to help index and access the latter. <eos> this model optimizes a normalizedcut graph-partitioning criterion and considers local tree constraints at the same time. <eos> the experimental results show the advantage of this model over viterbi-like, sequential alignment, under typical speech recognition errors.
studying natural language, and especially how people describe the world around them can help us better understand the visual world. <eos> in turn, it can also help us in the quest to generate natural language that describes this world in a human manner. <eos> we present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. <eos> unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. <eos> experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description ? <eos> making for more human-like annotations than previous approaches.
natural language systems trained on labeled data from one domain do not perform well on other domains. <eos> most adaptation algorithms proposed in the literature train a new model for the new domain using unlabeled data. <eos> however, it is time consuming to retrain big models or pipeline systems. <eos> moreover, the domain of a new target sentence may not be known, and one may not have significant amount of unlabeled data for every new domain. <eos> to pursue the goal of an open domain nlp ( train once, test anywhere ), we propose adut ( adaptation using label-preserving transformation ), an approach that avoids the need for retraining and does not require knowledge of the new domain, or any data from it. <eos> our approach applies simple label-preserving transformations to the target text so that the transformed text is more similar to the training domain ; it then applies the existing model on the transformed sentences and combines the predictions to produce the desired prediction on the target text. <eos> we instantiate adut for the case of semantic role labeling ( srl ) and show that it compares favorably with approaches that retrain their model on the target domain. <eos> specifically, this ? on the fly ? <eos> adaptation approach yields 13 % error reduction for a single parse system when adapting from the news wire text to fiction.
this paper shows that the performance of history-based models can be significantly improved by performing lookahead in the state space when making each classification decision. <eos> instead of simply using the best action output by the classifier, we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences. <eos> we present a perceptron-based parameter optimization method for this learning framework and show its convergence properties. <eos> the proposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. <eos> experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields ( crfs ) and structured perceptrons.
traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. <eos> we propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. <eos> our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function ( e.g., cosine ) of the projected vectors, and is able to efficiently handle a large number of training examples in the highdimensional space. <eos> evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.
in this article we present the rst spanish treebank, the first corpus annotated with rhetorical relations for this language. <eos> we describe the characteristics of the corpus, the annotation criteria, the annotation procedure, the inter-annotator agreement, and other related aspects. <eos> moreover, we show the interface that we have developed to carry out searches over the corpus ? <eos> annotated texts.
this paper describes the modeling of the morphosyntactic annotations of the multext-east corpora and lexicons as an owl/dl ontology. <eos> formalizing annotation schemes in owl/dl has the advantages of enabling formally specifying interrelationships between the various features and making logical inferences based on the relationships between them. <eos> we show that this approach provides us with a top-down perspective on a large set of morphosyntactic specifications for multiple languages, and that this perspective helps to identify and to resolve conceptual problems in the original specifications. <eos> furthermore, the ontological modeling allows us to link the multext-east specifications with repositories of annotation terminology such as the general ontology of linguistics descriptions or the iso tc37/sc4 data category registry.
this paper makes two contributions. <eos> first, we describe the hindi proposition bank that contains annotations of predicate argument structures of verb predicates. <eos> unlike propbanks in most other languages, the hind propbank is annotated on top of dependency structure, the hindi dependency treebank. <eos> we explore the similarities between dependency and predicate argument structures, so the propbank annotation can be faster and more accurate. <eos> second, we present a probabilistic rule-based system that maps syntactic dependents to semantic arguments. <eos> with simple rules, we classify about 47 % of the entire propbank arguments with over 90 % confidence. <eos> these preliminary results are promising ; they show how well these two frameworks are correlated. <eos> this can also be used to speed up our annotations.
for the implementation of the prosody prediction model, large scale annotated speech corpora have been widely applied. <eos> reliability among transcribers, however, was too low for successful learning of an automatic prosodic prediction. <eos> this paper reveals our observations on performance deterioration of the learning model due to inconsistent tagging of prosodic breaks in the established corpora. <eos> then, we suggest a method for consistent prosodic labeling among multiple transcribers. <eos> as a result, we obtain a corpus with consistent annotation of prosodic breaks. <eos> the estimated pairwise agreement of annotation of the main corpus is between 0.7477 and 0.7916, and the value of k is between 0.7057 and 0.7569. <eos> considering the estimated k, annotation of the main corpus has reliable consistency among multiple transcribers.
biasml is a novel annotation scheme with the purpose of identifying the presence as well as nuances of biased language within the subset of wikipedia articles dedicated to service providers. <eos> whereas wikipedia currently uses only manual flagging to detect possible bias, our scheme provides a foundation for the automating of bias flagging by improving upon the methodology of annotation schemes in classic sentiment analysis. <eos> we also address challenges unique to the task of identifying biased writing within the specific context of wikipedia ? s neutrality policy. <eos> we perform a detailed analysis of inter-annotator agreement, which shows that although the agreement scores for intra-sentential tags were relatively low, the agreement scores on the sentence and entry levels were encouraging ( 74.8 % and 66.7 %, respectively ). <eos> based on an analysis of our first implementation of our scheme, we suggest possible improvements to our guidelines, in hope that further rounds of annotation after incorporating them could provide appropriate data for use within a machine learning framework for automated detection of bias within wikipedia.
we describe a new interactive annotation scheme between a human annotator who carries out simplified annotations on cfg trees, and a statistical parser that converts the human annotations automatically into a richly annotated hpsg treebank. <eos> in order to check the proposed scheme ? s effectiveness, we performed automatic pseudo-annotations that emulate the system ? s idealized behavior and measured the performance of the parser trained on those annotations. <eos> in addition, we implemented a prototype system and conducted manual annotation experiments on a small test set.
the quality of annotated data is crucial for supervised learning. <eos> to eliminate errors in single annotated data, a second round of annotation is often used. <eos> however, is it absolutely necessary to double annotate every example ? <eos> we show that it is possible to reduce the amount of the second round of annotation by more than half without sacrificing the performance.
in this paper, we propose a crowdsourcing methodology for a single-step construction of both an empirically-derived sense inventory and the corresponding sense-annotated corpus. <eos> the methodology taps the intuitions of non-expert native speakers to create an expertquality resource, and natively lends itself to supplementing such a resource with additional information about the structure and reliability of the produced sense inventories. <eos> the resulting resource will provide several ways to empirically measure distances between related word senses, and will explicitly address the question of fuzzy boundaries between them.
this paper presents an evaluation of an automated quality assurance technique for a type of semantic representation known as a predicate argument structure. <eos> these representations are crucial to the development of an important class of corpus known as a proposition bank. <eos> previous work ( cohen and hunter, 2006 ) proposed and tested an analytical technique based on a simple discovery procedure inspired by classic structural linguistic methodology. <eos> cohen and hunter applied the technique manually to a small set of representations. <eos> here we test the feasibility of automating the technique, as well as the ability of the technique to scale to a set of semantic representations and to a corpus many times larger than that used by cohen and hunter. <eos> we conclude that the technique is completely automatable, uncovers missing sense distinctions and other bad semantic representations, and does scale well, performing at an accuracy of 69 % for identifying bad representations. <eos> we also report on the implications of our findings for the correctness of the semantic representations in propbank.
within the framework of the construction of a fact database, we defined guidelines to extract named entities, using a taxonomy based on an extension of the usual named entities definition. <eos> we thus defined new types of entities with broader coverage including substantivebased expressions. <eos> these extended named entities are hierarchical ( with types and components ) and compositional ( with recursive type inclusion and metonymy annotation ). <eos> human annotators used these guidelines to annotate a 1.3m word broadcast news corpus in french. <eos> this article presents the definition and novelty of extended named entity annotation guidelines, the human annotation of a global corpus and of a mini reference corpus, and the evaluation of annotations through the computation of inter-annotator agreements. <eos> finally, we discuss our approach and the computed results, and outline further work.
the creation of a gold standard corpus ( gsc ) is a very laborious and costly process. <eos> silver standard corpus ( ssc ) annotation is a very recent direction of corpus development which relies on multiple systems instead of human annotators. <eos> in this paper, we investigate the practical usability of an ssc when a machine learning system is trained on it and tested on an unseen benchmark gsc. <eos> the main focus of this paper is how an ssc can be maximally exploited. <eos> in this process, we inspect several hypotheses which might have influenced the idea of ssc creation. <eos> empirical results suggest that some of the hypotheses ( e.g. <eos> a positive impact of a large ssc despite of having wrong and missing annotations ) are not fully correct. <eos> we show that it is possible to automatically improve the quality and the quantity of the ssc annotations. <eos> we also observe that considering only those sentences of ssc which contain annotations rather than the full ssc results in a performance boost.
subjectivity and sentiment analysis ( ssa ) is an area that has been witnessing a flurry of novel research. <eos> however, only few attempts have been made to build ssa systems for morphologically-rich languages ( mrl ). <eos> in the current study, we report efforts to partially bridge this gap. <eos> we present a newly labeled corpus of modern standard arabic ( msa ) from the news domain manually annotated for subjectivity and domain at the sentence level. <eos> we summarize our linguisticallymotivated annotation guidelines and provide examples from our corpus exemplifying the different phenomena. <eos> throughout the paper, we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics.
we describe our efforts to apply the penn discourse treebank guidelines on a tamil corpus to create an annotated corpus of discourse relations in tamil. <eos> after conducting a preliminary exploratory study on tamil discourse connectives, we show our observations and results of a pilot experiment that we conducted by annotating a small portion of our corpus. <eos> our ultimate goal is to develop a tamil discourse relation bank that will be useful as a resource for further research in tamil discourse. <eos> furthermore, a study of the behavior of discourse connectives in tamil will also help in furthering the cross-linguistic understanding of discourse connectives.
this paper describes an annotated gold standard sample corpus of early modern german containing over 50,000 tokens of text manually annotated with pos tags, lemmas, and normalised spelling variants. <eos> the corpus is the first resource of its kind for this variant of german, and represents an ideal test bed for evaluating and adapting existing nlp tools on historical data. <eos> we describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects.
mae and mai are lightweight annotation and adjudication tools for corpus creation. <eos> dtds are used to define the annotation tags and attributes, including extent tags, link tags, and non-consuming tags. <eos> both programs are written in java and use a stand-alone sqlite database for storage and retrieval of annotation data. <eos> output is in stand-off xml.
in this paper, we first analyze and classify the empty categories in a hindi dependency treebank and then identify various discovery procedures to automatically detect the existence of these categories in a sentence. <eos> for this we make use of lexical knowledge along with the parsed output from a constraint based parser. <eos> through this work we show that it is possible to successfully discover certain types of empty categories while some other types are more difficult to identify. <eos> this work leads to the state-of-the-art system for automatic insertion of empty categories in the hindi sentence.
in this paper, we discuss some of the challenges of adequately applying a specification language to an annotation task, as embodied in a specific guideline. <eos> in particular, we discuss some issues with timeml motivated by error analysis on annotated tlinks in timebank. <eos> we introduce a document level information structure we call a narrative container ( nc ), designed to increase informativeness and accuracy of temporal relation identification. <eos> the narrative container is the default interval containing the events being discussed in the text, when no explicit temporal anchor is given. <eos> by exploiting this notion in the creation of a new temporal annotation over timebank, we were able to reduce inconsistencies and increase informativeness when compared to existing tlinks in timebank.
we describe an experiment on a temporal ordering task in this paper. <eos> we show that by selecting event pairs based on discourse structure and by modifying the pre-existent temporal classification scheme to fit the data better, we significantly improve inter-annotator agreement, as well as broaden the coverage of the task. <eos> we also present analysis of the current temporal classification scheme and propose ways to improve it in future work.
we describe the beginning stages of our work on summarizing chat, which is motivated by our observations concerning the information overload of us navy watchstanders. <eos> we describe the challenges of summarizing chat and focus on two chat-specific types of summarizations we are interested in : thread summaries and temporal summaries. <eos> we then discuss our plans for addressing these challenges and evaluation issues.
we present a novel unsupervised approach to the problem of multi-document summarization of scientific articles, in which the document collection is a list of papers cited together within the same source article, otherwise known as a co-citation. <eos> at the heart of the approach is a topic based clustering of fragments extracted from each co-cited article and relevance ranking using a query generated from the context surrounding the cocited list of papers. <eos> this analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. <eos> we present a system called scisumm that embodies this approach and apply it to the 2008 acl anthology. <eos> we evaluate this summarization system for relevant content selection using gold standard summaries prepared on principle based guidelines. <eos> evaluation with gold standard summaries demonstrates that our system performs better in content selection than an existing summarization system ( mead ). <eos> we present a detailed summary of our findings and discuss possible directions for future research.
this paper addresses the problem of summarizing decisions in spoken meetings : our goal is to produce a concise decision abstract for each meeting decision. <eos> we explore and compare token-level and dialogue act-level automatic summarization methods using both unsupervised and supervised learning frameworks. <eos> in the supervised summarization setting, and given true clusterings of decisionrelated utterances, we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. <eos> in the unsupervised summarization setting, we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques ( 0.22 rouge-f1 using lda-based topic models vs. 0.23 using svms ).
abstractive summarization has been a longstanding and long-term goal in automatic summarization, because systems that can generate abstracts demonstrate a deeper understanding of language and the meaning of documents than systems that merely extract sentences from those documents. <eos> genest ( 2009 ) showed that summaries from the top automatic summarizers are judged as comparable to manual extractive summaries, and both are judged to be far less responsive than manual abstracts, as the state of the art approaches the limits of extractive summarization, it becomes even more pressing to advance abstractive summarization. <eos> however, abstractive summarization has been sidetracked by questions of what qualifies as important information, and how do we find it ? <eos> the guided summarization task introduced at the text analysis conference 2010 attempts to neutralize both of these problems by introducing topic categories and lists of aspects that a responsive summary should address. <eos> this design results in more similar human models, giving the automatic summarizers a more focused target to pursue, and also provides detailed diagnostics of summary content, which can can help build better meaningoriented summarization systems.
we establish a novel task in the spirit of news summarization and topic detection and tracking ( tdt ) : daily determination of the topics newly popular with wikipedia readers. <eos> central to this effort is a new public dataset consisting of the hourly page view statistics of all wikipedia articles over the last three years. <eos> we give baseline results for the tasks of : discovering individual pages of interest, clustering these pages into coherent topics, and extracting the most relevant summarizing sentence for the reader. <eos> when compared to human judgements, our system shows the viability of this task, and opens the door to a range of exciting future work.
information graphics ( bar charts, line graphs, etc. ) <eos> in popular media generally have a discourse goal that contributes to achieving the communicative intent of a multimodal document. <eos> this paper presents our work on abstractive summarization of line graphs. <eos> our methodology involves hypothesizing the intended message of a line graph and using it as the core of a summary of the graphic. <eos> this core is then augmented with salient propositions that elaborate on the intended message.
unsupervised approaches to multi-document summarization consist of two steps : finding a content model of the documents to be summarized, and then generating a summary that best represents the most salient information of the documents. <eos> in this paper, we present a sentence selection objective for extractive summarization in which sentences are penalized for containing content that is specific to the documents they were extracted from. <eos> we modify an existing system, hiersum ( haghighi & vanderwende, 2009 ), to use our objective, which significantly outperforms the original hiersum in pairwise user evaluation. <eos> additionally, our rouge scores advance the current state-of-the-art for both supervised and unsupervised systems with statistical significance.
models of the acquisition of word segmentation are typically evaluated using phonemically transcribed corpora. <eos> accordingly, they implicitly assume that children know how to undo phonetic variation when they learn to extract words from speech. <eos> moreover, whereas models of language acquisition should perform similarly across languages, evaluation is often limited to english samples. <eos> using child-directed corpora of english, french and japanese, we evaluate the performance of state-of-the-art statistical models given inputs where phonetic variation has not been reduced. <eos> to do so, we measure segmentation robustness across different levels of segmental variation, simulating systematic allophonic variation or errors in phoneme recognition. <eos> we show that these models do not resist an increase in such variations and do not generalize to typologically different languages. <eos> from the perspective of early language acquisition, the results strengthen the hypothesis according to which phonological knowledge is acquired in large part before the construction of a lexicon.
the mapping from phonetic categories to acoustic cue values is highly flexible, and adapts rapidly in response to exposure. <eos> there is currently, however, no theoretical framework which captures the range of this adaptation. <eos> we develop a novel approach to modeling phonetic adaptation via a belief-updating model, and demonstrate that this model naturally unifies two adaptation phenomena traditionally considered to be distinct.
learning to group words into phrases without supervision is a hard task for nlp systems, but infants routinely accomplish it. <eos> we hypothesize that infants use acoustic cues to prosody, which nlp systems typically ignore. <eos> to evaluate the utility of prosodic information for phrase discovery, we present an hmmbased unsupervised chunker that learns from only transcribed words and raw acoustic correlates to prosody. <eos> unlike previous work on unsupervised parsing and chunking, we use neither gold standard part-of-speech tags nor punctuation in the input. <eos> evaluated on the switchboard corpus, our model outperforms several baselines that exploit either lexical or prosodic information alone, and, despite producing a flat structure, performs competitively with a state-of-the-art unsupervised lexicalized parser, with a substantial advantage in precision. <eos> our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for languagelearning infants.
we propose a statistical test for measuring grammatical productivity. <eos> we show that very young children ? s knowledge is consistent with a systematic grammar that independently combines linguistic units. <eos> to a testable extent, the usage-based approach to language and language learning, which emphasizes the role of lexically specific memorization, is inconsistent with the child language data. <eos> we also discuss the connection of this research with developments in computational and theoretical linguistics.
this paper defines a normal form for mcfgs that includes strongly equivalent representations of many mg variants, and presents an incremental priority-queue-based td recognizer for these mcfgs. <eos> after introducing mgs with overt phrasal movement, head movement and simple adjunction are added without change in the recognizer. <eos> the mg representation can be used directly, so that even rather sophisticated analyses of properly non-cf languages can be defined very succinctly. <eos> as with the similar stack-based cfmethods, finite memory suffices for the recognition of infinite languages, and a fully connected left context for probabilistic analysis is available at every point.
greater learnability has been offered as an explanation as to why certain properties appear in human languages more frequently than others. <eos> languages with greater learnability are more likely to be accurately transmitted from one generation of learners to the next. <eos> we explore whether such a learnability bias is sufficient to result in a property becoming prevalent across languages by formalizing language transmission using a linear model. <eos> we then examine the outcome of repeated transmission of languages using a mathematical analysis, a computer simulation, and an experiment with human participants, and show several ways in which greater learnability may not result in a property becoming prevalent. <eos> both the ways in which transmission failures occur and the relative number of languages with and without a property can affect whether the relationship between learnability and prevalence holds. <eos> our results show that simply finding a learnability bias is not sufficient to explain why a particular property is a linguistic universal, or even frequent among human languages.
the aim of this paper is to present a computational model of the dynamic composition and update of verb argument expectations using distributional memory, a state-of-the-art framework for distributional semantics. <eos> the experimental results conducted on psycholinguistic data sets show that the model is able to successfully predict the changes on the patient argument thematic fit produced by different types of verb agents.
driks university of groningen p.hendriks @ rug.nl abstract this paper presents a study of the effect of working memory load on the interpretation of pronouns in different discourse contexts : sto-ries with and without a topic shift. <eos> we present a computational model ( in act-r, anderson, 2007 ) to explain how referring subjects are used and interpreted. <eos> we furthermore report on an experiment that tests predictions that follow from simulations. <eos> the results of the experiment support the model predictions that wm load only affects the interpretation of pronouns in stories with a topic shift, but not in stories without a topic shift.
conversational participants tend to immediately and unconsciously adapt to each other ? s language styles : a speaker will even adjust the number of articles and other function words in their next utterance in response to the number in their partner ? s immediately preceding utterance. <eos> this striking level of coordination is thought to have arisen as a way to achieve social goals, such as gaining approval or emphasizing difference in status. <eos> but has the adaptation mechanism become so deeply embedded in the language-generation process as to become a reflex ? <eos> we argue that fictional dialogs offer a way to study this question, since authors create the conversations but don ? t receive the social benefits ( rather, the imagined characters do ). <eos> indeed, we find significant coordination across many families of function words in our large movie-script corpus. <eos> we also report suggestive preliminary findings on the effects of gender and other features ; e.g., surprisingly, for articles, on average, characters adapt more to females than to males.
atypical or idiosyncratic language is a characteristic of autism spectrum disorder ( asd ). <eos> in this paper, we discuss previous work identifying language errors associated with atypical language in asd and describe a procedure for reproducing those results. <eos> we describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument ( the ados ) for children with autism, children with developmental language disorder, and typically developing children. <eos> we then present methods for automatically extracting lexical and syntactic features from transcripts of children ? s speech to 1 ) identify certain syntactic and semantic errors that have previously been found to distinguish asd language from that of children with typical development ; and 2 ) perform diagnostic classification. <eos> our classifiers achieve results well above chance, demonstrating the potential for using nlp techniques to enhance neurodevelopmental diagnosis and atypical language analysis. <eos> we expect further improvement with additional data, features, and classification techniques.
since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complimented with the use of appropriate colours in information visualization and product marketing. <eos> yet, there is no comprehensive resource that captures concept ? colour associations. <eos> we present a method to create a large word ? colour association lexicon by crowdsourcing. <eos> we focus especially on abstract concepts and emotions to show that even though they can not be physically visualized, they too tend to have strong colour associations. <eos> finally, we show how word ? colour associations manifest themselves in language, and quantify usefulness of co-occurrence and polarity cues in automatically detecting colour associations.1
survival analysis is often used in medical and biological studies to examine the time until some specified event occurs, such as the time until death of terminally ill patients. <eos> in this paper, however, we apply survival analysis to eye movement data in order to model the survival function of fixation time distributions in reading. <eos> semiparametric regression modeling and novel evaluation methods for probabilistic models of eye movements are presented. <eos> survival models adjusting for the influence of linguistic and cognitive effects are shown to reduce prediction error within a critical time period, roughly between 150 and 250 ms following fixation onset.
the recent proliferation of political and social forums has given rise to a wealth of freely accessible naturalistic arguments. <eos> people can ? talk ? <eos> to anyone they want, at any time, in any location, about any topic. <eos> here we use a mechanical turk annotated corpus of forum discussions as a gold standard for the recognition of disagreement in online ideological forums. <eos> we analyze the utility of meta-post features, contextual features, dependency features and word-based features for signaling the disagreement relation. <eos> we show that using contextual and dialogic features we can achieve accuracies up to 68 % as compared to a unigram baseline of 63 %.
political blogs as a form of social media allow for an uniquely interactive form of political discourse. <eos> this is especially evident in focused blogs with a strong ideological identity. <eos> we investigate techniques to identify topics within the context of the community, which when discussed in a blog post evoke a discernible positive or negative collective opinion from readers who respond to posts in comments. <eos> this is done by using computational methods to assign sentiment polarity to blog comments and learning community specific models that summarize issues tackled by blogs and predict the polarity based on the topics discussed in a blog post.
microtexts, like sms messages, twitter posts, and facebook status updates, are a popular medium for real-time communication. <eos> in this paper, we investigate the writing conventions that different groups of users use to express themselves in microtexts. <eos> our empirical study investigates properties of lexical transformations as observed within twitter microtexts. <eos> the study reveals that different populations of users exhibit different amounts of shortened english terms and different shortening styles. <eos> the results reveal valuable insights into how human language technologies can be effectively applied to microtexts.
we examine sentiment analysis on twitter data. <eos> the contributions of this paper are : ( 1 ) we introduce pos-specific prior polarity features. <eos> ( 2 ) we explore the use of a tree kernel to obviate the need for tedious feature engineering. <eos> the new features ( in conjunction with previously proposed features ) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.
this paper explores the problem of detecting sentence-level forum authority claims in online discussions. <eos> using a maximum entropy model, we explore a variety of strategies for extracting lexical features in a sparse training scenario, comparing knowledge- and datadriven methods ( and combinations ). <eos> the augmentation of lexical features with parse context is also investigated. <eos> we find that certain markup features perform remarkably well alone, but are outperformed by data-driven selection of lexical features augmented with parse context.
we present the aawd corpus, a collection of 365 discussions drawn from wikipedia talk pages and annotated with labels capturing two kinds of social acts : alignment moves and authority claims. <eos> we describe these social acts and our annotation process, and analyze the resulting data set for interactions between participant status and social acts and between the social acts themselves.
hashtags are used in twitter to classify messages, propagate ideas and also to promote specific topics and people. <eos> in this paper, we present a linguistic-inspired study of how these tags are created, used and disseminated by the members of information networks. <eos> we study the propagation of hashtags in twitter grounded on models for the analysis of the spread of linguistic innovations in speech communities, that is, in groups of people whose members linguistically influence each other. <eos> differently from traditional linguistic studies, though, we consider the evolution of terms in a live and rapidly evolving stream of content, which can be analyzed in its entirety. <eos> in our experimental results, using a large collection crawled from twitter, we were able to identify some interesting aspects ? <eos> similar to those found in studies of ( offline ) speech ? <eos> that led us to believe that hashtags may effectively serve as models for characterizing the propagation of linguistic forms, including : ( 1 ) the existence of a ? preferential attachment process ?, that makes the few most common terms ever more popular, and ( 2 ) the relationship between the length of a tag and its frequency of use. <eos> the understanding of formation patterns of successful hashtags in twitter can be useful to increase the effectiveness of real-time streaming search algorithms.
user-contributed content is creating a surge on the internet. <eos> a list of ? buzzing topics ? <eos> can effectively monitor the surge and lead people to their topics of interest. <eos> yet a topic phrase alone, such as ? sxsw ?, can rarely present the information clearly. <eos> in this paper, we propose to explore a variety of text sources for summarizing the twitter topics, including the tweets, normalized tweets via a dedicated tweet normalization system, web contents linked from the tweets, as well as integration of different text sources. <eos> we employ the concept-based optimization framework for topic summarization, and conduct both automatic and human evaluation regarding the summary quality. <eos> performance differences are observed for different input sources and types of topics. <eos> we also provide a comprehensive analysis regarding the task challenges.
in this paper we investigate the connection between language and community membership of long time community participants through computational modeling techniques. <eos> we report on findings from an analysis of language usage within a popular online discussion forum with participation of thousands of users spanning multiple years. <eos> we find community norms of long time participants that are characterized by forum specific jargon and a style that is highly informal and shows familiarity with specific other participants and high emotional involvement in the discussion. <eos> we also find quantitative evidence of persistent shifts in language usage towards these norms across users over the course of the first year of community participation. <eos> our observed patterns suggests language stabilization after 8 or 9 months of participation.
email is an important way of communication in our daily life and it has become the subject of various nlp and social studies. <eos> in this paper, we focus on email formality and explore the factors that could affect the sender ? s choice of formality. <eos> as a case study, we use the enron email corpus to test how formality is affected by social distance, relative power, and the weight of imposition, as defined in brown and levinson ? s model of politeness ( 1987 ). <eos> our experiments show that their model largely holds in the enron corpus. <eos> we believe that the methodology proposed in the paper can be applied to other social media domains and be used to test other linguistic or social theories.
taking as a starting-point the development on cooccurrence techniques for several languages, we focus on the aspects that should be considered in a nv extraction task for basque. <eos> in basque, nv expressions are considered those combinations in which a noun, inflected or not, is co-occurring with a verb, as erabakia hartu ( ? to make a decision ? <eos> ), kontuan hartu ( ? to take into account ? ) <eos> and buruz jakin ( ? to know by heart ? ). <eos> a basic extraction system has been developed and evaluated against two references : a ) a reference which includes nv entries from several lexicographic works ; and b ) a manual evaluation by three experts of a random sample from the n-best lists.
one of the key issues in both natural language understanding and generation is the appropriate processing of multiword expressions ( mwes ). <eos> mwe can be defined as a semantic issue of a phrase where the meaning of the phrase may not be obtained from its constituents in a straightforward manner. <eos> this paper presents an approach of identifying bigram noun-noun mwes from a medium-size bengali corpus by clustering the semantically related nouns and incorporating a vector space model for similarity measurement. <eos> additional inclusion of the english wordnet : :similarity module also improves the results considerably. <eos> the present approach also contributes to locate clusters of the synonymous noun words present in a document. <eos> experimental results draw a satisfactory conclusion after analyzing the precision, recall and f-score values.
in this paper we present preliminary experiments that aim to reduce lexical data sparsity in statistical parsing by exploiting information about named entities. <eos> words in the wsj corpus are mapped to named entity clusters and a latent variable constituency parser is trained and tested on the transformed corpus. <eos> we explore two different methods for mapping words to entities, and look at the effect of mapping various subsets of named entity types. <eos> thus far, results show no improvement in parsing accuracy over the best baseline score ; we identify possible problems and outline suggestions for future directions.
multi-word expressions ( mwes ) account for a large portion of the language used in dayto-day interactions. <eos> a formal system that is flexible enough to model these large and often syntactically-rich non-compositional chunks as single units in naturally occurring text could considerably simplify large-scale semantic annotation projects, in which it would be undesirable to have to develop internal compositional analyses of common technical expressions that have specific idiosyncratic meanings. <eos> this paper will first define a notion of functorargument decomposition on phrase structure trees analogous to graph coloring, in which the tree is cast as a graph, and the elementary structures of a grammar formalism are colors. <eos> the paper then presents a formal argument that tree-rewriting systems, a class of grammar formalism that includes tree adjoining grammars, are able to produce a proper superset of the functor-argument decompositions that string-rewriting systems can produce.
in this paper, we investigate a supervised machine learning framework for automatically learning of english light verb constructions ( lvcs ). <eos> our system achieves an 86.3 % accuracy with a baseline ( chance ) performance of 52.2 % when trained with groups of either contextual or statistical features. <eos> in addition, we present an in-depth analysis of these contextual and statistical features and show that the system trained by these two types of cosmetically different features reaches similar performance empirically. <eos> however, in the situation where the surface structures of candidate lvcs are identical, the system trained with contextual features which contain information on surrounding words performs 16.7 % better. <eos> in this study, we also construct a balanced benchmark dataset with 2,162 sentences from bnc for english lvcs. <eos> and this data set is publicly available and is also a useful computational resource for research on mwes in general.
this paper describes a new part-of-speech tagger including multiword unit ( mwu ) identification. <eos> it is based on a conditional random field model integrating language-independent features, as well as features computed from external lexical resources. <eos> it was implemented in a finite-state framework composed of a preliminary finite-state lexical analysis and a crf decoding using weighted finitestate transducer composition. <eos> we showed that our tagger reaches state-of-the-art results for french in the standard evaluation conditions ( i.e. <eos> each multiword unit is already merged in a single token ). <eos> the evaluation of the tagger integrating mwu recognition clearly shows the interest of incorporating features based on mwu resources.
most work on evaluation of named-entity recognition has been done in the context of competitions, as a part of information extraction. <eos> there has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes : person, organization, and location. <eos> we report on a comparison of three state-ofthe-art named entity taggers : stanford, lbj, and identifinder. <eos> the taggers were compared with respect to : 1 ) agreement rate on the classification of entities by class, and 2 ) percentage of ambiguous entities ( belonging to more than one class ) co-occurring in a document. <eos> we found that the agreement between the taggers ranged from 34 % to 58 %, depending on the class and that more than 40 % of the globally ambiguous entities co-occur within the same document. <eos> we also propose a unit test based on the problems we encountered.
semantic role labeling annotation task depends on the correct identification of predicates, before identifying arguments and assigning them role labels. <eos> however, most predicates are not constituted only by a verb : they constitute complex predicates ( cps ) not yet available in a computational lexicon. <eos> in order to create a dictionary of cps, this study employs a corpus-based methodology. <eos> searches are guided by pos tags instead of a limited list of verbs or nouns, in contrast to similar studies. <eos> results include ( but are not limited to ) light and support verb constructions. <eos> these cps are classified into idiomatic and less idiomatic. <eos> this paper presents an in-depth analysis of this phenomenon, as well as an original resource containing a set of 773 annotated expressions. <eos> both constitute an original and rich contribution for nlp tools in brazilian portuguese that perform tasks involving semantics.
the identification and extraction of multiword expressions ( mwes ) currently deliver satisfactory results. <eos> however, the integration of these results into a wider application remains an issue. <eos> this is mainly due to the fact that the association measures ( ams ) used to detect mwes require a critical amount of data and that the mwe dictionaries can not account for all the lexical and syntactic variations inherent in mwes. <eos> in this study, we use an alternative technique to overcome these limitations. <eos> it consists in defining an n-gram frequency database that can be used to compute ams on-thefly, allowing the extraction procedure to efficiently process all the mwes in a text, even if they have not been previously observed.
this paper presents a procedure for extracting transfer rules for multiword expressions from parallel corpora for use in a rule based japanese-english mt system. <eos> we show that adding the multi-word rules improves translation quality and sketch ideas for learning more such rules.
the extensive use of multiword expressions ( mwe ) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. <eos> a mwe typically expresses concepts and ideas that usually can not be expressed by a single word. <eos> intuitively, with the appropriate treatment of mwes, the results of an information retrieval ( ir ) system could be improved. <eos> the aim of this paper is to apply techniques for the automatic extraction of mwes from corpora to index them as a single unit. <eos> experimental results show improvements on the retrieval of relevant documents when identifying mwes and treating them as a single indexing unit.
multi-word expressions ( mwes ) play an important role in all tasks that involve natural language processing. <eos> mwes in hindi are quite varied and many of these are of the types that are not encountered in english. <eos> in this paper, we examine different types of mwes encountered in hindi. <eos> many of these have not received adequate attention of investigators. <eos> for example, ? vaalaa ? <eos> constructs, doublets ( word-pairs ), replication, and a variety of verb group forms have not been explored as mwes. <eos> we examine these mwes from machine translation viewpoint. <eos> many of these are frequently used in day-to-day conversations and informal communication but are not that frequently encountered in a formal textual corpus. <eos> most of the conventional statistical methods for mwe identification use corpus with limited linguistic cues. <eos> these are found to be inadequate for detecting all types of mwes that exist in real life. <eos> in this paper, we present a stepwise methodology for mining hindi mwes using linguistic knowledge. <eos> interpretation and representation for some of these from machine translation perspective have also been explored.
in this paper, we describe our methods to detect noun compounds and light verb constructions in running texts. <eos> for noun compounds, dictionary-based methods and postagging seem to contribute most to the performance of the system whereas for light verb constructions, the combination of postagging, syntactic information and restrictions on the nominal and verbal component yield the best result. <eos> however, focusing on deverbal nouns proves to be beneficial for both types of mwes. <eos> the effect of syntax is negligible on noun compound detection whereas it is unambiguously helpful for identifying light verb constructions.
we introduce fipscoview, an on-line interface for dictionary-like visualisation of collocations detected from parallel corpora using a syntactically-informed extraction method. <eos>
this demo introduces a suite of web-based english lexical knowledge resources, called stringnet and stringnet navigator ( http : //nav.stringnet.org ), designed to provide access to the immense territory of multiword expressions that falls between what the lexical entries encode in lexicons on the one hand and what productive grammar rules cover on the other. <eos> stringnet ? s content consists of 1.6 billion hybrid n-grams, strings in which word forms and parts of speech grams can cooccur. <eos> subordinate and super-ordinate relations among hybrid n-grams are indexed, making stringnet a navigable web rather than a list. <eos> applications include error detection and correction tools and web browser-based tools that detect patterns in the webpages that a user browses.
the ngram statistics package ( text : :nsp ) is freely available open-source software that identifies ngrams, collocations and word associations in text. <eos> it is implemented in perl and takes advantage of regular expressions to provide very flexible tokenization and to allow for the identification of non-adjacent ngrams. <eos> it includes a wide range of measures of association that can be used to identify collocations.
we introduce several ideas that improve the performance of supervised information extraction systems with a pipeline architecture, when they are customized for new domains. <eos> we show that : ( a ) a combination of a sequence tagger with a rule-based approach for entity mention extraction yields better performance for both entity and relation mention extraction ; ( b ) improving the identification of syntactic heads of entity mentions helps relation extraction ; and ( c ) a deterministic inference engine captures some of the joint domain structure, even when introduced as a postprocessing step to a pipeline system. <eos> all in all, our contributions yield a 20 % relative increase in f1 score in a domain significantly different from the domains used during the development of our information extraction system.
many recent studies have been dedicated to the extraction of semantic connections between words. <eos> using such information at semantic level is likely to improve the performance of natural language processing ( nlp ) systems, such as text categorization, question answering, information extraction, etc. <eos> the scarcity of such resources in turkish, obstructs new improvements. <eos> there are many examples of semantic networks for english and other widely-used languages to lead the way for studies in turkish. <eos> in this study, developing a semantic network for turkish is aimed by using structural and string patterns in a dictionary. <eos> the results are promising, so that approximately two relations can be extracted from 3 definitions. <eos> the overall accuracy is 86 % if we consider the correct sense assignment, 94 % without considering word sense disambiguation.
in this paper, we have identified event and sentiment expressions at word level from the sentences of tempeval-2010 corpus and evaluated their association in terms of lexical equivalence and co-reference. <eos> a hybrid approach that consists of conditional random field ( crf ) based machine learning framework in conjunction with several rule based strategies has been adopted for event identification within the timeml framework. <eos> the strategies are based on semantic role labeling, wordnet relations and some handcrafted rules. <eos> the sentiment expressions are identified simply based on the cues that are available in the sentiment lexicons such as subjectivity wordlist, sentiwordnet and wordnet affect. <eos> the identification of lexical equivalence between event and sentiment expressions based on the part-of-speech ( pos ) categories is straightforward. <eos> the emotional verbs from verbnet have also been employed to improve the coverage of lexical equivalence. <eos> on the other hand, the association of sentiment and event has been analyzed using the notion of co-reference. <eos> the parsed dependency relations along with basic rhetoric knowledge help to identify the co-reference between event and sentiment expressions. <eos> manual evaluation on the 171 sentences of tempeval-2010 dataset yields the precision, recall and f-score values of 61.25 %, 70.29 % and 65.23 % respectively.
this paper introduces vignette semantics, a lexical semantic theory based on frame semantics that represents conceptual and graphical relations. <eos> we also describe a lexical resource that implements this theory, vignet, and its application in text-to-scene generation.
this paper suggests two ways of improving semantic role labeling ( srl ). <eos> first, we introduce a novel transition-based srl algorithm that gives a quite different approach to srl. <eos> our algorithm is inspired by shift-reduce parsing and brings the advantages of the transitionbased approach to srl. <eos> second, we present a self-learning clustering technique that effectively improves labeling accuracy in the test domain. <eos> for better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. <eos> all approaches are evaluated on the conll ? 09 english data. <eos> the new algorithm shows comparable results to another state-of-the-art system. <eos> the clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.
automatically-derived grammars, such as the split-and-merge model, have proven helpful in parsing ( petrov et al, 2006 ). <eos> as such grammars are refined, latent information is recovered which may be usable for linguistic tasks besides parsing. <eos> in this paper, we present and examine a new method of semantic relation classification : using automaticallyderived grammar rule clusters as a robust knowledge source for semantic relation classification. <eos> we examine performance of this feature group on the semeval 2010 relation classification corpus, and find that it improves performance over both more coarse-grained and more fine-grained syntactic and collocational features in semantic relation classification.
in this paper, we address the issue of automatically identifying null instantiated arguments in text. <eos> we refer to fillmore ? s theory of pragmatically controlled zero anaphora ( fillmore, 1986 ), which accounts for the phenomenon of omissible arguments using a lexically-based approach, and we propose a strategy for identifying implicit arguments in a text and finding their antecedents, given the overtly expressed semantic roles in the form of frame elements. <eos> to this purpose, we primarily rely on linguistic knowledge enriched with role frequency information collected from a training corpus. <eos> we evaluate our approach using the test set developed for the semeval task 10 and we highlight some issues of our approach. <eos> besides, we also point out some open problems related to the task definition and to the general phenomenon of null instantiated arguments, which needs to be better investigated and described in order to be captured from a computational point of view.
many prior studies have investigated the recovery of semantic arguments for nominal predicates. <eos> the models in many of these studies have assumed that arguments are independent of each other. <eos> this assumption simplifies the computational modeling of semantic arguments, but it ignores the joint nature of natural language. <eos> this paper presents a preliminary investigation into the joint modeling of implicit arguments for nominal predicates. <eos> the joint model uses propositional knowledge extracted from millions of internet webpages to help guide prediction.
we take the first steps towards augmenting a lexical resource, verbnet, with probabilistic information about coercive constructions. <eos> we focus on causedmotion as an example construction occurring with verbs for which it is a typical usage or for which it must be interpreted as extending the event semantics through coercion, which occurs productively and adds substantially to the relational semantics of a verb. <eos> however, through annotation we find that verbnet fails to accurately capture all usages of the construction. <eos> we use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against verbnet. <eos> we discuss how these methods will form the basis for enhancements for verbnet supporting more accurate analysis of the relational semantics of a verb across productive usages.
we present a model for the inclusion of semantic role annotations in the framework of confidence estimation for machine translation. <eos> the model has several interesting properties, most notably : 1 ) it only requires a linguistic processor on the ( generally well-formed ) source side of the translation ; 2 ) it does not directly rely on properties of the translation model ( hence, it can be applied beyond phrase-based systems ). <eos> these features make it potentially appealing for system ranking, translation re-ranking and user feedback evaluation. <eos> preliminary experiments in pairwise hypothesis ranking on five confidence estimation benchmarks show that the model has the potential to capture salient aspects of translation quality.
we argue that failing to capture the degree of contribution of each semantic frame in a sentence explains puzzling results in recent work on the meant family of semantic mt evaluation metrics, which have disturbingly indicated that dissociating semantic roles and fillers from their predicates actually improves correlation with human adequacy judgments even though, intuitively, properly segregating event frames should more accurately reflect the preservation of meaning. <eos> our analysis finds that both properly structured and flattened representations fail to adequately account for the contribution of each semantic frame to the overall sentence. <eos> we then show that the correlation of hmeant, the human variant of meant, can be greatly improved by introducing a simple length-based weighting scheme that approximates the degree of contribution of each semantic frame to the overall sentence. <eos> the new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame ? s contribution gives hmeant higher correlations than the previously bestperforming flattened model, as well as hter.
to facilitate the application of semantics in statistical machine translation, we propose a broad-coverage predicate-argument structure mapping technique using automated resources. <eos> our approach utilizes automatic syntactic and semantic parsers to generate chinese-english predicate-argument structures. <eos> the system produced a many-to-many argument mapping for all propbank argument types by computing argument similarity based on automatic word alignment, achieving 80.5 % f-score on numbered argument mapping and 64.6 % f-score on all arguments. <eos> by measuring predicate-argument structure similarity based on the argument mapping, and formulating the predicate-argument structure mapping problem as a linear-assignment problem, the system achieved 84.9 % f-score using automatic srl, only 3.7 % f-score lower than using gold standard srl. <eos> the mapping output covered 49.6 % of the annotated chinese predicates ( which contains predicateadjectives that often have no parallel annotations in english ) and 80.7 % of annotated english predicates, suggesting its potential as a valuable resource for improving word alignment and reranking mt output.
to increase the model coverage, sourcelanguage paraphrases have been utilized to boost smt system performance. <eos> previous work showed that word lattices constructed from paraphrases are able to reduce out-ofvocabulary words and to express inputs in different ways for better translation quality. <eos> however, such a word-lattice-based method suffers from two problems : 1 ) path duplications in word lattices decrease the capacities for potential paraphrases ; 2 ) lattice decoding in smt dramatically increases the search space and results in poor time efficiency. <eos> therefore, in this paper, we adopt word confusion networks as the input structure to carry source-language paraphrase information. <eos> similar to previous work, we use word lattices to build word confusion networks for merging of duplicated paths and faster decoding. <eos> experiments are carried out on small-, medium- and large-scale english ? <eos> chinese translation tasks, and we show that compared with the word-lattice-based method, the decoding time on three tasks is reduced significantly ( up to 79 % ) while comparable translation quality is obtained on the largescale task.
translation requires non-isomorphic transformation from the source to the target. <eos> however, non-isomorphism can be reduced by learning multi-word units ( mwus ). <eos> we present a novel way of representating sentence structure based on mwus, which are not necessarily continuous word sequences. <eos> our proposed method builds a simpler structure of mwus than words using words as vertices of a dependency structure. <eos> unlike previous studies, we collect many alternative structures in a packed forest. <eos> as an application of our proposed method, we extract translation rules in form of a source mwu-forest to the target string, and verify the rule coverage empirically. <eos> as a consequence, we improve the rule coverage compare to a previous work, while retaining the linear asymptotic complexity.
mistranslation of an ambiguous word can have a large impact on the understandability of a given sentence. <eos> in this article, we describe a thorough evaluation of the translation quality of ambiguous nouns in three different setups. <eos> we compared two statistical machine translation systems and one dedicated word sense disambiguation ( wsd ) system. <eos> our wsd system incorporates multilingual information and is independent from external lexical resources. <eos> word senses are derived automatically from word alignments on a parallel corpus. <eos> we show that the two wsd classifiers that were built for these experiments ( english ? <eos> french and english ? dutch ) outperform the smt system that was trained on the same corpus. <eos> this opens perspectives for the integration of our multilingual wsd module in a statistical machine translation framework, in order to improve the automated translation of ambiguous words, and by consequence make the translation output more understandable.
in this paper we propose several novel approaches to improve phrase reordering for statistical machine translation in the framework of maximum-entropy-based modeling. <eos> a smoothed prior probability is introduced to take into account the distortion effect in the priors. <eos> in addition to that we propose multiple novel distortion features based on syntactic parsing. <eos> a new metric is also introduced to measure the effect of distortion in the translation hypotheses. <eos> we show that both smoothed priors and syntax-based features help to significantly improve the reordering and hence the translation performance on a large-scale chinese-to-english machine translation task.
we show that reifying the rules from hyperedge weights to first-class graph nodes automatically gives us rule expectations in any kind of grammar expressible as a deductive system, without any explicit algorithm for calculating rule expectations ( such as the insideoutside algorithm ). <eos> this gives us expectation maximization training for any grammar class with a parsing algorithm that can be stated as a deductive system, for free. <eos> having such a framework in place accelerates turnover time for experimenting with new grammar classes and parsing algorithms ? to implement a grammar learner, only the parse forest construction has to be implemented.
we present a translation model based on dependency trees. <eos> the model adopts a treeto-string approach and extends phrasebased translation ( pbt ) by using the dependency tree of the source sentence for selecting translation options and for reordering them. <eos> decoding is done by translating each node in the tree and combining its translations with those of its head in alternative orders with respect to its siblings. <eos> reordering of the siblings exploits a heuristic based on the syntactic information from the parse tree which is learned from the corpus. <eos> the decoder uses the same phrase tables produced by a pbt system for looking up translations of single words or of partial sub-trees. <eos> a mathematical model is presented and experimental results are discussed.
we use hand-coded rules and graph-aligned logical dependencies to reorder english text towards chinese word order. <eos> we obtain a 1.5 % higher f-score for giza++ compared to running with unprocessed text. <eos> we describe this research and its implications for smt.
we consider scfg-basedmt systems that get syntactic category labels from parsing both the source and target sides of parallel training data. <eos> the resulting joint nonterminals often lead to needlessly large label sets that are not optimized for an mt scenario. <eos> this paper presents a method of iteratively coarsening a label set for a particular language pair and training corpus. <eos> we apply this label collapsing on chinese ? english and french ? english grammars, obtaining test-set improvements of up to 2.8 bleu, 5.2 ter, and 0.9 meteor on chinese ? english translation. <eos> an analysis of label collapsing ? s effect on the grammar and the decoding process is also given.
in this paper we present a novel approach of utilizing semantic role labeling ( srl ) information to improve hierarchical phrasebased machine translation. <eos> we propose an algorithm to extract srl-aware synchronous context-free grammar ( scfg ) rules. <eos> conventional hiero-style scfg rules will also be extracted in the same framework. <eos> special conversion rules are applied to ensure that when srl-aware scfg rules are used in derivation, the decoder only generates hypotheses with complete semantic structures. <eos> we perform machine translation experiments using 9 different chinese-english test-sets. <eos> our approach achieved an average bleu score improvement of 0.49 as well as 1.21 point reduction in ter.
ontologies and taxonomies are widely used to organize concepts providing the basis for activities such as indexing, and as background knowledge for nlp tasks. <eos> as such, translation of these resources would prove useful to adapt these systems to new languages. <eos> however, we show that the nature of these resources is significantly different from the ? free-text ? <eos> paradigm used to train most statistical machine translation systems. <eos> in particular, we see significant differences in the linguistic nature of these resources and such resources have rich additional semantics. <eos> we demonstrate that as a result of these linguistic differences, standard smt methods, in particular evaluation metrics, can produce poor performance. <eos> we then look to the task of leveraging these semantics for translation, which we approach in three ways : by adapting the translation system to the domain of the resource ; by examining if semantics can help to predict the syntactic structure used in translation ; and by evaluating if we can use existing translated taxonomies to disambiguate translations. <eos> we present some early results from these experiments, which shed light on the degree of success we may have with each approach.
a semantic feature for statistical machine translation, based on latent semantic indexing, is proposed and evaluated. <eos> the objective of the proposed feature is to account for the degree of similarity between a given input sentence and each individual sentence in the training dataset. <eos> this similarity is computed in a reduced vectorspace constructed by means of the latent semantic indexing decomposition. <eos> the computed similarity values are used as an additional feature in the log-linear model combination approach to statistical machine translation. <eos> in our implementation, the proposed feature is dynamically adjusted for each translation unit in the translation table according to the current input sentence to be translated. <eos> this model aims at favoring those translation units that were extracted from training sentences that are semantically related to the current input sentence being translated. <eos> experimental results on a spanish-to-english translation task on the bible corpus demonstrate a significant improvement on translation quality with respect to a baseline system.
we present a rule extractor for scfg-based mt that generalizes many of the contraints present in existing scfg extraction algorithms. <eos> our method ? s increased rule coverage comes from allowing multiple alignments, virtual nodes, and multiple tree decompositions in the extraction process. <eos> at decoding time, we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set, while our experiments with hierarchical grammar filtering indicate that more intelligent filtering schemes will also provide a key to future gains.
this paper proposes a novel application of a supervised topic model to do entity relation detection ( erd ). <eos> we adapt maximum entropy discriminant latent dirichlet allocation ( medlda ) with mixed membership for relation detection. <eos> the erd task is reformulated to fit into the topic modeling framework. <eos> our approach combines the benefits of both, maximum-likelihood estimation ( mle ) and max-margin estimation ( mme ), and the mixed membership formulation enables the system to incorporate heterogeneous features. <eos> we incorporate different features into the system and perform experiments on the ace 2005 corpus. <eos> our approach achieves better overall performance for precision, recall and fmeasure metrics as compared to svm-based and llda-based models.
we propose the use of a nonparametric bayesian model, the hierarchical dirichlet process ( hdp ), for the task of word sense induction. <eos> results are shown through comparison against latent dirichlet allocation ( lda ), a parametric bayesian model employed by brody and lapata ( 2009 ) for this task. <eos> we find that the two models achieve similar levels of induction quality, while the hdp confers the advantage of automatically inducing a variable number of senses per word, as compared to manually fixing the number of senses a priori, as in lda. <eos> this flexibility allows for the model to adapt to terms with greater or lesser polysemy, when evidenced by corpus distributional statistics. <eos> when trained on out-of-domain data, experimental results confirm the model ? s ability to make use of a restricted set of topically coherent induced senses, when then applied in a restricted domain.
edges of graphs that model real data can be seen as judgements whether pairs of objects are in relation with each other or not. <eos> so, one can evaluate the similarity of two graphs with a measure of agreement between judges classifying pairs of vertices into two categories ( connected or not connected ). <eos> when applied to synonymy networks, such measures demonstrate a surprisingly low agreement between various resources of the same language. <eos> this seems to suggest that the judgements on synonymy of lexemes of the same lexicon radically differ from one dictionary editor to another. <eos> in fact, even a strong disagreement between edges does not necessarily mean that graphs model a completely different reality : although their edges seem to disagree, synonymy resources may, at a coarser grain level, outline similar semantics. <eos> to investigate this hypothesis, we relied on shared common properties of real world data networks to look at the graphs at a more global level by using random walks. <eos> they enabled us to reveal a much better agreement between dense zones than between edges of synonymy graphs. <eos> these results suggest that although synonymy resources may disagree at the level of judgements on single pairs of words, they may nevertheless convey an essentially similar semantic information.
word sense induction ( wsi ) is an unsupervised approach for learning the multiple senses of a word. <eos> graph-based approaches to wsi frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses. <eos> we reinterpret graph-based wsi as community detection, a well studied problem in network science. <eos> the relations in the co-occurrence graph give rise to word communities, which distinguish senses. <eos> our results show competitive performance on the semeval-2010 wsi task.
a graph-based distance between wikipedia articles is defined using a random walk model, which estimates visiting probability ( vp ) between articles using two types of links : hyperlinks and lexical similarity relations. <eos> the vp to and from a set of articles is then computed, and approximations are proposed to make tractable the computation of semantic relatedness between every two texts in a large data set. <eos> the model is applied to document clustering on the 20 newsgroups data set. <eos> precision and recall are improved in comparison with previous textual distance algorithms.
in this paper, we present grawltcq, a new bootstrapping algorithm for building specialized terminology, corpora and queries, based on a graph model. <eos> we model links between documents, terms and queries, and use a random walk with restart algorithm to compute relevance propagation. <eos> we have evaluated grawltcq on an afp english corpus of 57,441 news over 10 categories. <eos> for corpora building, grawltcq outperforms the bootcat tool, which is vastly used in the domain. <eos> for 1,000 documents retrieved, we improve mean precision by 25 %. <eos> grawltcq has also shown to be faster and more robust than bootcat over iterations.
a key problem in document classification and clustering is learning the similarity between documents. <eos> traditional approaches include estimating similarity between feature vectors of documents where the vectors are computed using tf-idf in the bag-of-words model. <eos> however, these approaches do not work well when either similar documents do not use the same vocabulary or the feature vectors are not estimated correctly. <eos> in this paper, we represent documents and keywords using multiple layers of connected graphs. <eos> we pose the problem of simultaneously learning similarity between documents and keyword weights as an edge-weight regularization problem over the different layers of graphs. <eos> unlike most feature weight learning algorithms, we propose an unsupervised algorithm in the proposed framework to simultaneously optimize similarity and the keyword weights. <eos> we extrinsically evaluate the performance of the proposed similarity measure on two different tasks, clustering and classification. <eos> the proposed similarity measure outperforms the similarity measure proposed by ( muthukrishnan et al, 2010 ), a state-of-theart classification algorithm ( zhou and burges, 2007 ) and three different baselines on a variety of standard, large data sets.
we present the first work on applying sta-tistical techniques to unrestricted quanti-fier scope disambiguation ( qsd ), where there is no restriction on the type or the number of quantifiers in the sentence. <eos> we formulate unrestricted qsd as learning to build a directed acyclic graph ( dag ) and define evaluation metrics based on the properties of dags. <eos> previous work on sta-tistical scope disambiguation is very lim-ited, only considering sentences with two explicitly quantified noun phrases ( nps ). <eos> in addition, they only handle a restricted list of quantifiers. <eos> in our system, all nps, ex-plicitly quantified or not ( e.g. <eos> definites, bare singulars/plurals, etc. <eos> ), are considered for possible scope interactions. <eos> we present early results on applying a simple model to a small corpus. <eos> the preliminary results are encouraging, and we hope will motivate further research in this area.
usually unsupervised dependency parsing tries to optimize the probability of a corpus by modifying the dependency model that was presumably used to generate the corpus. <eos> in this article we explore a different view in which a dependency structure is among other things a partial order on the nodes in terms of centrality or saliency. <eos> under this assumption we model the partial order directly and derive dependency trees from this order. <eos> the result is an approach to unsupervised dependency parsing that is very different from standard ones in that it requires no training data. <eos> each sentence induces a model from which the parse is read off. <eos> our approach is evaluated on data from 12 different languages. <eos> two scenarios are considered : a scenario in which information about part-of-speech is available, and a scenario in which parsing relies only on word forms and distributional clusters. <eos> our approach is competitive to state-of-the-art in both scenarios.
using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries ( semi- ) automatically. <eos> the basic idea is based on the assumption that similar words have similar contexts across languages. <eos> the context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. <eos> the predecessors and successors. <eos> these different context positions are then combined into one context vector and compared across languages. <eos> however, previous research makes the ( implicit ) assumption that these different context positions should be weighted as equally important. <eos> furthermore, only the same context positions are compared with each other, for example the successor position in spanish is compared with the successor position in english. <eos> however, this is not necessarily always appropriate for languages like japanese and english. <eos> to overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is defined by a matrix. <eos> we define the optimal transformation matrix by using a bayesian probabilistic model, and show that it is feasible to find an approximate solution using markov chain monte carlo methods. <eos> our experiments demonstrate that our proposed method constantly improves translation accuracy.
in this article, we present a simple and effective approach for extracting bilingual lexicon from comparable corpora enhanced with parallel corpora. <eos> we make use of structural characteristics of the documents comprising the comparable corpus to extract parallel sentences with a high degree of quality. <eos> we then use state-of-the-art techniques to build a specialized bilingual lexicon from these sentences and evaluate the contribution of this lexicon when added to the comparable corpus-based alignment technique. <eos> finally, the value of this approach is demonstrated by the improvement of translation accuracy for medical words.
in this article we present a novel way of looking at the problem of automatic acquisition of pairs of translationally equivalent words from comparable corpora. <eos> we first present the standard and extended approaches traditionally dedicated to this task. <eos> we then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. <eos> the empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.
in this paper, we present two methods to use a noisy parallel news corpus to improve statistical machine translation ( smt ) systems. <eos> taking full advantage of the characteristics of our corpus and of existing resources, we use a bootstrapping strategy, whereby an existing smt engine is used both to detect parallel sentences in comparable data and to provide an adaptation corpus for translation models. <eos> mt experiments demonstrate the benefits of various combinations of these strategies.
we present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different articles about the same topics or events. <eos> the procedure consists of document pair extraction, sentence pair extraction, and fragment pair extraction. <eos> at each stage, we evaluate the intermediate results manually, and tune the later stages accordingly. <eos> with this minimally supervised approach, we achieve 62 % of accuracy on the paraphrase fragment pairs we collected and 67 % extracted from the msr corpus. <eos> the results look promising, given the minimal supervision of the approach, which can be further scaled up.
mining parallel data from comparable corpora is a promising approach for overcoming the data sparseness in statistical machine translation and other nlp applications. <eos> even if two comparable documents have few or no parallel sentence pairs, there is still potential for parallelism in the sub-sentential level. <eos> the ability to detect these phrases creates a valuable resource, especially for low-resource languages. <eos> in this paper we explore three phrase alignment approaches to detect parallel phrase pairs embedded in comparable sentences : the standard phrase extraction algorithm, which relies on the viterbi path ; a phrase extraction approach that does not rely on the viterbi path, but uses only lexical features ; and a binary classifier that detects parallel phrase pairs when presented with a large collection of phrase pair candidates. <eos> we evaluate the effectiveness of these approaches in detecting alignments for phrase pairs that have a known alignment in comparable sentence pairs. <eos> the results show that the non-viterbi alignment approach outperforms the other two approaches on f1 measure.
supervised learning algorithms for identifying comparable sentence pairs from a dominantly non-parallel corpora require resources for computing feature functions as well as training the classifier. <eos> in this paper we propose active learning techniques for addressing the problem of building comparable data for low-resource languages. <eos> in particular we propose strategies to elicit two kinds of annotations from comparable sentence pairs : class label assignment and parallel segment extraction. <eos> we also propose an active learning strategy for these two annotations that performs significantly better than when sampling for either of the annotations independently.
in this paper, we question the homogeneity of a large parallel corpus by measuring the similarity between various sub-parts. <eos> we compare results obtained using a general measure of lexical similarity based on ? 2 and by counting the number of discourse connectives. <eos> we argue that discourse connectives provide a more sensitive measure, revealing differences that are not visible with the general measure. <eos> we also provide evidence for the existence of specific characteristics defining translated texts as opposed to nontranslated ones, due to a universal tendency for explicitation.
while several recent works on dealing with large bilingual collections of texts, e.g. <eos> ( smith et al, 2010 ), seek for extracting parallel sentences from comparable corpora, we present paradocs, a system designed to recognize pairs of parallel documents in a ( large ) bilingual collection of texts. <eos> we show that this system outperforms a fair baseline ( enright and kondrak, 2007 ) in a number of controlled tasks. <eos> we applied it on the frenchenglish cross-language linked article pairs of wikipedia in order see whether parallel articles in this resource are available, and if our system is able to locate them. <eos> according to some manual evaluation we conducted, a fourth of the article pairs in wikipedia are indeed in translation relation, and paradocs identifies parallel or noisy parallel article pairs with a precision of 80 %.
as the title suggests, our paper deals with web discussion fora, whose content can be considered to be a special type of comparable corpora. <eos> we discuss the potential of this vast amount of data available now on the world wide web nearly for every language, regarding both general and common topics as well as the most obscure and specific ones. <eos> to illustrate our ideas, we propose a case study of seven wedding discussion fora in five languages.
in this paper we investigate automatic datatext alignment, i.e. <eos> the task of automatically aligning data records with textual descriptions, such that data tokens are aligned with the word strings that describe them. <eos> our methods make use of log likelihood ratios to estimate the strength of association between data tokens and text tokens. <eos> we investigate datatext alignment at the document level and at the sentence level, reporting results for several methodological variants as well as baselines. <eos> we find that log likelihood ratios provide a strong basis for predicting data-text alignment.
this paper introduces a new task of crosslingual slot filling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language. <eos> it is a very challenging task which suffers from both information extraction and machine translation errors. <eos> in this paper we analyze the types of errors produced by five different baseline approaches, and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora. <eos> without using any additional labeled data this new approach obtained 38.5 % relative improvement in precision and 86.7 % relative improvement in recall over several state-of-the-art approaches. <eos> the ultimate system outperformed monolingual slot filling pipelines built on much larger monolingual corpora.
we describe the design of a comparable corpus that spans all of the world ? s languages and facilitates large-scale cross-linguistic processing. <eos> this universal corpus consists of text collections aligned at the document and sentence level, multilingual wordlists, and a small set of morphological, lexical, and syntactic annotations. <eos> the design encompasses submission, storage, and access. <eos> submission preserves the integrity of the work, allows asynchronous updates, and facilitates scholarly citation. <eos> storage employs a cloud-hosted filestore containing normalized source data together with a database of texts and annotations. <eos> access is permitted to the filestore, the database, and an application programming interface. <eos> all aspects of the universal corpus are open, and we invite community participation in its design and implementation, and in supplying and using its data.
the paper presents an expectation maximization ( em ) algorithm for automatic generation of parallel and quasi-parallel data from any degree of comparable corpora ranging from parallel to weakly comparable. <eos> specifically, we address the problem of extracting related textual units ( documents, paragraphs or sentences ) relying on the hypothesis that, in a given corpus, certain pairs of translation equivalents are better indicators of a correct textual unit correspondence than other pairs of translation equivalents. <eos> we evaluate our method on mixed types of bilingual comparable corpora in six language pairs, obtaining state of the art accuracy figures.
we describe a set of techniques that have been developed while collecting parallel texts for russian-english language pair and building a corpus of parallel sentences for training a statistical machine translation system. <eos> we discuss issues of verifying potential parallel texts and filtering out automatically translated documents. <eos> finally we evaluate the quality of the 1-millionsentence corpus which we believe may be a useful resource for machine translation research.
in this paper, we present a first attempt to characterize the semantic deviance of composite expressions in distributional semantics. <eos> specifically, we look for properties of adjective-noun combinations within a vectorbased semantic space that might cue their lack of meaning. <eos> we evaluate four different compositionality models shown to have various levels of success in representing the meaning of an pairs : the simple additive and multiplicative models of mitchell and lapata ( 2008 ), and the linear-map-based models of guevara ( 2010 ) and baroni and zamparelli ( 2010 ). <eos> for each model, we generate composite vectors for a set of an combinations unattested in the source corpus and which have been deemed either acceptable or semantically deviant. <eos> we then compute measures that might cue semantic anomaly, and compare each model ? s results for the two classes of ans. <eos> our study shows that simple, unsupervised cues can indeed significantly tell unattested but acceptable ans apart from impossible, or deviant, ans, and that the simple additive and multiplicative models are the most effective in this task.
stemming from distributed representation theories, we investigate the interaction between distributed structure and distributional meaning. <eos> we propose a pure distributed tree ( dt ) and distributional distributed tree ( ddt ). <eos> dts and ddts are exploited for defining distributed tree kernels ( dtks ) and distributional distributed tree kernels ( ddtks ). <eos> we compare dtks and ddtks in two tasks : approximating tree kernels tk ( collins and duffy, 2002 ) ; performing textual entailment recognition ( rte ). <eos> results show that dtks correlate with tks and perform in rte better than ddtks. <eos> then, including distributional vectors in distributed structures is a very difficult task.
since its introduction into the nlp community, pointwise mutual information has proven to be a useful association measure in numerous natural language processing applications such as collocation extraction and word space models. <eos> in its original form, it is restricted to the analysis of two-way co-occurrences. <eos> nlp problems, however, need not be restricted to twoway co-occurrences ; often, a particular problem can be more naturally tackled when formulated as a multi-way problem. <eos> in this paper, we explore two multivariate generalizations of pointwise mutual information, and explore their usefulness and nature in the extraction of subject verb object triples.
this paper gives an overview of the shared task at the acl-hlt 2011 disco ( distributional semantics and compositionality ) workshop. <eos> we describe in detail the motivation for the shared task, the acquisition of datasets, the evaluation methodology and the results of participating systems. <eos> the task of assigning a numerical score for a phrase according to its compositionality showed to be hard. <eos> many groups reported features that intuitively should work, yet showed no correlation with the training data. <eos> the evaluation reveals that most systems outperform simple baselines, yet have difficulties in reliably assigning a compositionality score that closely matches the gold standard. <eos> overall, approaches based on word space models performed slightly better than methods relying solely on statistical association measures.
we considered a wide range of features for the disco 2011 shared task about compositionality prediction for word pairs, including coals-based endocentricity scores, compositionality scores based on distributional clusters, statistics about wordnet-induced paraphrases, hyphenation, and the likelihood of long translation equivalents in other languages. <eos> many of the features we considered correlated significantly with human compositionality scores, but in support vector regression experiments we obtained the best results using only coals-based endocentricity scores. <eos> our system was nevertheless the best performing system in the shared task, and average error reductions over a simple baseline in cross-validation were 13.7 % for english and 50.1 % for german.
this paper describes three systems from the university of minnesota, duluth that participated in the disco 2011 shared task that evaluated distributional methods of measuring semantic compositionality. <eos> all three systems approached this as a problem of collocation identification, where strong collocates are assumed to be minimally compositional. <eos> duluth1 relies on the t-score, whereas duluth-2 and duluth-3 rely on pointwise mutual information ( pmi ). <eos> duluth-1 was the top ranked system overall in coarse ? grained scoring, which was a 3-way category assignment where pairs were assigned values of high, medium, or low compositionality.
the measurement of relative compositionality of bigrams is crucial to identify multi-word expressions ( mwes ) in natural language processing ( nlp ) tasks. <eos> the article presents the experiments carried out as part of the participation in the shared task ? distributional semantics and compositionality ( disco ) ? <eos> organized as part of the disco workshop in aclhlt 2011. <eos> the experiments deal with various collocation based statistical approaches to compute the relative compositionality of three types of bigram phrases ( adjective-noun, verbsubject and verb-object combinations ). <eos> the experimental results in terms of both fine-grained and coarse-grained compositionality scores have been evaluated with the human annotated gold standard data. <eos> reasonable results have been obtained in terms of average point difference and coarse precision.
this paper reports on the participation of the nlp group at uned in the disco ? 2011 compositionality evaluation task. <eos> the aim of the task is to predict compositionality judgements assigned by human raters to candidate phrases, in english and german, from three common grammatical relations : adjectivenoun, subject-verb and subject-object. <eos> our participation is restricted to adjectivenoun relations in english. <eos> we explore the use of syntactic-based contexts obtained from large corpora to build classifiers that model the compositionality of the semantics of such pairs.
a description of a system for measuring the compositionality of collocations within the framework of the shared task of the distributional semantics and compositionality workshop ( disco 2011 ) is presented. <eos> the system exploits the intuition that a highly compositional collocation would tend to have a considerable semantic overlap with its constituents ( headword and modifier ) whereas a collocation with low compositionality would share little semantic content with its constituents. <eos> this intuition is operationalised via three configurations that exploit cosine similarity measures to detect the semantic overlap between the collocation and its constituents. <eos> the system performs competitively in the task.
in this paper, we highlight the problems of polysemy in word space models of compositionality detection. <eos> most models represent each word as a single prototype-based vector without addressing polysemy. <eos> we propose an exemplar-based model which is designed to handle polysemy. <eos> this model is tested for compositionality detection and it is found to outperform existing prototype-based models. <eos> we have participated in the shared task ( biemann and giesbrecht, 2011 ) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations.
in this paper, we present a system that automatically generates questions from natural language text using discourse connectives. <eos> we explore the usefulness of the discourse connectives for question generation ( qg ) that looks at the problem beyond sentence level. <eos> our work divides the qg task into content selection and question formation. <eos> content selection consists of finding the relevant part in text to frame question from while question formation involves sense disambiguation of the discourse connectives, identification of question type and applying syntactic transformations on the content. <eos> the system is evaluated manually for syntactic and semantic correctness.
identifying peer-review helpfulness is an important task for improving the quality of feedback received by students, as well as for helping students write better reviews. <eos> as we tailor standard product review analysis techniques to our peer-review domain, we notice that peerreview helpfulness differs not only between students and experts but also between types of experts. <eos> in this paper, we investigate how different types of perceived helpfulness might influence the utility of features for automatic prediction. <eos> our feature selection results show that certain low-level linguistic features are more useful for predicting student perceived helpfulness, while high-level cognitive constructs are more effective in modeling experts ? <eos> perceived helpfulness.
this paper presents genpex, a system for automatic generation of narrative probability exercises. <eos> generation of exercises in genpex is done in two steps. <eos> first, the system creates a specification of a solvable probability problem, based on input from the user ( a researcher or test developer ) who selects a specific question type and a narrative context for the problem. <eos> then, a text expressing the probability problem is generated. <eos> the user can tune the generated text by setting the values of some linguistic variation parameters. <eos> by varying the mathematical content of the exercise, its narrative context and the linguistic parameter settings, many different exercises can be produced. <eos> here we focus on the natural language generation part of genpex. <eos> after describing how the system works, we briefly present our first evaluation results, and discuss some aspects requiring further investigation.
automated testing of spoken language is the subject of much current research. <eos> elicited imitation ( ei ), or sentence repetition, is well suited for automated scoring, but does not directly test a broad range of speech communication skills. <eos> an oral proficiency interview ( opi ) tests a broad range of skills, but is not as well suited for automated scoring. <eos> some have suggested that ei can be used as a predictor of more general speech communication abilities. <eos> we examine ei for this purpose. <eos> a fully automated ei test is used to predict opi scores. <eos> experiments show strong correlation between predicted and actual opi scores. <eos> effectiveness of opi score prediction depends upon at least two important design decisions. <eos> one of these decisions is to base prediction primarily on acoustic measures, rather than on transcription. <eos> the other of these decisions is the choice of sentences, or ei test items, to be repeated. <eos> it is shown that both of these design decisions can greatly impact performance. <eos> it is also shown that the effectiveness of individual test items can be predicted.
structural events, ( i.e., the structure of clauses and disfluencies ) in spontaneous speech, are important components of human speaking and have been used to measure language development. <eos> however, they have not been actively used in automated speech assessment research. <eos> given the recent substantial progress on automated structural event detection on spontaneous speech, we investigated the detection of clause boundaries and interruption points of edit disfluencies on transcriptions of non-native speech data and extracted features from the detected events for speech assessment. <eos> compared to features computed on human-annotated events, the features computed on machine-generated events show promising correlations to holistic scores that reflect speaking proficiency levels.
for adult readers, an automated system can produce oral reading fluency ( orf ) scores ( e.g., words read correctly per minute ) that are consistent with scores provided by human evaluators ( balogh et al, 2005, and in press ). <eos> balogh ? s work on naal materials used passage-specific data to optimize statistical language models and scoring performance. <eos> the current study investigates whether or not an automated system can produce scores for young children ? s reading that are consistent with human scores. <eos> a novel aspect of the present study is that text-independent rule-based language models were employed ( cheng and townshend, 2009 ) to score reading passages that the system had never seen before. <eos> oral reading performances were collected over cell phones from 1st, 2nd, and 3rd grade children ( n = 95 ) in a classroom environment. <eos> readings were scored 1 ) in situ by teachers in the classroom, 2 ) later by expert scorers, and 3 ) by an automated system. <eos> statistical analyses provide evidence that machine words correct scores correlate well with scores provided by teachers and expert scorers, with all ( pearson ? s correlation coefficient ) r ? s > 0.98 at the individual response level, and all r ? s > 0.99 at the ? test ? <eos> level ( i.e., median scores out of 3 ).
in this paper, we present an automatic question generation system that can generate gap-fill questions for content in a document. <eos> gap-fill questions are fill-in-the-blank questions with multiple choices ( one correct answer and three distractors ) provided. <eos> the system finds the informative sentences from the document and generates gap-fill questions from them by first blanking keys from the sentences and then determining the distractors for these keys. <eos> syntactic and lexical features are used in this process without relying on any external resource apart from the information in the document. <eos> we evaluated our system on two chapters of a standard biology textbook and presented the results.
we present an empirical study of one-onone human tutoring dialogues in the domain of computer science data structures. <eos> we are interested in discovering effective tutoring strategies, that we frame as discovering which dialogue act ( da ) sequences correlate with learning. <eos> we employ multiple linear regression, to discover the strongest models that explain why students learn during one-on-one tutoring. <eos> importantly, we define ? flexible ? <eos> da sequence, in which extraneous das can easily be discounted. <eos> our experiments reveal several cognitively plausible da sequences which significantly correlate with learning outcomes.
research has shown that a number of factors, such as maturational constraints, previous language background, and attention, can have an effect on l2 acquisition. <eos> one related issue that remains to be explored is what factors make an individual word more easily learned. <eos> in this study we propose that word complexity, on both the phonetic and semantic levels, affect l2 vocabulary learning. <eos> two studies showed that words with simple grapheme-to-phoneme ratios were easier to learn than more phonetically complex words, and that words with two or fewer word senses were easier to learn that those with three or more.
we further work on detecting errors in postpositional particle usage by learners of korean by improving the training data and developing a complete pipeline of particle selection. <eos> we improve the data by filtering non-korean data and sampling instances to better match the particle distribution. <eos> our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization.
language sample analysis is an important technique used in measuring language development. <eos> at present, measures of grammatical complexity such as the index of productive syntax ( scarborough, 1990 ) are used to measure language development in early childhood. <eos> although these measures depict the overall competence in the usage of language, they do not provide for an analysis of the grammatical mistakes made by the child. <eos> in this paper, we explore the use of existing natural language processing ( nlp ) techniques to provide an insight into the processing of child language transcripts and challenges in automatic grammar checking. <eos> we explore the automatic detection of 6 types of verb related grammatical errors. <eos> we compare rule based systems to statistical systems and investigate the use of different features. <eos> we found the statistical systems performed better than the rule based systems for most of the error categories.
we introduce a method for learning to describe the attendant contexts of a given query for language learning. <eos> in our approach, we display phraseological information in the form of a summary of general patterns as well as lexical bundles anchored at the query. <eos> the method involves syntactical analyses and inverted file construction. <eos> at run-time, grammatical constructions and their lexical instantiations characterizing the usage of the given query are generated and displayed, aimed at improving learners ? <eos> deep vocabulary knowledge. <eos> we present a prototype system, grasp, that applies the proposed method for enhanced collocation learning. <eos> preliminary experiments show that language learners benefit more from grasp than conventional dictionary lookup. <eos> in addition, the information produced by grasp is potentially useful information for automatic or manual editing process.
learning a vocabulary word requires seeing it in multiple informative contexts. <eos> we describe a system to generate such contexts for a given word sense. <eos> rather than attempt to do word sense disambiguation on example contexts already generated or selected from a corpus, we compile information about the word sense into the context generation process. <eos> to evaluate the sense-appropriateness of the generated contexts compared to wordnet examples, three human judges chose which word sense ( s ) fit each example, blind to its source and intended sense. <eos> on average, one judge rated the generated examples as sense-appropriate, compared to two judges for the wordnet examples. <eos> although the system ? s precision was only half of wordnet ? s, its recall was actually higher than wordnet ? s, thanks to covering many senses for which wordnet lacks examples.
in this paper we present a methodology for creating concept map exercises for students. <eos> concept mapping is a common pedagogical exercise in which students generate a graphical model of some domain. <eos> our method automatically extracts knowledge representations from a textbook and uses them to generate concept maps. <eos> the purpose of the study is to generate and evaluate these concept maps according to their accuracy, completeness, and pedagogy.
this paper investigates two strategies for collecting readability assessments, an expert readers application intended to collect fine-grained readability assessments from language experts and a sort by readability application designed to be intuitive and open for everyone having internet access. <eos> we show that the data sets resulting from both annotation strategies are very similar. <eos> we conclude that crowdsourcing is a viable alternative to the opinions of language experts for readability prediction.
to overcome their substantial barriers to fluent reading, students with dyslexia need to be enticed to read more, and to read texts with carefully controlled lexical content. <eos> we describe and show examples from a prototype of the new r2aft story assembly engine, which generates an interactive text that has a ) variable plot and b ) lexical content which is individualized by decoding pattern.
we apply a previously reported measure of dialog cohesion to a corpus of spoken tutoring dialogs in which motivation was measured. <eos> we find that cohesion significantly predicts changes in student motivation, as measured with a modified mslq instrument. <eos> this suggests that non-intrusive dialog measures can be used to measure motivation during tutoring.
semantic distance is the degree of closeness between two pieces of text determined by their meaning. <eos> semantic distance is typically measured by analyzing a set of documents or a list of terms and assigning a metric based on the likeness of their meaning or the concept they represent. <eos> although related research provides some semantic-based algorithms, few applications exist. <eos> this work proposes a semanticbased approach for automatically identifying potential course equivalencies given their catalog descriptions. <eos> the method developed by li et al ( 2006 ) is extended in this paper to take a course description from one university as the input and suggest equivalent courses offered at another university. <eos> results are evaluated and future work is discussed.
we present a method that filters out nonscorable ( ns ) responses, such as responses with a technical difficulty, in an automated speaking proficiency assessment system. <eos> the assessment system described in this study first filters out the non-scorable responses and then predicts a proficiency score using a scoring model for the remaining responses. <eos> the data were collected from non-native speakers in two different countries, using two different item types in the proficiency assessment : items that elicit spontaneous speech and items that elicit recited speech. <eos> since the proportion of ns responses and the features available to the model differ according to the item type, an item type specific model was trained for each item type. <eos> the accuracy of the models ranged between 75 % and 79 % in spontaneous speech items and between 95 % and 97 % in recited speech items. <eos> two different groups of features, signal processing based features and automatic speech recognition ( asr ) based features, were implemented. <eos> the asr based models achieved higher accuracy than the non-asr based models.
this paper presents a method for identifying non-english speech, with the aim of supporting an automated speech proficiency scoring system for non-native speakers. <eos> the method uses a popular technique from the language identification domain, a single phone recognizer followed by multiple languagedependent language models. <eos> this method determines the language of a speech sample based on the phonotactic differences among languages. <eos> the method is intended for use with nonnative english speakers. <eos> therefore, the method must be able to distinguish nonenglish responses from non-native speakers ? <eos> english responses. <eos> this makes the task more challenging, as the frequent pronunciation errors of non-native speakers may weaken the phonetic and phonotactic distinction between english responses and non-english responses. <eos> in order to address this issue, the speaking rate measure was used to complement the language identification based features in the model. <eos> the accuracy of the method was 98 %, and there was 45 % relative error reduction over a system based on the conventional language identification technique. <eos> the model using both feature sets furthermore demonstrated an improvement in accuracy for speakers at all english proficiency levels.
we present a novel noisy channel model for correcting text produced by english as a second language ( esl ) authors. <eos> we model the english word choices made by esl authors as a random walk across an undirected bipartite dictionary graph composed of edges between english words and associated words in an author ? s native language. <eos> we present two such models, using cascades of weighted finitestate transducers ( wfsts ) to model language model priors, random walk-induced noise, and observed sentences, and expectation maximization ( em ) to learn model parameters after park and levy ( 2011 ). <eos> we show that such models can make intelligent word substitutions to improve grammaticality in an unsupervised setting.
we present a general and simple method to adapt an existing nlp tool in order to enable it to deal with historical varieties of languages. <eos> this approach consists basically in expanding the dictionary with the old word variants and in retraining the tagger with a small training corpus. <eos> we implement this approach for old spanish. <eos> the results of a thorough evaluation over the extended tool show that using this method an almost state-of-the-art performance is obtained, adequate to carry out quantitative studies in the humanities : 94.5 % accuracy for the main part of speech and 92.6 % for lemma. <eos> to our knowledge, this is the first time that such a strategy is adopted to annotate historical language varieties and we believe that it could be used as well to deal with other non-standard varieties of languages.
the paper describes a tagger for old czech ( 1200-1500 ad ), a fusional language with rich morphology. <eos> the practical restrictions ( no native speakers, limited corpora and lexicons, limited funding ) make old czech an ideal candidate for a resource-light crosslingual method that we have been developing ( e.g. <eos> hana et al, 2004 ; feldman and hana, 2010 ). <eos> we use a traditional supervised tagger. <eos> however, instead of spending years of effort to create a large annotated corpus of old czech, we approximate it by a corpus of modern czech. <eos> we perform a series of simple transformations to make a modern text look more like a text in old czech and vice versa. <eos> we also use a resource-light morphological analyzer to provide candidate tags. <eos> the results are worse than the results of traditional taggers, but the amount of language-specific work needed is minimal.
the goal of this study is to evaluate an ? offthe-shelf ? <eos> pos-tagger for modern german on historical data from the early modern period ( 1650-1800 ). <eos> with no specialised tagger available for this particular stage of the language, our findings will be of particular interest to smaller, humanities-based projects wishing to add pos annotations to their historical data but which lack the means or resources to train a pos tagger themselves. <eos> our study assesses the effects of spelling variation on the performance of the tagger, and investigates to what extent tagger performance can be improved by using ? normalised ? <eos> input, where spelling variants in the corpus are standardised to a modern form. <eos> our findings show that adding such a normalisation layer improves tagger performance considerably.
e-research explores the possibilities offered by ict for science and technology. <eos> its goal is to allow a better access to computing power, data and library resources. <eos> in essence eresearch is all about cyberstructure and being connected in ways that might change how we perceive scientific creation. <eos> the present work advocates open access to scientific data for linguists and language experts working within the humanities. <eos> by describing the modules of an online application, we would like to outline how a linguistic tool can help the linguist. <eos> work with data, from its creation to its integration into a publication is not rarely perceived as a chore. <eos> given the right tools however, it can become a meaningful part of the linguistic investigation. <eos> the standard format for linguistic data in the humanities is interlinear glosses. <eos> as such they represent a valuable resource even though linguists tend to disagree about the role and the methods by which data should influence linguistic exploration ( lehmann, 2004 ). <eos> in describing the components of our system we focus on the potential that this tool holds for real-time datasharing and continuous dissemination of research results throughout the life-cycle of a linguistic project.
the paper describes a tool developed to process historical ( slovene ) text, which annotates words in a tei encoded corpus with their modern-day equivalents, morphosyntactic tags and lemmas. <eos> such a tool is useful for developing historical corpora of highly-inflecting languages, enabling full text search in digital libraries of historical texts, for modernising such texts for today 's readers and making it simpler to correct ocr transcriptions.
in this paper, we report on how historical events are extracted from text within the semantics of history research project. <eos> the project aims at the creation of resources for a historical information retrieval system that can handle the time-based dynamics and varying perspectives of dutch historical archives. <eos> the historical event extraction module will be used for museum collections, allowing users to search for exhibits related to particular historical events or actors within time periods and geographic areas, extracted from accompanying text. <eos> we present here the methodology and tools used for the purpose of historical event extraction alongside with the first evaluation results.
cultural heritage institutions are making their digital content available and searchable online. <eos> digital metadata descriptions play an important role in this endeavour. <eos> this metadata is mostly manually created and often lacks detailed annotation, consistency and, most importantly, explicit semantic content descriptors which would facilitate online browsing and exploration of available information. <eos> this paper proposes the enrichment of existing cultural heritage metadata with automatically generated semantic content descriptors. <eos> in particular, it is concerned with metadata encoding archival descriptions ( ead ) and proposes to use automatic term recognition and term clustering techniques for knowledge acquisition and content-based document classification purposes.
most existing hlt pipelines assume the input is pure text or, at most, html and either ignore ( logical ) document structure or remove it. <eos> we argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved.
the arc project ( for architecture represented computationally ) is an attempt to reproduce in computer form the architectural historian ? s mental model of the gothic cathedral. <eos> this model includes the background information necessary to understand a natural language architectural description. <eos> our first task is to formalize the description of gothic cathedrals in a logical language, and provide a means for translating into this language from natural language. <eos> such a system could then be used by architectural historians and others to facilitate the task of gathering and using information from architectural descriptions. <eos> we believe the arc project will represent an important contribution to the preservation of cultural heritage, because it will offer a logical framework for understanding the description of landmark monuments of the past. <eos> this paper presents an outline of our plan for the arc system, and examines some of the issues we face in implementing it.
we present an end-to-end pipeline including a user interface for the production of wordlevel annotations for an opinion-mining task in the information technology ( it ) domain. <eos> our pre-annotation pipeline selects candidate sentences for annotation using results from a small amount of trained annotation to bias the random selection over a large corpus. <eos> our user interface reduces the need for the user to understand the ? meaning ? <eos> of opinion in our domain context, which is related to community reaction. <eos> it acts as a preliminary buffer against low-quality annotators. <eos> finally, our post-annotation pipeline aggregates responses and applies a more aggressive quality filter. <eos> we present positive results using two different evaluation philosophies and discuss how our design decisions enabled the collection of high-quality annotations under subjective and fine-grained conditions.
the voynich manuscript is an undeciphered document from medieval europe. <eos> we present current knowledge about the manuscript ? s text through a series of questions about its linguistic properties.
even though historical texts reveal a lot of interesting information on culture and social structure in the past, information access is limited and in most cases the only way to find the information you are looking for is to manually go through large volumes of text, searching for interesting text segments. <eos> in this paper we will explore the idea of facilitating this timeconsuming manual effort, using existing natural language processing techniques. <eos> attention is focused on automatically identifying verbs in early modern swedish texts ( 1550 ? 1800 ). <eos> the results indicate that it is possible to identify linguistic categories such as verbs in texts from this period with a high level of precision and recall, using morphological tools developed for present-day swedish, if the text is normalised into a more modern spelling before the morphological tools are applied.
today we have access to unprecedented amounts of literary texts. <eos> however, search still relies heavily on key words. <eos> in this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in both individual books and across very large collections. <eos> we introduce the concept of emotion word density, and using the brothers grimm fairy tales as example, we show how collections of text can be organized for better search. <eos> using the google books corpus we show how to determine an entity ? s emotion associations from cooccurring words. <eos> finally, we compare emotion words in fairy tales and novels, to show that fairy tales have a much wider range of emotion word densities than novels.
while the study of the connection between discourse patterns and personal identification is decades old, the study of these patterns using language technologies is relatively recent. <eos> in that more recent tradition we frame author age prediction from text as a regression problem. <eos> we explore the same task using three very different genres of data simultaneously : blogs, telephone conversations, and online forum posts. <eos> we employ a technique from domain adaptation that allows us to train a joint model involving all three corpora together as well as separately and analyze differences in predictive features across joint and corpusspecific aspects of the model. <eos> effective features include both stylistic ones ( such as pos patterns ) as well as content oriented ones. <eos> using a linear regression model based on shallow text features, we obtain correlations up to 0.74 and mean absolute errors between 4.1 and 6.8 years.
academic collaboration has often been at the forefront of scientific progress, whether amongst prominent established researchers, or between students and advisors. <eos> we suggest a theory of the different types of academic collaboration, and use topic models to computationally identify these in computational linguistics literature. <eos> a set of author-specific topics are learnt over the acl corpus, which ranges from 1965 to 2009. <eos> the models are trained on a per year basis, whereby only papers published up until a given year are used to learn that year ? s author topics. <eos> to determine the collaborative properties of papers, we use, as a metric, a function of the cosine similarity score between a paper ? s term vector and each author ? s topic signature in the year preceding the paper ? s publication. <eos> we apply this metric to examine questions on the nature of collaborations in computational linguistics research, finding that significant variations exist in the way people collaborate within different subfields.
in this paper we examine the sentence simplification problem as an english-to-english translation problem, utilizing a corpus of 137k aligned sentence pairs extracted by aligning english wikipedia and simple english wikipedia. <eos> this data set contains the full range of transformation operations including rewording, reordering, insertion and deletion. <eos> we introduce a new translation model for text simplification that extends a phrasebased machine translation approach to include phrasal deletion. <eos> evaluated based on three metrics that compare against a human reference ( bleu, word-f1 and ssa ) our new approach performs significantly better than two text compression techniques ( including t3 ) and the phrase-based translation system without deletion.
in this work, we present a scenario where contextual targeted paraphrasing of sub-sentential phrases is performed automatically to support the task of text revision. <eos> candidate paraphrases are obtained from a preexisting repertoire and validated in the context of the original sentence using information derived from the web. <eos> we report on experiments on french, where the original sentences to be rewritten are taken from a rewriting memory automatically extracted from the edit history of wikipedia.
we present a method for the sentence-level alignment of short simplified text to the original text from which they were adapted. <eos> our goal is to align a medium-sized corpus of parallel text, consisting of short news texts in spanish with their simplified counterpart. <eos> no training data is available for this task, so we have to rely on unsupervised learning. <eos> in contrast to bilingual sentence alignment, in this task we can exploit the fact that the probability of sentence correspondence can be estimated from lexical similarity between sentences. <eos> we show that the algoithm employed performs better than a baseline which approaches the problem with a tf*idf sentence similarity metric. <eos> the alignment algorithm is being used for the creation of a corpus for the study of text simplification in the spanish language.
paraphrase generation can be regarded as machine translation where source and target language are the same. <eos> we use the moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. <eos> data is derived from a recently released, large scale ( 2.1m tokens ) paraphrase corpus for dutch. <eos> preliminary results indicate that the phrase-based approach performs better in terms of nist scores and produces paraphrases at a greater distance from the source.
in our work we use an existing classifier to quantify and analyze the level of specific and general content in news documents and their human and automatic summaries. <eos> we discover that while human abstracts contain a more balanced mix of general and specific content, automatic summaries are overwhelmingly specific. <eos> we also provide an analysis of summary specificity and the summary quality scores assigned by people. <eos> we find that too much specificity could adversely affect the quality of content in the summary. <eos> our findings give strong evidence for the need for a new task in abstractive summarization : identification and generation of general sentences.
we examine the task of strict sentence intersection : a variant of sentence fusion in which the output must only contain the information present in all input sentences and nothing more. <eos> our proposed approach involves alignment and generalization over the input sentences to produce a generation lattice ; we then compare a standard search-based approach for decoding an intersection from this lattice to an integer linear program that preserves aligned content while minimizing the disfluency in interleaving text segments. <eos> in addition, we introduce novel evaluation strategies for intersection problems that employ entailmentstyle judgments for determining the validity of system-generated intersections. <eos> our experiments show that the proposed models produce valid intersections a majority of the time and that the segmented decoder yields advantages over the search-based approach.
we present a system for fusing sentences which are drawn from the same source document but have different content. <eos> unlike previous work, our approach is supervised, training on real-world examples of sentences fused by professional journalists in the process of editing news articles. <eos> like filippova and strube ( 2008 ), our system merges dependency graphs using integer linear programming. <eos> however, instead of aligning the inputs as a preprocess, we integrate the tasks of finding an alignment and selecting a merged sentence into a joint optimization problem, and learn parameters for this optimization using a structured online algorithm. <eos> evaluation by human judges shows that our technique produces fused sentences that are both informative and readable.
we propose a new, ambitious framework for abstractive summarization, which aims at selecting the content of a summary not from sentences, but from an abstract representation of the source documents. <eos> this abstract representation relies on the concept of information items ( init ), which we define as the smallest element of coherent information in a text or a sentence. <eos> our framework differs from previous abstractive summarization models in requiring a semantic analysis of the text. <eos> we present a first attempt made at developing a system from this framework, along with evaluation results for it from tac 2010. <eos> we also present related work, both from within and outside of the automatic summarization domain.
we present a method of creating disjunctive logical forms ( dlfs ) from aligned sentences for grammar-based paraphrase generation using the openccg broad coverage surface realizer. <eos> the method takes as input word-level alignments of two sentences that are paraphrases and projects these alignments onto the logical forms that result from automatically parsing these sentences. <eos> the projected alignments are then converted into phrasal edits for producing dlfs in both directions, where the disjunctions represent alternative choices at the level of semantic dependencies. <eos> the resulting dlfs are fed into the openccg realizer for n-best realization, using a pruning strategy that encourages lexical diversity. <eos> after merging, the approach yields an n-best list of paraphrases that contain grammatical alternatives to each original sentence, as well as paraphrases that mix and match content from the pair. <eos> a preliminary error analysis suggests that the approach could benefit from taking the word order in the original sentences into account. <eos> we conclude with a discussion of plans for future work, highlighting the method ? s potential use in enhancing automatic mt evaluation.
we present a substitution-only approach to sentence compression which ? tightens ? <eos> a sentence by reducing its character length. <eos> replacing phrases with shorter paraphrases yields paraphrastic compressions as short as 60 % of the original length. <eos> in support of this task, we introduce a novel technique for re-ranking paraphrases extracted from bilingual corpora. <eos> at high compression rates1 paraphrastic compressions outperform a state-of-the-art deletion model in an oracle experiment. <eos> for further compression, deleting from oracle paraphrastic compressions preserves more meaning than deletion alone. <eos> in either setting, paraphrastic compression shows promise for surpassing deletion-only methods.
this work surveys existing evaluation methodologies for the task of sentence compression, identifies their shortcomings, and proposes alternatives. <eos> in particular, we examine the problems of evaluating paraphrastic compression and comparing the output of different models. <eos> we demonstrate that compression rate is a strong predictor of compression quality and that perceived improvement over other models is often a side effect of producing longer output.
a growing body of work has highlighted the challenges of identifying the stance a speaker holds towards a particular topic, a task that involves identifying a holistic subjective disposition. <eos> we examine stance classification on a corpus of 4873 posts across 14 topics on convinceme.net, ranging from the playful to the ideological. <eos> we show that ideological debates feature a greater share of rebuttal posts, and that rebuttal posts are significantly harder to classify for stance, for both humans and trained classifiers. <eos> we also demonstrate that the number of subjective expressions varies across debates, a fact correlated with the performance of systems sensitive to sentimentbearing terms. <eos> we present results for identifing rebuttals with 63 % accuracy, and for identifying stance on a per topic basis that range from 54 % to 69 %, as compared to unigram baselines that vary between 49 % and 60 %. <eos> our results suggest that methods that take into account the dialogic context of such posts might be fruitful.
this paper presents a lexicon model for subjectivity description of dutch verbs that offers a framework for the development of sentiment analysis and opinion mining applications based on a deep syntactic-semantic approach. <eos> the model aims to describe the detailed subjectivity relations that exist between the participants of the verbs, expressing multiple attitudes for each verb sense. <eos> validation is provided by an annotation study that shows that these subtle subjectivity relations are reliably identifiable by human annotators.
this article reports on the methodology and the development of a complementary information source for the meaning of the synsets of princeton wordnet 3.0. <eos> this encoded information was built following the principles of the osgoodian differential semantics theory and consists of numerical values which represent the scaling of the connotative meanings along the multiple dimensions defined by pairs of antonyms ( factors ). <eos> depending on the selected factors, various facets of connotative meanings come under scrutiny and different types of textual subjective analysis may be conducted ( opinion mining, sentiment analysis ).
the paper presents a semi-automatic approach to creating sentiment dictionaries in many languages. <eos> we first produced high-level goldstandard sentiment dictionaries for two languages and then translated them automatically into third languages. <eos> those words that can be found in both target language word lists are likely to be useful because their word senses are likely to be similar to that of the two source languages. <eos> these dictionaries can be further corrected, extended and improved. <eos> in this paper, we present results that verify our triangulation hypothesis, by evaluating triangulated lists and comparing them to nontriangulated machine-translated word lists.
we propose a novel method to construct semantic orientation lexicons using large data and a thesaurus. <eos> to deal with large data, we use count-min sketch to store the approximate counts of all word pairs in a bounded space of 8gb. <eos> we use a thesaurus ( like roget ) to constrain near-synonymous words to have the same polarity. <eos> this framework can easily scale to any language with a thesaurus and a unzipped corpus size ? <eos> 50 gb ( 12 billion tokens ). <eos> we evaluate these lexicons intrinsically and extrinsically, and they perform comparable when compared to other existing lexicons.
locating documents carrying positive or negative favourability is an important application within media analysis. <eos> this paper presents some empirical results on the challenges facing a machine-learning approach to this kind of opinion mining. <eos> some of the challenges include : the often considerable imbalance in the distribution of positive and negative samples ; changes in the documents over time ; and effective training and quantification procedures for reporting results. <eos> this paper begins with three datasets generated by a media-analysis company, classifying documents in two ways : detecting the presence of favourability, and assessing negative vs. positive favourability. <eos> we then evaluate a machine-learning approach to automate the classification process. <eos> we explore the effect of using five different types of features, the robustness of the models when tested on data taken from a later time period, and the effect of balancing the input data by undersampling. <eos> we find varying choices for the optimum classifier, feature set and training strategy depending on the task and dataset.
sentiment analysis is one of the recent, highly dynamic fields in natural language processing. <eos> most existing approaches are based on word-level analysis of texts and are able to detect only explicit expressions of sentiment. <eos> in this paper, we present an approach towards automatically detecting emotions ( as underlying components of sentiment ) from contexts in which no clues of sentiment appear, based on commonsense knowledge. <eos> the resource we built towards this aim ? <eos> emotinet - is a knowledge base of concepts with associated affective value. <eos> preliminary evaluations show that this approach is appropriate for the task of implicit emotion detection, thus improving the performance of sentiment detection and classification in text.
to assist in the research of social networks in history, we develop machine-learning-based tools for the identification and classification of personal relationships. <eos> our case study focuses on the dutch social movement between 1870 and 1940, and is based on biographical texts describing the lives of notable people in this movement. <eos> we treat the identification and the labeling of relations between two persons into positive, neutral, and negative both as a sequence of two tasks and as a single task. <eos> we observe that our machine-learning classifiers, support vector machines, produce better generalization performance on the single task. <eos> we show how a complete social network can be built from these classifications, and provide a qualitative analysis of the induced network using expert judgements on samples of the network.
with the widespread use of email, we now have access to unprecedented amounts of text that we ourselves have written. <eos> in this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in many types of mail. <eos> we create a large word ? emotion association lexicon by crowdsourcing, and use it to compare emotions in love letters, hate mail, and suicide notes. <eos> we show that there are marked differences across genders in how they use emotion words in work-place email. <eos> for example, women use many words from the joy ? sadness axis, whereas men prefer terms from the fear ? trust axis. <eos> finally, we show visualizations that can help people track emotions in their emails.
this paper reports the development of japanese wordnet affect from the english wordnet affect lists with the help of english sentiwordnet and japanese wordnet. <eos> expanding the available synsets of the english wordnet affect using sentiwordnet, we have performed the translation of the expanded lists into japanese based on the synsetids in the japanese wordnet. <eos> a baseline system for emotion analysis of japanese sentences has been developed based on the japanese wordnet affect. <eos> the incorporation of morphology improves the performance of the system. <eos> overall, the system achieves average precision, recall and f-scores of 32.76 %, 53 % and 40.49 % respectively on 89 sentences of the japanese judgment corpus and 83.52 %, 49.58 % and 62.22 % on 1000 translated japanese sentences of the semeval 2007 affect sensing test corpus. <eos> different experimental outcomes and morphological analysis suggest that irrespective of the google translation error, the performance of the system could be improved by enhancing the japanese wordnet affect in terms of coverage.
in this paper, we focus on the impressions that people gain from reading articles in japanese newspapers, and we propose a method for extracting and quantifying these impressions in real numbers. <eos> the target impressions are limited to those represented by three bipolar scales, ? happy ? <eos> sad, ? <eos> ? glad ? <eos> angry, ? <eos> and ? peaceful ? <eos> strained, ? <eos> and the strength of each impression is computed as a real number between 1 and 7. <eos> first, we implement a method for computing impression values of articles using an impression lexicon. <eos> this lexicon represents a correlation between the words appearing in articles and the influence of these words on the readers ? <eos> impressions, and is created from a newspaper database using a word co-occurrence based method. <eos> we considered that some gaps would occur between values computed by such an unsupervised method and those judged by the readers, and we conducted experiments with 900 subjects to identify what gaps actually occurred. <eos> consequently, we propose a new approach that uses regression equations to correct impression values computed by the method. <eos> our investigation shows that accuracy is improved by a range of 23.2 % to 42.7 % by using regression equations.
recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. <eos> the advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. <eos> in this paper we propose a set of new feature selection schemes that use a content and syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. <eos> by focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. <eos> the results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches.
we introduce a new emotion classification task based on leary ? s rose, a framework for interpersonal communication. <eos> we present a small dataset of 740 dutch sentences, outline the annotation process and evaluate annotator agreement. <eos> we then evaluate the performance of several automatic classification systems when classifying individual sentences according to the four quadrants and the eight octants of leary ? s rose. <eos> svm-based classifiers achieve average f-scores of up to 51 % for 4-way classification and 31 % for 8-way classification, which is well above chance level. <eos> we conclude that emotion classification according to the interpersonal circumplex is a challenging task for both humans and machine learners. <eos> we expect classification performance to increase as context information becomes available in future versions of our dataset.
the research described in this work focuses on identifying key components for the task of irony detection. <eos> by means of analyzing a set of customer reviews, which are considered as ironic both in social and mass media, we try to find hints about how to deal with this task from a computational point of view. <eos> our objective is to gather a set of discriminating elements to represent irony. <eos> in particular, the kind of irony expressed in such reviews. <eos> to this end, we built a freely available data set with ironic reviews collected from amazon. <eos> such reviews were posted on the basis of an online viral effect ; i.e. <eos> contents whose effect triggers a chain reaction on people. <eos> the findings were assessed employing three classifiers. <eos> the results show interesting hints regarding the patterns and, especially, regarding the implications for sentiment analysis.
in most tasks related to opinion mining and sentiment analysis, it is necessary to compute the semantic orientation ( i.e., positive or negative evaluative implications ) of certain opinion expressions. <eos> recent works suggest that semantic orientation depends on application domains. <eos> moreover, we think that semantic orientation depends on the specific targets ( features ) that an opinion is applied to. <eos> in this paper, we introduce a technique to build domainspecific, feature-level opinion lexicons in a semi-supervised manner : we first induce a lexicon starting from a small set of annotated documents ; then, we expand it automatically from a larger set of unannotated documents, using a new graph-based ranking algorithm. <eos> our method was evaluated in three different domains ( headphones, hotels and cars ), using a corpus of product reviews which opinions were annotated at the feature level. <eos> we conclude that our method produces feature-level opinion lexicons with better accuracy and recall that domain-independent opinion lexicons using only a few annotated documents.
the new trend in sentiment classification is to use semantic features for representation of documents. <eos> we propose a semantic space based on wordnet senses for a supervised document-level sentiment classifier. <eos> not only does this show a better performance for sentiment classification, it also opens opportunities for building a robust sentiment classifier. <eos> we examine the possibility of using similarity metrics defined on wordnet to address the problem of not finding a sense in the training corpus. <eos> using three popular similarity metrics, we replace unknown synsets in the test set with a similar synset from the training set. <eos> an improvement of 6.2 % is seen with respect to baseline using this approach.
in this paper, we concentrate on the 3 of the tracks proposed in the ntcir 8 moat, concerning the classification of sentences according to their opinionatedness, relevance and polarity. <eos> we propose a method for the detection of opinions, relevance, and polarity classification, based on isr-wn ( a resource for the multidimensional analysis with relevant semantic trees of sentences using different wordnet-based information sources ). <eos> based on the results obtained, we can conclude that the resource and methods we propose are appropriate for the task, reaching the level of state-of-the-art approaches.
in recent years microblogs have taken on an important role in the marketing sphere, in which they have been used for sharing opinions and/or experiences about a product or service. <eos> companies and researchers have become interested in analysing the content generated over the most popular of these, the twitter platform, to harvest information critical for their online reputation management ( orm ). <eos> critical to this task is the efficient and accurate identification of tweets which refer to a company distinguishing them from those which do not. <eos> the aim of this work is to present and compare two different approaches to achieve this. <eos> the obtained results are promising while at the same time highlighting the difficulty of this task.
so tommaso caselli francesco rubino ilc ? a.zampolli ? <eos> ? <eos> cnr via g. moruzzi, 1- 56124 pisa { irene.russo } { tommaso.caselli } { francesco.rubino } @ ilc.cnr.it ester boldrini patricio mart ? nez-barco dsli ? <eos> university of alicante ap. <eos> de correos, 99 ? <eos> 03080 alicante { eboldrini } { patricio } @ dlsi.ua.es abstract in this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from italian newspaper articles ( la repubblica corpus ). <eos> our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. <eos> our approach has been evaluated with respect to manually annotated data. <eos> the results obtained so far are satisfying and support the validity of the methodology proposed.
in this study we investigate using an unsupervised generative learning method for subjectivity detection in text across different domains. <eos> we create an initial training set using simple lexicon information, and then evaluate a calibrated em ( expectation-maximization ) method to learn from unannotated data. <eos> we evaluate this unsupervised learning approach on three different domains : movie data, news resource, and meeting dialogues. <eos> we also perform a thorough analysis to examine impacting factors on unsupervised learning, such as the size and self-labeling accuracy of the initial training set. <eos> our experiments and analysis show inherent differences across domains and performance gain from calibration in em.
the aim of this paper is to present an approach to tackle the task of opinion question answering and text summarization. <eos> following the guidelines tac 2008 opinion summarization pilot task, we propose new methods for each of the major components of the process. <eos> in particular, for the information retrieval, opinion mining and summarization stages. <eos> the performance obtained improves with respect to the state of the art by approximately 12.50 %, thus concluding that the suggested approaches for these three components are adequate.
this paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language. <eos> starting from the union of small training data in target language and large translated examples in other languages, the transfer adaboost algorithm is applied to iteratively reduce the influence of low quality translated examples. <eos> alternatively, starting only from the training data in target language, the transfer self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set. <eos> these two algorithms are applied to sentence- and document-level cross lingual opinion analysis tasks, respectively. <eos> the evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data.
the bionlp shared task 2011, an information extraction task held over 6 months up to march 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. <eos> five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects.
the genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of bionlp shared task 2011. <eos> as its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers. <eos> after a 3-month system development period, 15 teams submitted their performance results on test cases. <eos> the results show the community has made a significant advancement in terms of both performance improvement and generalization.
this paper presents the preparation, resources, results and analysis of the epigenetics and post-translational modifications ( epi ) task, a main task of the bionlp shared task 2011. <eos> the task concerns the extraction of detailed representations of 14 protein and dna modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances. <eos> seven teams submitted final results to the epi task in the shared task, with the highest-performing system achieving 53 % f-score in the full task and 69 % f-score in the extraction of a simplified set of core event arguments.
this paper presents the preparation, resources, results and analysis of the infectious diseases ( id ) information extraction task, a main task of the bionlp shared task 2011. <eos> the id task represents an application and extension of the bionlp ? 09 shared task event extraction approach to full papers on infectious diseases. <eos> seven teams submitted final results to the task, with the highest-performing system achieving 56 % f-score in the full task, comparable to state-of-the-art performance in the established bionlp ? 09 task. <eos> the results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases.
in this paper we describe our approach to the bionlp 2011 shared task on biomedical event extraction from abstracts and full papers. <eos> we employ a joint inference system developed using the search-based structured prediction framework and show that it improves on a pipeline using the same features and it is better able to handle the domain shift from abstracts to full papers. <eos> in addition, we report on experiments using a simple domain adaptation method.
we describe the stanford entry to the bionlp 2011 shared task on biomolecular event extraction ( kim et al, 2011a ). <eos> our framework is based on the observation that event structures bear a close relation to dependency graphs. <eos> we show that if biomolecular events are cast as these pseudosyntactic structures, standard parsing tools ( maximum-spanning tree parsers and parse rerankers ) can be applied to perform event extraction with minimum domainspecific tuning. <eos> the vast majority of our domain-specific knowledge comes from the conversion to and from dependency graphs. <eos> our system performed competitively, obtaining 3rd place in the infectious diseases track ( 50.6 % f-score ), 5th place in epigenetics and post-translational modifications ( 31.2 % ), and 7th place in genia ( 50.0 % ). <eos> additionally, this system was part of the combined system in riedel et al ( 2011 ) to produce the highest scoring system in three out of the four event extraction tasks.
we present a joint model for biomedical event extraction and apply it to four tracks of the bionlp 2011 shared task. <eos> our model decomposes into three sub-models that concern ( a ) event triggers and outgoing arguments, ( b ) event triggers and incoming arguments and ( c ) protein-protein bindings. <eos> for efficient decoding we employ dual decomposition. <eos> our results are very competitive : with minimal adaptation of our model we come in second for two of the tasks ? right behind a version of the system presented here that includes predictions of the stanford event extractor as features. <eos> we also show that for the infectious diseases task using data from the genia track is a very effective way to improve accuracy.
we describe the faust entry to the bionlp 2011 shared task on biomolecular event extraction. <eos> the faust system explores several stacking models for combination using as base models the umass dual decomposition ( riedel and mccallum, 2011 ) and stanford event parsing ( mcclosky et al, 2011b ) approaches. <eos> we show that using stacking is a straightforward way to improving performance for event extraction and find that it is most effective when using a small set of stacking features and the base models use slightly different representations of the input data. <eos> the faust system obtained 1st place in three out of four tasks : 1st place in genia task 1 ( 56.0 % f-score ) and task 2 ( 53.9 % ), 2nd place in the epigenetics and post-translational modifications track ( 35.0 % ), and 1st place in the infectious diseases track ( 55.6 % ).
this paper presents the bacteria biotope task as part of the bionlp shared tasks 2011. <eos> the bacteria biotope task aims at extracting the location of bacteria from scientific web pages. <eos> bacteria location is a crucial knowledge in biology for phenotype studies. <eos> the paper details the corpus specification, the evaluation metrics, summarizes and discusses the participant results.
we present two related tasks of the bionlp shared tasks 2011 : bacteria gene renaming ( rename ) and bacteria gene interactions ( gi ). <eos> we detail the objectives, the corpus specification, the evaluation metrics, and we summarize the participants ? <eos> results. <eos> both issued from pubmed scientific literature abstracts, the rename task aims at extracting gene name synonyms, and the gi task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria.
this paper summarizes the protein coreference resolution task of bionlp shared task 2011. <eos> after 7 weeks of system development period, the task received final submissions from 6 teams. <eos> evaluation results show that state-of-the-art performance on the task can find 22.18 % of protein coreferences with the precision of 73.26 %. <eos> analysis of the submissions shows that several types of anaphoric expressions including definite expressions, which occupies a significant part of the problem, have not yet been solved.
this paper presents the entity relations ( rel ) task, a supporting task of the bionlp shared task 2011. <eos> the task concerns the extraction of two types of part-of relations between a gene/protein and an associated entity. <eos> four teams submitted final results for the rel task, with the highest-performing system achieving 57.7 % f-score. <eos> while experiments suggest use of the data can help improve event extraction performance, the task data has so far received only limited use in support of event extraction. <eos> the rel task continues as an open challenge, with all resources available from the shared task website.
kim, ellen riloff, nathan gilbert school of computing university of utah salt lake city, ut { youngjun, riloff, ngilbert } @ cs.utah.edu abstract to participate in the protein coreference section of the bionlp 2011 shared task, we use reconcile, a coreference resolution engine, by replacing some pre-processing components and adding a new mention detector. <eos> we got some improvement from training two separate classifiers for detecting anaphora and antecedent mentions. <eos> our system yielded the highest score in the task, f-score 34.05 % in partial mention, protein links, and system recall mode. <eos> we witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain.
this paper describes our event extraction system that participated in the bacteria biotopes task in bionlp shared task 2011. <eos> the system performs semi-supervised named entity recognition by leveraging additional information derived from external resources including a large amount of raw text. <eos> we also perform coreference resolution to deal with events having a large textual scope, which may span over several sentences ( or even paragraphs ). <eos> to create the training data for coreference resolution, we have manually annotated the corpus with coreference links. <eos> the overall f-score of event extraction was 33.2 at the official evaluation of the shared task, but it has been improved to 33.8 thanks to the refinement made after the submission deadline.
umr 8094 cnrs univ. <eos> paris 3 1 rue maurice arnoux f-92120 montrouge abstract this paper describes the system of the inra bibliome research group applied to the bacteria biotope ( bb ) task of the bi-onlp 2011 shared tasks. <eos> bacteria, geo-graphical locations and host entities were processed by a pattern-based approach and domain lexical resources. <eos> for the extraction of environment locations, we propose a framework based on semantic analysis sup-ported by an ontology of the biotope do-main. <eos> domain-specific rules were devel-oped for dealing with bacteria anaphora. <eos> official results show that our alvis system achieves the best performance of participat-ing systems.
this paper describes the supporting resources provided for the bionlp shared task 2011. <eos> these resources were constructed with the goal to alleviate some of the burden of system development from the participants and allow them to focus on the novel aspects of constructing their event extraction systems. <eos> with the availability of these resources we also seek to enable the evaluation of the applicability of specific tools and representations towards improving the performance of event extraction systems. <eos> additionally we supplied evaluation software and services and constructed a visualisation tool, stav, which visualises event extraction results and annotations. <eos> these resources helped the participants make sure that their final submissions and research efforts were on track during the development stages and evaluate their progress throughout the duration of the shared task. <eos> the visualisation software was also employed to show the differences between the gold annotations and those of the submitted results, allowing the participants to better understand the performance of their system. <eos> the resources, evaluation tools and visualisation tool are provided freely for research purposes and can be found at http : //sites.google.com/site/bionlpst/
the bacteria gene renaming ( rename ) task is a supporting task in the bionlp shared task 2011 ( bionlp-st ? 11 ). <eos> the task consists in extracting gene renaming acts and gene synonymy reminders in scientific texts about bacteria. <eos> in this paper, we present in details our method in three main steps : 1 ) the document segmentation into sentences, 2 ) the removal of the sentences exempt of renaming act ( false positives ) using both a gene nomenclature and supervised machine learning ( feature selection and svm ), 3 ) the linking of gene names by the target renaming relation in each sentence. <eos> our system ranked third at the official test with 64.4 % of f-measure. <eos> we also present here an effective post-competition improvement : the representation as svm features of regular expressions that detect combinations of trigger words. <eos> this increases the f-measure to 73.1 %.
building on technical advances from the bionlp 2009 shared task challenge, the 2011 challenge sets forth to generalize techniques to other complex biological event extraction tasks. <eos> in this paper, we present the implementation and evaluation of a signaturebased machine-learning technique to predict events from full texts of infectious disease documents. <eos> specifically, our approach uses novel signatures composed of traditional linguistic features and semantic knowledge to predict event triggers and their candidate arguments. <eos> using a leave-one out analysis, we report the contribution of linguistic and shallow semantic features in the trigger prediction and candidate argument extraction. <eos> lastly, we examine evaluations and posit causes for errors in our complex biological event extraction.
in this paper we describe a rule-based system developed for the bionlp 2011 genia event detection task. <eos> the system applies kybots ( knowledge yielding robots ) on annotated texts to extract bio-events involving proteins or genes. <eos> the main goal of this work is to verify the usefulness and portability of the kybot technology to the domain of biomedicine.
this paper describes a novel approach presented to the bionlp ? 11 shared task on genia event extraction. <eos> the approach consists of three steps. <eos> first, a dictionary is automatically constructed based on training datasets which is then used to detect candidate triggers and determine their event types. <eos> second, we apply a set of heuristic algorithms which use syntactic patterns and candidate triggers detected in the first step to extract biological events. <eos> finally, a post-processing is used to resolve regulatory events. <eos> we achieved an f-score of 43.94 % using the online evaluation system.
recently, the focus in the bionlp domain has shifted from binary relations to more expressive event representations, largely owing to the international popularity of the bionlp shared task ( st ) of 2009. <eos> this year, the st ? 11 provides a further generalization on three key aspects : text type, subject domain, and targeted event types. <eos> one of the supporting tasks established to provide more finegrained text predictions is the extraction of entity relations. <eos> we have implemented an extraction system for such non-causal relations between named entities and domain terms, applying semantic spaces and machine learning techniques. <eos> our system ranks second of four participating teams, achieving 37.04 % precision, 47.48 % recall and 41.62 % f-score.
we describe our approach for the genia event extraction in the main task of bionlp shared task 2011. <eos> there are two important parts in our method : event trigger annotation and event extraction. <eos> we use rules and dictionary to annotate event triggers. <eos> event extraction is based on patterns created from dependent graphs. <eos> we apply uima framework to support all stages in our system.
we introduce our incremental coreference resolution system for the bionlp 2011 shared task on protein/gene interaction. <eos> the benefits of an incremental architecture over a mentionpair model are : a reduction of the number of candidate pairs, a means to overcome the problem of underspecified items in pair-wise classification and the natural integration of global constraints such as transitivity. <eos> a filtering system takes into account specific features of different anaphora types. <eos> we do not apply machine learning, instead the system classifies with an empirically derived salience measure based on the dependency labels of the true mentions. <eos> the ontogene pipeline is used for preprocessing.
this paper presents our approach ( referred to as bioevent ) for protein-level complex event extraction, developed for the genia task ( kim et al, 2011b ) of the bionlp shared task 2011 ( kim et al, 2011a ). <eos> we developed a double layered machine learning approach which utilizes a state-of-the-art minimized feature set for each of the event types. <eos> we improved the best performing system of bionlp 2009 overall, and ranked first amongst 15 teams in finding ? localization ? <eos> events in 201112. <eos> bioevent is available at http : //bioevent.sourceforge.net/
we describe the system from the natural language processing group at microsoft research for the bionlp 2011 shared task. <eos> the task focuses on event extraction, identifying structured and potentially nested events from unannotated text. <eos> our approach follows a pipeline, first decorating text with syntactic information, then identifying the trigger words of complex events, and finally identifying the arguments of those events. <eos> the resulting system depends heavily on lexical and syntactic features. <eos> therefore, we explored methods of maintaining ambiguities and improving the syntactic representations, making the lexical information less brittle through clustering, and of exploring novel feature combinations and feature reduction. <eos> the system ranked 4th in the genia task with an f-measure of 51.5 %, and 3rd in the epi task with an f-measure of 64.9 %.
we participated in the bionlp shared task 2011, addressing the genia event extraction ( ge ) and the epigenetics and post-translational modifications ( epi ) tasks. <eos> a graph-based approach is employed to automatically learn rules for detecting biological events in the life-science literature. <eos> the event rules are learned by identifying the key contextual dependencies from full syntactic parsing of annotated text. <eos> event recognition is performed by searching for an isomorphism between event rules and the dependency graphs of sentences in the input texts. <eos> while we explored methods such as performance-based rule ranking to improve precision, we merged rules across multiple event types in order to increase recall. <eos> we achieved a 41.13 % f-score in detecting events of nine types in the task 1 of the ge task, and a 52.67 % f-score in identifying events across fifteen types in the core task of the epi task. <eos> our performance on both tasks is comparable to the state-of-the-art systems. <eos> our approach does not require any external domain-specific resources. <eos> the consistent performance on the two tasks supports the claim that the method generalizes well to extract events from different domains where training data is available.
the second bionlp shared task on event extraction ( bionlp-st ? 11 ) follows up the previous shared task competition with a focus on generalization with respect to text types, event types and subject domains. <eos> in this spirit, we re-engineered and extended our event extraction system, emphasizing linguistic generalizations and avoiding domain-, event typeor text type-specific optimizations. <eos> similar to our earlier system, syntactic dependencies form the basis of our approach. <eos> however, diverging from that system ? s more pragmatic nature, we more clearly distinguish the shared task concerns from a general semantic composition scheme, that is based on the notion of embedding. <eos> we apply our methodology to core bio-event extraction and speculation/negation detection tasks in three main tracks. <eos> our results demonstrate that such a general approach is viable and pinpoint some of its shortcomings.
we present a system for extracting biomedical events ( detailed descriptions of biomolecular interactions ) from research articles. <eos> this system was developed for the bionlp ? 11 shared task and extends our bionlp ? 09 shared task winning turku event extraction system. <eos> it uses support vector machines to first detect event-defining words, followed by detection of their relationships. <eos> the theme of the bionlp ? 11 shared task is generalization, extending event extraction to varied biomedical domains. <eos> our current system successfully predicts events for every domain case introduced in the bionlp ? 11 shared task, being the only system to participate in all eight tasks and all of their subtasks, with best performance in four tasks.
the conll-2011 shared task involved predicting coreference using ontonotes data. <eos> resources in this field have tended to be limited to noun phrase coreference, often on a restricted set of entities, such as ace entities. <eos> ontonotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types. <eos> ontonotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. <eos> this paper briefly describes the ontonotes annotation ( coreference and other layers ) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems. <eos> having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers ( parses, semantic roles, word senses, named entities and coreference ) that could support joint models, should help to energize ongoing research in the task of entity and event coreference.
this paper details the coreference resolution system submitted by stanford at the conll2011 shared task. <eos> our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. <eos> all these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. <eos> we participated in both the open and closed tracks and submitted results using both predicted and gold mentions. <eos> our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.
this paper describes the participation of relaxcor in the conll-2011 shared task : ? modeling unrestricted coreference in ontonotes ?. <eos> relaxcor is a constraint-based graph partitioning approach to coreference resolution solved by relaxation labeling. <eos> the approach combines the strengths of groupwise classifiers and chain formation methods in one global method.
this paper presents illinois-coref, a system for coreference resolution that participated in the conll-2011 shared task. <eos> we investigate two inference methods, best-link and all-link, along with their corresponding, pairwise and structured, learning protocols. <eos> within these, we provide a flexible architecture for incorporating linguistically-motivated constraints, several of which we developed and integrated. <eos> we compare and evaluate the inference approaches and the contribution of constraints, analyze the mistakes of the system, and discuss the challenges of resolving coreference for the ontonotes-4.0 data set.
in this paper, we describe a coreference solver based on the extensive use of lexical features and features extracted from dependency graphs of the sentences. <eos> the solver uses soon et al ( 2001 ) ? s classical resolution algorithm based on a pairwise classification of the mentions. <eos> we applied this solver to the closed track of the conll 2011 shared task ( pradhan et al, 2011 ). <eos> we carried out a systematic optimization of the feature set using cross-validation that led us to retain 24 features. <eos> using this set, we reached a muc score of 58.61 on the test set of the shared task. <eos> we analyzed the impact of the features on the development set and we show the importance of lexicalization as well as of properties related to dependency links in coreference resolution.
in this paper, we describe a machine learning system based on rule and tree ensembles for unrestricted coreference resolution. <eos> we use entropy guided transformation learning ( etl ) and decision trees as the base learners, and, respectively, etl committee and random forest as ensemble algorithms. <eos> our system is evaluated on the closed track of the conll 2011 shared task : modeling unrestricted coreference in ontonotes. <eos> a preliminary version of our system achieves the 6th best score out of 21 competitors in the conll 2011 shared task. <eos> here, we depict the system architecture and our experimental results and findings.
we present our end-to-end coreference resolution system, copa, which implements a global decision via hypergraph partitioning. <eos> in constrast to almost all previous approaches, we do not rely on separate classification and clustering steps, but perform coreference resolution globally in one step. <eos> copa represents each document as a hypergraph and partitions it with a spectral clustering algorithm. <eos> various types of relational features can be easily incorporated in this framwork. <eos> copa has participated in the open setting of the conll shared task on modeling unrestricted coreference.
because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 conll shared task ( pradhan et al., 2011 ). <eos> we investigate therefore multiobjective function optimization ( moo ) techniques based on genetic algorithms to optimize models according to multiple metrics simultaneously.
the paper presents a system for the conll2011 share task of coreference resolution. <eos> the system composes of two components : one for mentions detection and another one for their coreference resolution. <eos> for mentions detection, we adopted a number of heuristic rules from syntactic parse tree perspective. <eos> for coreference resolution, we apply svm by exploiting multiple syntactic and semantic features. <eos> the experiments on the conll-2011 corpus show that our rule-based mention identification system obtains a recall of 87.69 %, and the best result of the svm-based coreference resolution system is an average f-score 50.92 % of the muc, b-cubed and ceafe metrics.
in this paper we present sucre ( kobdani and schu ? tze, 2010 ) that is a modular coreference resolution system participating in the conll-2011 shared task : modeling unrestricted coreference in ontonote ( pradhan et al., 2011 ). <eos> the sucre ? s modular architecture provides a clean separation between data storage, feature engineering and machine learning algorithms.
this paper presents our error tolerable system for coreference resolution in conll2011 ( pradhan et al, 2011 ) shared task ( closed track ). <eos> different from most previous reported work, we detect mention candidates based on packed forest instead of single parse tree, and we use beam search algorithm based on the bell tree to create entities. <eos> experimental results show that our methods achieve promising results on the development set.
we introduce an incremental model for coreference resolution that competed in the conll 2011 shared task ( open regular ). <eos> we decided to participate with our baseline model, since it worked well with two other datasets. <eos> the benefits of an incremental over a mention-pair architecture are : a drastic reduction of the number of candidate pairs, a means to overcome the problem of underspecified items in pairwise classification and the natural integration of global constraints such as transitivity. <eos> we do not apply machine learning, instead the system uses an empirically derived salience measure based on the dependency labels of the true mentions. <eos> our experiments seem to indicate that such a system already is on par with machine learning approaches.
in this paper we describe the system with which we participated in the conll-2011 shared task on modelling coreference. <eos> our system is based on a cluster-ranking model proposed by rahman and ng ( 2009 ), with novel semantic features based on recent research on narrative event schema ( chambers and jurafsky, 2009 ). <eos> we demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. <eos> we also explore the impact of various features on our system ? s performance.
this paper describes our participation in the conll-2011 shared task for closed task. <eos> the approach used combines refined salience measure based pronominal resolution and crfs for non-pronominal resolution. <eos> in this work we also use machine learning based approach for identifying non-anaphoric pronouns.
this paper presents the coreference resolution system poly-co submitted to the closed track of the conll-2011 shared task. <eos> our system integrates a multilayer perceptron classifier in a pipeline approach. <eos> we describe the heuristic used to select the pairs of coreference candidates that are feeded to the network for training, and our feature selection method. <eos> the features used in our approach are based on similarity and identity measures, filtering informations, like gender and number, and other syntactic information.
our submission was a reduced version of the system described in haghighi and klein ( 2010 ), with extensions to improve mention detection to suit the ontonotes annotation scheme. <eos> including exact matching mention detection in this shared task added a new and challenging dimension to the problem, particularly for our system, which previously used a very permissive detection method. <eos> we improved this aspect of the system by adding filters based on the annotation scheme for ontonotes and analysis of system behavior on the development set. <eos> these changes led to improvements in coreference f-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the muc, b3, ceaf-e, ceaf-m and blanc, metrics, respectively, and a final task score of 47.10.
our system treats coreference resolution as an integer linear programming ( ilp ) problem. <eos> extending denis and baldridge ( 2007 ) and finkel andmanning ( 2008 ) ? s work, we exploit loose transitivity constraints on coreference pairs. <eos> instead of enforcing transitivity closure constraints, which brings o ( n3 ) complexity, we employ a strategy to reduce the number of constraints without large performance decrease, i.e., eliminating coreference pairs with probability below a threshold. <eos> experimental results show that it achieves a better performance than pairwise classifiers.
in this paper, we discuss the application of ubiu to the conll-2011 shared task on ? modeling unrestricted coreference ? <eos> in ontonotes. <eos> the shared task concentrates on the detection of coreference not only in noun phrases but also involving verbs. <eos> the information provided for the closed track included wordnet as well as corpus generated number and gender information. <eos> our system shows no improvement when using wordnet information, and the number information proved less reliable than the information in the part of speech tags.
in this paper, we describe the algorithms and experimental results of brandeis university in the participation of the conll task 2011 closed track. <eos> we report the features used in our system, and describe a novel cluster-based chaining algorithm to improve performance of coreference identification. <eos> we evaluate the system using the ontonotes data set and describe our results.
this paper describes our entry to the 2011 conll closed task ( pradhan et al, 2011 ) on modeling unrestricted coreference in ontonotes. <eos> our system is based on the reconcile coreference resolution research platform. <eos> reconcile is a general software infrastructure for the development of learning-based noun phrase ( np ) coreference resolution systems. <eos> our entry for the conll closed task is a configuration of reconcile intended to do well on ontonotes data. <eos> this paper describes our configuration of reconcile as well as the changes that we had to implement to integrate with the ontonotes task definition and data formats. <eos> we also present and discuss the performance of our system under different testing conditions on a withheld validation set.
this paper presents our participation in the conll-2011 shared task, modeling unrestricted coreference in ontonotes. <eos> coreference resolution, as a difficult and challenging problem in nlp, has attracted a lot of attention in the research community for a long time. <eos> its objective is to determine whether two mentions in a piece of text refer to the same entity. <eos> in our system, we implement mention detection and coreference resolution seperately. <eos> for mention detection, a simple classification based method combined with several effective features is developed. <eos> for coreference resolution, we propose a link type based pre-cluster pair model. <eos> in this model, pre-clustering of all the mentions in a single document is first performed. <eos> then for different link types, different classification models are trained to determine wheter two pre-clusters refer to the same entity. <eos> the final clustering results are generated by closest-first clustering method. <eos> official test results for closed track reveal that our method gives a muc f-score of 59.95 %, a b-cubed f-score of 63.23 %, and a ceaf f-score of 35.96 % on development dataset. <eos> when using gold standard mention boundaries, we achieve muc f-score of 55.48 %, b-cubed f-score of 61.29 %, and ceaf f-score of 32.53 %.
we investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems. <eos> the join evaluation and differences identification ( jedi ), finds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question. <eos> jedi provides evidence on the usefulness of a recent method, `1/`p-regularized regression ( obozinski et al, 2007 ). <eos> we evaluate against manually annotated success criteria from real users interacting with five different spoken user interfaces that give bus schedule information.
this paper describes a general and effective domain selection framework for multi-domain spoken dialogue systems that employ distributed domain experts. <eos> the framework consists of two processes : deciding if the current domain continues and estimating the probabilities for selecting other domains. <eos> if the current domain does not continue, the domain with the highest activation probability is selected. <eos> since those processes for each domain expert can be designed independently from other experts and can use a large variety of information, the framework achieves both extensibility and robustness against speech recognition errors. <eos> the results of an experiment using a corpus of dialogues between humans and a multi-domain dialogue system demonstrate the viability of the proposed framework.
with the evolution of online communication methods, conversations are increasingly handled via email, internet forums and other such methods. <eos> in this paper, we attempt to model lexical information in a context sensitive manner, encoding our belief that the use of language depends on the participants in the conversation. <eos> we model the discourse as a combination of the speaker, the addressee and other participants in the conversation as well as a context specific language model. <eos> in order to do this, we introduce a novel method based on an hmm with an exponential state space to capture speaker-addressee context. <eos> we also study the performance of topic modeling frameworks in conversational settings. <eos> we evaluate the models on the tasks of identifying the set of people present in any conversation, as well as identifying the speaker for every utterance in the conversation, and they show significant improvement over the baseline models.
we present a dialogue collection and enrichment framework that is designed to explore the learning and evaluation of dialogue policies for simple conversational characters using textual training data. <eos> to facilitate learning and evaluation, our framework enriches a collection of role-play dialogues with additional training data, including paraphrases of user utterances, and multiple independent judgments by external referees about the best policy response for the character at each point. <eos> as a case study, we use this framework to train a policy for a limited domain tactical questioning character, reaching promising performance. <eos> we also introduce an automatic policy evaluation metric that recognizes the validity of multiple conversational responses at each point in a dialogue. <eos> we use this metric to explore the variability in human opinion about optimal policy decisions, and to automatically evaluate several learned policies in our example domain.
human dialogue serves as a valuable model for learning the behavior of dialogue systems. <eos> hidden markov models ? <eos> sequential structure is well suited to modeling human dialogue, and their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process with a layer of implicit, highly influen-tial structure. <eos> hmms have been shown to be effective for a variety of descriptive and pre-dictive dialogue tasks. <eos> for task-oriented dia-logue, understanding the learning behavior of hmms is an important step toward building unsupervised models of human dialogue. <eos> this paper examines the behavior of hmms under six experimental conditions including different task-oriented feature sets and preprocessing approaches. <eos> the findings highlight the im-portance of providing hmm learning algo-rithms with rich task-based information. <eos> additionally, the results suggest how specific metrics should be used depending on whether the models will be employed primarily in a de-scriptive or predictive manner.
we present a novel scheme of spoken dialogue systems which uses the up-to-date information on the web. <eos> the scheme is based on information extraction which is defined by the predicate-argument ( p-a ) structure and realized by semantic parsing. <eos> based on the information structure, the dialogue system can perform question answering and also proactive information presentation. <eos> feasibility of this scheme is demonstrated with experiments using a domain of baseball news. <eos> in order to automatically select useful domain-dependent p-a templates, statistical measures are introduced, resulting to a completely unsupervised learning of the information structure given a corpus. <eos> similarity measures of p-a structures are also introduced to select relevant information. <eos> an experimental evaluation shows that the proposed system can make more relevant responses compared with the conventional ? bag-of-words ? <eos> scheme.
instruction giving can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. <eos> in this paper we present a novel algorithm for rapidly prototyping virtual instruction-giving agents from human-human corpora without manual annotation. <eos> automatically prototyping full-fledged dialogue systems from corpora is far from being a reality nowadays. <eos> our approach is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual worlds. <eos> we have defined an algorithm that, given a task-based corpus situated in a virtual world, which contains human instructor ? s speech acts and the user ? s responses as physical actions, generates a virtual instructor that robustly helps a user achieve a given task in the virtual world. <eos> we explain how this algorithm can be used for generating a virtual instructor for a game-like, task-oriented virtual world. <eos> we evaluate the virtual instructor with human users using task-oriented as well as user satisfaction metrics. <eos> we compare our results with both human and rule-based virtual instructors hand-coded for the same task.
natural language generators are faced with a multitude of different decisions during their generation process. <eos> we address the joint optimisation of navigation strategies and referring expressions in a situated setting with respect to task success and human-likeness. <eos> to this end, we present a novel, comprehensive framework that combines supervised learning, hierarchical reinforcement learning and a hierarchical information state. <eos> a human evaluation shows that our learnt instructions are rated similar to human instructions, and significantly better than the supervised learning baseline.
conventional speech recognition approaches usually wait until the user has finished talking before returning a recognition hypothesis. <eos> this results in spoken dialogue systems that are unable to react while the user is still speaking. <eos> incremental speech recognition ( isr ), where partial phrase results are returned during user speech, has been used to create more reactive systems. <eos> however, isr output is unstable and so prone to revision as more speech is decoded. <eos> this paper tackles the problem of stability in isr. <eos> we first present a method that increases the stability and accuracy of isr output, without adding delay. <eos> given that some revisions are unavoidable, we next present a pair of methods for predicting the stability and accuracy of isr results. <eos> taken together, we believe these approaches give isr more utility for real spoken dialogue systems.
we present the novel task of predicting temporal features of continuations of user input, while that input is still ongoing. <eos> we show that the remaining duration of an ongoing word, as well as the duration of the next can be predicted reasonably well, and we put this information to use in a system that synchronously completes a user ? s speech. <eos> while we focus on collaborative completions, the techniques presented here may also be useful for the alignment of back-channels and immediate turn-taking in an incremental sds, or to synchronously monitor the user ? s speech fluency for other reasons.
this paper provides a first assessment of a statistical dialog system in public use. <eos> in our dialog system there are four main recognition tasks, or slots ? <eos> bus route names, bus-stop locations, dates, and times. <eos> whereas a conventional system tracks a single value for each slot ? <eos> i.e., the speech recognizer ? s top hypothesis ? <eos> our statistical system tracks a distribution of many possible values over each slot. <eos> past work in lab studies has showed that this distribution improves robustness to speech recognition errors ; but to our surprise, we found the distribution yielded an increase in accuracy for only two of the four slots, and actually decreased accuracy in the other two. <eos> in this paper, we identify root causes for these differences in performance, including intrinsic properties of n-best lists, parameter settings, and the quality of statistical models. <eos> we synthesize our findings into a set of guidelines which aim to assist researchers and practitioners employing statistical techniques in future dialog systems.
generating temporal expressions ( te ) that are easy to understand, unambiguous, and reasonably short is a challenge for humans and spoken dialogue systems. <eos> rather than developing hand-written decision rules, we adopt a data-driven approach by collecting user feedback on a variety of possible tes in terms of task success, ambiguity, and user preference. <eos> the data collected in this work is freely available to the research community. <eos> these data were then used to train a simulated user and a reinforcement learning policy that learns an adaptive temporal expression generation strategy for a variety of contexts. <eos> we evaluate our learned policy both in simulation and with real users and show that this data-driven adaptive policy is a significant improvement over a rule-based adaptive policy, leading to a 24 % increase in perceived task completion, while showing a small increase in actual task completion, and a 16 % decrease in call duration. <eos> this means that dialogues are more efficient and that users are also more confident about the appointment that they have agreed with the system.
detecting levels of interest from speakers is a new problem in spoken dialog understanding with significant impact on real world business applications. <eos> previous work has focused on the analysis of traditional acoustic signals and shallow lexical features. <eos> in this paper, we present a novel hierarchical fusion learning model that takes feedback from previous multistream predictions of prominent seed samples into account and uses a mean cosine similarity measure to learn rules that improve reclassification. <eos> our method is domain-independent and can be adapted to other speech and language processing areas where domain adaptation is expensive to perform. <eos> incorporating discriminative term frequency and inverse document frequency ( dtfidf ), lexical affect scoring, and low and high level prosodic and acoustic features, our experiments outperform the published results of all systems participating in the 2010 interspeech paralinguistic affect subchallenge.
user satisfaction is a common evaluation metric in task-oriented dialogue systems, whereas tutorial dialogue systems are often evaluated in terms of student learning gain. <eos> however, user satisfaction is also important for such systems, since it may predict technology acceptance. <eos> we present a detailed satisfaction questionnaire used in evaluating the beetle ii system ( revu-nl ), and explore the underlying components of user satisfaction using factor analysis. <eos> we demonstrate interesting patterns of interaction between interpretation quality, satisfaction and the dialogue policy, highlighting the importance of more finegrained evaluation of user satisfaction.
in this work we describe the modeling and prediction of interaction quality ( iq ) in spoken dialogue systems ( sds ) using support vector machines. <eos> the model can be employed to estimate the quality of the ongoing interaction at arbitrary points in a spoken humancomputer interaction. <eos> we show that the use of 52 completely automatic features characterizing the system-user exchange significantly outperforms state-of-the-art approaches. <eos> the model is evaluated on publically available data from the cmu let ? s go bus information system. <eos> it reaches a performance of 61.6 % unweighted average recall when discriminating between 5 classes ( good to very poor ). <eos> it can be further shown that incorporating knowledge about the user ? s emotional state does hardly improve the performance.
sms dictation by voice is becoming a viable alternative providing a convenient method for texting in a variety of environments. <eos> contextual knowledge should be used to improve performance. <eos> we propose to add topic knowledge as part of the contextual awareness of both texting partners during sms conversations. <eos> topics can be used for speech applications, if the relation between the conversed topics and the choice of words in sms dialogs is measurable. <eos> in this study, we collected an sms corpus, developed a topic annotation scheme, and built a topic hierarchy in a tree structure. <eos> we validated our topic assignments and tree structure by the agglomerative information bottleneck method, which also proved the measurability of the interrelation between topics and wording. <eos> to quantify this relation we propose a na ? ve classification method based on the calculation of topic distinctive word lists and compare the classifiers ? <eos> topic recognition capabilities for sms dialogs with unigram language models. <eos> the results demonstrate that the relation between topic and wording is significant and can be integrated into sms dictation.
many discourse connectives can signal several types of relations between sentences. <eos> their automatic disambiguation, i.e. <eos> the labeling of the correct sense of each occurrence, is important for discourse parsing, but could also be helpful to machine translation. <eos> we describe new approaches for improving the accuracy of manual annotation of three discourse connectives ( two english, one french ) by using parallel corpora. <eos> an appropriate set of labels for each connective can be found using information from their translations. <eos> our results for automatic disambiguation are state-of-the-art, at up to 85 % accuracy using surface features. <eos> using feature analysis, contextual features are shown to be useful across languages and connectives.
we propose a method for modelling how dialogue moves influence and are influenced by the agents ? <eos> preferences. <eos> we extract constraints on preferences and dependencies among them, even when they are expressed indirectly, by exploiting discourse structure. <eos> our method relies on a study of 20 dialogues chosen at random from the verbmobil corpus. <eos> we then test the algorithms predictions against the judgements of naive annotators on 3 random unseen dialogues. <eos> the average annotator-algorithm agreement and the average inter-annotator agreement show that our method is reliable.
we present a method of evaluating the immediate performance impact of user state misclassifications in spoken dialogue systems. <eos> we illustrate the method with a tutoring system that adapts to student uncertainty over and above correctness. <eos> first we define a ranking of user states representing local performance. <eos> second, we compare user state trajectories when the first state is accurately classified versus misclassified. <eos> trajectories are quantified using a previously proposed metric representing the likelihood of transitioning from one user state to another. <eos> comparison of the two sets of trajectories shows whether user state misclassifications change the likelihood of subsequent higher or lower ranked states, relative to accurate classification. <eos> our tutoring system results illustrate the case where user state misclassification increases the likelihood of negative performance trajectories as compared to accurate classification.
instructional efficacy of automated conversational agents designed to help small groups of students achieve higher learning outcomes can be improved by the use of social interaction strategies. <eos> these strategies help the tutor agent manage the attention of the students while delivering useful instructional content. <eos> two technical challenges involving the use of social interaction strategies include determining the appropriate policy for triggering these strategies and regulating the amount of social behavior performed by the tutor. <eos> in this paper, a comparison of six different triggering policies is presented. <eos> we find that a triggering policy learnt from human behavior in combination with a filter that keeps the amount of social behavior comparable to that performed by human tutors offers the most effective solution to the these challenges.
mental modeling is crucial for natural humanrobot interactions ( hri ). <eos> yet, effective mechanisms that enable reasoning about and communication of mental states are not available. <eos> we propose to utilize adverbial cues, routinely employed by humans, for this goal and present a novel algorithm that integrates adverbial modifiers with belief revision and expression, phrasing utterances based on gricean conversational maxims. <eos> the algorithm is demonstrated in a simple hri scenario.
this paper presents a progressively challenging series of experiments that investigate clarification subdialogues to resolve the words in noisy transcriptions of user utterances. <eos> we focus on user utterances where the user ? s specific intent requires little additional inference, given sufficient understanding of the form. <eos> we learned decision-making strategies for a dialogue manager from run-time features of our spoken dialogue system and from observation of human wizards we had embedded within it. <eos> results show that noisy asr can be resolved based on predictions from context about what a user might say, and that dialogue management strategies for clarifications of linguistic form benefit from access to features from spoken language understanding.
this paper addresses a first step toward a spoken dialogue system that evokes user ? s spontaneous backchannels. <eos> we construct an hmm-based dialogue-style text-to-speech ( tts ) system that generates human-like cues that evoke users ? <eos> backchannels. <eos> a spoken dialogue system for information navigation was implemented and the tts was evaluated in terms of evoked user backchannels. <eos> we conducted user experiments and demonstrated that the user backchannels evoked by our tts are more informative for the system in detecting users ? <eos> feelings than those by conventional reading-style tts.
this paper reports on an experiment that investigates clarification subdialogues in intentionally noisy speech recognition. <eos> the architecture learns weights for mixtures of grounding strategies from examples provided by a human wizard embedded in the system. <eos> results indicate that the architecture learns to eliminate misunderstandings reliably despite high word error rate.
we present a novel annotation scheme for cross-cultural argumentation and persuasion dialogues. <eos> this scheme is an adaptation of existing coding schemes on negotiation, following a review of literature on cross-cultural differences in negotiation styles. <eos> the scheme has been refined through application to coding both two-party and multi-party negotiation dialogues in three different domains, and is general enough to be applicable to different domains with few if any extensions. <eos> dialogues annotated with the scheme have been used to successfully learn culture-specific dialogue policies for argumentation and persuasion.
we present an approach to performing automated evaluations of pipeline architectures in natural language dialogue systems. <eos> our approach addresses some of the difficulties that arise in such automated evaluations, including the lack of consensus among human annotators about the correct outputs within the processing pipeline, the availability of multiple acceptable system responses to some user utterances, and the complex relationship between system responses and internal processing results. <eos> our approach includes the development of a corpus of richly annotated target dialogues, simulations of the pipeline processing that could occur in these dialogues, and an analysis of how system responses vary based on internal processing results within the pipeline. <eos> we illustrate our approach in two implemented virtual human dialogue systems.
linguistic markers of personality traits have been studied extensively, but few crosscultural studies exist. <eos> in this paper, we evaluate how native speakers of american english and arabic perceive personality traits and naturalness of english utterances that vary along the dimensions of verbosity, hedging, lexical and syntactic alignment, and formality. <eos> the utterances are the turns within dialogue fragments that are presented as text transcripts to the workers of amazon ? s mechanical turk. <eos> the results of the study suggest that all four dimensions can be used as linguistic markers of all personality traits by both language communities. <eos> a further comparative analysis shows cross-cultural differences for some combinations of measures of personality traits and naturalness, the dimensions of linguistic variability and dialogue acts.
we present a new approach to dialogue management based on the use of multiple, interconnected policies. <eos> instead of capturing the complexity of the interaction in a single large policy, the dialogue manager operates with a collection of small local policies combined concurrently and hierarchically. <eos> the metacontrol of these policies relies on an activation vector updated before and after each turn.
world model framework designed to facilitate human-robot communication meghann lomas, e. vincent cross ii, jonathan darvill, r. christopher garrett, michael kopack, and kenneth whitebread lockheed martin advanced technology laboratories 3 executive campus, suite 600, cherry hill, nj 08002 1 856.792.9681 { mlomas, ecross, jdarvill, rgarrett, mkopack, kwhitebr } @ atl.lmco.com abstract we describe a novel world model frame-work designed to support situated human-robot communication through improved mutual knowledge about the physical world. <eos> this work focuses on enabling a robot to store and use semantic information from a human located in the same envi-ronment as the robot and respond using human-understandable terminology. <eos> this facilitates information sharing between a robot and a human and subsequently pro-motes team-based operations. <eos> herein, we present motivation for our world model, an overview of the world model, a discussion of proof-of-concept simulations, and future work.
within our ongoing effort to develop a computational model to understand multi-modal human dialogue in the field of elderly care, this paper focuses on pronominal and deictic co-reference resolution. <eos> after describing our data collection effort, we discuss our annotation scheme. <eos> we developed a co-reference model that employs both a simple notion of markable type, and multiple statistical models. <eos> our results show that knowing the type of the markable, and the presence of simultaneous pointing gestures improve co-reference resolution for personal and deictic pronouns.
many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users ? <eos> affect. <eos> previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models. <eos> thus, we wish to investigate how more minor differences, like small dialogue content changes and switching from a wizarded system to a fully automated system, influence the performance of our affect detection models. <eos> we perform a post-hoc experiment where we use various data sets to train multiple models, and compare against a test set from the most recent version of our dialogue system. <eos> analyzing these results strongly suggests that these differences do impact these models ? <eos> performance.
error-return plots show the rate of error ( misunderstanding ) against the rate of nonreturn ( non-understanding ) for natural language processing systems. <eos> they are a useful visual tool for judging system performance when other measures such as recall/precision and detection-error tradeoff are less informative, specifically when a system is judged on the correctness of its responses, but may elect to not return a response.
we apply a paradise-style evaluation to a human-human dialogue corpus that was collected to support the design of a spoken dialogue system for library transactions. <eos> the book request dialogue task we investigate is informational in nature : a book request is considered successful if the librarian is able to identify a specific book for the patron. <eos> paradise assumes that user satisfaction can be modeled as a regression over task success and dialogue costs. <eos> the paradise model we derive includes features that characterize two types of qualitative features. <eos> the first has to do with the specificity of the communicative goals, given a request for an item. <eos> the second has to do with the number and location of overlapping turns, which can sometimes signal rapport between the speakers.
the semantic annotation of dialogue corpora permits building efficient language understanding applications for supporting enjoyable and effective human-machine interactions. <eos> nevertheless, the annotation process could be costly, time-consuming and complicated, particularly the more expressive is the semantic formalism. <eos> in this work, we propose a bootstrapping architecture for the semantic annotation of dialogue corpora with rich structures, based on dependency syntax and frame semantics.
this paper describes an implemented monolingual text-to-text generation system. <eos> the system takes monologue and transforms it to two-participant dialogue. <eos> the system uses mappings between discourse relations in text and dialogue acts in dialogue. <eos> these mappings are extracted from a parallel monologue and dialogue corpus.
we present beetle ii, a tutorial dialogue system which accepts unrestricted language input and supports experimentation with different dialogue strategies. <eos> our first system evaluation compared two dialogue policies. <eos> the resulting corpus was used to study the impact of different tutoring and error recovery strategies on user satisfaction and student interaction style. <eos> it can also be used in the future to study a wide range of research issues in dialogue systems.
when a robot is situated in an environment containing multiple possible interaction partners, it has to make decisions about when to engage specific users and how to detect and react appropriately to actions of the users that might signal the intention to interact. <eos> in this demonstration we present the integration of an engagement model in an existing dialog system based on interaction patterns. <eos> as a sample scenario, this enables the humanoid robot nao to play a quiz game with multiple participants.
this demonstration will illustrate an interactive immersive computer game, pomy, designed to help korean speakers learn english. <eos> this system allows learners to exercise their visual and aural senses, receiving a full immersion experience to increase their memory and concentration abilities to a greatest extent. <eos> in pomy, learners can have free conversations with game characters and receive corrective feedback to their errors. <eos> game characters show various emotional expressions based on learners ? <eos> input to keep learners motivated. <eos> through this system, learners can repeatedly practice conversations in everyday life setting in a foreign language with no embarrassment.
we demonstrate a dialogue system and the accompanying authoring tools that are designed to allow authors with little or no experience in building dialogue systems to rapidly build advanced question-answering characters. <eos> to date seven such virtual characters have been built by non-experts using this architecture and tools. <eos> here we demonstrate one such character, pfc sean avery, which was developed by a non-expert in 3 months.
the automatic content linking device is a just-in-time document retrieval system that monitors an ongoing dialogue or monologue and enriches it with potentially related documents from local repositories or from the web. <eos> the documents are found using queries that are built from the dialogue words, obtained through automatic speech recognition. <eos> results are displayed in real time to the dialogue participants, or to people watching a recorded dialogue or a talk. <eos> the system can be demonstrated in both settings.
the workshop on statistical machine translation ( wmt ) has become one of acl ? s flagship workshops, held annually since 2006. <eos> in addition to soliciting papers from the research community, wmt also features a shared translation task for evaluating mt systems. <eos> this shared task is notable for having manual evaluation as its cornerstone. <eos> the workshop ? s overview paper, playing a descriptive and administrative role, reports the main results of the evaluation without delving deep into analyzing those results. <eos> the aim of this paper is to investigate and explain some interesting idiosyncrasies in the reported results, which only become apparent when performing a more thorough analysis of the collected annotations. <eos> our analysis sheds some light on how the reported results should ( and should not ) be interpreted, and also gives rise to some helpful recommendation for the organizers of wmt.
reordering is a major challenge for machine translation between distant languages. <eos> recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. <eos> here we present a simple framework for evaluating word order independently of lexical choice by comparing the system ? s reordering of a source sentence to reference reordering data generated from manually word-aligned translations. <eos> when used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming endto-end machine translation experiments. <eos> a novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. <eos> we show that how the alignments are generated can significantly effect the robustness of the evaluation. <eos> we also outline some ways in which this framework has allowed our group to analyze reordering errors for english to japanese machine translation.
this paper presents the results of the wmt11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. <eos> we conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. <eos> we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. <eos> this year featured a haitian creole to english task translating sms messages sent to an emergency response service in the aftermath of the haitian earthquake. <eos> we also conducted a pilot ? tunable metrics ? <eos> task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.
we present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence. <eos> the system employs a statistical classifier trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences. <eos> development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a pcfg parser.
this paper proposes a new automatic machine translation evaluation metric : amber, which is based on the metric bleu but incorporates recall, extra penalties, and some text processing variants. <eos> there is very little linguistic information in amber. <eos> we evaluate its system-level correlation and sentence-level consistency scores with human rankings from the wmt shared evaluation task ; amber achieves state-of-the-art performance.
this paper describes the submission from the national university of singapore to the wmt 2011 shared evaluation task and the tunable metric task. <eos> our entry is tesla in three different configurations : tesla-m, tesla-f, and the new tesla-b.
this paper describes meteor 1.3, our submission to the 2011 emnlp workshop on statistical machine translation automatic evaluation metric tasks. <eos> new metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. <eos> we include ranking and adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced tuning version shown to outperform bleu in minimum error rate training for a phrase-based urdu-english system.
sempos is an automatic metric of machine translation quality for czech and english focused on content words. <eos> it correlates well with human judgments but it is computationally costly and hard to adapt to other languages because it relies on a deep-syntactic analysis of the system output and the reference. <eos> to remedy this, we attempt at approximating sempos using only tagger output and a few heuristics. <eos> at a little expense in correlation to human judgments, we can evaluate mt systems much faster. <eos> additionally, we describe our submission to the tunable metrics task in wmt11.
current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations. <eos> we propose a truly automatic evaluation metric based on ibm1 lexicon probabilities which does not need any reference translations. <eos> several variants of ibm1 scores are systematically explored in order to find the most promising directions. <eos> correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the statistical machine translation workshop. <eos> five different european languages are taken into account : english, spanish, french, german and czech. <eos> the results show that the ibm1 scores are competitive with the classic evaluation metrics, the most promising being ibm1 scores calculated on morphemes and pos-4grams.
we propose the use of morphemes for automatic evaluation of machine translation output, and systematically investigate a set of f score and bleu score based metrics calculated on words, morphemes and pos tags along with all corresponding combinations. <eos> correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the statistical machine translation workshop. <eos> machine translation outputs in five different european languages are used : english, spanish, french, german and czech. <eos> the results show that the f scores which take into account morphemes and pos tags are the most promising metrics.
we describe our submissions to the wmt11 shared mt evaluation task : mterater and mterater-plus. <eos> both are machine-learned metrics that use features from e-rater r ?, an automated essay scoring engine designed to assess writing proficiency. <eos> despite using only features from e-rater and without comparing to translations, mterater achieves a sentencelevel correlation with human rankings equivalent to bleu. <eos> since mterater only assesses fluency, we build a meta-metric, mteraterplus, that incorporates adequacy by combining mterater with other mt evaluation metrics and heuristics. <eos> this meta-metric has a higher correlation with human rankings than either mterater or individual mt metrics alone. <eos> however, we also find that e-rater features may not have significant impact on correlation in every case.
we describe tine, a new automatic evaluation metric for machine translation that aims at assessing segment-level adequacy. <eos> lexical similarity and shallow-semantics are used as indicators of adequacy between machine and reference translations. <eos> the metric is based on the combination of a lexical matching component and an adequacy component. <eos> lexical matching is performed comparing bagsof-words without any linguistic annotation. <eos> the adequacy component consists in : i ) using ontologies to align predicates ( verbs ), ii ) using semantic roles to align predicate arguments ( core arguments and modifiers ), and iii ) matching predicate arguments using distributional semantics. <eos> tine ? s performance is comparable to that of previous metrics at segment level for several language pairs, with average kendall ? s tau correlation from 0.26 to 0.29. <eos> we show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as bleu.
automatic evaluation metrics are fundamentally important for machine translation, allowing comparison of systems performance and efficient training. <eos> current evaluation metrics fall into two classes : heuristic approaches, like bleu, and those using supervised learning trained on human judgement data. <eos> while many trained metrics provide a better match against human judgements, this comes at the cost of including lots of features, leading to unwieldy, non-portable and slow metrics. <eos> in this paper, we introduce a new trained metric, rose, which only uses simple features that are easy portable and quick to compute. <eos> in addition, rose is sentence-based, as opposed to document-based, allowing it to be used in a wider range of settings. <eos> results show that rose performs well on many tasks, such as ranking system and syntactic constituents, with results competitive to bleu. <eos> moreover, this still holds when rose is trained on human judgements of translations into a different language compared with that use in testing.
the past few years have seen an increasing interest in using amazon ? s mechanical turk for purposes of collecting data and performing annotation tasks. <eos> one such task is the mass evaluation of system output in a variety of tasks. <eos> in this paper, we present maise, a package that allows researchers to evaluate the output of their ai system ( s ) using human judgments collected via amazon ? s mechanical turk, greatly streamlining the process. <eos> maise is open source, easy to run, and platform-independent. <eos> the core of maise ? s codebase was used for the manual evaluation of wmt10, and the completed package is being used again in the current evaluation for wmt11. <eos> in this paper, we describe the main features, functionality, and usage of maise, which is now available for download and use.
this paper describes the development operated into many for the 2011 wmt system combination evaluation campaign. <eos> hypotheses from french/english and english/french mt systems were combined with a new version of many, an open source system combination software based on confusion networks decoding currently developed at lium. <eos> many has been updated in order to optimize decoder parameters with mert, which proves to find better weights. <eos> the system combination yielded significant improvements in bleu score when applied on system combination data from two languages.
this paper presents the submissions of the pattern recognition and human language technology ( prhlt ) group to the system combination task of the sixth workshop on statistical machine translation ( wmt 2011 ). <eos> each submissions is generated by a multi-system minimum bayes risk ( mbr ) technique. <eos> our technique uses the mbr decision rule and a linear combination of the component systems ? <eos> probability distributions to search for the minimum risk translation among all the sentences in the target language.
this paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 workshop on machine translation ? s system combination task. <eos> we show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments. <eos> humans judged our combination the best on eight of ten tracks.
rwth participated in the system combination task of the sixth workshop on statistical machine translation ( wmt 2011 ). <eos> for three language pairs, we combined 6 to 14 systems into a single consensus translation. <eos> a three-level metacombination scheme combining six different system combination setups with three different engines was applied on the french ? english language pair. <eos> depending on the language pair, improvements versus the best single system are in the range of +1.9 % and +2.5 % abs. <eos> on bleu, and between ? 1.8 % and ? 2.4 % abs. <eos> on ter. <eos> novel techniques compared with rwth ? s submission to wmt 2010 include two additional system combination engines, an additional word alignment technique, meta combination, and additional optimization techniques.
bbn submitted system combination outputs for czech-english, german-english, spanishenglish, and french-english language pairs. <eos> all combinations were based on confusion network decoding. <eos> the confusion networks were built using incremental hypothesis alignment algorithm with flexible matching. <eos> a novel bi-gram count feature, which can penalize bi-grams not present in the input hypotheses corresponding to a source sentence, was introduced in addition to the usual decoder features. <eos> the system combination weights were tuned using a graph based expected bleu as the objective function while incrementally expanding the networks to bi-gram and 5-gram contexts. <eos> the expected bleu tuning described in this paper naturally generalizes to hypergraphs and can be used to optimize thousands of weights. <eos> the combination gained about 0.5-4.0 bleu points over the best individual systems on the official wmt11 language pairs. <eos> a 39 system multisource combination achieved an 11.1 bleu point gain.
this paper describes the uzh system that was used for the wmt 2011 system combination shared task submission. <eos> we participated in the system combination task for the translation directions de ? en and en ? de. <eos> the system uses moses as a backbone, with the outputs of the 2 ? 3 best individual systems being integrated through additional phrase tables. <eos> the system compares well to other system combination submissions, with no other submission being significantly better. <eos> a bleu-based comparison to the individual systems, however, indicates that it achieves no significant gains over the best individual system.
this paper describes the jhu system combination scheme used in wmt-11. <eos> the jhu system combination is based on confusion network alignment, and inherited the framework developed by ( karakos et al, 2008 ). <eos> we improved our core system combination algorithm by making use of ter-plus, which was originally designed for string alignment, for alignment of confusion networks. <eos> experimental results on french-english, germanenglish, czech-english and spanish-english combination tasks show significant improvements on bleu and ter by up to 2 points on average, compared to the best individual system output, and improvements compared with the results produced by itg which we used in wmt-10.
we consider using online language models for translating multiple streams which naturally arise on the web. <eos> after establishing that using just one stream can degrade translations on different domains, we present a series of simple approaches which tackle the problem of maintaining translation performance on all streams in small space. <eos> by exploiting the differing throughputs of each stream and how the decoder translates prior test points from each stream, we show how translation performance can equal specialised, per-stream language models, but do this in a single language model using far less space. <eos> our results hold even when adding three billion tokens of additional text as a background language model.
we present kenlm, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. <eos> the probing data structure uses linear probing hash tables and is designed for speed. <eos> compared with the widelyused srilm, our probing model is 2.4 times as fast while using 57 % of the memory. <eos> the trie data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. <eos> trie simultaneously uses less memory than the smallest lossless baseline and less cpu than the fastest baseline. <eos> our code is open-source1, thread-safe, and integrated into the moses, cdec, and joshua translation systems. <eos> this paper describes the several performance techniques used and presents benchmarks against alternative implementations.
in past evaluations for machine translation of european languages, it could be shown that the translation performance of smt systems can be increased by integrating a bilingual language model into a phrase-based smt system. <eos> in the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model. <eos> we analyzed the effect of bilingual language models and show where they could help to better model the translation process. <eos> we could show improvements of translation quality on german-to-english and arabic-to-english. <eos> in addition, for the arabic-to-english task, training an extra bilingual language model on the pos tags instead of the surface word forms led to further improvements.
we describe an approach for generating a ranked list of candidate document translation pairs without the use of bilingual dictionary or machine translation system. <eos> we developed this approach as an initial, filtering step, for extracting parallel text from large, multilingual ? but non-parallel ? <eos> corpora. <eos> we represent bilingual documents in a vector space whose basis vectors are the overlapping tokens found in both languages of the collection. <eos> using this representation, weighted by tf ? idf, we compute cosine document similarity to create a ranked list of candidate document translation pairs. <eos> unlike cross-language information retrieval, where a ranked list in the target language is evaluated for each source query, we are interested in, and evaluate, the more difficult task of finding translated document pairs. <eos> we first perform a feasibility study of our approach on parallel collections in multiple languages, representing multiple language families and scripts. <eos> the approach is then applied to a large bilingual collection of around 800k books. <eos> to avoid the computational cost of ) ( 2no document pair comparisons, we employ locality sensitive hashing ( lsh ) approximation algorithm for cosine similarity, which reduces our time complexity to ) log ( nno.
languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. <eos> to address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. <eos> by integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. <eos> we use a simple heuristic process to extract agreement constraints for german and test our approach on an english-german system trained on wmt data, achieving a small improvement in translation accuracy as measured by bleu.
the quality of arabic-english statistical machine translation often suffers as a result of standard phrase-based smt systems ? <eos> inability to perform long-range re-orderings, specifically those needed to translate vso-ordered arabic sentences. <eos> this problem is further exacerbated by the low performance of arabic parsers on subject and subject span detection. <eos> in this paper, we present two parse ? fuzzification ? <eos> techniques which allow the translation system to select among a range of possible s ? v re-orderings. <eos> with this approach, we demonstrate a 0.3-point improvement in bleu score ( 69 % of the maximum possible using gold parses ), and a corresponding improvement in the percentage of syntactically well-formed subjects under a manual evaluation.
paraphrases are useful for statistical machine translation ( smt ) and natural language processing tasks. <eos> distributional paraphrase generation is independent of parallel texts and syntactic parses, and hence is suitable also for resource-poor languages, but tends to erroneously rank antonyms, trend-contrasting, and polarity-dissimilar candidates as good paraphrases. <eos> we present here a novel method for improving distributional paraphrasing by filtering out such candidates. <eos> we evaluate it in simulated low and mid-resourced smt tasks, translating from english to two quite different languages. <eos> we show statistically significant gains in english-to-chinese translation quality, up to 1 bleu from nonfiltered paraphrase-augmented models ( 1.6 bleu from baseline ). <eos> we also show that yielding gains in translation to arabic, a morphologically rich language, is not straightforward.
in many languages the use of compound words is very productive. <eos> a common practice to reduce sparsity consists in splitting compounds in the training data. <eos> when this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. <eos> furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. <eos> we present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. <eos> we also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources.
statistical machine translation systems are normally optimised for a chosen gain function ( metric ) by using mert to find the best model weights. <eos> this algorithm suffers from stability problems and can not scale beyond 20-30 features. <eos> we present an alternative algorithm for discriminative training of phrasebasedmt systems, samplerank, which scales to hundreds of features, equals or beats mert on both small and medium sized systems, and permits the use of sentence or document level features. <eos> samplerank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function.
we present an empirical study of instance selection techniques for machine translation. <eos> in an active learning setting, instance selection minimizes the human effort by identifying the most informative sentences for translation. <eos> in a transductive learning setting, selection of training instances relevant to the test set improves the final translation quality. <eos> after reviewing the state of the art in the field, we generalize the main ideas in a class of instance selection algorithms that use feature decay. <eos> feature decay algorithms increase diversity of the training set by devaluing features that are already included. <eos> we show that the feature decay rate has a very strong effect on the final translation quality whereas the initial feature values, inclusion of higher order features, or sentence length normalizations do not. <eos> we evaluate the best instance selection methods using a standard moses baseline using the whole 1.6 million sentence english-german section of the europarl corpus. <eos> we show that selecting the best 3000 training sentences for a specific test sentence is sufficient to obtain a score within 1 bleu of the baseline, using 5 % of the training data is sufficient to exceed the baseline, and a ? <eos> 2 bleu improvement over the baseline is possible by optimally selected subset of the training data. <eos> in out-of-domain translation, we are able to reduce the training set size to about 7 % and achieve a similar performance with the baseline.
most of the freely available parallel data to train the translation model of a statistical machine translation system comes from very specific sources ( european parliament, united nations, etc ). <eos> therefore, there is increasing interest in methods to perform an adaptation of the translation model. <eos> a popular approach is based on unsupervised training, also called self-enhancing. <eos> both only use monolingual data to adapt the translation model. <eos> in this paper we extend the previous work and provide new insight in the existing methods. <eos> we report results on the translation between french and english. <eos> improvements of up to 0.5 bleu were observed with respect to a very competitive baseline trained on more than 280m words of human translated parallel data.
this work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing probabilistic latent semantic analysis ( plsa ) during model training. <eos> during inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language ? s vocabulary, from which we perform minimum discrimination information ( mdi ) adaptation on a background language model ( lm ). <eos> we apply our approach on the english-french iwslt 2010 ted talk exercise, and report a 15 % reduction in perplexity and relative bleu and nist improvements of 3 % and 2.4 %, respectively over a baseline only using a 5-gram background lm over the entire translation task. <eos> our topic modeling approach is simpler to construct than its counterparts.
this paper describes limsi ? s submissions to the sixth workshop on statistical machine translation. <eos> we report results for the frenchenglish and german-english shared translation tasks in both directions. <eos> our systems use n-code, an open source statistical machine translation system based on bilingual n-grams. <eos> for the french-english task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. <eos> in particular, using a simple filtering strategy helped to improve both processing time and translation quality. <eos> to translate from english to french and german, we also investigated the use of the soul language model in machine translation and showed significant improvements with a 10-gram soul model. <eos> we also briefly report experiments with several alternatives to the standard n-best mert procedure, leading to a significant speed-up.
we present a translation model enriched with shallow syntactic and semantic information about the source language. <eos> base-phrase labels and semantic role labels are incorporated into an hierarchical model by creating shallow semantic ? trees ?. <eos> results show an increase in performance of up to 6 % in bleu scores for english-spanish translation over a standard phrase-based smt baseline.
we present the results we obtain using our regmt system, which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs. <eos> our training instance selection methods perform feature decay for proper selection of training instances, which plays an important role to learn correct feature mappings. <eos> regmt uses l2 regularized regression as well as l1 regularized regression for sparse regression estimation of target features. <eos> we present translation results using our training instance selection methods, translation results using graph decoding, system combination results with regmt, and performance evaluation with the f1 measure over target features as a metric for evaluating translation quality.
we use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation. <eos> this method called ? reverse self-training ? <eos> improves the decoder ? s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. <eos> in small-data setting. <eos> we empirically evaluate the gains for several pairs of european languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words. <eos> we also provide a description of the systems we submitted to wmt11 shared task.
this paper describes the german-english translation system developed by the ark research group at carnegie mellon university for the sixth workshop on machine translation ( wmt11 ). <eos> we present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including : feature engineering to improve modeling of the derivation structure of translations ; better handing of oovs ; and using development set translations into other languages to create additional pseudoreferences for training.
this paper presents the system we developed for the 2011 wmt haitian creole ? english sms featured translation task. <eos> applying standard statistical machine translation methods to noisy real-world sms data in a low-density language setting such as haitian creole poses a unique set of challenges, which we attempt to address in this work. <eos> along with techniques to better exploit the limited available training data, we explore the benefits of several methods for alleviating the additional noise inherent in the sms and transforming it to better suite the assumptions of our hierarchical phrase-based model system. <eos> we show that these methods lead to significant improvements in bleu score over the baseline.
in this paper we describe our hybrid machine translation system with which we participated in the wmt11 shared translation task for the english ? german language pair. <eos> our system was able to outperform its rbmt baseline and turned out to be the best-scored participating system in the manual evaluation. <eos> to achieve this, we extended an existing, rule-based mt system with a module for stochastic selection of analysis parse trees that allowed to better cope with parsing errors during the system ? s analysis phase. <eos> due to the integration into the analysis phase of the rbmt engine, we are able to preserve the benefits of a rule-based translation system such as proper generation of target language text. <eos> additionally, we used a statistical tool for terminology extraction to improve the lexicon of the rbmt system. <eos> we report results from both automated metrics and human evaluation efforts, including examples which show how the proposed approach can improve machine translation quality.
we present the carnegie mellon university stat-xfer group submission to the wmt 2011 shared translation task. <eos> we built a hybrid syntactic mt system for french ? english using the joshua decoder and an automatically acquired scfg. <eos> new work for this year includes training data selection and grammar filtering. <eos> expanded training data selection significantly increased translation scores and lowered oov rates, while results on grammar filtering were mixed.
this paper describes the phrase-based smt systems developed for our participation in the wmt11 shared translation task. <eos> translations for english ? german and english ? french were generated using a phrase-based translation system which is extended by additional models such as bilingual and fine-grained pos language models, pos-based reordering, lattice phrase extraction and discriminative word alignment. <eos> furthermore, we present a special filtering method for the english-french giga corpus and the phrase scoring step in the training is parallelized.
this paper describes the statistical machine translation system submitted to the wmt11 featured translation task, which involves translating haitian creole sms messages into english. <eos> in our experiments we try to address the issue of noise in the training data, as well as the lack of parallel training data. <eos> spelling normalization is applied to reduce out-of-vocabulary words in the corpus. <eos> using semantic role labeling rules we expand the available training corpus. <eos> additionally we investigate extracting parallel sentences from comparable data to enhance the available parallel data.
this paper presents the liu system for the wmt 2011 shared task for translation between german and english. <eos> for english ? <eos> german we attempted to improve the translation tables with a combination of standard statistical word alignments and phrase-based word alignments. <eos> for german ? english translation we tried to make the german text more similar to the english text by normalizing german morphology and performing rule-based clause reordering of the german text. <eos> this resulted in small improvements for both translation directions.
monotrans2 is a translation system that combines machine translation ( mt ) with human computation using two crowds of monolingual source ( haitian creole ) and target ( english ) speakers. <eos> we report on its use in the wmt 2011 haitian creole to english translation task, showing that monotrans2 translated 38 % of the sentences well compared to google translate ? s 25 %.
in this paper we describe the institute for logic, language and computation ( university of amsterdam ) phrase-based statistical machine translation system for englishto-german translation proposed within the emnlp-wmt 2011 shared task. <eos> the main novelty of the submitted system is a syntaxdriven pre-translation reordering algorithm implemented as source string permutation via transfer of the source-side syntax tree.
this paper describes the upm system for translation task at the emnlp 2011 workshop on statistical machine translation ( http : //www.statmt.org/wmt11/ ), and it has been used for both directions : spanish-english and english-spanish. <eos> this system is based on moses with two new modules for pre and post processing the sentences. <eos> the main contribution is the method proposed ( based on the similarity with the source language test set ) for selecting the sentences for training the models and adjusting the weights. <eos> with system, we have obtained a 23.2 bleu for spanish-english and 21.7 bleu for englishspanish.
this paper describes an experiment in which we try to automatically correct mistakes in grammatical agreement in english to czech mt outputs. <eos> we perform several rule-based corrections on sentences parsed to dependency trees. <eos> we prove that it is possible to improve the mt quality of majority of the systems participating in wmt shared task. <eos> we made both automatic ( bleu ) and manual evaluations.
accuracy of dependency parsers is one of the key factors limiting the quality of dependencybased machine translation. <eos> this paper deals with the influence of various dependency parsing approaches ( and also different training data size ) on the overall performance of an english-to-czech dependency-based statistical translation system implemented in the treex framework. <eos> we also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of bleu.
we describe our system for the news commentary translation task of wmt 2011. <eos> the submitted run for the french-english direction is a combination of two moses-based systems developed at lig and lia laboratories. <eos> we report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and rover to combine n-best list of hypotheses output by different systems.
this work describes the haitian-cre ? ole to english statistical machine translation system built by barcelona media innovation center ( bm ) and institute for infocomm research ( i2r ) for the 6th workshop on statistical machine translation ( wmt 2011 ). <eos> our system carefully processes the available data and uses it in a standard phrase-based system enhanced with a source context semantic feature that helps conducting a better lexical selection and a feature orthogonalization procedure that helps making mert optimization more reliable and stable. <eos> our system was ranked first ( among a total of 9 participant systems ) by the conducted human evaluation.
this paper describes the machine translation ( mt ) system developed by the transducens research group, from universitat d ? alacant, spain, for the wmt 2011 shared translation task. <eos> we submitted a hybrid system for the spanish ? english language pair consisting of a phrase-based statistical mt system whose phrase table was enriched with bilingual phrase pairs matching transfer rules and dictionary entries from the apertium shallowtransfer rule-based mt platform. <eos> our hybrid system outperforms, in terms of bleu, gtm and meteor, a standard phrase-based statistical mt system trained on the same corpus, and received the second best bleu score in the automatic evaluation.
this paper describes the development of french ? english and english ? french statistical machine translation systems for the 2011 wmt shared task evaluation. <eos> our main systems were standard phrase-based statistical systems based on the moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems. <eos> additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words.
we report results on translation of sms messages from haitian creole to english. <eos> we show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents. <eos> we also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora.
we present progress on joshua, an opensource decoder for hierarchical and syntaxbased machine translation. <eos> the main focus is describing thrax, a flexible, open source synchronous context-free grammar extractor. <eos> thrax extracts both hierarchical ( chiang, 2007 ) and syntax-augmented machine translation ( zollmann and venugopal, 2006 ) grammars. <eos> it is built on apache hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats.
we present the dfki hybrid translation system at the wmt workshop 2011. <eos> three smt and two rbmt systems are combined at the level of the final translation output. <eos> the translation results show that our hybrid system significantly outperformed individual systems by exploring strengths of both rule-based and statistical translations.
this paper describes the system presented for the english-spanish translation task by the collaboration between ceu-uch and upv for 2011 wmt. <eos> a comparison of independent phrase-based translation models interpolation for each available training corpora were tested, giving an improvement of 0.4 bleu points over the baseline. <eos> output n -best lists were rescored via a target neural network language model. <eos> an improvement of one bleu point over the baseline was obtained adding the two features, giving 31.5 bleu and 57.9 ter for the primary system, computed over lowercased and detokenized outputs. <eos> the system was positioned second in the final ranking.
we describe our experiments with hierarchical phrase-based machine translation for the wmt 2011 shared task. <eos> we trained a system for all 8 translation directions between english on one side and czech, german, spanish or french on the other side, though we focused slightly more on the english-to-czech direction. <eos> we provide a detailed description of our configuration and data so the results are replicable.
in this paper, we propose that mt is an important technology in crisis events, something that can and should be an integral part of a rapid-response infrastructure. <eos> by integrating mt services directly into a messaging infrastructure ( whatever the type of messages being serviced, e.g., text messages, twitter feeds, blog postings, etc. <eos> ), mt can be used to provide first pass translations into a majority language, which can be more effectively triaged and then routed to the appropriate aid agencies. <eos> if done right, mt can dramatically increase the speed by which relief can be provided. <eos> to ensure that mt is a standard tool in the arsenal of tools needed in crisis events, we propose a preliminary crisis cookbook, the contents of which could be translated into the relevant language ( s ) by volunteers immediately after a crisis event occurs. <eos> the resulting data could then be made available to relief groups on the ground, as well as to providers of mt services. <eos> we also note that there are significant contributions that our community can make to relief efforts through continued work on our research, especially that research which makes mt more viable for under-resourced languages.
a growing body of machine translation research aims to exploit lexical patterns ( e.g., ngrams and phrase pairs ) with gaps ( simard et al., 2005 ; chiang, 2005 ; xiong et al, 2011 ). <eos> typically, these ? gappy patterns ? <eos> are discovered using heuristics based on word alignments or local statistics such as mutual information. <eos> in this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. <eos> we exploit bayesian nonparametrics and collapsed gibbs sampling to discover salient patterns in a corpus. <eos> we evaluate the patterns qualitatively and also add them as features to an mt system, reporting promising preliminary results.
we provide a general algorithmic schema for translation rule extraction and show that several popular extraction methods ( including phrase pair extraction, hierarchical phrase pair extraction, and ghkm extraction ) can be viewed as specific instances of this schema. <eos> this work is primarily intended as a survey of the dominant extraction paradigms, in which we make explicit the close relationship between these approaches, and establish a language for future hybridizations. <eos> this facilitates a generic and extensible implementation of alignment-based extraction methods.
we present a novel approach for extracting a minimal synchronous context-free grammar ( scfg ) for hiero-style statistical machine translation using a non-parametric bayesian framework. <eos> our approach is designed to extract rules that are licensed by the word alignments and heuristically extracted phrase pairs. <eos> our bayesian model limits the number of scfg rules extracted, by sampling from the space of all possible hierarchical rules ; additionally our informed prior based on the lexical alignment probabilities biases the grammar to extract high quality rules leading to improved generalization and the automatic identification of commonly re-used rules. <eos> we show that our bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the bleu score.
a major weakness of extant statistical machine translation ( smt ) systems is their lack of a proper training procedure. <eos> phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many. <eos> in this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures. <eos> the tractability of the whole enterprise is achieved through an efficient implementation of the conditional random fields ( crfs ) model using a weighted finite-state transducers library. <eos> this approach is experimentally contrasted with several conventional phrase-based systems.
we present a nonparametric bayesian approach to extract a structured database of entities from text. <eos> neither the number of entities nor the fields that characterize each entity are provided in advance ; the only supervision is a set of five prototype examples. <eos> our method jointly accomplishes three tasks : ( i ) identifying a set of canonical entities, ( ii ) inferring a schema for the fields that describe each entity, and ( iii ) matching entities to their references in raw text. <eos> empirical evaluation shows that the approach learns an accurate database of entities and a sensible model of name structure.
cross-lingual lexical substitution ( clls ) is the task that aims at providing for a target word in context, several alternative substitute words in another language. <eos> the proposed sets of translations may come from external resources or be extracted from textual data. <eos> in this paper, we apply for the first time an unsupervised cross-lingual wsd method to this task. <eos> the method exploits the results of a cross-lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity. <eos> we evaluate the impact of using clustering information for clls by applying the wsd method to the semeval-2010 clls data set. <eos> our system performs better on the ? out-of-ten ? <eos> measure than the systems that participated in the semeval task, and is ranked medium on the other measures. <eos> we analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting.
the unsupervised data oriented parsing ( udop ) approach has been repeatedly reported to achieve state of the art performance in experiments on parsing of different corpora. <eos> at the same time the approach is demanding both in computation time and memory. <eos> this paper describes an approach which decreases these demands. <eos> first the problem is translated into the generation of probabilistic bottom up tree automata ( pbta ). <eos> then it is explained how solving two standard problems for these automata results in a reduction in the size of the grammar. <eos> the reduction of the grammar size by using efficient algorithms for pbtas is the main contribution of this paper. <eos> experiments suggest that this leads to a reduction in grammar size by a factor of 2. <eos> this paper also suggests some extensions of the original udop algorithm that are made possible or aided by the use of tree automata.
the development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research. <eos> the primary advantage of these methods is that they do not require annotated data to learn a model. <eos> however, this advantage makes them difficult to evaluate against a manually labeled gold standard. <eos> using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. <eos> instead, we argue that the rarely used in-context evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. <eos> finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.
the automatic generation of entity profiles from unstructured text, such as knowledge base population, if applied in a multi-lingual setting, generates the need to align such profiles from multiple languages in an unsupervised manner. <eos> this paper describes an unsupervised and language-independent approach to mine name translation pairs from entity profiles, using wikipedia infoboxes as a stand-in for high quality entity profile extraction. <eos> pairs are initially found using expressions that are written in language-independent forms ( such as dates and numbers ), and new translations are then mined from these pairs. <eos> the algorithm then iteratively bootstraps from these translations to learn more pairs and more translations. <eos> the algorithm maintains a high precision, over 95 %, for the majority of its iterations, with a slightly lower precision of 85.9 % and an f-score of 76 %. <eos> a side effect of the name mining algorithm is the unsupervised creation of a translation lexicon between the two languages, with an accuracy of 64 %. <eos> we also duplicate three state-of-the-art name translation mining methods and use two existing name translation gazetteers to compare with our approach. <eos> comparisons show our approach can effectively augment the results from each of these alternative methods and resources.
there is high demand for automated tools that assign polarity to microblog content such as tweets ( twitter posts ), but this is challenging due to the terseness and informality of tweets in addition to the wide variety and rapid evolution of language in twitter. <eos> it is thus impractical to use standard supervised machine learning techniques dependent on annotated training examples. <eos> we do without such annotations by using label propagation to incorporate labels from a maximum entropy classifier trained on noisy labels and knowledge about word types encoded in a lexicon, in combination with the twitter follower graph. <eos> results on polarity classification for several datasets show that our label propagation approach rivals a model supervised with in-domain annotated tweets, and it outperforms the noisily supervised classifier it exploits as well as a lexicon-based polarity ratio classifier.
in this paper, we give a treatment to the problem of bilingual part-of-speech induction with parallel data. <eos> we demonstrate that na ? <eos> ? ve optimization of log-likelihood with joint mrfs suffers from a severe problem of local maxima, and suggest an alternative ? <eos> using contrastive estimation for estimation of the parameters. <eos> our experiments show that estimating the parameters this way, using overlapping features with joint mrfs performs better than previous work on the 1984 dataset.
training efficient statistical approaches for natural language understanding generally requires data with segmental semantic annotations. <eos> unfortunately, building such resources is costly. <eos> in this paper, we propose an approach that produces annotations in an unsupervised way. <eos> the first step is an implementation of latent dirichlet alocation that produces a set of topics with probabilities for each topic to be associated with a word in a sentence. <eos> this knowledge is then used as a bootstrap to infer a segmentation of a word sentence into topics using either integer linear optimisation or stochastic word alignment models ( ibm models ) to produce the final semantic annotation. <eos> the relation between automaticallyderived topics and task-dependent concepts is evaluated on a spoken dialogue task with an available reference annotation.
the amount of data produced in usergenerated content continues to grow at a staggering rate. <eos> however, the text found in these media can deviate wildly from the standard rules of orthography, syntax and even semantics and present significant problems to downstream applications which make use of this noisy data. <eos> in this paper we present a novel unsupervised method for extracting domainspecific lexical variants given a large volume of text. <eos> we demonstrate the utility of this method by applying it to normalize text messages found in the online social media service, twitter, into their most likely standard english versions. <eos> our method yields a 20 % reduction in word error rate over an existing state-of-theart approach.
in this paper we apply lightly-supervised training to a hierarchical phrase-based statistical machine translation system. <eos> we employ bitexts that have been built by automatically translating large amounts of monolingual data as additional parallel training corpora. <eos> we explore different ways of using this additional data to improve our system. <eos> our results show that integrating a second translation model with only non-hierarchical phrases extracted from the automatically generated bitexts is a reasonable approach. <eos> the translation performance matches the result we achieve with a joint extraction on all training bitexts while the system is kept smaller due to a considerably lower overall number of phrases.
recent years ? <eos> most efficient approaches for language understanding are statistical. <eos> these approaches benefit from a segmental semantic annotation of corpora. <eos> to reduce the production cost of such corpora, this paper proposes a method that is able to match first identified concepts with word sequences in an unsupervised way. <eos> this method based on automatic alignment is used by an understanding system based on conditional random fields and is evaluated on a spoken dialogue task using either manual or automatic transcripts.
resolving ambiguity associated with names found on the web, wikipedia or medical texts is a very challenging task, which has been of great interest to the research community. <eos> we propose a novel approach to disambiguating names using latent dirichlet allocation, where the learned topics represent the underlying senses of the ambiguous name. <eos> we conduct a detailed evaluation on multiple data sets containing ambiguous person, location and organization names and for multiple languages such as english, spanish, romanian and bulgarian. <eos> we conduct comparative studies with existing approaches and show a substantial improvement of 15 to 35 % in task accuracy.
word sense induction ( wsi ) is an unsupervised learning approach to discovering the different senses of a word from its contextual uses. <eos> a core challenge to wsi approaches is distinguishing between related and possibly similar senses of a word. <eos> current wsi evaluation techniques have yet to analyze the specific impact of similarity on accuracy. <eos> therefore, we present a new wsi evaluation that quantifies the relationship between the relatedness of a word ? s senses and the ability of a wsi algorithm to distinguish between them. <eos> furthermore, we perform an analysis on sense confusions in semeval-2 wsi task according to sense similarity. <eos> both analyses for a representative selection of clustering-based wsi approaches reveals that performance is most sensitive to the clustering algorithm and not the lexical features used.
aphasia treatment for the recovery of lost communication functionalities is possible through frequent and intense speech therapy sessions. <eos> in this sense, speech and language technology may provide important support in improving the recovery process. <eos> the aim of the project vithea ( virtual therapist for aphasia treatment ) is to develop an on-line system designed to behave as a virtual therapist, guiding the patient in performing training exercises in a simple and intuitive fashion. <eos> in this paper, the fundamental components of the vithea system are presented, with particular emphasis on the speech recognition module. <eos> furthermore, we report encouraging automatic word naming recognition results using data collected from speech therapy sessions.
this paper describes modifications to acoustic speech signals produced by speakers with dysarthria in order to make those utterances more intelligible to typical listeners. <eos> these modifications include the correction of tempo, the adjustment of formant frequencies in sonorants, the removal of aberrant voicing, the deletion of phoneme insertion errors, and the replacement of erroneously dropped phonemes. <eos> through simple evaluations of intelligibility with na ? <eos> ? ve listeners, we show that the correction of phoneme errors results in the greatest increase in intelligibility and is therefore a desirable mechanism for the eventual creation of augmentative application software for individuals with dysarthria.
in this paper, we examine the idea of technology-assisted co-construction, where the communication partner of an aac user can make guesses about the intended messages, which are included in the user ? s word completion/prediction interface. <eos> we run some human trials to simulate this new interface concept, with subjects predicting words as the user ? s intended message is being generated in real time with specified typing speeds. <eos> results indicate that people can provide substantial keystroke savings by providing word completion or prediction, but that the savings are not as high as n-gram language models. <eos> interestingly, the language model and human predictions are complementary in certain key ways ? <eos> humans doing a better job in some circumstances on contextually salient nouns. <eos> we discuss implications of the enhanced coconstruction interface for real-time message generation in aac direct selection devices.
advances in natural language generation and speech processing techniques, combined with changes in the commercial landscape, have brought within reach dramatic improvements in augmentative alternative communication ( aac ). <eos> these improvements, though overwhelmingly positive, amplify a family of personal data use problems. <eos> this paper argues that the aac design and implementation process needs to identify and address personal data use problems. <eos> accordingly, this paper explores personal data management problems and proposes responses. <eos> this paper is situated in the context of aac technology but the responses could be generalised for other communities affected by low digital literacy, low literacy levels and cognitive challenges.
in this paper, we examine several methods for including dynamic, contextually-sensitive binary codes within indirect selection typing methods using a grid with fixed symbol positions. <eos> using huffman codes derived from a character n-gram model, we investigate both synchronous ( fixed latency highlighting ) and asynchronous ( self-paced using long versus short press ) scanning. <eos> additionally, we look at methods that allow for scanning past a target and returning to it versus methods that remove unselected items from consideration. <eos> finally, we investigate a novel method for displaying the binary codes for each symbol to the user, rather than using cell highlighting, as the means for identifying the required input sequence for the target symbol. <eos> we demonstrate that dynamic coding methods for fixed position grids can be tailored for very diverse user requirements.
this paper describes our work on improving access to the content of multimodal documents containing line graphs in popular media for people with visual impairments. <eos> we provide an overview of our implemented system, including our method for recognizing and conveying the intended message of a line graph. <eos> the textual description of the graphic generated by our system is presented at the most relevant point in the document. <eos> we also describe ongoing work into obtaining additional propositions that elaborate on the intended message, and examine the potential benefits of analyzing the text and graphical content together in order to extend our system to produce summaries of entire multimodal documents.
this paper describes the integration of commonly used screen readers, namely, nvda and orca with text to speech ( tts ) systems for indian languages. <eos> a participatory design approach was followed in the development of the integrated system to ensure that the expectations of visually challenged people are met. <eos> given that india is a multilingual country ( 22 official languages ), a uniform framework for an integrated text-to-speech synthesis systems with screen readers across six indian languages are developed, which can be easily extended to other languages as well. <eos> since indian languages are syllable centred, syllable-based concatenative speech synthesizers are built. <eos> this paper describes the development and evaluation of syllable-based indian language text-to-speech ( tts ) synthesis system ( around festival tts ) with orca and nvda, for linux and windows environments respectively. <eos> tts systems for six indian languages, namely, hindi, tamil, marathi, bengali, malayalam and telugu were built. <eos> usability studies of the screen readers were performed. <eos> the system usability was evaluated by a group of visually challenged people based on a questionnaire provided to them. <eos> and a mean opinion score ( mos ) of 62.27 % was achieved.
in this paper, we propose a new approach to readability assessment with a specific view to the task of text simplification : the intended audience includes people with low literacy skills and/or with mild cognitive impairment. <eos> read ? it represents the first advanced readability assessment tool for what concerns italian, which combines traditional raw text features with lexical, morpho-syntactic and syntactic information. <eos> in read ? it readability assessment is carried out with respect to both documents and sentences where the latter represents an important novelty of the proposed approach creating the prerequisites for aligning the readability assessment step with the text simplification process. <eos> read ? it shows a high accuracy in the document classification task and promising results in the sentence classification scenario.
this paper describes a categorization module for improving the performance of a spanish into spanish sign language ( lse ) translation system. <eos> this categorization module replaces spanish words with associated tags. <eos> when implementing this module, several alternatives for dealing with non-relevant words have been studied. <eos> nonrelevant words are spanish words not relevant in the translation process. <eos> the categorization module has been incorporated into a phrase-based system and a statistical finite state transducer ( sfst ). <eos> the evaluation results reveal that the bleu has increased from 69.11 % to 78.79 % for the phrase-based system and from 69.84 % to 75.59 % for the sfst. <eos> keywords : source language categorization, speech into sign language translation. <eos> lengua de signos espa ? ola ( lse ).
communication is an essential part of our life. <eos> though, not only communication is the key ? <eos> it is all about emotional ( prosodic ) communication. <eos> due to empirical research, people, who are augmentative communicators and speak with a voice output communication aid, want to express their emotions in the same way as everybody else ? <eos> it is one of their deepest interests ( portnuff, 2006 ; hoffmann and w ? lfing, 2010 ). <eos> so far, current devices lack the opportunity of emotional utterances. <eos> this circumstance leads not only to a huge usability deficit, but furthermore, it is an obstacle to develop emotional competence and to behave as well as regulate one ? s emotion adequately ( blackstone and wilkins, 2009 ). <eos> this article aims to increase the sensitivity for the importance of emotional communication. <eos> furthermore, it tries to give first suggestions for implementing an usable device that supports users with a voice output communication aid to express emotional utterances. <eos> this could be done by using phrase-generation, as mentiond by vanderheyden and pennigton ( 1998 ).
this paper describes a machine translation system that offers many deaf and hearingimpaired people the chance to access published information in arabic by translating text into their first language, arabic sign language ( arsl ). <eos> the system was created under the close guidance of a team that included three deaf native signers and one arsl interpreter. <eos> we discuss problems inherent in the design and development of such translation systems and review previous arsl machine translation systems, which all too often demonstrate a lack of collaboration between engineers and the deaf community. <eos> we describe and explain in detail both the adapted translation approach chosen for the proposed system and the arsl corpus that we collected for this purpose. <eos> the corpus has 203 signed sentences ( with 710 distinct signs ) with content restricted to the domain of instructional language as typically used in deaf education. <eos> evaluation shows that the system produces translated sign sentences outputs with an average word error rate of 46.7 % and an average position error rate of 29.4 % using leave-oneout cross validation. <eos> the most frequent source of errors is missing signs in the corpus ; this could be addressed in future by collecting more corpus material.
a corpus of easy-to-read texts in combination with a base vocabulary pool for swedish was used in order to build a basic vocabulary. <eos> the coverage of these entries by symbols in an existing aac database was then assessed. <eos> we finally suggest a method for enriching the expressive power of the aac language by combining existing symbols and in this way illustrate additional concepts.
numerical information is very common in all kinds of documents from newspapers and magazines to household bills and wage slips. <eos> however, many people find it difficult to understand, particularly people with poor education and disabilities. <eos> sometimes numerical information is presented with hedges that modify the meaning. <eos> a numerical hedge is a word or phrase employed to indicate explicitly that some loss of precision has taken place ( e.g., ? around ? ) <eos> and it may also indicate the direction of approximation ( e.g., ? more than ? ). <eos> this paper presents a study of the use of numerical hedges that is part of research investigating the process of rewriting difficult numerical expressions in simpler ways. <eos> we carried out a survey in which experts in numeracy were asked to simplify a range of proportion expressions and analysed the results to obtain guidelines for automating the simplification task.
the simple english wikipedia provides a simplified version of wikipedia 's english articles for readers with special needs. <eos> however, there are fewer efforts to make information in wikipedia in other languages accessible to a large audience. <eos> this work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a simple portuguese wikipedia on demand, based on user interactions with the main portuguese wikipedia. <eos> our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words, while our system can correctly simplify 22,200 occurrences, with estimated f-measure 77.2 %.
reading comprehension activities are an authentic task including a rich, language-based context, which makes them an interesting reallife challenge for research into automatic content analysis. <eos> for textual entailment research, content assessment of reading comprehension exercises provides an interesting opportunity for extrinsic, real-purpose evaluation, which also supports the integration of context and task information into the analysis. <eos> in this paper, we discuss the first results for content assessment of reading comprehension activities for german and present results which are competitive with the current state of the art for english. <eos> diving deeper into the results, we provide an analysis in terms of the different question types and the ways in which the information asked for is encoded in the text. <eos> we then turn to analyzing the role of the question and argue that the surface-based account of information that is given in the question should be replaced with a more sophisticated, linguistically informed analysis of the information structuring of the answer in the context of the question that it is a response to.
while modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. <eos> we present a probabilistic approach for this task which covers aspects such as differentiating various resources by their reliability levels, considering the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. <eos> the impact of our model components is validated by evaluations, which also show that its performance is in line with the best published entailment systems.
this paper addresses context matching in textual inference. <eos> we formulate the task under the contextual preferences framework which broadly captures contextual aspects of inference. <eos> we propose a generic classificationbased scheme under this framework which coherently attends to context matching in inference and may be employed in any inferencebased task. <eos> as a test bed for our scheme we use the name-based text categorization ( tc ) task. <eos> we define an integration of contextual preferences into the tc setting and present a concrete self-supervised model which instantiates the generic scheme and is applied to address context matching in the tc task. <eos> experiments on standard tc datasets show that our approach outperforms the state of the art in context modeling for name-based tc.
we address two issues related to the development of systems for recognizing textual entailment. <eos> the first is the impossibility to capitalize on lessons learned over the different datasets available, due to the changing nature of traditional rte evaluation settings. <eos> the second is the lack of simple ways to assess the results achieved by our system on a given training corpus, and figure out its real potential on unseen test data. <eos> our contribution is the extension of an open-source rte package with an automatic way to explore the large search space of possible configurations, in order to select the most promising one over a given dataset. <eos> from the developers ? <eos> point of view, the efficiency and ease of use of the system, together with the good results achieved on all previous rte datasets, represent a useful support, providing an immediate term of comparison to position the results of their approach.
common evaluation metrics for paraphrase patterns do not necessarily correlate with extrinsic recognition task performance. <eos> we propose a metric which gives weight to lexical variety in paraphrase patterns ; our proposed metric has a positive correlation with paraphrase recognition task performance, with a pearson correlation of 0.5~0.7 ( k=10, with ? strict ? <eos> judgment ) in a statistically significant level ( p-value < 0.01 ).
ambiguities are ubiquitous in natural language and pose a major challenge for the automatic interpretation of natural language expressions. <eos> in this paper we focus on different types of lexical ambiguities that play a role in the context of ontology-based question answering, and explore strategies for capturing and resolving them. <eos> we show that by employing underspecification techniques and by using ontological reasoning in order to filter out inconsistent interpretations as early as possible, the overall number of interpretations can be effectively reduced by 44 %.
intervals and the events that occur in them are encoded as strings, elaborating on a conception of events as ? intervals cum description. ? <eos> notions of satisfaction in interval temporal logics are formulated in terms of strings, and the possibility of computing these via finite-state machines/transducers is investigated. <eos> this opens up temporal semantics to finite-state methods, with entailments that are decidable insofar as these can be reduced to inclusions between regular languages.
reasoning about ordinary human situations and activities requires the availability of diverse types of knowledge, including expectations about the probable results of actions and the lexical entailments for many predicates. <eos> we describe initial work to acquire such a collection of conditional ( if ? then ) knowledge by exploiting presuppositional discourse patterns ( such as ones involving ? but ?, ? yet ?, and ? hoping to ? ) <eos> and abstracting the matched material into general rules.
we introduce bless, a data set specifically designed for the evaluation of distributional semantic models. <eos> bless contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. <eos> it is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. <eos> we discuss the motivations for bless, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models.
unsupervised methods of semantic relations extraction rely on a similarity measure between lexical units. <eos> similarity measures differ both in kinds of information they use and in the ways how this information is transformed into a similarity score. <eos> this paper is making a step further in the evaluation of the available similarity measures within the context of semantic relation extraction. <eos> we compare 21 baseline measures ? <eos> 8 knowledge-based, 4 corpus-based, and 9 web-based metrics with the bless dataset. <eos> our results show that existing similarity measures provide significantly different results, both in general performances and in relation distributions. <eos> we conclude that the results suggest developing a combined similarity measure.
we present a distributional semantic model combining text- and image-based features. <eos> we evaluate this multimodal semantic model on simulating similarity judgments, concept clustering and the bless benchmark. <eos> when integrated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, suggesting that the two sources of information are complementary.
this paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. <eos> raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods. <eos> our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. <eos> the results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection.
we present a distributional vector space model that incorporates latent dirichlet allocation in order to capture the semantic relation holding between adjectives and nouns along interpretable dimensions of meaning : the meaning of adjective-noun phrases is characterized in terms of ontological attributes that are prominent in their compositional semantics. <eos> the model is evaluated in a similarity prediction task based on paired adjective-noun phrases from the mitchell and lapata ( 2010 ) benchmark data. <eos> comparing our model against a high-dimensional latent word space, we observe qualitative differences that shed light on different aspects of similarity conveyed by both models and suggest integrating their complementary strengths.
this paper presents a novel approach for automatic detection of semantic change of words based on distributional similarity models. <eos> we show that the method obtains good results with respect to a reference ranking produced by human raters. <eos> the evaluation also analyzes the performance of frequency-based methods, comparing them to the similarity method proposed.
recent research on multilingual statistical machine translation ( smt ) focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs. <eos> this paper proposes a new method to translate a dialect language into a foreign language by integrating transliteration approaches based on bayesian co-segmentation ( bcs ) models with pivot-based smt approaches. <eos> the advantages of the proposed method with respect to standard smt approaches are three fold : ( 1 ) it uses a standard language as the pivot language and acquires knowledge about the relation between dialects and the standard language automatically, ( 2 ) it reduces the translation task complexity by using monotone decoding techniques, ( 3 ) it reduces the number of features in the log-linear model that have to be estimated from bilingual data. <eos> experimental results translating four japanese dialects ( kumamoto, kyoto, okinawa, osaka ) into four indo-european languages ( english, german, russian, hindi ) and two asian languages ( chinese, korean ) revealed that the proposed method improves the translation quality of dialect translation tasks and outperforms standard pivot translation approaches concatenating smt engines for the majority of the investigated language pairs.
this paper is about improving the quality of arabic-english statistical machine translation ( smt ) on dialectal arabic text using morphological knowledge. <eos> we present a light-weight rule-based approach to producing modern standard arabic ( msa ) paraphrases of dialectal arabic out-of-vocabulary ( oov ) words and low frequency words. <eos> our approach extends an existing msa analyzer with a small number of morphological clitics, and uses transfer rules to generate paraphrase lattices that are input to a state-of-the-art phrasebased smt system. <eos> this approach improves bleu scores on a blind test set by 0.56 absolute bleu ( or 1.5 % relative ). <eos> a manual error analysis of translated dialectal words shows that our system produces correct translations in 74 % of the time for oovs and 60 % of the time for low frequency words.
small, manually assembled corpora may be available for less dominant languages and dialects, but producing web-scale resources remains a challenge. <eos> even when considerable quantities of text are present on the web, finding this text, and distinguishing it from related languages in the same region can be difficult. <eos> for example less dominant variants of english ( e.g. <eos> new zealander, singaporean, canadian, irish, south african ) may be found under their respective national domains, but will be partially mixed with englishes of the british and us varieties, perhaps through syndication of journalism, or the local reuse of text by multinational companies. <eos> less formal dialectal usage may be scattered more widely over the internet through mechanisms such as wiki or blog authoring. <eos> here we automatically construct a corpus of hiberno-english ( english as spoken in ireland ) using a variety of methods : filtering by national domain, filtering by orthographic conventions, and bootstrapping from a set of irelandspecific terms ( slang, place names, organisations ). <eos> we evaluate the national specificity of the resulting corpora by measuring the incidence of topical terms, and several grammatical constructions that are particular to hiberno-english. <eos> the results show that domain filtering is very effective for isolating text that is topic-specific, and orthographic classification can exclude some non-irish texts, but that selected seeds are necessary to extract considerable quantities of more informal, dialectal text.
while most dialectological research so far focuses on phonetic and lexical phenomena, we use recent fieldwork in the domain of dialect syntax to guide the development of multidialectal natural language processing tools. <eos> in particular, we develop a set of rules that transform standard german sentence structures into syntactically valid swiss german sentence structures. <eos> these rules are sensitive to the dialect area, so that the dialects of more than 300 towns are covered. <eos> we evaluate the transformation rules on a standard german treebank and obtain accuracy figures of 85 % and above for most rules. <eos> we analyze the most frequent errors and discuss the benefit of these transformations for various natural language processing tasks.
this paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialect-form text, given that a computational description of the standard morphology is available. <eos> the goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available nlp tools that only assume standard-form input. <eos> the results show that a learning method based on inductive logic programming quickly converges to the correct model with respect to many phonological and morphological differences that are regular in nature.
we explore variability involved in speech with a non-native accent. <eos> we first employ a combination of knowledge-based and datadriven approaches for the analysis of pronunciation variants between l1 ( german ) and target l2 ( slovak ). <eos> knowledge gained in this two-step process is then used in adapting acoustic models and the lexicon. <eos> we focus on modifications in the pronunciation dictionary and speech rate. <eos> our results show that the recognition of german-accented slovak is significantly improved with techniques modeling slow l2 speech, and that the adaptation of the pronunciation dictionary yields only insignificant gains.
this paper describes a method for selecting an appropriate phone set in dialect speech synthesis for a so far undescribed dialect by applying hidden markov model ( hmm ) based training and clustering methods. <eos> in this pilot study we show how a phone set derived from the phonetic surface can be optimized given a small amount of dialect speech training data.
this paper reports the results of the wordnet.ptglobal project, an extension of wordnet.pt to all portuguese varieties. <eos> profiting from a theoretical model of high level explanatory adequacy and from a convenient and flexible development tool, wordnet.ptglobal achieves a rich and multipurpose lexical resource, suitable for contrastive studies and for a vast range of language-based applications covering all portuguese varieties.
sentence compression has attracted much interest in recent years, but most sentence compressors are extractive, i.e., they only delete words. <eos> there is a lack of appropriate datasets to train and evaluate abstractive sentence compressors, i.e., methods that apart from deleting words can also rephrase expressions. <eos> we present a new dataset that contains candidate extractive and abstractive compressions of source sentences. <eos> the candidate compressions are annotated with human judgements for grammaticality and meaning preservation. <eos> we discuss how the dataset was created, and how it can be used in generate-and-rank abstractive sentence compressors. <eos> we also report experimental results with a novel abstractive sentence compressor that uses the dataset.
recent years have seen a trend towards empirically motivated and more data-driven approaches in the field of referring expression generation ( reg ). <eos> much of this work has focussed on initial reference to objects in visual scenes. <eos> while this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. <eos> to move this research forward, we require data sets built around increasingly complex scenes, and we need much larger data sets to accommodate their higher dimensionality. <eos> to control the complexity, we also need to adopt a hypothesis-driven approach to scene design. <eos> in this paper, we describe gre3d7, the largest corpus of humanproduced distinguishing descriptions available to date, discuss the hypotheses that underlie its design, and offer a number of analyses of the 4480 descriptions it contains.
currently there is little agreement about, or even discussion of, methodologies for taskbased evaluation of nlg systems. <eos> i discuss one specific issue in this area, namely the importance of control vs the importance of ecological validity ( real-world context ), and suggest that perhaps we need to put more emphasis on ecological validity in nlg evaluations.
linguistic patterns reflect the regularities of natural language and their applicability is acknowledged in several natural language processing tasks. <eos> particularly, in the task of question generation, many systems depend on patterns to generate questions from text. <eos> the approach we follow relies on patterns that convey lexical, syntactic and semantic information, automatically learned from largescale corpora. <eos> in this paper we discuss the impact of varying several parameters during pattern learning and matching in the question generation task. <eos> in particular, we introduce semantics ( by means of named entities ) in our lexico-syntactic patterns. <eos> we evaluate and compare the number and quality of the learned patterns and the matched text segments. <eos> also, we detail the influence of the patterns in the generation of natural language questions.
this paper shows that using linguistically motivated features for english that-complementizer choice in an averaged perceptron model for classification can improve upon the prediction accuracy of a state-of-the-art realization ranking model. <eos> we report results on a binary classification task for predicting the presence/absence of a that-complementizer using features adapted from jaeger ? s ( 2010 ) investigation of the uniform information density principle in the context of that-mentioning. <eos> our experiments confirm the efficacy of the features based on jaeger ? s work, including information density ? based features. <eos> the experiments also show that the improvements in prediction accuracy apply to cases in which the presence of a that-complementizer arguably makes a substantial difference to fluency or intelligiblity. <eos> our ultimate goal is to improve the performance of a ranking model for surface realization, and to this end we conclude with a discussion of how we plan to combine the local complementizer-choice features with those in the global ranking model.
we discuss the preferred ordering of elements of binomials ( e.g., conjunctions such as fish and chips, lager and lime, exciting and interesting ) and provide a detailed critique of benor and levy ? s probabilistic account of english binomials. <eos> in particular, we discuss the extent to which their approach is suitable as a model of language generation. <eos> we describe resources we have developed for the investigation of binomials using a combination of parsed corpora and very large unparsed corpora. <eos> we discuss the use of these resources in developing models of binomial ordering, concentrating in particular on the evaluation issues which arise.
reversible stochastic attribute-value grammars ( de kok et al, 2011 ) use one model for parse disambiguation and fluency ranking. <eos> such a model encodes preferences with respect to syntax, fluency, and appropriateness of logical forms, as weighted features. <eos> reversible models are built on the premise that syntactic preferences are shared between parse disambiguation and fluency ranking. <eos> given that reversible models also use features that are specific to parsing or generation, there is the possibility that the model is trained to rely on these directional features. <eos> if this is true, the premise that preferences are shared between parse disambiguation and fluency ranking does not hold. <eos> in this work, we compare and apply feature selection techniques to extract the most discriminative features from directional and reversible models. <eos> we then analyse the contributions of different classes of features, and show that reversible models do rely on task-independent features.
